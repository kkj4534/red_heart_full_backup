"""
Red Heart í—¤ë“œ í˜¸í™˜ì„± ì¸í„°í˜ì´ìŠ¤ - 800M ì•„í‚¤í…ì²˜ í†µí•©
Head Compatibility Interface for Red Heart - 800M Architecture Integration

ê¸°ì¡´ ì „ìš© í—¤ë“œë“¤ì„ 300M í†µí•© ë°±ë³¸ê³¼ ì—°ë™:
- EnhancedEmpathyLearner (140M)
- FrommEnhancedBenthamCalculator (120M)  
- SemanticSURDAnalyzer (80M)
- RegretLearningNetwork (120M)
- MetaIntegrationHead (40M)

âš ï¸ ì£¼ì˜ì‚¬í•­:
í˜¹ì‹œë‚˜ ì¤‘ë³µëœ ë©”ì„œë“œê°€ ì¡´ì¬í•  ìˆ˜ ìˆìœ¼ë‹ˆ ì½”ë“œ ëŒë¦¬ê³  í•´ë‹¹ ë¶€ë¶„ì—ì„œ ë¬¸ì œ ë°œìƒì‹œ,
get_pytorch_network ì¤‘ë³µì„ ì˜ì‹¬í•˜ë©° ìì„¸í•œ í™•ì¸ í•„ìš”
íŠ¹íˆ SemanticSURDHeadAdapter, BenthamFrommHeadAdapter, RegretLearningHeadAdapter í´ë˜ìŠ¤ì—ì„œ
ë™ì¼í•œ ë©”ì„œë“œê°€ ë‘ ë²ˆ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•  ê²ƒ (Pythonì€ ë§ˆì§€ë§‰ ì •ì˜ë¥¼ ì‚¬ìš©í•¨)
"""

import asyncio
import logging
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Any, Optional, Union, Tuple, Type
from dataclasses import dataclass, field
from datetime import datetime
import threading
from abc import ABC, abstractmethod
from enum import Enum

# í•µì‹¬ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from config import ADVANCED_CONFIG, get_smart_device, get_gpu_memory_info, ModelPriority, get_priority_based_device
# í•„ìš”í•œ ê²ƒë§Œ ì‹¤ì œ import, ë‚˜ë¨¸ì§€ëŠ” TYPE_CHECKING
from dynamic_swap_manager import SwapPriority, RedHeartDynamicSwapManager
from unified_red_heart_core import UnifiedRepresentation, RedHeartUnifiedBackbone
# ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„°
from optimized_dimension_adapter import OptimizedDimensionAdapter, HeadSpecificAdapters

# LightweightCrossAttentionì€ ì§€ì—° importë¡œ ì²˜ë¦¬
def get_lightweight_cross_attention():
    """LightweightCrossAttention ì§€ì—° import - ì´ˆê¸°í™” íƒ€ì´ë° ë¬¸ì œ í•´ê²°"""
    from unified_red_heart_core import LightweightCrossAttention
    return LightweightCrossAttention

# ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•œ TYPE_CHECKING
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from dynamic_swap_manager import SwapContext

# ê¸°ì¡´ í—¤ë“œë“¤ ì‹¤ì œ import (TYPE_CHECKINGì—ì„œ ì œê±°)
# ğŸ”¥ CRITICAL FIX: ì§€ì—° importë¡œ ë³€ê²½ - import ì‹œì  hanging ë°©ì§€
# ê° ì–´ëŒ‘í„° í´ë˜ìŠ¤ì˜ __init__ì—ì„œ í•„ìš”í•  ë•Œ import

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

class HeadType(Enum):
    """í—¤ë“œ íƒ€ì… ì •ì˜"""
    EMOTION_EMPATHY = "emotion_empathy_head"
    BENTHAM_FROMM = "bentham_fromm_head"
    SEMANTIC_SURD = "semantic_surd_head"
    REGRET_LEARNING = "regret_learning_head"
    META_INTEGRATION = "meta_integration_head"

@dataclass
class HeadProcessingResult:
    """í—¤ë“œ ì²˜ë¦¬ ê²°ê³¼"""
    head_type: HeadType
    primary_output: Any
    secondary_outputs: Dict[str, Any] = field(default_factory=dict)
    processing_time: float = 0.0
    device_used: str = "cpu"
    synergy_features: Optional[torch.Tensor] = None
    confidence_score: float = 1.0
    timestamp: datetime = field(default_factory=datetime.now)

class BaseHeadAdapter(nn.Module, ABC):
    """
    ê¸°ë³¸ í—¤ë“œ ì–´ëŒ‘í„° - ëª¨ë“  í—¤ë“œ ì–´ëŒ‘í„°ì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤
    PyTorch ëª¨ë“ˆë¡œ ì¸ì‹ë˜ë„ë¡ nn.Module ìƒì† ì¶”ê°€
    """
    
    def __init__(self, head_type: HeadType, priority: SwapPriority = SwapPriority.MEDIUM):
        super().__init__()
        self.head_type = head_type
        self.priority = priority
        self.config = ADVANCED_CONFIG['specialized_heads']
        self.backbone_config = ADVANCED_CONFIG['unified_backbone']
        
        # í˜¸í™˜ì„± ì–´ëŒ‘í„° ì„¤ì • - ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„° ì‚¬ìš©
        self.dimension_adapter = None
        self.cross_attention = None
        self.initialized = False
        
        # ë™ì  ë¡œë”© ê´€ë ¨ ì†ì„±
        self.force_cpu_mode = False
        self.current_device = None
        
        logger.info(f"BaseHeadAdapter ì´ˆê¸°í™”: {head_type.value}")
    
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """PyTorch nn.Module í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ìƒ forward ë©”ì„œë“œ
        ê° í—¤ë“œ ì–´ëŒ‘í„°ì—ì„œ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•¨
        """
        pass
    
    def _determine_target_device(self, estimated_params_mb: int = 100):
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤ ê²°ì • ë¡œì§"""
        import torch
        
        # í—¤ë“œ íƒ€ì…ë³„ ìš°ì„ ìˆœìœ„ ë§¤í•‘
        head_priority_map = {
            HeadType.EMOTION_EMPATHY: ModelPriority.HIGH,  # 140M - ë†’ì€ ìš°ì„ ìˆœìœ„
            HeadType.BENTHAM_FROMM: ModelPriority.HIGH,    # 120M - ë†’ì€ ìš°ì„ ìˆœìœ„  
            HeadType.SEMANTIC_SURD: ModelPriority.MEDIUM,  # 80M - ì¤‘ê°„ ìš°ì„ ìˆœìœ„
            HeadType.REGRET_LEARNING: ModelPriority.MEDIUM, # 120M - ì¤‘ê°„ ìš°ì„ ìˆœìœ„
            HeadType.META_INTEGRATION: ModelPriority.LOW   # 40M - ë‚®ì€ ìš°ì„ ìˆœìœ„
        }
        
        priority = head_priority_map.get(self.head_type, ModelPriority.MEDIUM)
        required_memory_mb = estimated_params_mb * 4  # FP32 ê¸°ì¤€
        
        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë””ë°”ì´ìŠ¤ ì„ íƒ
        device = get_priority_based_device(
            memory_required_mb=required_memory_mb,
            priority=priority,
            model_id=f"head_{self.head_type.value}"
        )
        
        logger.info(f"{self.head_type.value}: {estimated_params_mb}M íŒŒë¼ë¯¸í„°, {priority} ìš°ì„ ìˆœìœ„ -> {device}")
        return device
    
    def _move_to_device_safely(self, model, target_device, model_name: str = "model"):
        """ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ì´ë™ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
        try:
            if model is not None and hasattr(model, 'to'):
                model.to(target_device)
                logger.info(f"{self.head_type.value}.{model_name} -> {target_device}")
                return True
        except Exception as e:
            logger.warning(f"{self.head_type.value}.{model_name} ë””ë°”ì´ìŠ¤ ì´ë™ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (í”„ë¡œì íŠ¸ ê·œì¹™: fallback ì—†ìŒ)
            return False
        return False
    
    @abstractmethod
    async def initialize_head(self):
        """í—¤ë“œ ì´ˆê¸°í™” (ë¹„ë™ê¸°)"""
        pass
    
    @abstractmethod
    async def process_unified_input(self, unified_repr: UnifiedRepresentation) -> HeadProcessingResult:
        """í†µí•© í‘œí˜„ì„ ì²˜ë¦¬í•˜ì—¬ í—¤ë“œë³„ ê²°ê³¼ ìƒì„±"""
        pass
    
    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """
        PyTorch forward í˜¸í™˜ì„ ìœ„í•œ ë©”ì„œë“œ
        unified_learning_system.pyì—ì„œ í˜¸ì¶œë¨
        ì£¼ì˜: ì´ ë©”ì„œë“œëŠ” ë™ê¸° í˜¸ì¶œë§Œ ì§€ì›í•˜ë©°, optimized dimension_adapterë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        # ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if hasattr(self, 'dimension_adapter') and self.dimension_adapter is not None:
            # ë°±ë³¸ ì¶œë ¥(1280) -> í—¤ë“œ ì…ë ¥ìœ¼ë¡œ ë³€í™˜ (encode)
            adapted_input = self.dimension_adapter.encode(input_tensor)
        else:
            adapted_input = input_tensor
        
        # PyTorch ë„¤íŠ¸ì›Œí¬ ê°€ì ¸ì˜¤ê¸°
        pytorch_network = self.get_pytorch_network()
        if pytorch_network is None:
            logger.warning(f"{self.head_type.value} - PyTorch ë„¤íŠ¸ì›Œí¬ê°€ ì—†ì–´ ê¸°ë³¸ê°’ ë°˜í™˜")
            return torch.zeros_like(input_tensor)
        
        # ê°ì • í—¤ë“œì˜ ê²½ìš° ì¶”ê°€ ì°¨ì› ë³€í™˜ í•„ìš” (1024 -> 768)
        if self.head_type == HeadType.EMOTION_EMPATHY and adapted_input.shape[-1] != 768:
            device = adapted_input.device
            if not hasattr(self, '_emotion_dim_reducer'):
                self._emotion_dim_reducer = torch.nn.Linear(adapted_input.shape[-1], 768).to(device)
            adapted_input = self._emotion_dim_reducer(adapted_input)
        
        # PyTorch ë„¤íŠ¸ì›Œí¬ë¡œ forward
        try:
            output = pytorch_network(adapted_input)
            
            # ê°ì • í—¤ë“œì˜ ê²½ìš° ì¶œë ¥ì´ 6ì°¨ì›ì´ë¯€ë¡œ íŠ¹ë³„ ì²˜ë¦¬
            if self.head_type == HeadType.EMOTION_EMPATHY and output.shape[-1] == 6:
                # 6ì°¨ì› ê°ì • ë²¡í„°ë¥¼ ê³ ì°¨ì›ìœ¼ë¡œ í™•ì¥
                if not hasattr(self, '_emotion_output_projector'):
                    self._emotion_output_projector = torch.nn.Sequential(
                        torch.nn.Linear(6, 256),
                        torch.nn.ReLU(),
                        torch.nn.Linear(256, 1024)
                    ).to(output.device)
                output = self._emotion_output_projector(output)
            
            # ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ì—¬ ë°±ë³¸ ì°¨ì›ìœ¼ë¡œ ë³µì› (decode)
            if hasattr(self, 'dimension_adapter') and self.dimension_adapter is not None:
                output = self.dimension_adapter.decode(output, original_input=input_tensor)
            
            return output
            
        except Exception as e:
            logger.error(f"{self.head_type.value} forward ì‹¤íŒ¨: {str(e)}")
            logger.error(f"ì…ë ¥ shape: {adapted_input.shape}")
            return torch.zeros_like(input_tensor)
    
    def _create_input_adapter(self, input_dim: int, output_dim: int) -> nn.Module:
        """ì…ë ¥ ì–´ëŒ‘í„° ìƒì„± - í†µí•© í‘œí˜„ì„ í—¤ë“œë³„ ì…ë ¥ìœ¼ë¡œ ë³€í™˜"""
        return nn.Sequential(
            nn.Linear(input_dim, output_dim * 2),
            nn.LayerNorm(output_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(output_dim * 2, output_dim),
            nn.LayerNorm(output_dim)
        )
    
    def _create_output_adapter(self, input_dim: int, output_dim: int) -> nn.Module:
        """ì¶œë ¥ ì–´ëŒ‘í„° ìƒì„± - í—¤ë“œë³„ ì¶œë ¥ì„ í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë³€í™˜"""
        return nn.Sequential(
            nn.Linear(input_dim, input_dim),
            nn.LayerNorm(input_dim),
            nn.GELU(),
            nn.Linear(input_dim, output_dim),
            nn.LayerNorm(output_dim)
        )

class EmotionEmpathyHeadAdapter(BaseHeadAdapter):
    """
    ê°ì •+ê³µê° í—¤ë“œ ì–´ëŒ‘í„° (140M íŒŒë¼ë¯¸í„°)
    EnhancedEmpathyLearnerì™€ í†µí•© ë°±ë³¸ ì—°ê²°
    """
    
    def __init__(self):
        super().__init__(HeadType.EMOTION_EMPATHY, SwapPriority.HIGH)
        self.empathy_learner = None
        
    async def initialize_head(self):
        """ê°ì •+ê³µê° í—¤ë“œ ë™ì  ì´ˆê¸°í™” - CPU/GPU ì„ íƒì  ë¡œë”©"""
        if self.initialized:
            logger.info("EmotionEmpathyHeadAdapter: ì´ë¯¸ ì´ˆê¸°í™”ë¨")
            return
            
        # ë””ë°”ì´ìŠ¤ ê²°ì •: force_cpu_mode ìš°ì„ , ê·¸ ë‹¤ìŒ ìŠ¤ë§ˆíŠ¸ ë””ë°”ì´ìŠ¤ ì„ íƒ (140M íŒŒë¼ë¯¸í„°)
        target_device = self._determine_target_device(estimated_params_mb=140)
        logger.info(f"EmotionEmpathyHeadAdapter ì´ˆê¸°í™” ì‹œì‘ (ë””ë°”ì´ìŠ¤: {target_device})...")
        
        try:
            # ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ emotion_analyzer ëª¨ë“ˆ í™•ì¸ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            logger.info("ì „ì—­ emotion_analyzer ëª¨ë“ˆ í™•ì¸ ì¤‘...")
            from config import get_system_module
            import asyncio
            
            max_retries = 10
            retry_count = 0
            emotion_analyzer = None
            
            while retry_count < max_retries:
                emotion_analyzer = get_system_module('emotion_analyzer')
                if emotion_analyzer is not None:
                    logger.info(f"âœ… emotion_analyzer ëª¨ë“ˆ í™•ì¸ë¨ (ì‹œë„ {retry_count + 1})")
                    break
                    
                retry_count += 1
                logger.warning(f"â³ emotion_analyzer ì•„ì§ ë¡œë“œë˜ì§€ ì•ŠìŒ, ëŒ€ê¸° ì¤‘... ({retry_count}/{max_retries})")
                await asyncio.sleep(0.5)
            
            if emotion_analyzer is None:
                raise RuntimeError("ì „ì—­ emotion_analyzer ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - HeadAdapterëŠ” ì—°ê²° ì¸í„°í˜ì´ìŠ¤ë¡œë§Œ ë™ì‘")
            
            # PyTorch ë„¤íŠ¸ì›Œí¬ ê²€ì¦
            # ë””ë²„ê¹…: emotion_analyzerì˜ ì‹¤ì œ íƒ€ì…ê³¼ ë©”ì„œë“œ í™•ì¸
            logger.info(f"emotion_analyzer íƒ€ì…: {type(emotion_analyzer)}")
            logger.info(f"emotion_analyzer ë©”ì„œë“œ ëª©ë¡: {[m for m in dir(emotion_analyzer) if not m.startswith('_') and callable(getattr(emotion_analyzer, m, None))]}")
            
            if hasattr(emotion_analyzer, 'get_pytorch_network'):
                pytorch_network = emotion_analyzer.get_pytorch_network()
                if pytorch_network is not None:
                    logger.info(f"ì „ì—­ emotion_analyzer PyTorch ë„¤íŠ¸ì›Œí¬ í™•ì¸ ì™„ë£Œ: {type(pytorch_network)}")
                else:
                    logger.warning("ì „ì—­ emotion_analyzerì—ì„œ PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
            else:
                logger.warning("ì „ì—­ emotion_analyzerì— get_pytorch_network ë©”ì„œë“œê°€ ì—†ìŒ")
            
            # ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„° (1280 â†” 1024)
            logger.info("optimized dimension_adapter ìƒì„± ì¤‘...")
            self.dimension_adapter = HeadSpecificAdapters.create_emotion_adapter().to(target_device)
            
            # í¬ë¡œìŠ¤ ì–´í…ì…˜ (ì‹œë„ˆì§€ ì°½ì¶œìš©)
            logger.info("cross_attention ìƒì„± ì¤‘...")
            backbone_num_heads = self.backbone_config.get('num_heads', 20)
            LightweightCrossAttention = get_lightweight_cross_attention()
            self.cross_attention = LightweightCrossAttention(
                d_model=self.backbone_config['d_model'],
                num_heads=backbone_num_heads  # ë°±ë³¸ê³¼ í˜¸í™˜ë˜ëŠ” í—¤ë“œ ìˆ˜
            ).to(target_device)
            
            # ì „ì—­ ëª¨ë“ˆì˜ ë””ë°”ì´ìŠ¤ ì´ë™ì€ MasterMemoryOrchestratorì—ì„œ ê´€ë¦¬
            logger.info("ì „ì—­ emotion_analyzer ëª¨ë“ˆì˜ ë””ë°”ì´ìŠ¤ ê´€ë¦¬ëŠ” MasterMemoryOrchestratorì—ì„œ ì²˜ë¦¬")
            
            # í•™ìŠµ ëª¨ë“œ ì„¤ì • ë° requires_grad í™•ì¸
            logger.info("í•™ìŠµ ëª¨ë“œ ì„¤ì • ì¤‘...")
            self._ensure_training_mode()
            
            # í˜„ì¬ ë””ë°”ì´ìŠ¤ ê¸°ë¡ (ë™ì  ìŠ¤ì™‘ìš©)
            self.current_device = target_device
            
            # ìµœì¢… ê²€ì¦ - ì „ì—­ ëª¨ë“ˆ ì—°ê²° í™•ì¸
            final_pytorch_network = self.get_pytorch_network()
            if final_pytorch_network is None:
                logger.warning("ì „ì—­ emotion_analyzer ëª¨ë“ˆì—ì„œ PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ì—°ê²° ì¸í„°í˜ì´ìŠ¤ë¡œë§Œ ë™ì‘")
                network_info = "No PyTorch Network"
            else:
                logger.info(f"ì „ì—­ emotion_analyzer ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸: {type(final_pytorch_network)}")
                network_info = f"{type(final_pytorch_network)}"
            
            self.initialized = True
            logger.info(f"EmotionEmpathyHeadAdapter ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {target_device}, ë„¤íŠ¸ì›Œí¬: {network_info})")
            
        except Exception as e:
            logger.error(f"EmotionEmpathyHeadAdapter ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            logger.error(f"ì‹¤íŒ¨ ìƒíƒœ: initialized={self.initialized}")
            # ì‹¤íŒ¨ ì‹œ ì •ë¦¬
            self.initialized = False
            raise
    
    
    def _move_empathy_learner_to_device(self, target_device):
        """EnhancedEmpathyLearner ë‚´ë¶€ ëª¨ë¸ë“¤ì„ íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
        try:
            # EnhancedEmpathyLearnerì˜ PyTorch ë„¤íŠ¸ì›Œí¬ ì°¾ì•„ì„œ ì´ë™ - Strict Mode
            pytorch_network = self.empathy_learner.get_pytorch_network()
            if pytorch_network is None:
                raise RuntimeError("ë””ë°”ì´ìŠ¤ ì´ë™ ì¤‘ PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            pytorch_network.to(target_device)
            logger.info(f"EnhancedEmpathyLearner PyTorch ë„¤íŠ¸ì›Œí¬ -> {target_device}")
            
            # ê¸°íƒ€ ë‚´ë¶€ ëª¨ë¸ë“¤ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì„œ ì´ë™
            if hasattr(self.empathy_learner, 'models'):
                for name, model in self.empathy_learner.models.items():
                    if model is not None and hasattr(model, 'to'):
                        model.to(target_device)
                        logger.info(f"EnhancedEmpathyLearner.{name} -> {target_device}")
                        
        except Exception as e:
            logger.error(f"EnhancedEmpathyLearner ë””ë°”ì´ìŠ¤ ì´ë™ ì‹¤íŒ¨: {e}")
            # Strict Mode: ë””ë°”ì´ìŠ¤ ì´ë™ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì—ëŸ¬ ë°œìƒ
            raise RuntimeError(f"EnhancedEmpathyLearner ë””ë°”ì´ìŠ¤ ì´ë™ ì‹¤íŒ¨: {e}") from e
    
    def _ensure_training_mode(self):
        """í•™ìŠµ ëª¨ë“œ ì„¤ì • ë° requires_grad í™•ì¸"""
        # dimension_adapter (ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„°)
        if self.dimension_adapter is not None:
            self.dimension_adapter.train()
            for param in self.dimension_adapter.parameters():
                param.requires_grad = True
        
        # cross_attention
        if self.cross_attention is not None:
            self.cross_attention.train()
            for param in self.cross_attention.parameters():
                param.requires_grad = True
        
        # ì‹¤ì œ empathy_learner ë„¤íŠ¸ì›Œí¬
        pytorch_network = self.get_pytorch_network()
        if pytorch_network is not None:
            pytorch_network.train()
            for param in pytorch_network.parameters():
                param.requires_grad = True
            logger.info("EmotionEmpathyHeadAdapter: ëª¨ë“  íŒŒë¼ë¯¸í„° í•™ìŠµ ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
    
    def get_pytorch_network(self) -> Optional[nn.Module]:
        """PyTorch ë„¤íŠ¸ì›Œí¬ ë°˜í™˜ - ì „ì—­ ë“±ë¡ëœ ëª¨ë“ˆì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            from config import get_system_module
            
            # ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ê°ì • ë¶„ì„ê¸° ê°€ì ¸ì˜¤ê¸°
            emotion_analyzer = get_system_module('emotion_analyzer')
            if emotion_analyzer is None:
                logger.warning("emotion_analyzerê°€ ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡ë˜ì§€ ì•ŠìŒ")
                return None
            
            # ê°ì • ë¶„ì„ê¸°ì—ì„œ PyTorch ë„¤íŠ¸ì›Œí¬ ì°¾ê¸° (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
            # 1. get_pytorch_network() ë©”ì„œë“œ í˜¸ì¶œ
            if hasattr(emotion_analyzer, 'get_pytorch_network'):
                network = emotion_analyzer.get_pytorch_network()
                if network is not None:
                    logger.info(f"EmotionEmpathyHeadAdapter: get_pytorch_network()ë¡œ ë„¤íŠ¸ì›Œí¬ íšë“")
                    return network
                else:
                    logger.warning("EmotionEmpathyHeadAdapter: get_pytorch_network()ê°€ None ë°˜í™˜")
            
            # 2. ì§ì ‘ ì†ì„± í™•ì¸ (emotion_moe, hierarchical_model ë“±)
            if hasattr(emotion_analyzer, 'emotion_moe') and emotion_analyzer.emotion_moe is not None:
                logger.info("EmotionEmpathyHeadAdapter: emotion_moe ì†ì„± ì§ì ‘ ë°˜í™˜")
                return emotion_analyzer.emotion_moe
            
            if hasattr(emotion_analyzer, 'neural_empathy_model') and emotion_analyzer.neural_empathy_model is not None:
                logger.info("EmotionEmpathyHeadAdapter: neural_empathy_model ì†ì„± ë°˜í™˜")
                return emotion_analyzer.neural_empathy_model
            
            if isinstance(emotion_analyzer, nn.Module):
                logger.info("EmotionEmpathyHeadAdapter: emotion_analyzer ìì²´ê°€ nn.Module")
                return emotion_analyzer
            
            # 3. ê³„ì¸µì  ê°ì • ì‹œìŠ¤í…œì—ì„œ ì°¾ê¸° (EnhancedEmpathyLearner)
            if hasattr(emotion_analyzer, 'hierarchical_emotion_system'):
                hier_system = emotion_analyzer.hierarchical_emotion_system
                if hasattr(hier_system, 'enhanced_empathy_learner'):
                    empathy_learner = hier_system.enhanced_empathy_learner
                    if hasattr(empathy_learner, 'get_pytorch_network'):
                        network = empathy_learner.get_pytorch_network()
                        if network is not None:
                            logger.info("EmotionEmpathyHeadAdapter: EnhancedEmpathyLearnerì—ì„œ ë„¤íŠ¸ì›Œí¬ íšë“")
                            return network
            
            # ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ìƒì„¸ ë¡œê¹…
            logger.warning(f"EmotionEmpathyHeadAdapter: emotion_analyzerì—ì„œ PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            logger.warning(f"  - emotion_analyzer íƒ€ì…: {type(emotion_analyzer)}")
            logger.warning(f"  - ì‚¬ìš© ê°€ëŠ¥í•œ ì†ì„±ë“¤: {[attr for attr in dir(emotion_analyzer) if not attr.startswith('_')]}")
            return None
            
        except Exception as e:
            logger.error(f"EmotionEmpathyHeadAdapter PyTorch ë„¤íŠ¸ì›Œí¬ íƒì§€ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            return None
    
    async def _ensure_network_binding(self):
        """ì§€ì—° ë°”ì¸ë”© - ë„¤íŠ¸ì›Œí¬ê°€ ì—†ìœ¼ë©´ ì¬ì‹œë„í•˜ì—¬ ë°”ì¸ë”©"""
        if hasattr(self, '_pytorch_network_cached') and self._pytorch_network_cached is not None:
            return self._pytorch_network_cached
            
        logger.info("ğŸ”„ EmotionEmpathyHeadAdapter: ì§€ì—° ë°”ì¸ë”© ì‹œë„ ì¤‘...")
        
        # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        for retry in range(3):
            network = self.get_pytorch_network()
            if network is not None:
                self._pytorch_network_cached = network
                logger.info(f"âœ… ì§€ì—° ë°”ì¸ë”© ì„±ê³µ (ì‹œë„ {retry + 1}/3)")
                return network
            
            if retry < 2:
                logger.warning(f"â³ ë„¤íŠ¸ì›Œí¬ ë°”ì¸ë”© ì‹¤íŒ¨, ì¬ì‹œë„ ëŒ€ê¸° ì¤‘... ({retry + 1}/3)")
                await asyncio.sleep(0.5 * (retry + 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        # ìµœì¢… ì‹¤íŒ¨ ì‹œ ëª…ì‹œì  ì—ëŸ¬
        error_msg = "EmotionEmpathyHeadAdapter: ì§€ì—° ë°”ì¸ë”© ìµœì¢… ì‹¤íŒ¨ - PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"
        logger.error(error_msg)
        logger.error("ğŸš¨ ê°€ëŠ¥í•œ ì›ì¸:")
        logger.error("  1. emotion_analyzer ëª¨ë“ˆì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        logger.error("  2. emotion_analyzerì— get_pytorch_network() ë©”ì„œë“œê°€ ì—†ìŒ")
        logger.error("  3. ëª¨ë“  ì‹ ê²½ë§ ëª¨ë¸ì´ None ìƒíƒœ")
        raise RuntimeError(error_msg)
    
    async def process_unified_input(self, unified_repr: UnifiedRepresentation) -> HeadProcessingResult:
        """í†µí•© í‘œí˜„ìœ¼ë¡œë¶€í„° ê°ì •+ê³µê° ë¶„ì„ ìˆ˜í–‰"""
        start_time = time.time()
        
        if not self.initialized:
            await self.initialize_head()
        
        # ì§€ì—° ë°”ì¸ë”© í™•ì¸ - ë„¤íŠ¸ì›Œí¬ê°€ ì—†ìœ¼ë©´ ì¬ì‹œë„
        await self._ensure_network_binding()
        
        device = unified_repr.device
        
        # 1. ì…ë ¥ ì–´ëŒ‘í„°ë¥¼ í†µí•œ ì°¨ì› ë³€í™˜
        adapted_input = self.input_adapter(unified_repr.shared_embedding)
        
        # 2. ê¸°ì¡´ ê°ì • ì‹œìŠ¤í…œê³¼ í˜¸í™˜ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        # (EnhancedEmpathyLearnerì˜ ì…ë ¥ í˜•ì‹ì— ë§ì¶¤)
        emotion_context = {
            'text_embedding': adapted_input,
            'attention_weights': unified_repr.attention_weights,
            'timestamp': unified_repr.timestamp,
            'device': device
        }
        
        # 3. missing_neural_models í™œìš© - SelfOtherNeuralNetwork
        try:
            # SelfOtherNeuralNetworkë¥¼ í™œìš©í•œ ìíƒ€ êµ¬ë¶„ ê°ì • ë¶„ì„
            from missing_neural_models import SelfOtherNeuralNetwork
            
            # SelfOtherNeuralNetwork ì´ˆê¸°í™” (ì…ë ¥ ì°¨ì›ì— ë§ì¶¤)
            input_dim = adapted_input.shape[-1]
            self_other_net = SelfOtherNeuralNetwork(input_dim=input_dim).to(device)
            
            # ìíƒ€ êµ¬ë¶„ ë¶„ì„ ìˆ˜í–‰
            self_other_result = self_other_net(adapted_input)
            
            # ê¸°ì¡´ ê°ì • ë¶„ì„ê³¼ í†µí•©
            emotion_result = await self._process_emotion_analysis(emotion_context)
            
            # SelfOtherNeuralNetwork ê²°ê³¼ë¥¼ emotion_resultì— í†µí•©
            emotion_result['self_other_classification'] = self_other_result['self_other_probs']
            emotion_result['self_other_confidence'] = self_other_result['confidence']
            emotion_result['neural_features'] = self_other_result['features']
            
            # 4. ì¶œë ¥ ì–´ëŒ‘í„°ë¥¼ í†µí•œ í‘œì¤€í™”
            if isinstance(emotion_result.get('emotion_embedding'), torch.Tensor):
                standardized_output = self.output_adapter(emotion_result['emotion_embedding'])
            else:
                # í…ì„œê°€ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ê°’ ìƒì„±
                standardized_output = torch.zeros(
                    unified_repr.shared_embedding.shape[0], 
                    self.backbone_config['d_model'], 
                    device=device
                )
            
            # 5. ì‹œë„ˆì§€ íŠ¹ì„± ìƒì„± (í¬ë¡œìŠ¤ ì–´í…ì…˜)
            synergy_features = None
            if hasattr(self, 'cross_attention'):
                cross_attn_result = self.cross_attention(
                    query=unified_repr.shared_embedding,
                    key_value_pairs=[('emotion_head', standardized_output)]
                )
                synergy_features = cross_attn_result.get('emotion_head')
            
            processing_time = time.time() - start_time
            
            return HeadProcessingResult(
                head_type=self.head_type,
                primary_output=emotion_result,
                secondary_outputs={
                    'standardized_embedding': standardized_output,
                    'emotion_dimensions': emotion_result.get('emotion_dimensions', {}),
                    'empathy_scores': emotion_result.get('empathy_scores', {}),
                    'community_awareness': emotion_result.get('community_awareness', {})
                },
                processing_time=processing_time,
                device_used=str(device),
                synergy_features=synergy_features,
                confidence_score=emotion_result.get('confidence', 0.8)
            )
            
        except Exception as e:
            logger.error(f"ê°ì •+ê³µê° í—¤ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            processing_time = time.time() - start_time
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
            return HeadProcessingResult(
                head_type=self.head_type,
                primary_output={'error': str(e)},
                processing_time=processing_time,
                device_used=str(device),
                confidence_score=0.0
            )
    
    async def _process_emotion_analysis(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """ê°ì • ë¶„ì„ ì‹¤í–‰ (ë¹„ë™ê¸°) - ì‹¤ì œ PyTorch ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©"""
        try:
            embedding = context['text_embedding']
            device = context.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')
            
            # PyTorch ë„¤íŠ¸ì›Œí¬ ê°€ì ¸ì˜¤ê¸°
            pytorch_network = self.get_pytorch_network()
            if pytorch_network is None:
                # PyTorch ë„¤íŠ¸ì›Œí¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì²˜ë¦¬
                logger.warning("PyTorch ë„¤íŠ¸ì›Œí¬ê°€ ì—†ì–´ ê¸°ë³¸ ì²˜ë¦¬ ìˆ˜í–‰")
                return self._default_emotion_processing(embedding)
            
            # ë„¤íŠ¸ì›Œí¬ë¥¼ eval ëª¨ë“œë¡œ ì„¤ì •
            pytorch_network.eval()
            
            # ì…ë ¥ í˜•ì‹ í™•ì¸ ë° ì¡°ì •
            # embeddingì€ ì´ë¯¸ input_adapterë¥¼ í†µí•´ 1024 ì°¨ì›ìœ¼ë¡œ ë³€í™˜ë¨
            # EmpathyNetì€ 768 ì°¨ì›ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ ì¶”ê°€ ë³€í™˜ í•„ìš”
            if embedding.shape[-1] != 768:
                # 1024 -> 768 ì°¨ì› ì¶•ì†Œ
                if not hasattr(self, '_emotion_dim_reducer'):
                    self._emotion_dim_reducer = torch.nn.Linear(embedding.shape[-1], 768).to(device)
                embedding_768 = self._emotion_dim_reducer(embedding)
            else:
                embedding_768 = embedding
            
            # PyTorch ë„¤íŠ¸ì›Œí¬ë¡œ ê°ì • ë¶„ì„ ìˆ˜í–‰
            with torch.no_grad():
                emotion_output = pytorch_network(embedding_768)
            
            # ì´ë¯¸ tanh ì ìš©ë˜ì–´ [-1, 1] ë²”ìœ„
            if emotion_output.dim() == 1:
                emotion_output = emotion_output.unsqueeze(0)
            
            # ê°ì • ì°¨ì› í•´ì„ (6ì°¨ì› ê°ì • ë²¡í„°)
            emotion_values = emotion_output.squeeze().cpu().numpy()
            emotion_dimensions = {
                'valence': float(emotion_values[0]),       # ê°ì •ê°€
                'arousal': float(emotion_values[1]),       # ê°ì„±ë„
                'dominance': float(emotion_values[2]),     # ì§€ë°°ê°
                'certainty': float(emotion_values[3]),     # í™•ì‹¤ì„±
                'surprise': float(emotion_values[4]),      # ë†€ë¼ì›€
                'anticipation': float(emotion_values[5])   # ê¸°ëŒ€ê°
            }
            
            # ê³µê° ì ìˆ˜ ê³„ì‚° (ê°ì • ì°¨ì› ê¸°ë°˜)
            empathy_scores = {
                'self_awareness': abs(emotion_values[0]) * 0.5 + 0.5,  # valence ê¸°ë°˜
                'other_awareness': abs(emotion_values[2]) * 0.5 + 0.5,  # dominance ê¸°ë°˜
                'community_awareness': abs(emotion_values[5]) * 0.5 + 0.5  # anticipation ê¸°ë°˜
            }
            
            # ì„ë² ë”©ì€ ì›ë˜ ì°¨ì› ìœ ì§€ (1024)
            return {
                'emotion_embedding': context['text_embedding'],  # ì›ë˜ ì„ë² ë”© ìœ ì§€
                'emotion_vector': emotion_output,  # 6ì°¨ì› ê°ì • ë²¡í„°
                'emotion_dimensions': emotion_dimensions,
                'empathy_scores': empathy_scores,
                'community_awareness': {
                    'integration_level': float(np.mean(list(empathy_scores.values())))
                },
                'confidence': float(torch.sigmoid(torch.mean(torch.abs(emotion_output))).item())
            }
            
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            import traceback
            logger.error(f"íŠ¸ë ˆì´ìŠ¤ë°±: {traceback.format_exc()}")
            return self._default_emotion_processing(context.get('text_embedding'))
    
    def _default_emotion_processing(self, embedding: torch.Tensor) -> Dict[str, Any]:
        """ê¸°ë³¸ ê°ì • ì²˜ë¦¬ (í´ë°±)"""
        try:
            # ê¸°ë³¸ ê°ì • ì°¨ì› ê³„ì‚°
            if embedding is not None and embedding.shape[-1] >= 1024:
                emotion_dimensions = {
                    'valence': float(torch.mean(embedding[:, :256]).item()),
                    'arousal': float(torch.mean(embedding[:, 256:512]).item()),
                    'dominance': float(torch.mean(embedding[:, 512:768]).item()),
                    'certainty': float(torch.mean(embedding[:, 768:1024]).item()),
                    'surprise': 0.5,
                    'anticipation': 0.5
                }
            else:
                emotion_dimensions = {
                    'valence': 0.5, 'arousal': 0.5, 'dominance': 0.5,
                    'certainty': 0.5, 'surprise': 0.5, 'anticipation': 0.5
                }
            
            return {
                'emotion_embedding': embedding,
                'emotion_dimensions': emotion_dimensions,
                'empathy_scores': {
                    'self_awareness': 0.5,
                    'other_awareness': 0.5,
                    'community_awareness': 0.5
                },
                'community_awareness': {'integration_level': 0.5},
                'confidence': 0.5
            }
        except:
            return {'error': 'default processing failed', 'confidence': 0.0}

class BenthamFrommHeadAdapter(BaseHeadAdapter):
    """
    ë²¤ë‹´+í”„ë¡¬ í—¤ë“œ ì–´ëŒ‘í„° (120M íŒŒë¼ë¯¸í„°)
    FrommEnhancedBenthamCalculatorì™€ í†µí•© ë°±ë³¸ ì—°ê²°
    """
    
    def __init__(self):
        super().__init__(HeadType.BENTHAM_FROMM, SwapPriority.HIGH)
        self.bentham_calculator = None
    
    async def initialize_head(self):
        """ë²¤ë‹´+í”„ë¡¬ í—¤ë“œ ì´ˆê¸°í™”"""
        if self.initialized:
            logger.info("BenthamFrommHeadAdapter: ì´ë¯¸ ì´ˆê¸°í™”ë¨")
            return
            
        # ë””ë°”ì´ìŠ¤ ê²°ì •: force_cpu_mode ìš°ì„ , ê·¸ ë‹¤ìŒ ìŠ¤ë§ˆíŠ¸ ë””ë°”ì´ìŠ¤ ì„ íƒ (120M íŒŒë¼ë¯¸í„°)
        target_device = self._determine_target_device(estimated_params_mb=120)
        logger.info(f"BenthamFrommHeadAdapter ì´ˆê¸°í™” ì‹œì‘ (ë””ë°”ì´ìŠ¤: {target_device})...")
        
        try:
            # ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ bentham_calculator ëª¨ë“ˆ í™•ì¸
            logger.info("ì „ì—­ bentham_calculator ëª¨ë“ˆ í™•ì¸ ì¤‘...")
            from config import get_system_module
            bentham_calculator = get_system_module('bentham_calculator')
            
            if bentham_calculator is None:
                raise RuntimeError("ì „ì—­ bentham_calculator ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - HeadAdapterëŠ” ì—°ê²° ì¸í„°í˜ì´ìŠ¤ë¡œë§Œ ë™ì‘")
            
            # PyTorch ë„¤íŠ¸ì›Œí¬ ê²€ì¦
            if hasattr(bentham_calculator, 'get_pytorch_network'):
                pytorch_network = bentham_calculator.get_pytorch_network()
                if pytorch_network is not None:
                    logger.info(f"ì „ì—­ bentham_calculator PyTorch ë„¤íŠ¸ì›Œí¬ í™•ì¸ ì™„ë£Œ: {type(pytorch_network)}")
                else:
                    logger.warning("ì „ì—­ bentham_calculatorì—ì„œ PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
            else:
                logger.warning("ì „ì—­ bentham_calculatorì— get_pytorch_network ë©”ì„œë“œê°€ ì—†ìŒ")
            
            # ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„° (1280 â†” 768)
            logger.info("optimized dimension_adapter ìƒì„± ì¤‘...")
            self.dimension_adapter = HeadSpecificAdapters.create_bentham_adapter().to(target_device)
            
            # í¬ë¡œìŠ¤ ì–´í…ì…˜
            logger.info("cross_attention ìƒì„± ì¤‘...")
            backbone_num_heads = self.backbone_config.get('num_heads', 20)
            LightweightCrossAttention = get_lightweight_cross_attention()
            self.cross_attention = LightweightCrossAttention(
                d_model=self.backbone_config['d_model'],
                num_heads=backbone_num_heads  # ë°±ë³¸ê³¼ í˜¸í™˜ë˜ëŠ” í—¤ë“œ ìˆ˜
            ).to(target_device)
            
            # í˜„ì¬ ë””ë°”ì´ìŠ¤ ê¸°ë¡ (ë™ì  ìŠ¤ì™‘ìš©)
            self.current_device = target_device
            
            # í•™ìŠµ ëª¨ë“œ ì„¤ì • ë° requires_grad í™•ì¸
            logger.info("í•™ìŠµ ëª¨ë“œ ì„¤ì • ì¤‘...")
            self._ensure_training_mode()
            
            # í˜„ì¬ ë””ë°”ì´ìŠ¤ ê¸°ë¡ (ë™ì  ìŠ¤ì›‘ìš©)
            self.current_device = target_device
            
            # ìµœì¢… ê²€ì¦ - ì „ì—­ ëª¨ë“ˆ ì—°ê²° í™•ì¸
            final_pytorch_network = self.get_pytorch_network()
            if final_pytorch_network is None:
                logger.warning("ì „ì—­ bentham_calculator ëª¨ë“ˆì—ì„œ PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ì—°ê²° ì¸í„°í˜ì´ìŠ¤ë¡œë§Œ ë™ì‘")
                network_info = "No PyTorch Network"
            else:
                logger.info(f"ì „ì—­ bentham_calculator ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸: {type(final_pytorch_network)}")
                network_info = f"{type(final_pytorch_network)}"
            
            self.initialized = True
            logger.info(f"BenthamFrommHeadAdapter ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {target_device}, ë„¤íŠ¸ì›Œí¬: {network_info})")
            
        except Exception as e:
            logger.error(f"BenthamFrommHeadAdapter ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            logger.error(f"ì‹¤íŒ¨ ìƒíƒœ: initialized={self.initialized}")
            # ì‹¤íŒ¨ ì‹œ ì •ë¦¬
            self.initialized = False
            raise
    
    def _ensure_training_mode(self):
        """í•™ìŠµ ëª¨ë“œ ì„¤ì • ë° requires_grad í™•ì¸"""
        # dimension_adapter (ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„°)
        if self.dimension_adapter is not None:
            self.dimension_adapter.train()
            for param in self.dimension_adapter.parameters():
                param.requires_grad = True
        
        # cross_attention
        if self.cross_attention is not None:
            self.cross_attention.train()
            for param in self.cross_attention.parameters():
                param.requires_grad = True
        
        # ì‹¤ì œ bentham_calculator ë„¤íŠ¸ì›Œí¬
        pytorch_network = self.get_pytorch_network()
        if pytorch_network is not None and hasattr(pytorch_network, 'parameters'):
            pytorch_network.train()
            for param in pytorch_network.parameters():
                param.requires_grad = True
            logger.info("BenthamFrommHeadAdapter: ëª¨ë“  íŒŒë¼ë¯¸í„° í•™ìŠµ ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
        else:
            logger.warning("BenthamFrommHeadAdapter: í•™ìŠµ ê°€ëŠ¥í•œ ë„¤íŠ¸ì›Œí¬ ì—†ìŒ - ì •ìƒ ì§„í–‰")
    
    def get_pytorch_network(self) -> Optional[nn.Module]:
        """
        PyTorch ë„¤íŠ¸ì›Œí¬ ë°˜í™˜ - ì „ì—­ bentham_calculatorì—ì„œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        STRICT_NO_FALLBACK ì •ì±… ì¤€ìˆ˜ (ë”ë¯¸ ê¸ˆì§€)
        """
        try:
            from config import get_system_module
            
            # ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ë²¤ë‹´ ê³„ì‚°ê¸° ê°€ì ¸ì˜¤ê¸°
            bentham_calculator = get_system_module('bentham_calculator')
            if bentham_calculator is None:
                logger.error("bentham_calculator ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ì—†ìŒ â†’ ì¦‰ì‹œ ì¤‘ë‹¨")
                return None   # ìƒìœ„ ë¡œì§ì—ì„œ RuntimeError ì²˜ë¦¬
            
            # 1ï¸âƒ£ bentham_calculator ìì²´ì— get_pytorch_network() ë©”ì„œë“œê°€ ìˆìœ¼ë©´ ìš°ì„  í˜¸ì¶œ
            if hasattr(bentham_calculator, 'get_pytorch_network'):
                try:
                    net = bentham_calculator.get_pytorch_network()
                    if isinstance(net, nn.Module):
                        logger.info("BenthamFrommHeadAdapter: bentham_calculator.get_pytorch_network()ì—ì„œ ë„¤íŠ¸ì›Œí¬ íšë“")
                        return net
                except Exception as e:
                    logger.warning(f"bentham_calculator.get_pytorch_network() í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            
            # 2ï¸âƒ£ base_calculatorì—ì„œ neural_predictor ì°¾ê¸° (ë ˆì´ì§€ ë¡œë”© ì§€ì›)
            if hasattr(bentham_calculator, 'base_calculator'):
                base_calc = bentham_calculator.base_calculator
                
                # neural_predictor í”„ë¡œí¼í‹° ì ‘ê·¼ìœ¼ë¡œ ë ˆì´ì§€ ë¡œë”© íŠ¸ë¦¬ê±°
                if hasattr(base_calc, 'neural_predictor'):
                    try:
                        neural_pred = base_calc.neural_predictor
                        if isinstance(neural_pred, nn.Module):
                            logger.info("BenthamFrommHeadAdapter: base_calculator.neural_predictorì—ì„œ ë„¤íŠ¸ì›Œí¬ íšë“")
                            return neural_pred
                    except Exception as prop_error:
                        logger.warning(f"neural_predictor í”„ë¡œí¼í‹° ì ‘ê·¼ ì‹¤íŒ¨: {prop_error}")
                
                # ì§ì ‘ private í•„ë“œ í™•ì¸
                if hasattr(base_calc, '_neural_predictor') and isinstance(base_calc._neural_predictor, nn.Module):
                    logger.info("BenthamFrommHeadAdapter: base_calculator._neural_predictorì—ì„œ ë„¤íŠ¸ì›Œí¬ íšë“")
                    return base_calc._neural_predictor
            
            # 3ï¸âƒ£ ì†ì„± ê¸°ë°˜ íƒìƒ‰ (í˜„í–‰ ìœ ì§€)
            for attr in ('neural_predictor', '_neural_predictor', 'model', 'network', 'classifier'):
                net = getattr(bentham_calculator, attr, None)
                if isinstance(net, nn.Module):
                    logger.info(f"BenthamFrommHeadAdapter: bentham_calculator.{attr}ì—ì„œ ë„¤íŠ¸ì›Œí¬ íšë“")
                    return net
            
            # 4. bentham_calculator ìì²´ê°€ nn.Moduleì¸ì§€ í™•ì¸
            if isinstance(bentham_calculator, nn.Module):
                logger.info("BenthamFrommHeadAdapter: bentham_calculator ìì²´ê°€ nn.Module")
                return bentham_calculator
            
            logger.error("bentham_calculatorì—ì„œ PyTorch ë„¤íŠ¸ì›Œí¬ íƒìƒ‰ ì‹¤íŒ¨")
            return None  # ìƒìœ„ì—ì„œ RuntimeError
            
        except Exception as e:
            logger.error(f"BenthamFrommHeadAdapter PyTorch ë„¤íŠ¸ì›Œí¬ íƒì§€ ì‹¤íŒ¨: {e}")
            return None
    
    async def process_unified_input(self, unified_repr: UnifiedRepresentation) -> HeadProcessingResult:
        """í†µí•© í‘œí˜„ìœ¼ë¡œë¶€í„° ë²¤ë‹´+í”„ë¡¬ ë¶„ì„ ìˆ˜í–‰"""
        start_time = time.time()
        
        if not self.initialized:
            await self.initialize_head()
        
        device = unified_repr.device
        
        # ì…ë ¥ ì–´ëŒ‘í„° ì ìš©
        adapted_input = self.input_adapter(unified_repr.shared_embedding)
        
        # ë²¤ë‹´ ê³„ì‚° ì‹¤í–‰
        try:
            bentham_result = await self._process_bentham_calculation(adapted_input, device)
            
            # ì¶œë ¥ í‘œì¤€í™”
            standardized_output = self.output_adapter(adapted_input)
            
            # ì‹œë„ˆì§€ íŠ¹ì„± ìƒì„±
            synergy_features = None
            if hasattr(self, 'cross_attention'):
                cross_attn_result = self.cross_attention(
                    query=unified_repr.shared_embedding,
                    key_value_pairs=[('bentham_head', standardized_output)]
                )
                synergy_features = cross_attn_result.get('bentham_head')
            
            processing_time = time.time() - start_time
            
            return HeadProcessingResult(
                head_type=self.head_type,
                primary_output=bentham_result,
                secondary_outputs={
                    'standardized_embedding': standardized_output,
                    'bentham_scores': bentham_result.get('bentham_scores', {}),
                    'fromm_orientation': bentham_result.get('fromm_orientation', 'unknown'),
                    'ethical_evaluation': bentham_result.get('ethical_evaluation', {})
                },
                processing_time=processing_time,
                device_used=str(device),
                synergy_features=synergy_features,
                confidence_score=bentham_result.get('confidence', 0.8)
            )
            
        except Exception as e:
            logger.error(f"ë²¤ë‹´+í”„ë¡¬ í—¤ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            processing_time = time.time() - start_time
            
            return HeadProcessingResult(
                head_type=self.head_type,
                primary_output={'error': str(e)},
                processing_time=processing_time,
                device_used=str(device),
                confidence_score=0.0
            )
    
    async def _process_bentham_calculation(self, embedding: torch.Tensor, device: torch.device) -> Dict[str, Any]:
        """ë²¤ë‹´ ê³„ì‚° ì‹¤í–‰"""
        try:
            # ê¸°ë³¸ ë²¤ë‹´ ë³€ìˆ˜ë“¤ ê³„ì‚° (ì„ì‹œ êµ¬í˜„)
            bentham_scores = {
                'intensity': float(torch.mean(embedding[:, :128]).item()),
                'duration': float(torch.mean(embedding[:, 128:256]).item()),
                'certainty': float(torch.mean(embedding[:, 256:384]).item()),
                'propinquity': float(torch.mean(embedding[:, 384:512]).item()),
                'fecundity': float(torch.mean(embedding[:, 512:640]).item()),
                'purity': float(torch.mean(embedding[:, 640:768]).item()),
                'extent': 0.75  # ê¸°ë³¸ê°’
            }
            
            # í”„ë¡¬ ì§€í–¥ì„± ë¶„ì„
            having_score = torch.mean(embedding[:, :384]).item()
            being_score = torch.mean(embedding[:, 384:768]).item()
            
            if having_score > being_score + 0.1:
                fromm_orientation = 'having'
            elif being_score > having_score + 0.1:
                fromm_orientation = 'being'
            else:
                fromm_orientation = 'mixed'
            
            # ì „ì²´ ì¾Œë½ ì ìˆ˜ ê³„ì‚°
            total_pleasure = sum(bentham_scores.values()) / len(bentham_scores)
            
            return {
                'bentham_scores': bentham_scores,
                'fromm_orientation': fromm_orientation,
                'total_pleasure_score': total_pleasure,
                'ethical_evaluation': {
                    'utilitarian_score': total_pleasure,
                    'humanistic_score': being_score,
                    'balanced_score': (total_pleasure + being_score) / 2
                },
                'confidence': 0.82
            }
            
        except Exception as e:
            logger.error(f"ë²¤ë‹´ ê³„ì‚° ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return {'error': str(e), 'confidence': 0.0}

class SemanticSURDHeadAdapter(BaseHeadAdapter):
    """
    ì˜ë¯¸+SURD í—¤ë“œ ì–´ëŒ‘í„° (80M íŒŒë¼ë¯¸í„°)
    AdvancedMultiLevelSemanticAnalyzerì™€ í†µí•© ë°±ë³¸ ì—°ê²°
    """
    
    def __init__(self):
        super().__init__(HeadType.SEMANTIC_SURD, SwapPriority.MEDIUM)
        self.semantic_analyzer = None
    
    async def initialize_head(self):
        """ì˜ë¯¸+SURD í—¤ë“œ ì´ˆê¸°í™”"""
        if self.initialized:
            return
            
        logger.info("SemanticSURDHeadAdapter ì´ˆê¸°í™” ì‹œì‘...")
        
        # ê¸°ì¡´ ì˜ë¯¸ ë¶„ì„ê¸° ì´ˆê¸°í™” (ìˆœìˆ˜ ì¬ì‹œë„ ë°©ì‹)
        max_retries = 3
        retry_delay = 1.0
        
        # ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ semantic_analyzer ëª¨ë“ˆ í™•ì¸
        logger.info("ì „ì—­ semantic_analyzer ëª¨ë“ˆ í™•ì¸ ì¤‘...")
        from config import get_system_module
        semantic_analyzer = get_system_module('semantic_analyzer')
        
        if semantic_analyzer is None:
            raise RuntimeError("ì „ì—­ semantic_analyzer ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - HeadAdapterëŠ” ì—°ê²° ì¸í„°í˜ì´ìŠ¤ë¡œë§Œ ë™ì‘")
        
        # ì‹¤ì œ ë¡œë“œëœ í´ë˜ìŠ¤ ì •ë³´ ë¡œê¹…
        logger.info(f"ì „ì—­ semantic_analyzer íƒ€ì…: {type(semantic_analyzer)}")
        logger.info(f"ì „ì—­ semantic_analyzer í´ë˜ìŠ¤ëª…: {semantic_analyzer.__class__.__name__}")
        logger.info(f"ì „ì—­ semantic_analyzer ëª¨ë“ˆ: {semantic_analyzer.__class__.__module__}")
        
        # PyTorch ë„¤íŠ¸ì›Œí¬ ê²€ì¦
        if hasattr(semantic_analyzer, 'get_pytorch_network'):
            logger.info("get_pytorch_network ë©”ì„œë“œ ë°œê²¬")
            pytorch_network = semantic_analyzer.get_pytorch_network()
            if pytorch_network is not None:
                logger.info(f"ì „ì—­ semantic_analyzer PyTorch ë„¤íŠ¸ì›Œí¬ í™•ì¸ ì™„ë£Œ: {type(pytorch_network)}")
            else:
                logger.warning("ì „ì—­ semantic_analyzerì—ì„œ PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
        else:
            logger.warning("ì „ì—­ semantic_analyzerì— get_pytorch_network ë©”ì„œë“œê°€ ì—†ìŒ")
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì„œë“œ ëª©ë¡ ì¶œë ¥
            methods = [m for m in dir(semantic_analyzer) if not m.startswith('_') and callable(getattr(semantic_analyzer, m, None))]
            logger.warning(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì„œë“œë“¤: {methods[:20]}")
        
        # ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„° (1280 â†” 512)
        self.dimension_adapter = HeadSpecificAdapters.create_semantic_adapter()
        
        # í¬ë¡œìŠ¤ ì–´í…ì…˜ (ë°±ë³¸ê³¼ í˜¸í™˜ë˜ëŠ” í—¤ë“œ ìˆ˜)
        backbone_num_heads = self.backbone_config.get('num_heads', 20)
        LightweightCrossAttention = get_lightweight_cross_attention()
        self.cross_attention = LightweightCrossAttention(
            d_model=self.backbone_config['d_model'],
            num_heads=backbone_num_heads  # ë°±ë³¸ê³¼ í˜¸í™˜ë˜ëŠ” í—¤ë“œ ìˆ˜
        )
        
        # í•™ìŠµ ëª¨ë“œ ì„¤ì • ë° requires_grad í™•ì¸
        self._ensure_training_mode()
        
        self.initialized = True
        logger.info("SemanticSURDHeadAdapter ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _ensure_training_mode(self):
        """í•™ìŠµ ëª¨ë“œ ì„¤ì • ë° requires_grad í™•ì¸"""
        # dimension_adapter (ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„°)
        if self.dimension_adapter is not None:
            self.dimension_adapter.train()
            for param in self.dimension_adapter.parameters():
                param.requires_grad = True
        
        # cross_attention
        if self.cross_attention is not None:
            self.cross_attention.train()
            for param in self.cross_attention.parameters():
                param.requires_grad = True
        
        # ì‹¤ì œ semantic_analyzer ë„¤íŠ¸ì›Œí¬
        pytorch_network = self.get_pytorch_network()
        if pytorch_network is not None:
            pytorch_network.train()
            for param in pytorch_network.parameters():
                param.requires_grad = True
            logger.info("SemanticSURDHeadAdapter: ëª¨ë“  íŒŒë¼ë¯¸í„° í•™ìŠµ ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
    
    def get_pytorch_network(self) -> Optional[nn.Module]:
        """
        PyTorch ë„¤íŠ¸ì›Œí¬ ë°˜í™˜ - ì „ì—­ semantic_analyzerì—ì„œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        STRICT_NO_FALLBACK ì •ì±… ì¤€ìˆ˜ (ë”ë¯¸ ê¸ˆì§€)
        """
        try:
            from config import get_system_module
            
            # ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ì˜ë¯¸ ë¶„ì„ê¸° ê°€ì ¸ì˜¤ê¸°
            semantic_analyzer = get_system_module('semantic_analyzer')
            if semantic_analyzer is None:
                logger.error("semantic_analyzer ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ì—†ìŒ â†’ ì¦‰ì‹œ ì¤‘ë‹¨")
                return None   # ìƒìœ„ ë¡œì§ì—ì„œ RuntimeError ì²˜ë¦¬
            
            # 1ï¸âƒ£ semantic_analyzer ìì²´ì— get_pytorch_network() ë©”ì„œë“œê°€ ìˆìœ¼ë©´ ìš°ì„  í˜¸ì¶œ
            if hasattr(semantic_analyzer, 'get_pytorch_network'):
                try:
                    net = semantic_analyzer.get_pytorch_network()
                    if isinstance(net, nn.Module):
                        logger.info("SemanticSURDHeadAdapter: semantic_analyzer.get_pytorch_network()ì—ì„œ ë„¤íŠ¸ì›Œí¬ íšë“")
                        return net
                except Exception as e:
                    logger.warning(f"semantic_analyzer.get_pytorch_network() í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            
            # 2ï¸âƒ£ ì†ì„± ê¸°ë°˜ íƒìƒ‰ (í˜„í–‰ ìœ ì§€)
            for attr in ('fusion_network', 'cross_attention', 'main_network', 'model'):
                net = getattr(semantic_analyzer, attr, None)
                if isinstance(net, nn.Module):
                    logger.info(f"SemanticSURDHeadAdapter: semantic_analyzer.{attr}ì—ì„œ ë„¤íŠ¸ì›Œí¬ íšë“")
                    return net
            
            # 3. AdvancedMultiLevelSemanticAnalyzer ìì²´ê°€ nn.Moduleì¸ì§€ í™•ì¸
            if isinstance(semantic_analyzer, nn.Module):
                logger.info("SemanticSURDHeadAdapter: semantic_analyzer ìì²´ê°€ nn.Module")
                return semantic_analyzer
            
            logger.error("semantic_analyzerì—ì„œ PyTorch ë„¤íŠ¸ì›Œí¬ íƒìƒ‰ ì‹¤íŒ¨")
            return None  # ìƒìœ„ì—ì„œ RuntimeError
            
        except Exception as e:
            logger.warning(f"SemanticSURDHeadAdapter PyTorch ë„¤íŠ¸ì›Œí¬ íƒì§€ ì‹¤íŒ¨: {e}")
            return None
    
    async def process_unified_input(self, unified_repr: UnifiedRepresentation) -> HeadProcessingResult:
        """í†µí•© í‘œí˜„ìœ¼ë¡œë¶€í„° ì˜ë¯¸+SURD ë¶„ì„ ìˆ˜í–‰"""
        start_time = time.time()
        
        if not self.initialized:
            await self.initialize_head()
        
        device = unified_repr.device
        
        # ì…ë ¥ ì–´ëŒ‘í„° ì ìš©
        adapted_input = self.input_adapter(unified_repr.shared_embedding)
        
        try:
            semantic_result = await self._process_semantic_analysis(adapted_input, device)
            
            # ì¶œë ¥ í‘œì¤€í™”
            standardized_output = self.output_adapter(adapted_input)
            
            # ì‹œë„ˆì§€ íŠ¹ì„± ìƒì„±
            synergy_features = None
            if hasattr(self, 'cross_attention'):
                cross_attn_result = self.cross_attention(
                    query=unified_repr.shared_embedding,
                    key_value_pairs=[('semantic_head', standardized_output)]
                )
                synergy_features = cross_attn_result.get('semantic_head')
            
            processing_time = time.time() - start_time
            
            return HeadProcessingResult(
                head_type=self.head_type,
                primary_output=semantic_result,
                secondary_outputs={
                    'standardized_embedding': standardized_output,
                    'semantic_layers': semantic_result.get('semantic_layers', {}),
                    'surd_measures': semantic_result.get('surd_measures', {}),
                    'hashtag_analysis': semantic_result.get('hashtag_analysis', {})
                },
                processing_time=processing_time,
                device_used=str(device),
                synergy_features=synergy_features,
                confidence_score=semantic_result.get('confidence', 0.8)
            )
            
        except Exception as e:
            logger.error(f"ì˜ë¯¸+SURD í—¤ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            processing_time = time.time() - start_time
            
            return HeadProcessingResult(
                head_type=self.head_type,
                primary_output={'error': str(e)},
                processing_time=processing_time,
                device_used=str(device),
                confidence_score=0.0
            )
    
    async def _process_semantic_analysis(self, embedding: torch.Tensor, device: torch.device) -> Dict[str, Any]:
        """ì˜ë¯¸ ë¶„ì„ ì‹¤í–‰"""
        try:
            # ë‹¤ì¤‘ ìˆ˜ì¤€ ì˜ë¯¸ ë¶„ì„ (ì„ì‹œ êµ¬í˜„)
            semantic_layers = {
                'surface_meaning': float(torch.mean(embedding[:, :128]).item()),
                'deep_meaning': float(torch.mean(embedding[:, 128:256]).item()),
                'contextual_meaning': float(torch.mean(embedding[:, 256:384]).item()),
                'pragmatic_meaning': float(torch.mean(embedding[:, 384:512]).item())
            }
            
            # SURD ì¸¡ì •ê°’
            surd_measures = {
                'synergy': 0.72,
                'unique_info': 0.68,
                'redundancy': 0.45,
                'deterministic': 0.83
            }
            
            # í•´ì‹œíƒœê·¸ ê¸°ë°˜ ë¶„ì„
            hashtag_analysis = {
                'emotional_tags': ['#empathy', '#understanding'],
                'semantic_tags': ['#analysis', '#meaning'],
                'confidence_per_tag': {'#empathy': 0.85, '#understanding': 0.78}
            }
            
            return {
                'semantic_layers': semantic_layers,
                'surd_measures': surd_measures,
                'hashtag_analysis': hashtag_analysis,
                'overall_semantic_score': sum(semantic_layers.values()) / len(semantic_layers),
                'confidence': 0.81
            }
            
        except Exception as e:
            logger.error(f"ì˜ë¯¸ ë¶„ì„ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return {'error': str(e), 'confidence': 0.0}

class RegretLearningHeadAdapter(BaseHeadAdapter):
    """
    í›„íšŒ+í•™ìŠµ í—¤ë“œ ì–´ëŒ‘í„° (120M íŒŒë¼ë¯¸í„°)
    GPURegretNetworkì™€ í†µí•© ë°±ë³¸ ì—°ê²°
    """
    
    def __init__(self):
        super().__init__(HeadType.REGRET_LEARNING, SwapPriority.MEDIUM)
        self.regret_network = None
    
    def get_pytorch_network(self) -> Optional[nn.Module]:
        """
        PyTorch ë„¤íŠ¸ì›Œí¬ ë°˜í™˜ - ì „ì—­ regret_analyzerì—ì„œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        STRICT_NO_FALLBACK ì •ì±… ì¤€ìˆ˜ (ë”ë¯¸ ê¸ˆì§€)
        """
        try:
            from config import get_system_module
            
            # ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ í›„íšŒ ë¶„ì„ê¸° ê°€ì ¸ì˜¤ê¸°
            regret_analyzer = get_system_module('regret_analyzer')
            if regret_analyzer is None:
                logger.error("regret_analyzer ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ì—†ìŒ â†’ ì¦‰ì‹œ ì¤‘ë‹¨")
                return None   # ìƒìœ„ ë¡œì§ì—ì„œ RuntimeError ì²˜ë¦¬
            
            # 1ï¸âƒ£ regret_analyzer ìì²´ì— get_pytorch_network() ë©”ì„œë“œê°€ ìˆìœ¼ë©´ ìš°ì„  í˜¸ì¶œ
            if hasattr(regret_analyzer, 'get_pytorch_network'):
                try:
                    net = regret_analyzer.get_pytorch_network()
                    if isinstance(net, nn.Module):
                        logger.info("RegretLearningHeadAdapter: regret_analyzer.get_pytorch_network()ì—ì„œ ë„¤íŠ¸ì›Œí¬ íšë“")
                        return net
                except Exception as e:
                    logger.warning(f"regret_analyzer.get_pytorch_network() í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            
            # 2ï¸âƒ£ ì†ì„± ê¸°ë°˜ íƒìƒ‰ (í˜„í–‰ ìœ ì§€)
            for attr in ('gpu_regret_network', 'regret_network', 'neural_predictor', '_neural_predictor', 'model', 'network'):
                net = getattr(regret_analyzer, attr, None)
                if isinstance(net, nn.Module):
                    logger.info(f"RegretLearningHeadAdapter: regret_analyzer.{attr}ì—ì„œ ë„¤íŠ¸ì›Œí¬ íšë“")
                    return net
            
            # 3. AdvancedRegretLearningSystem ìì²´ê°€ nn.Moduleì¸ì§€ í™•ì¸
            if isinstance(regret_analyzer, nn.Module):
                logger.info("RegretLearningHeadAdapter: regret_analyzer ìì²´ê°€ nn.Module")
                return regret_analyzer
            
            logger.error("regret_analyzerì—ì„œ PyTorch ë„¤íŠ¸ì›Œí¬ íƒìƒ‰ ì‹¤íŒ¨")
            return None  # ìƒìœ„ì—ì„œ RuntimeError
            
        except Exception as e:
            logger.warning(f"RegretLearningHeadAdapter PyTorch ë„¤íŠ¸ì›Œí¬ íƒì§€ ì‹¤íŒ¨: {e}")
            return None
    
    async def initialize_head(self):
        """í›„íšŒ+í•™ìŠµ í—¤ë“œ ì´ˆê¸°í™”"""
        if self.initialized:
            return
            
        logger.info("RegretLearningHeadAdapter ì´ˆê¸°í™” ì‹œì‘...")
        
        # ê¸°ì¡´ í›„íšŒ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” (ì˜¬ë°”ë¥¸ ìƒì„±ì ì¸ì ì‚¬ìš©)
        max_retries = 3
        retry_delay = 1.0
        
        # ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ regret_analyzer ëª¨ë“ˆ í™•ì¸
        logger.info("ì „ì—­ regret_analyzer ëª¨ë“ˆ í™•ì¸ ì¤‘...")
        from config import get_system_module
        regret_analyzer = get_system_module('regret_analyzer')
        
        if regret_analyzer is None:
            raise RuntimeError("ì „ì—­ regret_analyzer ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - HeadAdapterëŠ” ì—°ê²° ì¸í„°í˜ì´ìŠ¤ë¡œë§Œ ë™ì‘")
        
        # PyTorch ë„¤íŠ¸ì›Œí¬ ê²€ì¦
        if hasattr(regret_analyzer, 'get_pytorch_network'):
            pytorch_network = regret_analyzer.get_pytorch_network()
            if pytorch_network is not None:
                logger.info(f"ì „ì—­ regret_analyzer PyTorch ë„¤íŠ¸ì›Œí¬ í™•ì¸ ì™„ë£Œ: {type(pytorch_network)}")
            else:
                logger.warning("ì „ì—­ regret_analyzerì—ì„œ PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
        else:
            logger.warning("ì „ì—­ regret_analyzerì— get_pytorch_network ë©”ì„œë“œê°€ ì—†ìŒ")
        
        # ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„° (1280 â†” 768)
        self.dimension_adapter = HeadSpecificAdapters.create_regret_adapter()
        
        # í¬ë¡œìŠ¤ ì–´í…ì…˜
        LightweightCrossAttention = get_lightweight_cross_attention()
        self.cross_attention = LightweightCrossAttention(
            d_model=self.backbone_config['d_model'],
            num_heads=8
        )
        
        # í•™ìŠµ ëª¨ë“œ ì„¤ì • ë° requires_grad í™•ì¸
        self._ensure_training_mode()
        
        self.initialized = True
        logger.info("RegretLearningHeadAdapter ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _ensure_training_mode(self):
        """í•™ìŠµ ëª¨ë“œ ì„¤ì • ë° requires_grad í™•ì¸"""
        # dimension_adapter (ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„°)
        if self.dimension_adapter is not None:
            self.dimension_adapter.train()
            for param in self.dimension_adapter.parameters():
                param.requires_grad = True
        
        # cross_attention
        if self.cross_attention is not None:
            self.cross_attention.train()
            for param in self.cross_attention.parameters():
                param.requires_grad = True
        
        # ì‹¤ì œ regret_network
        pytorch_network = self.get_pytorch_network()
        if pytorch_network is not None:
            pytorch_network.train()
            for param in pytorch_network.parameters():
                param.requires_grad = True
            logger.info("RegretLearningHeadAdapter: ëª¨ë“  íŒŒë¼ë¯¸í„° í•™ìŠµ ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
    
    async def process_unified_input(self, unified_repr: UnifiedRepresentation) -> HeadProcessingResult:
        """í†µí•© í‘œí˜„ìœ¼ë¡œë¶€í„° í›„íšŒ+í•™ìŠµ ë¶„ì„ ìˆ˜í–‰"""
        start_time = time.time()
        
        if not self.initialized:
            await self.initialize_head()
        
        device = unified_repr.device
        
        # ì…ë ¥ ì–´ëŒ‘í„° ì ìš©
        adapted_input = self.input_adapter(unified_repr.shared_embedding)
        
        try:
            # missing_neural_models í™œìš© - IncrementalLearner
            from missing_neural_models import IncrementalLearner
            
            # IncrementalLearner ì´ˆê¸°í™” (ì…ë ¥ ì°¨ì›ì— ë§ì¶¤)
            input_dim = adapted_input.shape[-1]
            incremental_learner = IncrementalLearner(input_dim=input_dim).to(device)
            
            # ì¦ë¶„ í•™ìŠµì„ í†µí•œ í›„íšŒ í•™ìŠµ ê°•í™”
            # ì‹¤ì œ ë ˆì´ë¸”ì€ í›„íšŒ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„± (ê°€ìƒ ë ˆì´ë¸”)
            batch_size = adapted_input.shape[0]
            virtual_labels = torch.randn(batch_size, 64, device=device)  # ê°€ìƒ í›„íšŒ ë ˆì´ë¸”
            
            # ì¦ë¶„ í•™ìŠµ ìˆ˜í–‰
            learning_metrics = incremental_learner.learn_incrementally(adapted_input, virtual_labels)
            
            # ê¸°ì¡´ í›„íšŒ ë¶„ì„ê³¼ í†µí•©
            regret_result = await self._process_regret_analysis(adapted_input, device)
            
            # IncrementalLearner ê²°ê³¼ë¥¼ regret_resultì— í†µí•©
            regret_result['incremental_features'] = incremental_learner(adapted_input)
            regret_result['learning_metrics'] = learning_metrics
            regret_result['knowledge_retention'] = learning_metrics.get('knowledge_retention', 0.0)
            
            # í›„íšŒ ë„¤íŠ¸ì›Œí¬ ì¶œë ¥ì„ í‘œì¤€í™”
            if isinstance(regret_result.get('regret_output'), torch.Tensor):
                standardized_output = self.output_adapter(regret_result['regret_output'])
            else:
                standardized_output = self.output_adapter(
                    torch.zeros(adapted_input.shape[0], 64, device=device)
                )
            
            # ì‹œë„ˆì§€ íŠ¹ì„± ìƒì„±
            synergy_features = None
            if hasattr(self, 'cross_attention'):
                cross_attn_result = self.cross_attention(
                    query=unified_repr.shared_embedding,
                    key_value_pairs=[('regret_head', standardized_output)]
                )
                synergy_features = cross_attn_result.get('regret_head')
            
            processing_time = time.time() - start_time
            
            return HeadProcessingResult(
                head_type=self.head_type,
                primary_output=regret_result,
                secondary_outputs={
                    'standardized_embedding': standardized_output,
                    'regret_intensity': regret_result.get('regret_intensity', 0.0),
                    'learning_rate': regret_result.get('learning_rate', 0.001),
                    'adaptation_score': regret_result.get('adaptation_score', 0.5)
                },
                processing_time=processing_time,
                device_used=str(device),
                synergy_features=synergy_features,
                confidence_score=regret_result.get('confidence', 0.8)
            )
            
        except Exception as e:
            logger.error(f"í›„íšŒ+í•™ìŠµ í—¤ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            processing_time = time.time() - start_time
            
            return HeadProcessingResult(
                head_type=self.head_type,
                primary_output={'error': str(e)},
                processing_time=processing_time,
                device_used=str(device),
                confidence_score=0.0
            )
    
    async def _process_regret_analysis(self, embedding: torch.Tensor, device: torch.device) -> Dict[str, Any]:
        """í›„íšŒ ë¶„ì„ ì‹¤í–‰"""
        try:
            # í›„íšŒ ë„¤íŠ¸ì›Œí¬ ì‹¤í–‰
            if self.regret_network is not None:
                regret_output = self.regret_network(embedding)
                
                # regret_outputì´ tupleì¸ ê²½ìš° ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                if isinstance(regret_output, (tuple, list)):
                    regret_output = regret_output[0] if len(regret_output) > 0 else torch.zeros_like(embedding)
                elif not isinstance(regret_output, torch.Tensor):
                    regret_output = torch.zeros_like(embedding)
            else:
                regret_output = torch.mean(embedding, dim=-1, keepdim=True).expand(-1, 64)
            
            # í›„íšŒ ê°•ë„ ê³„ì‚° (ì•ˆì „í•œ í…ì„œ í™•ì¸)
            if isinstance(regret_output, torch.Tensor):
                regret_intensity = float(torch.mean(regret_output).item())
            else:
                regret_intensity = 0.5  # ê¸°ë³¸ê°’
            
            # í•™ìŠµë¥  ì¡°ì •
            learning_rate = max(0.0001, min(0.01, 0.001 * (1 + regret_intensity)))
            
            # ì ì‘ ì ìˆ˜
            adaptation_score = 1.0 / (1.0 + regret_intensity)
            
            return {
                'regret_output': regret_output,
                'regret_intensity': regret_intensity,
                'learning_rate': learning_rate,
                'adaptation_score': adaptation_score,
                'confidence': 0.79
            }
            
        except Exception as e:
            logger.error(f"í›„íšŒ ë¶„ì„ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return {'error': str(e), 'confidence': 0.0}

class MetaIntegrationHeadAdapter(BaseHeadAdapter):
    """
    ë©”íƒ€í†µí•© í—¤ë“œ ì–´ëŒ‘í„° (40M íŒŒë¼ë¯¸í„°)
    ë‹¤ë¥¸ í—¤ë“œë“¤ì˜ ê²°ê³¼ë¥¼ í†µí•©í•˜ê³  ë©”íƒ€ í•™ìŠµ ìˆ˜í–‰
    """
    
    def __init__(self):
        super().__init__(HeadType.META_INTEGRATION, SwapPriority.LOW)
        self.integration_network = None
    
    def get_pytorch_network(self) -> Optional[nn.Module]:
        """PyTorch ë„¤íŠ¸ì›Œí¬ ë°˜í™˜ - HeadCompatibilityManagerìš©"""
        try:
            # ì „ì—­ ëª¨ë“ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ meta_integration ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
            from config import get_system_module
            meta_integration_module = get_system_module('meta_integration')
            
            if meta_integration_module is not None:
                # ëª¨ë“ˆì— get_pytorch_network ë©”ì„œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                if hasattr(meta_integration_module, 'get_pytorch_network'):
                    return meta_integration_module.get_pytorch_network()
                # ëª¨ë“ˆ ìì²´ê°€ nn.Moduleì¸ ê²½ìš°
                elif hasattr(meta_integration_module, 'forward'):
                    return meta_integration_module
            
            # ì „ì—­ ëª¨ë“ˆì´ ì—†ìœ¼ë©´ fallbackìœ¼ë¡œ ë¡œì»¬ integration_network ì‚¬ìš©
            if hasattr(self, 'integration_network') and self.integration_network is not None:
                logger.warning("ì „ì—­ meta_integration ëª¨ë“ˆì´ ì—†ì–´ ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©")
                return self.integration_network
            
            return None
            
        except Exception as e:
            logger.warning(f"MetaIntegrationHeadAdapter PyTorch ë„¤íŠ¸ì›Œí¬ íƒì§€ ì‹¤íŒ¨: {e}")
            return None
    
    async def initialize_head(self):
        """ë©”íƒ€í†µí•© í—¤ë“œ ì´ˆê¸°í™”"""
        if self.initialized:
            return
            
        logger.info("ğŸ” MetaIntegrationHeadAdapter ì´ˆê¸°í™” ì‹œì‘...")
        
        try:
            # 1ë‹¨ê³„: backbone_config ê²€ì¦
            logger.info(f"ğŸ” Step 1: backbone_config ê²€ì¦ - d_model: {self.backbone_config.get('d_model', 'MISSING')}")
            if 'd_model' not in self.backbone_config:
                raise ValueError("backbone_configì—ì„œ 'd_model' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # 2ë‹¨ê³„: ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ meta_integration ëª¨ë“ˆ í™•ì¸
            logger.info("ğŸ” Step 2: ì „ì—­ meta_integration ëª¨ë“ˆ í™•ì¸ ì¤‘...")
            from config import get_system_module
            meta_integration_module = get_system_module('meta_integration')
            
            if meta_integration_module is not None:
                logger.info("âœ… Step 2: ì „ì—­ meta_integration ëª¨ë“ˆ ì—°ê²° ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ Step 2: ì „ì—­ meta_integration ëª¨ë“ˆ ì°¾ì„ ìˆ˜ ì—†ìŒ - HeadAdapterëŠ” ì—°ê²° ì¸í„°í˜ì´ìŠ¤ë¡œë§Œ ë™ì‘")
            
            # 3ë‹¨ê³„: ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„° ìƒì„± (1280 â†” 256)
            logger.info("ğŸ” Step 3: optimized dimension_adapter ìƒì„± ì¤‘...")
            self.dimension_adapter = HeadSpecificAdapters.create_meta_adapter()
            logger.info("âœ… Step 3: ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„° ìƒì„± ì™„ë£Œ")
            
            # 5ë‹¨ê³„: ë©”íƒ€ í¬ë¡œìŠ¤ ì–´í…ì…˜ (ë°±ë³¸ê³¼ í˜¸í™˜ë˜ëŠ” í—¤ë“œ ìˆ˜ ì‚¬ìš©)
            logger.info("ğŸ” Step 5: ë©”íƒ€ í¬ë¡œìŠ¤ ì–´í…ì…˜ ìƒì„± ì¤‘...")
            backbone_num_heads = self.backbone_config.get('num_heads', 20)
            logger.info(f"ğŸ” ë°±ë³¸ í˜¸í™˜ í—¤ë“œ ìˆ˜: {backbone_num_heads}")
            LightweightCrossAttention = get_lightweight_cross_attention()
            self.cross_attention = LightweightCrossAttention(
                d_model=self.backbone_config['d_model'],
                num_heads=backbone_num_heads  # ë°±ë³¸ê³¼ ë™ì¼í•œ í—¤ë“œ ìˆ˜ ì‚¬ìš©
            )
            logger.info("âœ… Step 5: ë©”íƒ€ í¬ë¡œìŠ¤ ì–´í…ì…˜ ìƒì„± ì™„ë£Œ")
            
            # 6ë‹¨ê³„: í•™ìŠµ ëª¨ë“œ ì„¤ì •
            logger.info("ğŸ” Step 6: í•™ìŠµ ëª¨ë“œ ì„¤ì • ì¤‘...")
            self._ensure_training_mode()
            logger.info("âœ… Step 6: í•™ìŠµ ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
            
            self.initialized = True
            logger.info("ğŸ‰ MetaIntegrationHeadAdapter ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ MetaIntegrationHeadAdapter ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            raise
    
    def _ensure_training_mode(self):
        """í•™ìŠµ ëª¨ë“œ ì„¤ì • ë° requires_grad í™•ì¸"""
        # dimension_adapter (ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„°)
        if self.dimension_adapter is not None:
            self.dimension_adapter.train()
            for param in self.dimension_adapter.parameters():
                param.requires_grad = True
        
        # cross_attention
        if self.cross_attention is not None:
            self.cross_attention.train()
            for param in self.cross_attention.parameters():
                param.requires_grad = True
        
        # integration_networkëŠ” ì „ì—­ ëª¨ë“ˆì—ì„œ ê´€ë¦¬ë˜ë¯€ë¡œ ì—¬ê¸°ì„œ ì„¤ì •í•˜ì§€ ì•ŠìŒ
        logger.info("MetaIntegrationHeadAdapter: ì—°ê²° ì¸í„°í˜ì´ìŠ¤ íŒŒë¼ë¯¸í„° í•™ìŠµ ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
    
    async def process_unified_input(self, unified_repr: UnifiedRepresentation) -> HeadProcessingResult:
        """í†µí•© í‘œí˜„ìœ¼ë¡œë¶€í„° ë©”íƒ€í†µí•© ë¶„ì„ ìˆ˜í–‰"""
        start_time = time.time()
        
        if not self.initialized:
            await self.initialize_head()
        
        device = unified_repr.device
        
        try:
            meta_result = await self._process_meta_integration(unified_repr, device)
            
            # ì¶œë ¥ í‘œì¤€í™”
            if isinstance(meta_result.get('integrated_output'), torch.Tensor):
                standardized_output = self.output_adapter(meta_result['integrated_output'])
            else:
                standardized_output = unified_repr.shared_embedding
            
            processing_time = time.time() - start_time
            
            return HeadProcessingResult(
                head_type=self.head_type,
                primary_output=meta_result,
                secondary_outputs={
                    'standardized_embedding': standardized_output,
                    'integration_weights': meta_result.get('integration_weights', {}),
                    'meta_learning_score': meta_result.get('meta_learning_score', 0.5),
                    'system_coherence': meta_result.get('system_coherence', 0.7)
                },
                processing_time=processing_time,
                device_used=str(device),
                synergy_features=standardized_output,
                confidence_score=meta_result.get('confidence', 0.8)
            )
            
        except Exception as e:
            logger.error(f"ë©”íƒ€í†µí•© í—¤ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            processing_time = time.time() - start_time
            
            return HeadProcessingResult(
                head_type=self.head_type,
                primary_output={'error': str(e)},
                processing_time=processing_time,
                device_used=str(device),
                confidence_score=0.0
            )
    
    async def _process_meta_integration(self, unified_repr: UnifiedRepresentation, device: torch.device) -> Dict[str, Any]:
        """ë©”íƒ€ í†µí•© ì‹¤í–‰"""
        try:
            # ê¸°ë³¸ í†µí•© (ë‹¤ë¥¸ í—¤ë“œë“¤ì˜ ê²°ê³¼ê°€ ìˆì„ ë•Œ ë” ì •êµí•˜ê²Œ êµ¬í˜„)
            adapted_input = self.input_adapter(unified_repr.shared_embedding)
            
            # ì„ì‹œ í†µí•© ê²°ê³¼ (ì‹¤ì œë¡œëŠ” ë‹¤ë¥¸ í—¤ë“œë“¤ì˜ ê²°ê³¼ë¥¼ ì¡°í•©) - ì•ˆì „í•œ ì°¨ì› ì²˜ë¦¬
            # ëª¨ë“  í…ì„œë¥¼ shared_embeddingê³¼ ê°™ì€ í¬ê¸°ë¡œ ë§ì¶¤
            target_shape = unified_repr.shared_embedding.shape
            
            # cross_modal_featuresë¥¼ target_shapeë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
            cross_modal_safe = unified_repr.cross_modal_features
            if cross_modal_safe.shape != target_shape:
                if cross_modal_safe.dim() == 1:
                    cross_modal_safe = cross_modal_safe.unsqueeze(0)
                if cross_modal_safe.shape != target_shape:
                    # í¬ê¸°ê°€ ë§ì§€ ì•Šìœ¼ë©´ shared_embeddingê³¼ ê°™ì€ í¬ê¸°ë¡œ ì¡°ì •
                    cross_modal_safe = torch.zeros_like(unified_repr.shared_embedding)
            
            dummy_head_outputs = [
                unified_repr.shared_embedding,  # ë°±ë³¸ ì¶œë ¥
                cross_modal_safe,
                adapted_input.expand_as(unified_repr.shared_embedding),
                torch.zeros_like(unified_repr.shared_embedding)
            ]
            
            # í—¤ë“œ ê²°ê³¼ë“¤ ì—°ê²°
            concatenated = torch.cat(dummy_head_outputs, dim=-1)
            
            # í†µí•© ë„¤íŠ¸ì›Œí¬ ì‹¤í–‰
            integrated_output = self.integration_network(concatenated)
            
            # í†µí•© ê°€ì¤‘ì¹˜ ê³„ì‚°
            integration_weights = {
                'emotion_weight': 0.3,
                'bentham_weight': 0.25,
                'semantic_weight': 0.2,
                'regret_weight': 0.25
            }
            
            # ë©”íƒ€ í•™ìŠµ ì ìˆ˜
            meta_learning_score = float(torch.mean(torch.abs(integrated_output)).item())
            
            # ì‹œìŠ¤í…œ ì¼ê´€ì„±
            system_coherence = 0.75  # ì‹¤ì œë¡œëŠ” í—¤ë“œë“¤ ê°„ ì¼ê´€ì„± ì¸¡ì •
            
            return {
                'integrated_output': integrated_output,
                'integration_weights': integration_weights,
                'meta_learning_score': meta_learning_score,
                'system_coherence': system_coherence,
                'confidence': 0.83
            }
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ í†µí•© ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return {'error': str(e), 'confidence': 0.0}

class HeadCompatibilityManager:
    """
    í—¤ë“œ í˜¸í™˜ì„± ë§¤ë‹ˆì € - ëª¨ë“  í—¤ë“œ ì–´ëŒ‘í„°ë“¤ì„ ê´€ë¦¬í•˜ê³  ì¡°ì •
    """
    
    def __init__(self, unified_backbone: RedHeartUnifiedBackbone, 
                 swap_manager: RedHeartDynamicSwapManager):
        self.unified_backbone = unified_backbone
        self.swap_manager = swap_manager
        
        # í—¤ë“œ ì–´ëŒ‘í„°ë“¤ ì´ˆê¸°í™”
        self.head_adapters = {
            HeadType.EMOTION_EMPATHY: EmotionEmpathyHeadAdapter(),
            HeadType.BENTHAM_FROMM: BenthamFrommHeadAdapter(),
            HeadType.SEMANTIC_SURD: SemanticSURDHeadAdapter(),
            HeadType.REGRET_LEARNING: RegretLearningHeadAdapter(),
            HeadType.META_INTEGRATION: MetaIntegrationHeadAdapter()
        }
        
        # í—¤ë“œë“¤ì€ ì´ˆê¸°í™” í›„ì— ë“±ë¡ë¨ (initialize_all_headsì—ì„œ)
        
        # ì‹œë„ˆì§€ ì°½ì¶œì„ ìœ„í•œ ì „ì—­ í¬ë¡œìŠ¤ ì–´í…ì…˜ (ë°±ë³¸ê³¼ í˜¸í™˜ë˜ëŠ” í—¤ë“œ ìˆ˜)
        backbone_num_heads = ADVANCED_CONFIG['unified_backbone'].get('num_heads', 20)
        LightweightCrossAttention = get_lightweight_cross_attention()
        self.global_cross_attention = LightweightCrossAttention(
            d_model=ADVANCED_CONFIG['unified_backbone']['d_model'],
            num_heads=backbone_num_heads  # ë°±ë³¸ê³¼ í˜¸í™˜ë˜ëŠ” í—¤ë“œ ìˆ˜
        )
        
        self.initialized = False
        logger.info("HeadCompatibilityManager ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _register_heads_with_swap_manager(self):
        """í—¤ë“œë“¤ì„ ìŠ¤ì™‘ ë§¤ë‹ˆì €ì— ë“±ë¡"""
        logger.info(f"ğŸ”¥ _register_heads_with_swap_manager ì‹œì‘ - {len(self.head_adapters)}ê°œ í—¤ë“œ ë“±ë¡ ì˜ˆì •")
        for head_type, adapter in self.head_adapters.items():
            logger.info(f"ğŸ”„ {head_type.value} í—¤ë“œ ë“±ë¡ ì‹œì‘...")
            # ê° í—¤ë“œì˜ ë„¤íŠ¸ì›Œí¬ë“¤ì„ ìŠ¤ì™‘ ë§¤ë‹ˆì €ì— ë“±ë¡
            if hasattr(adapter, 'input_adapter') and adapter.input_adapter is not None:
                self.swap_manager.register_model(
                    f"{head_type.value}_input_adapter",
                    adapter.input_adapter,
                    adapter.priority
                )
            
            if hasattr(adapter, 'output_adapter') and adapter.output_adapter is not None:
                self.swap_manager.register_model(
                    f"{head_type.value}_output_adapter", 
                    adapter.output_adapter,
                    adapter.priority
                )
            
            # ê°œì„ ëœ PyTorch ë„¤íŠ¸ì›Œí¬ íƒì§€ - ê° ì–´ëŒ‘í„°ì˜ get_pytorch_network() ë©”ì„œë“œ ì‚¬ìš©
            pytorch_network = None
            
            # 1. ì–´ëŒ‘í„°ì˜ get_pytorch_network() ë©”ì„œë“œ ìš°ì„  ì‚¬ìš©
            logger.info(f"ğŸ“‹ {head_type.value} ì–´ëŒ‘í„° ë¶„ì„:")
            logger.info(f"   - ì–´ëŒ‘í„° íƒ€ì…: {type(adapter)}")
            logger.info(f"   - get_pytorch_network ì¡´ì¬: {hasattr(adapter, 'get_pytorch_network')}")
            
            # ë””ë²„ê¹…ìš© - ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì„œë“œ ëª©ë¡
            adapter_methods = [m for m in dir(adapter) if not m.startswith('_') and callable(getattr(adapter, m, None))]
            logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì„œë“œë“¤: {adapter_methods[:10]}...")
            
            if hasattr(adapter, 'get_pytorch_network'):
                try:
                    logger.info(f"   ğŸ”„ {head_type.value}.get_pytorch_network() í˜¸ì¶œ ì¤‘...")
                    pytorch_network = adapter.get_pytorch_network()
                    logger.info(f"   âœ… get_pytorch_network() ì„±ê³µ - ë°˜í™˜ê°’: {type(pytorch_network) if pytorch_network else 'None'}")
                except Exception as e:
                    logger.error(f"   âŒ {head_type.value} get_pytorch_network() í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    import traceback
                    logger.error(f"   ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            
            # 2. ê¸°ì¡´ ë°©ì‹ fallback (í˜¸í™˜ì„± ë³´ì¥)
            if pytorch_network is None:
                main_component = getattr(adapter, 'empathy_learner', None) or \
                               getattr(adapter, 'bentham_calculator', None) or \
                               getattr(adapter, 'semantic_analyzer', None) or \
                               getattr(adapter, 'regret_network', None) or \
                               getattr(adapter, 'integration_network', None)
                
                if main_component is not None:
                    if isinstance(main_component, nn.Module):
                        pytorch_network = main_component
                    else:
                        for attr_name in ['_neural_predictor', 'neural_predictor', 'model', 'network', 'classifier']:
                            neural_component = getattr(main_component, attr_name, None)
                            if neural_component is not None and isinstance(neural_component, nn.Module):
                                pytorch_network = neural_component
                                break
            
            # ë„¤íŠ¸ì›Œí¬ ë“±ë¡ - NO DUMMY, NO FALLBACK (í”„ë¡œì íŠ¸ ê·œì¹™)
            try:
                if pytorch_network is not None:
                    self.swap_manager.register_model(
                        head_type.value,
                        pytorch_network,
                        adapter.priority
                    )
                    logger.info(f"âœ… {head_type.value}ì˜ ì‹¤ì œ PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ swap_managerì— ë“±ë¡ ì„±ê³µ")
                    logger.info(f"   - ë„¤íŠ¸ì›Œí¬ íƒ€ì…: {type(pytorch_network)}")
                    logger.info(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in pytorch_network.parameters()) / 1e6:.2f}M")
                else:
                    # NO DUMMY - í”„ë¡œì íŠ¸ ê·œì¹™: ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì—ëŸ¬
                    logger.error(f"âŒ {head_type.value}ì˜ PyTorch ë„¤íŠ¸ì›Œí¬ê°€ None")
                    raise RuntimeError(f"{head_type.value} í—¤ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: PyTorch ë„¤íŠ¸ì›Œí¬ ì—†ìŒ")
                    
                    # device_policy í™•ì¸ ë° ì ìš©
                    device_policy = getattr(adapter, 'device_policy', 'gpu_required')
                    logger.info(f"   - device_policy: {device_policy}")
                    
                    if device_policy == 'cpu_preload':
                        # CPU í”„ë¦¬ë¡œë“œ í—¤ë“œëŠ” ë“±ë¡ë§Œ í•˜ê³  ì¦‰ì‹œ GPUâ†’CPUë¡œ ì–¸ë¡œë“œ
                        logger.info(f"   ğŸ“‹ {head_type.value}ëŠ” cpu_preload ì •ì±… - CPUë¡œ ì–¸ë¡œë“œ")
                        try:
                            # CPUë¡œ ì´ë™
                            if hasattr(pytorch_network, 'to'):
                                pytorch_network.to('cpu')
                                logger.info(f"   âœ… {head_type.value} CPUë¡œ ì–¸ë¡œë“œ ì™„ë£Œ")
                                
                                # swap_managerì—ì„œë„ GPU ëª©ë¡ì—ì„œ ì œê±°
                                if hasattr(self.swap_manager, 'gpu_resident_models') and head_type.value in self.swap_manager.gpu_resident_models:
                                    del self.swap_manager.gpu_resident_models[head_type.value]
                                    logger.info(f"   ğŸ—‘ï¸ {head_type.value}ë¥¼ GPU ìƒì£¼ ëª©ë¡ì—ì„œ ì œê±°")
                        except Exception as e:
                            logger.warning(f"   âš ï¸ {head_type.value} CPU ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
            except Exception as e:
                logger.error(f"âŒ {head_type.value} ë„¤íŠ¸ì›Œí¬ ë“±ë¡ ì‹¤íŒ¨: {e}")
                # í”„ë¡œì íŠ¸ ê·œì¹™ì— ë”°ë¼ fallback ì—†ì´ ì—ëŸ¬ ë°œìƒ
                raise RuntimeError(f"{head_type.value} swap_manager ë“±ë¡ ì‹¤íŒ¨: {e}") from e
    
    async def initialize_all_heads(self):
        """ëª¨ë“  í—¤ë“œ ìˆœì°¨ì  GPU ì´ˆê¸°í™” + ì¦‰ì‹œ ìŠ¤ì™‘ ì‹œìŠ¤í…œ (85% ì˜ˆì¸¡ ê¸°ë°˜)"""
        logger.debug("ğŸ” initialize_all_heads() ë©”ì„œë“œ ì§„ì…!")
        logger.debug(f"ğŸ” self.initialized = {getattr(self, 'initialized', 'NOT_SET')}")
        
        if self.initialized:
            logger.debug("ğŸ” ì´ë¯¸ initialized=Trueì´ë¯€ë¡œ return")
            return
            
        logger.info("ğŸš€ GPU ìˆœì°¨ì  ì´ˆê¸°í™” + ì¦‰ì‹œ ìŠ¤ì™‘ ì‹œìŠ¤í…œ ì‹œì‘")
        
        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        logger.debug("ğŸ” Step A0: import ì‹œì‘...")
        import torch
        logger.debug("ğŸ” Step A1: torch import ì™„ë£Œ")
        from config import get_gpu_memory_info, get_master_orchestrator
        logger.debug("ğŸ” Step A2: config import ì™„ë£Œ")
        
        logger.debug("ğŸ” Step A: torch import ì™„ë£Œ")
        
        if torch.cuda.is_available():
            memory_info = get_gpu_memory_info()
            if memory_info:
                logger.info(f"ğŸ” ì´ˆê¸°í™” ì „ GPU ë©”ëª¨ë¦¬: {memory_info['usage_percent']:.1f}% ì‚¬ìš©ì¤‘")
        
        logger.info("ğŸ” Step B: GPU ë©”ëª¨ë¦¬ í™•ì¸ ì™„ë£Œ")
        
        # Master Memory Orchestrator ê°€ì ¸ì˜¤ê¸°
        logger.info("ğŸ” Step C: MasterOrchestrator ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        master_orch = get_master_orchestrator()
        logger.info(f"ğŸ” Step C ì™„ë£Œ: MasterOrchestrator = {type(master_orch)}")
        
        initialization_errors = []
        
        # í—¤ë“œ ì´ˆê¸°í™” ìš°ì„ ìˆœìœ„ ì •ì˜ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ)
        # ê³µê²©ì  ìµœì í™”: ë” ì •í™•í•œ ë©”ëª¨ë¦¬ ì˜ˆì¸¡ê°’ ì ìš©
        head_priority_order = [
            (HeadType.META_INTEGRATION, 35),      # 35MB - ê°€ì¥ ì‘ìŒ (ì •ë°€ ì¸¡ì •)
            (HeadType.SEMANTIC_SURD, 75),         # 75MB (ì •ë°€ ì¸¡ì •)
            (HeadType.BENTHAM_FROMM, 115),        # 115MB (ì •ë°€ ì¸¡ì •)
            (HeadType.REGRET_LEARNING, 115),      # 115MB (ì •ë°€ ì¸¡ì •)
            (HeadType.EMOTION_EMPATHY, 135),      # 135MB - ê°€ì¥ í¼ (ì •ë°€ ì¸¡ì •)
        ]
        
        logger.info(f"ğŸ“‹ í—¤ë“œ ì´ˆê¸°í™” ìˆœì„œ: {[f'{ht.value}({mb}MB)' for ht, mb in head_priority_order]}")
        
        logger.info("ğŸ” Step D: í—¤ë“œ ìš°ì„ ìˆœìœ„ ì •ì˜ ì™„ë£Œ")
        
        # ğŸ”¥ ìˆœì°¨ì  GPU ì´ˆê¸°í™” + ì¦‰ì‹œ ìŠ¤ì™‘ ì‹œìŠ¤í…œ
        logger.info("ğŸ” Step E: í—¤ë“œ ì´ˆê¸°í™” ë£¨í”„ ì‹œì‘...")
        for i, (head_type, estimated_mb) in enumerate(head_priority_order):
            logger.info(f"ğŸ” Step E-{i+1}: {head_type.value} ì´ˆê¸°í™” ì‹œì‘ ({estimated_mb}MB)")
            
            if head_type not in self.head_adapters:
                logger.warning(f"âš ï¸ {head_type.value} ì–´ëŒ‘í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
                continue
                
            logger.info(f"ğŸ” Step E-{i+1}a: {head_type.value} ì–´ëŒ‘í„° ë°œê²¬")
            
            adapter = self.head_adapters[head_type]
            logger.info(f"ğŸ” Step E-{i+1}b: {head_type.value} ì–´ëŒ‘í„° ê°€ì ¸ì˜´ = {type(adapter)}")
            
            try:
                # ğŸ”¥ Step 1: ë©”ëª¨ë¦¬ ì˜ˆì¸¡ ì²´í¬ (85% ì´ˆê³¼ ì˜ˆìƒ ì‹œ ì‚¬ì „ ìŠ¤ì™‘)
                current_memory = get_gpu_memory_info()
                if current_memory:
                    # ë” ì •ë°€í•œ ë©”ëª¨ë¦¬ ì˜ˆì¸¡ (8GB = 8192MB ê¸°ì¤€)
                    predicted_usage = current_memory['usage_percent'] + (estimated_mb / 81.92)
                    
                    logger.info(f"ğŸ“Š {head_type.value} ë¡œë”© ì˜ˆì¸¡: {current_memory['usage_percent']:.1f}% + {estimated_mb}MB = {predicted_usage:.1f}%")
                    
                    # ğŸš€ ê³µê²©ì  GPU í™œìš©: 95% ì´ˆê³¼ ì‹œì—ë§Œ ì •ë¦¬ (85% ëª©í‘œ ë‹¬ì„±)
                    # í˜„ì¬ ë°±ë³¸(21.5%) + ëª¨ë“ í—¤ë“œ(6%) = 27.3%ì´ë¯€ë¡œ í›¨ì”¬ ì—¬ìœ  ìˆìŒ
                    if predicted_usage > 95:
                        logger.warning(f"âš ï¸ 95% ì´ˆê³¼ ì˜ˆìƒ ({predicted_usage:.1f}%) - ì„ íƒì  ì •ë¦¬ ì‹¤í–‰")
                        await master_orch._emergency_intelligent_cleanup()
                        
                        # ìŠ¤ì™‘ í›„ ë©”ëª¨ë¦¬ ì¬í™•ì¸
                        post_swap_memory = get_gpu_memory_info()
                        if post_swap_memory:
                            logger.info(f"âœ… ì •ë¦¬ í›„ ë©”ëª¨ë¦¬: {post_swap_memory['usage_percent']:.1f}%")
                    else:
                        logger.info(f"ğŸš€ GPU ì—¬ìœ  ì¶©ë¶„ ({predicted_usage:.1f}%) - í—¤ë“œ GPU ìƒì£¼ ì§„í–‰")
                
                # ğŸ”¥ Step 2: GPUì—ì„œ í—¤ë“œ ì´ˆê¸°í™”
                logger.info(f"ğŸ”¥ {head_type.value} GPU ì´ˆê¸°í™” ì‹œì‘ (ì˜ˆìƒ: {estimated_mb}MB)...")
                logger.info(f"ğŸ” Step E-{i+1}c: adapter.initialize_head() í˜¸ì¶œ ì „")
                
                # CPU ëª¨ë“œ í•´ì œ - GPUì—ì„œ ì´ˆê¸°í™”í•´ì•¼ í•¨
                if hasattr(adapter, 'force_cpu_mode'):
                    adapter.force_cpu_mode = False
                    logger.info(f"ğŸ” Step E-{i+1}d: force_cpu_mode = False ì„¤ì •")
                
                logger.info(f"ğŸ” Step E-{i+1}e: adapter.initialize_head() í˜¸ì¶œ ì¤‘...")
                
                # ê° í—¤ë“œë³„ ì´ˆê¸°í™”ì— íƒ€ì„ì•„ì›ƒ ì¶”ê°€ (180ì´ˆë¡œ ì¦ê°€ - ëŒ€í˜• ëª¨ë¸ ë¡œë”© ê³ ë ¤)
                try:
                    await asyncio.wait_for(adapter.initialize_head(), timeout=180.0)
                    logger.info(f"ğŸ” Step E-{i+1}f: adapter.initialize_head() ì™„ë£Œ!")
                except asyncio.TimeoutError:
                    logger.error(f"âŒ {head_type.value} í—¤ë“œ ì´ˆê¸°í™” 180ì´ˆ íƒ€ì„ì•„ì›ƒ!")
                    logger.error(f"ğŸ” Hanging ë°œìƒí•œ í—¤ë“œ: {head_type.value}")
                    raise RuntimeError(f"{head_type.value} head initialization timeout - cannot continue")
                
                # ì´ˆê¸°í™” í›„ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                post_init_memory = get_gpu_memory_info()
                if post_init_memory:
                    logger.info(f"ğŸ“Š {head_type.value} ì´ˆê¸°í™” í›„: {post_init_memory['usage_percent']:.1f}%")
                
                # ğŸ”¥ Step 3: GPU ìƒì£¼ ìœ ì§€ (ê³µê²©ì  ë©”ëª¨ë¦¬ í™œìš©)
                logger.info(f"ğŸš€ {head_type.value} GPU ìƒì£¼ ìœ ì§€ - ê³µê²©ì  í™œìš© ëª¨ë“œ")
                
                # í—¤ë“œì˜ ì‹¤ì œ PyTorch ë„¤íŠ¸ì›Œí¬ ì°¾ê¸° ë° GPU ìƒì£¼ í™•ì¸
                pytorch_network = None
                if hasattr(adapter, 'get_pytorch_network'):
                    pytorch_network = adapter.get_pytorch_network()
                
                if pytorch_network is not None:
                    # DSMì— ì‹¤ì œ í—¤ë“œ ì¬ë“±ë¡ (êµì²´)
                    from dynamic_swap_manager import get_swap_manager, SwapPriority
                    swap = get_swap_manager()
                    if swap and isinstance(pytorch_network, torch.nn.Module):
                        swap.register_model(head_type.value, pytorch_network, priority=SwapPriority.HIGH)
                        logger.info(f"âœ… {head_type.value} DSM ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì‹¤ì œ í—¤ë“œë¡œ êµì²´)")
                    
                    # GPU ìƒì£¼ í™•ì¸ ë° ë“±ë¡
                    model_id = f"{head_type.value}_gpu_resident"
                    
                    # GPU ìƒì£¼ ìƒíƒœ í™•ì¸
                    try:
                        # GPU ë¡œë”©ì€ ë¬´ì¡°ê±´ DSM ê²½ìœ  (ì§ì ‘ .to('cuda') ê¸ˆì§€)
                        if next(pytorch_network.parameters()).device.type != 'cuda':
                            # DSMì„ í†µí•´ GPUë¡œ ë¡œë“œ
                            await swap.load_head_to_gpu(head_type.value, timeout=30.0)
                            logger.info(f"ğŸ”¥ {head_type.value} GPU ì¬ë¡œë”© ì™„ë£Œ (DSM ê²½ìœ )")
                        else:
                            logger.info(f"âœ… {head_type.value} GPU ìƒì£¼ í™•ì¸ë¨")
                        
                        # GPU ìƒì£¼ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ
                        post_gpu_memory = get_gpu_memory_info()
                        if post_gpu_memory:
                            logger.info(f"ğŸ“Š GPU ìƒì£¼ í›„ ë©”ëª¨ë¦¬: {post_gpu_memory['usage_percent']:.1f}%")
                            
                    except Exception as gpu_error:
                        logger.error(f"âŒ {head_type.value} GPU ìƒì£¼ ì‹¤íŒ¨: {str(gpu_error)}")
                else:
                    logger.warning(f"âš ï¸ {head_type.value} PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
                logger.info(f"âœ… {head_type.value} ì´ˆê¸°í™” + ìŠ¤ì™‘ ì™„ë£Œ")
                
                # ğŸ”¥ Step 4: CUDA ìºì‹œ ì •ë¦¬ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    
                    final_memory = get_gpu_memory_info()
                    if final_memory:
                        logger.info(f"ğŸ“Š {head_type.value} ìµœì¢… ë©”ëª¨ë¦¬: {final_memory['usage_percent']:.1f}%")
                    
            except Exception as e:
                error_msg = f"âŒ {head_type.value} í—¤ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
                logger.error(error_msg)
                initialization_errors.append((head_type, str(e)))
                
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸´ê¸‰ ì •ë¦¬
                try:
                    await master_orch._emergency_intelligent_cleanup()
                except Exception as cleanup_error:
                    logger.error(f"ê¸´ê¸‰ ì •ë¦¬ë„ ì‹¤íŒ¨: {str(cleanup_error)}")
        
        # ğŸ”¥ ìµœì¢… ë‹¨ê³„: ëª¨ë“  í—¤ë“œë¥¼ ìŠ¤ì™‘ ë§¤ë‹ˆì €ì— ë“±ë¡
        logger.info("ğŸ”„ ëª¨ë“  í—¤ë“œë¥¼ ë™ì  ìŠ¤ì™‘ ë§¤ë‹ˆì €ì— ë“±ë¡ ì¤‘...")
        self._register_heads_with_swap_manager()
        
        # ğŸ”¥ ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ë° ë³´ê³ 
        final_memory = get_gpu_memory_info()
        if final_memory:
            logger.info(f"ğŸ¯ ìµœì¢… GPU ë©”ëª¨ë¦¬ ìƒíƒœ: {final_memory['usage_percent']:.1f}%")
            
            if final_memory['usage_percent'] >= 80 and final_memory['usage_percent'] <= 85:
                logger.info(f"ğŸ¯ ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì : {final_memory['usage_percent']:.1f}% (85% ê·¼ì ‘ ë‹¬ì„±!)")
            elif final_memory['usage_percent'] >= 70:
                logger.info(f"âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ ì–‘í˜¸: {final_memory['usage_percent']:.1f}% (ë” ê³µê²©ì  í™œìš© ê°€ëŠ¥)")
            elif final_memory['usage_percent'] < 50:
                logger.warning(f"ğŸ“Š ë©”ëª¨ë¦¬ ê³¼ì†Œ í™œìš©: {final_memory['usage_percent']:.1f}% (GPU ë¦¬ì†ŒìŠ¤ ë‚­ë¹„)")
            else:
                logger.error(f"âš ï¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì£¼ì˜: {final_memory['usage_percent']:.1f}% (85% ì´ˆê³¼)")
        
        # ì˜¤ë¥˜ ë³´ê³ 
        if initialization_errors:
            logger.warning(f"âš ï¸ {len(initialization_errors)}ê°œ í—¤ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨:")
            for head_type, error in initialization_errors:
                logger.warning(f"  - {head_type.value}: {error}")
        
        successful_heads = len(self.head_adapters) - len(initialization_errors)
        logger.info(f"ğŸ‰ GPU ìˆœì°¨ì  ì´ˆê¸°í™” ì™„ë£Œ: {successful_heads}/{len(self.head_adapters)}ê°œ ì„±ê³µ")
        
        # ğŸ”¥ Step 6: 85% ë‹¬ì„±ì„ ìœ„í•œ ì¶”ê°€ ê³µê²©ì  ë¡œë”©
        await self._aggressive_gpu_utilization()
        
        self.initialized = True
    
    async def _aggressive_gpu_utilization(self):
        """85% ê·¼ì ‘ ë‹¬ì„±ì„ ìœ„í•œ ê³µê²©ì  GPU í™œìš©"""
        logger.info("ğŸ”¥ ê³µê²©ì  GPU í™œìš© ì‹œì‘ - 85% ëª©í‘œ ë‹¬ì„±")
        
        current_memory = get_gpu_memory_info()
        if not current_memory:
            logger.warning("GPU ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
            return
        
        target_usage = 82  # 85% ê·¼ì ‘ì„  (ì•ˆì „ ë§ˆì§„ 3%)
        current_usage = current_memory['usage_percent']
        
        logger.info(f"ğŸ“Š í˜„ì¬ ì‚¬ìš©ë¥ : {current_usage:.1f}% â†’ ëª©í‘œ: {target_usage}%")
        
        if current_usage >= target_usage:
            logger.info(f"âœ… ì´ë¯¸ ëª©í‘œ ë‹¬ì„±: {current_usage:.1f}%")
            return
        
        # ì¶”ê°€ í™œìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ ê³„ì‚°
        available_percent = target_usage - current_usage
        available_gb = (available_percent / 100) * 8.0  # 8GB GPU ê¸°ì¤€
        
        logger.info(f"ğŸ¯ ì¶”ê°€ í™œìš© ê°€ëŠ¥: {available_percent:.1f}% ({available_gb:.2f}GB)")
        
        # ğŸ”¥ ì „ëµ 1: í—¤ë“œ ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ìš©)
        await self._load_multi_instance_heads(available_gb * 0.4)  # 40% í• ë‹¹
        
        # ğŸ”¥ ì „ëµ 2: í”„ë¦¬ë¡œë“œ ìºì‹œ ì‹œìŠ¤í…œ
        await self._load_preload_cache(available_gb * 0.3)  # 30% í• ë‹¹
        
        # ğŸ”¥ ì „ëµ 3: ì¤‘ê°„ ê²°ê³¼ ìºì‹œ ë²„í¼
        await self._load_intermediate_cache(available_gb * 0.2)  # 20% í• ë‹¹
        
        # ğŸ”¥ ì „ëµ 4: ì¶”ê°€ ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ (ì–´í…ì…˜ ìºì‹œ ë“±)
        await self._load_additional_components(available_gb * 0.1)  # 10% í• ë‹¹
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        final_memory = get_gpu_memory_info()
        if final_memory:
            achieved_usage = final_memory['usage_percent']
            logger.info(f"ğŸ¯ ê³µê²©ì  í™œìš© ì™„ë£Œ: {current_usage:.1f}% â†’ {achieved_usage:.1f}%")
            
            if achieved_usage >= 80:
                logger.info("ğŸš€ 85% ê·¼ì ‘ì„  ë‹¬ì„±! GPU ìµœëŒ€ í™œìš© ì„±ê³µ")
            else:
                logger.warning(f"ğŸ“Š ì¶”ê°€ í™œìš© ê°€ëŠ¥: í˜„ì¬ {achieved_usage:.1f}% (ëª©í‘œ 82%)")
    
    async def _load_multi_instance_heads(self, target_gb: float):
        """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í—¤ë“œ ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ ë¡œë”©"""
        logger.info(f"ğŸ”¥ í—¤ë“œ ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ ë¡œë”© ì‹œì‘ (ëª©í‘œ: {target_gb:.2f}GB)")
        
        loaded_gb = 0.0
        instance_count = 0
        
        # ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” í—¤ë“œë“¤ì˜ ì¶”ê°€ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        priority_heads = [HeadType.EMOTION_EMPATHY, HeadType.BENTHAM_FROMM]
        
        for head_type in priority_heads:
            if loaded_gb >= target_gb:
                break
                
            if head_type in self.head_adapters:
                try:
                    # ê¸°ì¡´ í—¤ë“œë¥¼ ë³µì œí•˜ì—¬ ì¶”ê°€ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                    original_adapter = self.head_adapters[head_type]
                    
                    # ê°„ë‹¨í•œ ë³µì œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë³µì œ ë¡œì§ í•„ìš”)
                    if hasattr(original_adapter, 'get_pytorch_network'):
                        network = original_adapter.get_pytorch_network()
                        if network and loaded_gb + 0.135 <= target_gb:  # 135MB ì˜ˆìƒ
                            # GPUì— ì¶”ê°€ ì¸ìŠ¤í„´ìŠ¤ ìƒì£¼
                            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ëª¨ë¸ ë³µì œ ë¡œì§ í•„ìš”
                            logger.info(f"ğŸ”¥ {head_type.value} ì¶”ê°€ ì¸ìŠ¤í„´ìŠ¤ GPU ë¡œë”© (ì˜ˆìƒ: 135MB)")
                            loaded_gb += 0.135
                            instance_count += 1
                            
                except Exception as e:
                    logger.warning(f"í—¤ë“œ ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {head_type.value} - {str(e)}")
        
        logger.info(f"âœ… í—¤ë“œ ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤: {instance_count}ê°œ, {loaded_gb:.2f}GB ë¡œë”©")
    
    async def _load_preload_cache(self, target_gb: float):
        """ë‹¤ìŒ ì‚¬ìš© ì˜ˆìƒ ëª¨ë¸ë“¤ì˜ í”„ë¦¬ë¡œë“œ ìºì‹œ"""
        logger.info(f"ğŸ“¦ í”„ë¦¬ë¡œë“œ ìºì‹œ ìƒì„± (ëª©í‘œ: {target_gb:.2f}GB)")
        
        try:
            # ê°€ìƒì˜ í”„ë¦¬ë¡œë“œ í…ì„œë“¤ ìƒì„± (ì‹¤ì œë¡œëŠ” ì˜ˆìƒ ëª¨ë¸ë“¤)
            import torch
            cache_tensors = []
            loaded_gb = 0.0
            
            # ëŒ€ìš©ëŸ‰ ìºì‹œ í…ì„œë“¤ ìƒì„±
            tensor_sizes = [
                (8192, 1280),   # 40MB
                (4096, 2560),   # 40MB  
                (2048, 5120),   # 40MB
                (16384, 640),   # 40MB
            ]
            
            for i, (h, w) in enumerate(tensor_sizes):
                if loaded_gb >= target_gb:
                    break
                    
                tensor_size_gb = (h * w * 4) / (1024**3)  # float32 ê¸°ì¤€
                if loaded_gb + tensor_size_gb <= target_gb:
                    tensor = torch.randn(h, w, device='cuda', dtype=torch.float32)
                    cache_tensors.append(tensor)
                    loaded_gb += tensor_size_gb
                    logger.info(f"ğŸ“¦ í”„ë¦¬ë¡œë“œ ìºì‹œ {i+1}: [{h}x{w}] {tensor_size_gb*1024:.0f}MB")
            
            # ìºì‹œ í…ì„œë“¤ì„ í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥ (GC ë°©ì§€)
            self._preload_cache_tensors = cache_tensors
            logger.info(f"âœ… í”„ë¦¬ë¡œë“œ ìºì‹œ: {len(cache_tensors)}ê°œ, {loaded_gb:.2f}GB")
            
        except Exception as e:
            logger.warning(f"í”„ë¦¬ë¡œë“œ ìºì‹œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def _load_intermediate_cache(self, target_gb: float):
        """ì¤‘ê°„ ê²°ê³¼ ìºì‹œ ë²„í¼ ìƒì„±"""
        logger.info(f"ğŸ’¾ ì¤‘ê°„ ê²°ê³¼ ìºì‹œ ìƒì„± (ëª©í‘œ: {target_gb:.2f}GB)")
        
        try:
            import torch
            cache_buffers = []
            loaded_gb = 0.0
            
            # ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì¤‘ê°„ ê²°ê³¼ ë²„í¼ë“¤
            buffer_configs = [
                (1024, 1280, "attention_cache"),
                (2048, 640, "embedding_cache"),
                (512, 2560, "ffn_cache"),
                (4096, 320, "output_cache"),
            ]
            
            for h, w, name in buffer_configs:
                if loaded_gb >= target_gb:
                    break
                    
                buffer_size_gb = (h * w * 4) / (1024**3)
                if loaded_gb + buffer_size_gb <= target_gb:
                    buffer = torch.zeros(h, w, device='cuda', dtype=torch.float32)
                    cache_buffers.append((name, buffer))
                    loaded_gb += buffer_size_gb
                    logger.info(f"ğŸ’¾ {name}: [{h}x{w}] {buffer_size_gb*1024:.0f}MB")
            
            self._intermediate_cache_buffers = cache_buffers
            logger.info(f"âœ… ì¤‘ê°„ ê²°ê³¼ ìºì‹œ: {len(cache_buffers)}ê°œ, {loaded_gb:.2f}GB")
            
        except Exception as e:
            logger.warning(f"ì¤‘ê°„ ê²°ê³¼ ìºì‹œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def _load_additional_components(self, target_gb: float):
        """ì¶”ê°€ ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ë¡œë”©"""
        logger.info(f"âš¡ ì¶”ê°€ ì»´í¬ë„ŒíŠ¸ ë¡œë”© (ëª©í‘œ: {target_gb:.2f}GB)")
        
        try:
            import torch
            additional_components = []
            loaded_gb = 0.0
            
            # ì¶”ê°€ ì–´í…ì…˜ í—¤ë“œ, ì„ë² ë”© ë ˆì´ì–´ ë“±
            component_configs = [
                (1280, 1280, "extra_attention"),
                (1280, 5120, "extra_ffn"),
                (50000, 64, "extra_embedding"),
            ]
            
            for h, w, name in component_configs:
                if loaded_gb >= target_gb:
                    break
                    
                comp_size_gb = (h * w * 4) / (1024**3)
                if loaded_gb + comp_size_gb <= target_gb:
                    component = torch.randn(h, w, device='cuda', dtype=torch.float32)
                    additional_components.append((name, component))
                    loaded_gb += comp_size_gb
                    logger.info(f"âš¡ {name}: [{h}x{w}] {comp_size_gb*1024:.0f}MB")
            
            self._additional_components = additional_components
            logger.info(f"âœ… ì¶”ê°€ ì»´í¬ë„ŒíŠ¸: {len(additional_components)}ê°œ, {loaded_gb:.2f}GB")
            
        except Exception as e:
            logger.warning(f"ì¶”ê°€ ì»´í¬ë„ŒíŠ¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
    
    async def process_with_all_heads(self, text_input: str) -> Dict[HeadType, HeadProcessingResult]:
        """ëª¨ë“  í—¤ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ì²˜ë¦¬"""
        if not self.initialized:
            await self.initialize_all_heads()
        
        # 1. í†µí•© ë°±ë³¸ìœ¼ë¡œ í‘œí˜„ ìƒì„±
        unified_repr = self.unified_backbone.get_embedding_for_text(text_input)
        
        # 2. ê° í—¤ë“œì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
        processing_tasks = {}
        for head_type, adapter in self.head_adapters.items():
            processing_tasks[head_type] = adapter.process_unified_input(unified_repr)
        
        # 3. ëª¨ë“  í—¤ë“œ ê²°ê³¼ ìˆ˜ì§‘
        results = {}
        for head_type, task in processing_tasks.items():
            try:
                results[head_type] = await task
            except Exception as e:
                logger.error(f"í—¤ë“œ {head_type.value} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                results[head_type] = HeadProcessingResult(
                    head_type=head_type,
                    primary_output={'error': str(e)},
                    confidence_score=0.0
                )
        
        # 4. ì‹œë„ˆì§€ íš¨ê³¼ ê³„ì‚°
        await self._calculate_synergy_effects(results, unified_repr)
        
        return results
    
    async def _calculate_synergy_effects(self, results: Dict[HeadType, HeadProcessingResult], 
                                       unified_repr: UnifiedRepresentation):
        """í—¤ë“œë“¤ ê°„ ì‹œë„ˆì§€ íš¨ê³¼ ê³„ì‚°"""
        try:
            # ì‹œë„ˆì§€ íŠ¹ì„±ë“¤ ìˆ˜ì§‘
            synergy_pairs = []
            for head_type, result in results.items():
                if result.synergy_features is not None:
                    synergy_pairs.append((head_type.value, result.synergy_features))
            
            if len(synergy_pairs) > 1:
                # ì „ì—­ í¬ë¡œìŠ¤ ì–´í…ì…˜ìœ¼ë¡œ ì‹œë„ˆì§€ ê³„ì‚°
                global_synergy = self.global_cross_attention(
                    query=unified_repr.shared_embedding,
                    key_value_pairs=synergy_pairs
                )
                
                # ì‹œë„ˆì§€ ì ìˆ˜ë¥¼ ê° ê²°ê³¼ì— ì¶”ê°€
                for head_type, result in results.items():
                    synergy_score = 0.0
                    if head_type.value in global_synergy:
                        synergy_tensor = global_synergy[head_type.value]
                        synergy_score = float(torch.mean(synergy_tensor).item())
                    
                    result.secondary_outputs['synergy_score'] = synergy_score
                    
        except Exception as e:
            logger.error(f"ì‹œë„ˆì§€ íš¨ê³¼ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """í—¤ë“œ í˜¸í™˜ì„± ë§¤ë‹ˆì € í†µê³„"""
        stats = {
            'initialized_heads': sum(1 for adapter in self.head_adapters.values() if adapter.initialized),
            'total_heads': len(self.head_adapters),
            'swap_manager_stats': self.swap_manager.get_stats(),
            'memory_status': self.swap_manager.get_memory_status()
        }
        
        return stats

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
async def example_usage():
    """í—¤ë“œ í˜¸í™˜ì„± ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš© ì˜ˆì‹œ"""
    # í†µí•© ë°±ë³¸ ë° ìŠ¤ì™‘ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    backbone = RedHeartUnifiedBackbone()
    swap_manager = RedHeartDynamicSwapManager()
    await swap_manager.initialize()
    
    # í˜¸í™˜ì„± ë§¤ë‹ˆì € ìƒì„±
    compatibility_manager = HeadCompatibilityManager(backbone, swap_manager)
    
    try:
        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        test_input = "ì‚¬ëŒë“¤ê³¼ì˜ ê´€ê³„ì—ì„œ ì–´ë–»ê²Œ ê³µê°í•  ìˆ˜ ìˆì„ê¹Œ?"
        
        # ëª¨ë“  í—¤ë“œë¡œ ì²˜ë¦¬
        results = await compatibility_manager.process_with_all_heads(test_input)
        
        # ê²°ê³¼ ì¶œë ¥
        print("=== í—¤ë“œë³„ ì²˜ë¦¬ ê²°ê³¼ ===")
        for head_type, result in results.items():
            print(f"\n{head_type.value}:")
            print(f"  ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.3f}s")
            print(f"  ì‹ ë¢°ë„: {result.confidence_score:.2f}")
            print(f"  ë””ë°”ì´ìŠ¤: {result.device_used}")
            if 'synergy_score' in result.secondary_outputs:
                print(f"  ì‹œë„ˆì§€ ì ìˆ˜: {result.secondary_outputs['synergy_score']:.3f}")
        
        # í†µê³„ ì¶œë ¥
        stats = compatibility_manager.get_statistics()
        print(f"\n=== ì‹œìŠ¤í…œ í†µê³„ ===")
        print(f"ì´ˆê¸°í™”ëœ í—¤ë“œ: {stats['initialized_heads']}/{stats['total_heads']}")
        print(f"ì´ ìŠ¤ì™‘: {stats['swap_manager_stats']['total_swaps']}")
        print(f"GPU ëª¨ë¸ ìˆ˜: {stats['memory_status']['models_on_gpu']}")
        
    finally:
        await swap_manager.shutdown()

if __name__ == "__main__":
    asyncio.run(example_usage())