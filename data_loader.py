"""
Red Heart AI ë°ì´í„° ë¡œë”
Claude APIë¡œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ ë¡œë“œ
"""

import json
import torch
import numpy as np
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class PreprocessedDataLoader:
    """Claude APIë¡œ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”"""
    
    def __init__(self, data_path: str = "/mnt/c/large_project/linux_red_heart/claude_api_preprocessing/claude_preprocessed_complete.json"):
        """
        Args:
            data_path: ì „ì²˜ë¦¬ëœ JSON íŒŒì¼ ê²½ë¡œ
        """
        self.data_path = Path(data_path)
        self.data = None
        self.embeddings_cache = {}
        
        # ë°ì´í„° ë¡œë“œ
        self._load_data()
    
    def _load_data(self):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
        if not self.data_path.exists():
            raise FileNotFoundError(f"ì „ì²˜ë¦¬ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_path}")
        
        logger.info(f"ðŸ“¥ ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë“œ ì¤‘: {self.data_path}")
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        # ë°ì´í„° êµ¬ì¡° íŒŒì•…
        if isinstance(raw_data, list):
            self.data = raw_data
        elif isinstance(raw_data, dict) and 'samples' in raw_data:
            self.data = raw_data['samples']
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹ìž…ë‹ˆë‹¤")
        
        logger.info(f"âœ… {len(self.data)}ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ")
        
        # ì²« ë²ˆì§¸ ìƒ˜í”Œ êµ¬ì¡° ì¶œë ¥
        if self.data:
            sample = self.data[0]
            logger.info("ðŸ“Š ìƒ˜í”Œ êµ¬ì¡°:")
            for key in sample.keys():
                if isinstance(sample[key], dict):
                    logger.info(f"  - {key}: {list(sample[key].keys())}")
                elif isinstance(sample[key], list):
                    logger.info(f"  - {key}: List[{len(sample[key])}]")
                else:
                    logger.info(f"  - {key}: {type(sample[key]).__name__}")
    
    def get_batch(self, indices: List[int]) -> Dict[str, torch.Tensor]:
        """ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ ë° í…ì„œ ë³€í™˜
        
        Args:
            indices: ìƒ˜í”Œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë°°ì¹˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ (í…ì„œ)
        """
        batch_data = {
            'texts': [],
            'embeddings': [],
            'emotion_labels': [],
            'bentham_scores': [],
            'surd_metrics': [],
            'regret_factors': []
        }
        
        for idx in indices:
            sample = self.data[idx]
            
            # í…ìŠ¤íŠ¸
            if 'text' in sample:
                batch_data['texts'].append(sample['text'])
            elif 'post' in sample:
                batch_data['texts'].append(sample['post'])
            
            # ìž„ë² ë”© (ì—†ìœ¼ë©´ ìƒì„± í•„ìš”)
            if 'embedding' in sample:
                batch_data['embeddings'].append(sample['embedding'])
            else:
                # ìž„ì‹œë¡œ 768ì°¨ì› ì œë¡œ ë²¡í„° (ë‚˜ì¤‘ì— ì‹¤ì œ ì¸ì½”ë” ì‚¬ìš©)
                batch_data['embeddings'].append(np.zeros(768))
            
            # ê°ì • ë¼ë²¨
            if 'emotions' in sample:
                emotions = sample['emotions']
                if isinstance(emotions, dict):
                    # 7ê°œ ê¸°ë³¸ ê°ì • ì¶”ì¶œ
                    emotion_vector = [
                        emotions.get('joy', 0.0),
                        emotions.get('anger', 0.0),
                        emotions.get('surprise', 0.0),
                        emotions.get('disgust', 0.0),
                        emotions.get('sadness', 0.0),
                        emotions.get('shame', 0.0),
                        emotions.get('fear', 0.0)
                    ]
                else:
                    emotion_vector = emotions[:7]
                batch_data['emotion_labels'].append(emotion_vector)
            
            # ë²¤ë‹´ ì ìˆ˜
            if 'bentham_scores' in sample:
                bentham = sample['bentham_scores']
                if isinstance(bentham, dict):
                    bentham_vector = [
                        bentham.get('intensity', 0.5),
                        bentham.get('duration', 0.5),
                        bentham.get('certainty', 0.5),
                        bentham.get('propinquity', 0.5),
                        bentham.get('fecundity', 0.5),
                        bentham.get('purity', 0.5),
                        bentham.get('extent', 0.5)
                    ]
                else:
                    bentham_vector = bentham
                batch_data['bentham_scores'].append(bentham_vector)
            
            # SURD ë©”íŠ¸ë¦­
            if 'surd_metrics' in sample:
                surd = sample['surd_metrics']
                if isinstance(surd, dict):
                    surd_vector = [
                        surd.get('selection', 0.5),
                        surd.get('uncertainty', 0.5),
                        surd.get('risk', 0.5),
                        surd.get('decision', 0.5)
                    ]
                else:
                    surd_vector = surd
                batch_data['surd_metrics'].append(surd_vector)
            
            # í›„íšŒ íŒ©í„°
            if 'regret_factor' in sample:
                batch_data['regret_factors'].append([sample['regret_factor']])
            elif 'regret' in sample:
                batch_data['regret_factors'].append([sample['regret']])
            else:
                batch_data['regret_factors'].append([0.5])
        
        # í…ì„œ ë³€í™˜
        result = {}
        
        # í…ìŠ¤íŠ¸ëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€
        if batch_data['texts']:
            result['texts'] = batch_data['texts']
        
        # ë‚˜ë¨¸ì§€ëŠ” í…ì„œë¡œ ë³€í™˜
        if batch_data['embeddings']:
            result['embeddings'] = torch.tensor(batch_data['embeddings'], dtype=torch.float32)
        
        if batch_data['emotion_labels']:
            result['emotion_labels'] = torch.tensor(batch_data['emotion_labels'], dtype=torch.float32)
        
        if batch_data['bentham_scores']:
            result['bentham_scores'] = torch.tensor(batch_data['bentham_scores'], dtype=torch.float32)
        
        if batch_data['surd_metrics']:
            result['surd_metrics'] = torch.tensor(batch_data['surd_metrics'], dtype=torch.float32)
        
        if batch_data['regret_factors']:
            result['regret_factors'] = torch.tensor(batch_data['regret_factors'], dtype=torch.float32)
        
        return result
    
    def split_data(self, train_ratio: float = 0.8, val_ratio: float = 0.1) -> Tuple[List, List, List]:
        """ë°ì´í„°ì…‹ ë¶„í• 
        
        Args:
            train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
            val_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            
        Returns:
            (train_data, val_data, test_data)
        """
        n = len(self.data)
        n_train = int(n * train_ratio)
        n_val = int(n * val_ratio)
        
        # ì¸ë±ìŠ¤ ì…”í”Œ
        indices = np.random.permutation(n)
        
        train_indices = indices[:n_train]
        val_indices = indices[n_train:n_train+n_val]
        test_indices = indices[n_train+n_val:]
        
        train_data = [self.data[i] for i in train_indices]
        val_data = [self.data[i] for i in val_indices]
        test_data = [self.data[i] for i in test_indices]
        
        logger.info(f"ðŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        logger.info(f"  - í•™ìŠµ: {len(train_data)} ìƒ˜í”Œ")
        logger.info(f"  - ê²€ì¦: {len(val_data)} ìƒ˜í”Œ")
        logger.info(f"  - í…ŒìŠ¤íŠ¸: {len(test_data)} ìƒ˜í”Œ")
        
        return train_data, val_data, test_data
    
    def __len__(self):
        return len(self.data) if self.data else 0
    
    def __getitem__(self, idx):
        return self.data[idx]


class TargetMapper:
    """ë°°ì¹˜ ë°ì´í„°ì—ì„œ íƒ€ê¹ƒ ì¶”ì¶œ"""
    
    @staticmethod
    def extract_emotion_target(batch_data: Dict[str, torch.Tensor]) -> torch.Tensor:
        """ê°ì • íƒ€ê¹ƒ ì¶”ì¶œ
        
        Args:
            batch_data: ë°°ì¹˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            ê°ì • íƒ€ê¹ƒ í…ì„œ (ë¶„ë¥˜: argmax ì¸ë±ìŠ¤ ë˜ëŠ” íšŒê·€: ë²¡í„°)
        """
        if 'emotion_labels' in batch_data:
            # íšŒê·€ íƒœìŠ¤í¬: ì „ì²´ ê°ì • ë²¡í„° ë°˜í™˜
            return batch_data['emotion_labels']
        else:
            # ê¸°ë³¸ê°’: ê· ë“± ë¶„í¬
            batch_size = len(batch_data.get('texts', [1]))
            return torch.ones(batch_size, 7) / 7.0
    
    @staticmethod
    def extract_bentham_target(batch_data: Dict[str, torch.Tensor]) -> torch.Tensor:
        """ë²¤ë‹´ íƒ€ê¹ƒ ì¶”ì¶œ"""
        if 'bentham_scores' in batch_data:
            return batch_data['bentham_scores']
        else:
            batch_size = len(batch_data.get('texts', [1]))
            return torch.ones(batch_size, 7) * 0.5
    
    @staticmethod
    def extract_surd_target(batch_data: Dict[str, torch.Tensor]) -> torch.Tensor:
        """SURD íƒ€ê¹ƒ ì¶”ì¶œ"""
        if 'surd_metrics' in batch_data:
            return batch_data['surd_metrics']
        else:
            batch_size = len(batch_data.get('texts', [1]))
            return torch.ones(batch_size, 4) * 0.5
    
    @staticmethod
    def extract_regret_target(batch_data: Dict[str, torch.Tensor]) -> torch.Tensor:
        """í›„íšŒ íƒ€ê¹ƒ ì¶”ì¶œ"""
        if 'regret_factors' in batch_data:
            return batch_data['regret_factors']
        else:
            batch_size = len(batch_data.get('texts', [1]))
            return torch.ones(batch_size, 1) * 0.5