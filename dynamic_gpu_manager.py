#!/usr/bin/env python3
"""
Dynamic GPU Memory Manager for Red Heart AI System
вЈЎВаЂ GPU вЕћвфевдг Ж┤ђвдгВъљ - вфевЇИв│ё вДъВХцьўЋ вЕћвфевдг ьЋавІ╣

Features:
- вфевЇИв│ё Ж░юв│ё вЕћвфевдг ьЋавІ╣ Ваёвъх
- klue/bert-base вЊ▒ Вўцв▓ёьЌцвЊю ВюёьЌў вфевЇИ ВЋѕВаЋВё▒ в│┤ВъЦ
- вЕћВЮИ ьЋЎВіх ьїїВЮ┤ьћёвЮ╝ВЮИ вЈЎВаЂ ВхюВаЂьЎћ
- ВІцВІюЖ░ё вЕћвфевдг вфевІѕьё░вДЂ в░Ј ВА░ВаЋ
"""

import torch
import gc
import time
import logging
from typing import Dict, Optional, Tuple, List
from dataclasses import dataclass
from contextlib import contextmanager
import threading
from enum import Enum

class ModelRiskLevel(Enum):
    """вфевЇИ Вўцв▓ёьЌцвЊю ВюёьЌўвЈё вХёвЦў"""
    LOW = "low"           # ВЋѕВаёьЋю вфевЇИвЊц
    MEDIUM = "medium"     # ВцЉЖ░ё ВюёьЌўвЈё
    HIGH = "high"         # klue/bert-base вЊ▒ Вўцв▓ёьЌцвЊю ВюёьЌў вфевЇИ

@dataclass
class GPUMemoryProfile:
    """GPU вЕћвфевдг ьћёвАюьЋё"""
    model_name: str
    risk_level: ModelRiskLevel
    base_allocation: float      # ЖИ░в│И ьЋавІ╣вЪЅ (в╣ёВюе)
    max_allocation: float       # Вхювїђ ьЋавІ╣вЪЅ (в╣ёВюе)
    min_allocation: float       # ВхюВєї ьЋавІ╣вЪЅ (в╣ёВюе)
    priority: int              # Вџ░ВёаВѕюВюё (1=ВхюЖ│а)
    stable_mode: bool          # ВЋѕВаЋ вфевЊю ВѓгВџЕ ВЌгвХђ

class DynamicGPUManager:
    """вЈЎВаЂ GPU вЕћвфевдг Ж┤ђвдгВъљ"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger = logging.getLogger(__name__)
        
        # GPU В┤Ю вЕћвфевдг ВаЋв│┤
        if torch.cuda.is_available():
            self.total_memory = torch.cuda.get_device_properties(0).total_memory
            self.total_memory_gb = self.total_memory / (1024**3)
        else:
            self.total_memory = 0
            self.total_memory_gb = 0
        
        # вфевЇИв│ё вЕћвфевдг ьћёвАюьЋё ВёцВаЋ
        self.memory_profiles = self._initialize_memory_profiles()
        
        # ьўёВъг вАювЊювљю вфевЇИ ВХћВаЂ
        self.loaded_models: Dict[str, torch.nn.Module] = {}
        self.model_memory_usage: Dict[str, float] = {}
        
        # вЈЎВаЂ ьЋавІ╣ВЮё ВюёьЋю вЕћвфевдг ьњђ
        self.reserved_memory = 0.1  # 10% ВІюВіцьЁю ВўѕВЋй
        self.available_memory_ratio = 1.0 - self.reserved_memory
        
        # ВіцваѕвЊю ВЋѕВаёВё▒
        self.lock = threading.Lock()
        
        self.logger.info(f"Dynamic GPU Manager initialized - Total GPU Memory: {self.total_memory_gb:.1f}GB")
    
    def _initialize_memory_profiles(self) -> Dict[str, GPUMemoryProfile]:
        """вфевЇИв│ё вЕћвфевдг ьћёвАюьЋё В┤ѕЖИ░ьЎћ"""
        profiles = {
            # Вўцв▓ёьЌцвЊю ВюёьЌў вфевЇИвЊц - ВЋѕВаЋВё▒ Вџ░Вёа
            'klue/bert-base': GPUMemoryProfile(
                model_name='klue/bert-base',
                risk_level=ModelRiskLevel.HIGH,
                base_allocation=0.15,      # 15% Ж│аВаЋ (ьўёВъг ВёцВаЋ ВюаВДђ)
                max_allocation=0.20,       # Вхювїђ 20%
                min_allocation=0.10,       # ВхюВєї 10%
                priority=1,                # ВхюЖ│а Вџ░ВёаВѕюВюё
                stable_mode=True           # ВЋѕВаЋ вфевЊю ьЎюВё▒ьЎћ
            ),
            'beomi/KcELECTRA-base-v2022': GPUMemoryProfile(
                model_name='beomi/KcELECTRA-base-v2022',
                risk_level=ModelRiskLevel.HIGH,
                base_allocation=0.15,
                max_allocation=0.20,
                min_allocation=0.10,
                priority=2,
                stable_mode=True
            ),
            
            # ВцЉЖ░ё ВюёьЌўвЈё вфевЇИвЊц
            'j-hartmann/emotion-english-distilroberta-base': GPUMemoryProfile(
                model_name='j-hartmann/emotion-english-distilroberta-base',
                risk_level=ModelRiskLevel.MEDIUM,
                base_allocation=0.25,
                max_allocation=0.35,
                min_allocation=0.15,
                priority=3,
                stable_mode=False
            ),
            
            # ВЋѕВаёьЋю вфевЇИвЊц - вЈЎВаЂ ВхюВаЂьЎћ ВаЂВџЕ
            'main_learning_pipeline': GPUMemoryProfile(
                model_name='main_learning_pipeline',
                risk_level=ModelRiskLevel.LOW,
                base_allocation=0.40,      # 40% ЖИ░в│И ьЋавІ╣
                max_allocation=0.70,       # Вхювїђ 70% Ж╣їВДђ ьЎЋВъЦ Ж░ђвіЦ
                min_allocation=0.20,       # ВхюВєї 20% в│┤ВъЦ
                priority=4,
                stable_mode=False          # вЈЎВаЂ ВхюВаЂьЎћ ьЎюВё▒ьЎћ
            ),
            'neural_networks': GPUMemoryProfile(
                model_name='neural_networks',
                risk_level=ModelRiskLevel.LOW,
                base_allocation=0.30,
                max_allocation=0.60,
                min_allocation=0.15,
                priority=5,
                stable_mode=False
            ),
            
            # ВЃѕвАю ВХћЖ░ђвљю вфевЊѕвЊц
            'legal_expert_system': GPUMemoryProfile(
                model_name='legal_expert_system',
                risk_level=ModelRiskLevel.LOW,
                base_allocation=0.10,      # 10% ЖИ░в│И ьЋавІ╣ (вѓ┤ВъЦ вфевЇИ ЖИ░в░ў)
                max_allocation=0.15,       # Вхювїђ 15%
                min_allocation=0.05,       # ВхюВєї 5%
                priority=6,
                stable_mode=False          # вЈЎВаЂ ВхюВаЂьЎћ ьЎюВё▒ьЎћ
            ),
            'three_view_scenario_system': GPUMemoryProfile(
                model_name='three_view_scenario_system',
                risk_level=ModelRiskLevel.LOW,
                base_allocation=0.15,      # 15% ЖИ░в│И ьЋавІ╣ (ВІаЖ▓йвДЮ ЖИ░в░ў)
                max_allocation=0.25,       # Вхювїђ 25%
                min_allocation=0.08,       # ВхюВєї 8%
                priority=7,
                stable_mode=False          # вЈЎВаЂ ВхюВаЂьЎћ ьЎюВё▒ьЎћ
            ),
            'phase_controller_hook': GPUMemoryProfile(
                model_name='phase_controller_hook',
                risk_level=ModelRiskLevel.LOW,
                base_allocation=0.05,      # 5% ЖИ░в│И ьЋавІ╣ (Вё▒віЦ вфевІѕьё░вДЂ)
                max_allocation=0.10,       # Вхювїђ 10%
                min_allocation=0.02,       # ВхюВєї 2%
                priority=8,
                stable_mode=False          # вЈЎВаЂ ВхюВаЂьЎћ ьЎюВё▒ьЎћ
            )
        }
        return profiles
    
    @contextmanager
    def allocate_memory(self, model_name: str, dynamic_boost: bool = False):
        """
        вфевЇИв│ё вЕћвфевдг ьЋавІ╣ В╗еьЁЇВіцьіИ вДцвІѕВађ
        
        Args:
            model_name: вфевЇИ ВЮ┤вдё
            dynamic_boost: вЈЎВаЂ вХђВіцьіИ ьЎюВё▒ьЎћ ВЌгвХђ
        """
        profile = self.memory_profiles.get(model_name)
        if not profile:
            # ВЋївацВДђВДђ ВЋіВЮђ вфевЇИВЮђ ВцЉЖ░ё ВюёьЌўвЈёвАю В▓ўвдг
            profile = GPUMemoryProfile(
                model_name=model_name,
                risk_level=ModelRiskLevel.MEDIUM,
                base_allocation=0.25,
                max_allocation=0.35,
                min_allocation=0.15,
                priority=10,
                stable_mode=True
            )
        
        with self.lock:
            try:
                # вЕћвфевдг ьЋавІ╣
                allocated_memory = self._allocate_model_memory(profile, dynamic_boost)
                self.model_memory_usage[model_name] = allocated_memory
                
                self.logger.info(f"­ЪћД {model_name} вЕћвфевдг ьЋавІ╣: {allocated_memory:.1%} ({allocated_memory * self.total_memory_gb:.1f}GB)")
                
                yield allocated_memory
                
            finally:
                # вЕћвфевдг ьЋ┤Ваю
                self._release_model_memory(model_name)
                self.logger.info(f"­ЪЌЉ№ИЈ {model_name} вЕћвфевдг ьЋ┤Ваю ВЎёвБї")
    
    def _allocate_model_memory(self, profile: GPUMemoryProfile, dynamic_boost: bool) -> float:
        """ВІцВаю вЕћвфевдг ьЋавІ╣ вАюВДЂ"""
        if not torch.cuda.is_available():
            return 0.0
        
        # ьўёВъг вЕћвфевдг ВЃЂьЃю ьЎЋВЮИ
        current_allocated = torch.cuda.memory_allocated() / self.total_memory
        current_reserved = torch.cuda.memory_reserved() / self.total_memory
        available_ratio = self.available_memory_ratio - current_reserved
        
        # ьЋавІ╣вЪЅ Ж▓░ВаЋ - вдгв»ИьіИ ВаЉЖи╝ вгИВаю ьЋ┤Ж▓░
        if profile.stable_mode:
            # ВЋѕВаЋ вфевЊю: Ж│аВаЋ ьЋавІ╣
            target_allocation = profile.base_allocation
            self.logger.info(f"­Ъћњ {profile.model_name}: ВЋѕВаЋ вфевЊю - Ж│аВаЋ ьЋавІ╣ {target_allocation:.1%}")
        else:
            # вЈЎВаЂ вфевЊю: ВЃЂьЎЕВЌљ вћ░вЮ╝ ВА░ВаЋ (вЇћ в│┤ВѕўВаЂ ВаЉЖи╝)
            if dynamic_boost and available_ratio > 0.6:  # 0.5 Рєњ 0.6Вю╝вАю ВЃЂьќЦ
                # ВЌгВюа вЕћвфевдгЖ░ђ ВХЕвХёьЋўвЕ┤ ьЎЋВъЦ ьЋавІ╣ (вЇћ в│┤ВѕўВаЂ)
                target_allocation = min(profile.max_allocation, available_ratio * 0.7)  # 0.8 Рєњ 0.7вАю Ж░љВєї
                self.logger.info(f"РџА {profile.model_name}: вЈЎВаЂ вХђВіцьіИ - ьЎЋВъЦ ьЋавІ╣ {target_allocation:.1%}")
            elif available_ratio > 0.3:  # ВхюВєї ВЌгВюавХё В▓┤ьЂг ВХћЖ░ђ
                # ЖИ░в│И ьЋавІ╣ (вЇћ в│┤ВѕўВаЂ)
                target_allocation = min(profile.base_allocation, available_ratio * 0.5)  # 0.6 Рєњ 0.5вАю Ж░љВєї
                self.logger.info(f"­ЪћД {profile.model_name}: ЖИ░в│И ьЋавІ╣ {target_allocation:.1%}")
            else:
                # вЕћвфевдг вХђВА▒ ВІю ВхюВєї ьЋавІ╣
                target_allocation = min(profile.min_allocation, available_ratio * 0.8)
                self.logger.warning(f"Рџа№ИЈ {profile.model_name}: вЕћвфевдг вХђВА▒ - ВхюВєї ьЋавІ╣ {target_allocation:.1%}")
        
        # ВхюВєї/Вхювїђ ВаюьЋю ВаЂВџЕ
        target_allocation = max(profile.min_allocation, target_allocation)
        target_allocation = min(profile.max_allocation, target_allocation)
        
        # PyTorch вЕћвфевдг ВёцВаЋ
        try:
            if target_allocation > 0:
                # вЕћвфевдг В║љВІю ВаЋвдг
                torch.cuda.empty_cache()
                gc.collect()
                
                # вЕћвфевдг ьЋавІ╣вЪЅ ВёцВаЋ (ВІцВаю ьЋавІ╣ВЮђ вфевЇИ вАювћЕ ВІю)
                self.logger.info(f"­Ъј» ВхюВбЁ ьЋавІ╣: {target_allocation:.1%} ({target_allocation * self.total_memory_gb:.1f}GB)")
                
        except Exception as e:
            self.logger.warning(f"вЕћвфевдг ьЋавІ╣ ВцЉ ВўцвЦў: {e}")
            # ВЋѕВаёьЋю ЖИ░в│ИЖ░њВю╝вАю fallback
            target_allocation = profile.min_allocation
        
        return target_allocation
    
    def _release_model_memory(self, model_name: str):
        """вфевЇИ вЕћвфевдг ьЋ┤Ваю"""
        if model_name in self.model_memory_usage:
            del self.model_memory_usage[model_name]
        
        if model_name in self.loaded_models:
            del self.loaded_models[model_name]
        
        # GPU вЕћвфевдг ВаЋвдг
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
    
    def get_memory_status(self) -> Dict[str, float]:
        """ьўёВъг GPU вЕћвфевдг ВЃЂьЃю в░ўьЎў"""
        if not torch.cuda.is_available():
            return {"error": "CUDA not available"}
        
        allocated_bytes = torch.cuda.memory_allocated()
        reserved_bytes = torch.cuda.memory_reserved()
        total_bytes = self.total_memory
        
        return {
            "total_gb": total_bytes / (1024**3),
            "allocated_gb": allocated_bytes / (1024**3),
            "reserved_gb": reserved_bytes / (1024**3),
            "available_gb": (total_bytes - reserved_bytes) / (1024**3),
            "utilization_percent": (allocated_bytes / total_bytes) * 100,
            "active_models": list(self.model_memory_usage.keys()),
            "model_allocations": self.model_memory_usage.copy()
        }
    
    def optimize_for_learning(self) -> bool:
        """ьЋЎВіх ВхюВаЂьЎћ вфевЊю ьЎюВё▒ьЎћ"""
        try:
            # вЕћвфевдг ВаЋвдг
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
            
            # ьЋЎВіх ВаёВџЕ вЕћвфевдг ьњђ ВёцВаЋ
            memory_status = self.get_memory_status()
            available_memory = memory_status.get("available_gb", 0)
            
            if available_memory > 4.0:  # 4GB ВЮ┤ВЃЂ ВЌгВюа
                self.logger.info(f"­Ъџђ ьЋЎВіх ВхюВаЂьЎћ вфевЊю ьЎюВё▒ьЎћ - ВѓгВџЕ Ж░ђвіЦ вЕћвфевдг: {available_memory:.1f}GB")
                return True
            else:
                self.logger.warning(f"Рџа№ИЈ вЕћвфевдг вХђВА▒Вю╝вАю ьЋЎВіх ВхюВаЂьЎћ ВаюьЋю - ВѓгВџЕ Ж░ђвіЦ: {available_memory:.1f}GB")
                return False
                
        except Exception as e:
            self.logger.error(f"ьЋЎВіх ВхюВаЂьЎћ ВцЉ ВўцвЦў: {e}")
            return False
    
    def emergency_cleanup(self):
        """в╣ёВЃЂ вЕћвфевдг ВаЋвдг"""
        self.logger.warning("­Ъџе в╣ёВЃЂ вЕћвфевдг ВаЋвдг ВІцьќЅ")
        
        # вфевЊа вфевЇИ ьЋ┤Ваю
        for model_name in list(self.loaded_models.keys()):
            self._release_model_memory(model_name)
        
        # Ж░ЋВаю вЕћвфевдг ВаЋвдг
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        gc.collect()
        self.logger.info("РюЁ в╣ёВЃЂ вЕћвфевдг ВаЋвдг ВЎёвБї")

# ВаёВЌГ GPU Ж┤ђвдгВъљ ВЮИВіцьё┤Віц
_gpu_manager = None

def get_gpu_manager() -> DynamicGPUManager:
    """ВаёВЌГ GPU Ж┤ђвдгВъљ ВЮИВіцьё┤Віц в░ўьЎў"""
    global _gpu_manager
    if _gpu_manager is None:
        _gpu_manager = DynamicGPUManager()
    return _gpu_manager

# ьјИВЮў ьЋеВѕўвЊц
def allocate_gpu_memory(model_name: str, dynamic_boost: bool = False):
    """GPU вЕћвфевдг ьЋавІ╣ (В╗еьЁЇВіцьіИ вДцвІѕВађ)"""
    return get_gpu_manager().allocate_memory(model_name, dynamic_boost)

def get_gpu_status() -> Dict[str, float]:
    """ьўёВъг GPU ВЃЂьЃю ВА░ьџї"""
    return get_gpu_manager().get_memory_status()

def optimize_gpu_for_learning() -> bool:
    """ьЋЎВіхВџЕ GPU ВхюВаЂьЎћ"""
    return get_gpu_manager().optimize_for_learning()

def emergency_gpu_cleanup():
    """в╣ёВЃЂ GPU вЕћвфевдг ВаЋвдг"""
    get_gpu_manager().emergency_cleanup()