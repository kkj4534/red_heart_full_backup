#!/usr/bin/env python3
"""
Red Heart AI ìµœì¢… í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ
730M íŒŒë¼ë¯¸í„° ëª¨ë¸ì˜ 60 ì—í­ í•™ìŠµ with Advanced Techniques
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import numpy as np
import logging
import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import gc
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸
from training.enhanced_checkpoint_manager import EnhancedCheckpointManager
from training.lr_sweep_optimizer import LRSweepOptimizer
from training.sweet_spot_detector import SweetSpotDetector
from training.parameter_crossover_system import ParameterCrossoverSystem
from training.oom_handler import OOMHandler
from training.advanced_training_techniques import AdvancedTrainingManager

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from config import ADVANCED_CONFIG, get_device
from data_loader import PreprocessedDataLoader
from sentence_transformer_singleton import get_sentence_transformer

# ì‹¤ì œ ëª¨ë¸ ëª¨ë“ˆë“¤
from unified_backbone import RedHeartUnifiedBackbone
from unified_heads import EmotionHead, BenthamHead, RegretHead, SURDHead
from analyzer_neural_modules import create_neural_analyzers
from advanced_analyzer_wrappers import create_advanced_analyzer_wrappers
from phase_neural_networks import Phase0ProjectionNet, Phase2CommunityNet, HierarchicalEmotionIntegrator
try:
    from emotion_dsp_simulator import EmotionDSPSimulator, DynamicKalmanFilter
except ImportError:
    EmotionDSPSimulator = None
    DynamicKalmanFilter = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('RedHeart.UnifiedTrainingFinal')


class UnifiedTrainingConfig:
    """í†µí•© í•™ìŠµ ì„¤ì •"""
    
    def __init__(self):
        # ëª¨ë¸ ì„¤ì • (730M íŒŒë¼ë¯¸í„°)
        self.model_params = 730_000_000
        self.hidden_dim = 1280
        self.num_layers = 18
        self.num_heads = 20
        
        # í•™ìŠµ ì„¤ì •
        self.total_epochs = 60
        self.micro_batch_size = 2  # ì•ˆì •ì„±ì„ ìœ„í•´ 2ë¡œ ì‹œì‘
        self.gradient_accumulation = 32  # ìœ íš¨ ë°°ì¹˜ = 64
        self.base_lr = 1e-4
        
        # LR ìŠ¤ìœ• ì„¤ì • (ë…ë¦½ ì‹¤í–‰)
        self.lr_sweep_enabled = False  # ë³¸ í•™ìŠµì—ì„œëŠ” ë¹„í™œì„±í™”
        self.lr_sweep_range = (1e-5, 1e-2)
        self.lr_sweep_points = 5
        
        # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
        self.checkpoint_interval = 2  # ì§ìˆ˜ ì—í­ë§ˆë‹¤ ì €ì¥ (30ê°œ)
        self.checkpoint_dir = "training/checkpoints_final"
        
        # Advanced Training
        self.enable_label_smoothing = True
        self.enable_rdrop = True
        self.enable_ema = True
        self.enable_llrd = True
        self.label_smoothing = 0.1
        self.rdrop_alpha = 1.0
        self.ema_decay = 0.999
        
        # OOM í•¸ë“¤ë§
        self.enable_oom_handler = True
        self.memory_threshold = 0.85
        self.min_batch_size = 1
        
        # Sweet Spot & Crossover
        self.enable_sweet_spot = True
        self.enable_crossover = True
        self.crossover_strategy = 'selective'
        
        # ë°ì´í„° ì„¤ì •
        self.data_dir = "for_learn_dataset"
        self.validation_split = 0.1
        self.num_workers = 4
        
        # ë¡œê¹…
        self.log_interval = 10
        self.verbose = False  # ìƒì„¸ ì¶œë ¥ ì„¤ì •
        self.val_interval = 100


class UnifiedModel(nn.Module):
    """Red Heart AI 730M í†µí•© ëª¨ë¸"""
    
    def __init__(self, config: UnifiedTrainingConfig, device=None):
        super().__init__()
        self.config = config
        self.device = device if device else torch.device('cpu')
        
        # ë°±ë³¸ ì„¤ì •
        backbone_config = {
            'input_dim': 768,
            'd_model': 896,
            'num_layers': 8,
            'num_heads': 14,
            'feedforward_dim': 3584,
            'dropout': 0.1,
            'task_dim': 896
        }
        
        # ë°±ë³¸ ì´ˆê¸°í™” (90.6M)
        self.backbone = RedHeartUnifiedBackbone(backbone_config)
        
        # í—¤ë“œ ì´ˆê¸°í™” (153M)
        self.emotion_head = EmotionHead(input_dim=896)
        self.bentham_head = BenthamHead(input_dim=896)
        self.regret_head = RegretHead(input_dim=896) 
        self.surd_head = SURDHead(input_dim=896)
        
        # ì‹ ê²½ë§ ë¶„ì„ê¸° (368M) - nn.ModuleDictë¡œ ê°ì‹¸ì„œ parameters()ì— í¬í•¨ë˜ë„ë¡
        analyzers_dict = create_neural_analyzers(input_dim=896)
        self.neural_analyzers = nn.ModuleDict(analyzers_dict)
        # ê° analyzerë¥¼ deviceë¡œ ì´ë™
        if self.device and self.device != torch.device('cpu'):
            self.neural_analyzers = self.neural_analyzers.to(self.device)
        
        # Advanced ë¶„ì„ê¸° ë˜í¼ (112M) - translator ì´ˆê¸°í™” í›„ ìƒì„±
        self.advanced_wrappers = None  # ë‚˜ì¤‘ì— ì´ˆê¸°í™”
        
        # Phase ë„¤íŠ¸ì›Œí¬ (4.3M)
        self.phase0_net = Phase0ProjectionNet()
        self.phase2_net = Phase2CommunityNet()
        self.hierarchical_integrator = HierarchicalEmotionIntegrator()
        
        # DSP & ì¹¼ë§Œ í•„í„° (2.3M)
        if EmotionDSPSimulator is not None:
            self.dsp_simulator = EmotionDSPSimulator({'hidden_dim': 384})
            self.kalman_filter = DynamicKalmanFilter(state_dim=7)
        else:
            self.dsp_simulator = None
            self.kalman_filter = None
        
    def forward(self, x, task='emotion', return_all=False):
        """ìˆœì „íŒŒ - ëª¨ë“  ëª¨ë“ˆ ì‚¬ìš© (730M ì „ì²´)
        
        Args:
            x: ì…ë ¥ í…ì„œ
            task: í˜„ì¬ í•™ìŠµ ì¤‘ì¸ íƒœìŠ¤í¬
            return_all: ëª¨ë“  ì¶œë ¥ ë°˜í™˜ ì—¬ë¶€ (í•™ìŠµ ì‹œ True)
            
        Returns:
            return_all=False: í•´ë‹¹ íƒœìŠ¤í¬ì˜ ì¶œë ¥ í…ì„œ
            return_all=True: dict with 'head_output', 'neural_output', 'wrapper_output'
        """
        # ì…ë ¥ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (í•„ìš”ì‹œ)
        if x.device != self.backbone.parameters().__next__().device:
            x = x.to(self.backbone.parameters().__next__().device)
        
        # ë°±ë³¸ ì²˜ë¦¬ (90.6M)
        backbone_outputs = self.backbone(x, task=task)
        
        # íƒœìŠ¤í¬ë³„ íŠ¹ì§• ì¶”ì¶œ
        if task in backbone_outputs:
            features = backbone_outputs[task]
        else:
            features = torch.stack(list(backbone_outputs.values())).mean(dim=0)
        
        outputs = {}
        
        # 1. í—¤ë“œ ì¶œë ¥ (153M)
        if task == 'emotion':
            head_output = self.emotion_head(features)
            if isinstance(head_output, dict):
                head_output = head_output.get('emotions', head_output.get('emotion_logits', list(head_output.values())[0]))
        elif task == 'bentham':
            head_output = self.bentham_head(features)
            if isinstance(head_output, dict):
                head_output = head_output.get('bentham_scores', list(head_output.values())[0])
        elif task == 'regret':
            head_output = self.regret_head(features)
            if isinstance(head_output, dict):
                head_output = head_output.get('regret_score', list(head_output.values())[0])
        elif task == 'surd':
            head_output = self.surd_head(features)
            if isinstance(head_output, dict):
                head_output = head_output.get('surd_values', output.get('surd_scores', list(head_output.values())[0]))
        else:
            head_output = self.emotion_head(features)
            if isinstance(head_output, dict):
                head_output = head_output.get('emotions', head_output.get('emotion_logits', list(head_output.values())[0]))
        
        outputs['head'] = head_output
        
        # 2. Neural Analyzers ì¶œë ¥ (368.2M)
        if self.neural_analyzers and task in self.neural_analyzers:
            neural_output = self.neural_analyzers[task](features)
            if isinstance(neural_output, dict):
                # dictë©´ ì²« ë²ˆì§¸ í…ì„œ ì¶”ì¶œ
                neural_output = list(neural_output.values())[0] if neural_output else features
            outputs['neural'] = neural_output
        else:
            outputs['neural'] = features  # fallback
        
        # 3. Advanced Wrappers ì¶œë ¥ (112M) - ì´ˆê¸°í™”ëœ ê²½ìš°ë§Œ
        if self.advanced_wrappers and task in self.advanced_wrappers:
            wrapper_output = self.advanced_wrappers[task](features)
            if isinstance(wrapper_output, dict):
                wrapper_output = list(wrapper_output.values())[0] if wrapper_output else features
            outputs['wrapper'] = wrapper_output
        else:
            outputs['wrapper'] = features  # fallback
        
        # 4. Phase Networks (4.3M)
        if hasattr(self, 'phase0_net') and self.phase0_net:
            phase0_out = self.phase0_net(features)
            outputs['phase0'] = phase0_out
        
        # 5. DSP & Kalman (2.3M)
        if hasattr(self, 'dsp_simulator') and self.dsp_simulator and task == 'emotion':
            # DSPëŠ” emotion íƒœìŠ¤í¬ì—ì„œë§Œ ì‚¬ìš©
            dsp_out = self.dsp_simulator.process(head_output)
            outputs['dsp'] = dsp_out
        
        # return_allì´ë©´ ëª¨ë“  ì¶œë ¥ ë°˜í™˜ (í•™ìŠµ ì‹œ ì‚¬ìš©)
        if return_all:
            return outputs
        else:
            # ê¸°ë³¸: head ì¶œë ¥ë§Œ ë°˜í™˜
            return head_output


class UnifiedTrainer:
    """í†µí•© í•™ìŠµ ê´€ë¦¬ì"""
    
    def __init__(self, config: UnifiedTrainingConfig):
        self.config = config
        self.device = get_device()
        self.verbose = config.verbose  # V2ì™€ ë™ì¼í•˜ê²Œ verbose ì„¤ì •
        
        logger.info("=" * 70)
        logger.info("Red Heart AI ìµœì¢… í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info(f"  - ëª¨ë¸ í¬ê¸°: {config.model_params/1e6:.0f}M íŒŒë¼ë¯¸í„°")
        logger.info(f"  - ì´ ì—í­: {config.total_epochs}")
        logger.info(f"  - ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {config.micro_batch_size} (GA={config.gradient_accumulation})")
        logger.info(f"  - ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info("=" * 70)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_model()
        
        # ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
        self._initialize_dataloaders()
        
        # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
        self._initialize_optimizer()
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.global_step = 0
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.no_param_update = False  # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í”Œë˜ê·¸
        
    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì €
        self.checkpoint_manager = EnhancedCheckpointManager(
            checkpoint_dir=self.config.checkpoint_dir,
            max_checkpoints=30,
            save_interval=self.config.checkpoint_interval
        )
        
        # LR ìŠ¤ìœ• ì˜µí‹°ë§ˆì´ì €
        if self.config.lr_sweep_enabled:
            self.lr_sweep = LRSweepOptimizer(
                base_lr=self.config.base_lr,
                sweep_range=self.config.lr_sweep_range,
                num_sweep_points=self.config.lr_sweep_points
            )
        
        # Sweet Spot íƒì§€ê¸°
        if self.config.enable_sweet_spot:
            self.sweet_spot_detector = SweetSpotDetector(
                window_size=5,
                stability_threshold=0.01,
                patience=10
            )
        
        # Parameter Crossover
        if self.config.enable_crossover:
            self.crossover_system = ParameterCrossoverSystem(
                crossover_strategy=self.config.crossover_strategy,
                blend_ratio=0.7
            )
        
        # OOM í•¸ë“¤ëŸ¬
        if self.config.enable_oom_handler:
            self.oom_handler = OOMHandler(
                initial_batch_size=self.config.micro_batch_size,
                min_batch_size=self.config.min_batch_size,
                gradient_accumulation=self.config.gradient_accumulation,
                memory_threshold=self.config.memory_threshold
            )
        
        # Advanced Training Manager
        self.training_manager = AdvancedTrainingManager(
            enable_label_smoothing=self.config.enable_label_smoothing,
            enable_rdrop=self.config.enable_rdrop,
            enable_ema=self.config.enable_ema,
            enable_llrd=self.config.enable_llrd,
            label_smoothing=self.config.label_smoothing,
            rdrop_alpha=self.config.rdrop_alpha,
            ema_decay=self.config.ema_decay
        )
        
        logger.info("âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™” - v2 ë°©ì‹ ì°¨ìš© (ìˆœì°¨ì  GPU ë¡œë“œ)"""
        logger.info("ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘ (ìˆœì°¨ì  GPU ë¡œë“œ ë°©ì‹)...")
        
        # ì‹¤ì œ 730M ëª¨ë¸ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤ ì „ë‹¬)
        self.model = UnifiedModel(self.config, device=self.device)
        
        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        if self.device.type == 'cuda':
            gpu_mem_before = torch.cuda.memory_allocated() / (1024**3)
            logger.info(f"  ì´ˆê¸° GPU ë©”ëª¨ë¦¬: {gpu_mem_before:.2f}GB")
        
        # ìˆœì°¨ì ìœ¼ë¡œ ì»´í¬ë„ŒíŠ¸ë¥¼ GPUë¡œ ì´ë™ (7GBê¹Œì§€ í™œìš©)
        # 1. ë°±ë³¸ (90.6M) - í•­ìƒ GPU
        self.model.backbone = self.model.backbone.to(self.device)
        logger.info(f"  âœ… ë°±ë³¸ GPU ë¡œë“œ (90.6M)")
        
        # 2. ëª¨ë“  í—¤ë“œ (153M) - GPUì— ìœ ì§€ (ì´ì „ì—” í•„ìš”ì‹œë§Œ ë¡œë“œ)
        self.model.emotion_head = self.model.emotion_head.to(self.device)
        logger.info(f"  âœ… ê°ì • í—¤ë“œ GPU ë¡œë“œ (38.3M)")
        
        self.model.bentham_head = self.model.bentham_head.to(self.device)
        logger.info(f"  âœ… ë²¤ë‹´ í—¤ë“œ GPU ë¡œë“œ (38.3M)")
        
        self.model.regret_head = self.model.regret_head.to(self.device)
        logger.info(f"  âœ… í›„íšŒ í—¤ë“œ GPU ë¡œë“œ (38.3M)")
        
        self.model.surd_head = self.model.surd_head.to(self.device)
        logger.info(f"  âœ… SURD í—¤ë“œ GPU ë¡œë“œ (38.3M)")
        
        # 3. Translator ëª¨ë“ˆ ì´ˆê¸°í™” (Advanced ë¶„ì„ê¸° ì˜ì¡´ì„±)
        logger.info("  ğŸ”„ Translator ëª¨ë“ˆ ì´ˆê¸°í™” ì¤‘...")
        try:
            from config import get_system_module, register_system_module
            existing_translator = get_system_module('translator')
            if existing_translator is None:
                from local_translator import LocalTranslator
                translator = LocalTranslator()
                register_system_module('translator', translator)
                logger.info("  âœ… LocalTranslator ì´ˆê¸°í™” ë° ì „ì—­ ë“±ë¡ ì™„ë£Œ")
            else:
                logger.info("  â„¹ï¸ Translatorê°€ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        except Exception as e:
            logger.error(f"  âŒ Translator ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.warning("     Advanced Emotion Wrapperê°€ ì œí•œë©ë‹ˆë‹¤")
        
        # Translator ì´ˆê¸°í™” í›„ Advanced Wrappers ìƒì„±
        if self.model.advanced_wrappers is None:
            logger.info("  ğŸ”§ Advanced Wrappers ìƒì„± ì¤‘...")
            from advanced_analyzer_wrappers import create_advanced_analyzer_wrappers
            wrappers_dict = create_advanced_analyzer_wrappers()
            # nn.ModuleDictë¡œ ê°ì‹¸ì„œ parameters()ì— í¬í•¨ë˜ë„ë¡
            self.model.advanced_wrappers = nn.ModuleDict(wrappers_dict) if wrappers_dict else None
            
            # Advanced Wrappers íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
            wrapper_params = 0
            if self.model.advanced_wrappers:
                for name, wrapper in self.model.advanced_wrappers.items():
                    if hasattr(wrapper, 'parameters'):
                        params = sum(p.numel() for p in wrapper.parameters())
                        wrapper_params += params
            logger.info(f"  âœ… Advanced Wrappers ìƒì„± ì™„ë£Œ ({wrapper_params/1e6:.1f}M)")
        
        # 4. Neural Analyzers (368M) - GPU ì—¬ìœ  ìˆìœ¼ë©´ ë¡œë“œ  
        if hasattr(self.model, 'neural_analyzers') and self.model.neural_analyzers:
            try:
                for name, analyzer in self.model.neural_analyzers.items():
                    self.model.neural_analyzers[name] = analyzer.to(self.device)
                    logger.info(f"  âœ… {name} ë¶„ì„ê¸° GPU ë¡œë“œ")
            except RuntimeError as e:
                if 'out of memory' in str(e).lower():
                    logger.warning(f"  âš ï¸ Neural AnalyzersëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ CPU ìœ ì§€")
                else:
                    raise
        
        # 5. Advanced Wrappers (112M) - GPU ì—¬ìœ  ìˆìœ¼ë©´ ë¡œë“œ
        if hasattr(self.model, 'advanced_wrappers') and self.model.advanced_wrappers:
            try:
                for name, wrapper in self.model.advanced_wrappers.items():
                    self.model.advanced_wrappers[name] = wrapper.to(self.device)
                    logger.info(f"  âœ… {name} Wrapper GPU ë¡œë“œ")
            except RuntimeError as e:
                if 'out of memory' in str(e).lower():
                    logger.warning(f"  âš ï¸ Advanced WrappersëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ CPU ìœ ì§€")
                else:
                    raise
        
        # 6. Phase Networks (4.3M) - ì‘ìœ¼ë‹ˆê¹Œ GPUë¡œ
        if hasattr(self.model, 'phase0_net') and self.model.phase0_net:
            self.model.phase0_net = self.model.phase0_net.to(self.device)
            logger.info(f"  âœ… Phase0 ë„¤íŠ¸ì›Œí¬ GPU ë¡œë“œ")
        
        if hasattr(self.model, 'phase2_net') and self.model.phase2_net:
            self.model.phase2_net = self.model.phase2_net.to(self.device)
            logger.info(f"  âœ… Phase2 ë„¤íŠ¸ì›Œí¬ GPU ë¡œë“œ")
        
        if hasattr(self.model, 'hierarchical_integrator') and self.model.hierarchical_integrator:
            self.model.hierarchical_integrator = self.model.hierarchical_integrator.to(self.device)
            logger.info(f"  âœ… Hierarchical Integrator GPU ë¡œë“œ")
        
        # 7. DSP & Kalman (2.3M) - ì‘ìœ¼ë‹ˆê¹Œ GPUë¡œ
        if hasattr(self.model, 'dsp_simulator') and self.model.dsp_simulator:
            self.model.dsp_simulator = self.model.dsp_simulator.to(self.device)
            logger.info(f"  âœ… DSP Simulator GPU ë¡œë“œ")
        
        if hasattr(self.model, 'kalman_filter') and self.model.kalman_filter:
            self.model.kalman_filter = self.model.kalman_filter.to(self.device)
            logger.info(f"  âœ… Kalman Filter GPU ë¡œë“œ")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if self.device.type == 'cuda':
            gpu_mem_after = torch.cuda.memory_allocated() / (1024**3)
            logger.info(f"  ìµœì¢… GPU ë©”ëª¨ë¦¬: {gpu_mem_after:.2f}GB (ì¦ê°€: {gpu_mem_after - gpu_mem_before:.2f}GB)")
            
            # ì „ì²´ GPU ë©”ëª¨ë¦¬ ì •ë³´
            total_gpu = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            logger.info(f"  GPU ì‚¬ìš©ë¥ : {gpu_mem_after/total_gpu*100:.1f}% / {total_gpu:.1f}GB")
        
        # Advanced Training ì´ˆê¸°í™”
        self.training_manager.initialize(
            model=self.model,
            num_classes=6,
            base_lr=self.config.base_lr
        )
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸ (v2ì²˜ëŸ¼ ê° ì»´í¬ë„ŒíŠ¸ë³„ë¡œ ê³„ì‚°)
        total_params = 0
        
        # ë°±ë³¸
        backbone_params = sum(p.numel() for p in self.model.backbone.parameters())
        total_params += backbone_params
        logger.info(f"  ë°±ë³¸: {backbone_params/1e6:.1f}M")
        
        # í—¤ë“œë“¤
        head_params = 0
        for name in ['emotion_head', 'bentham_head', 'regret_head', 'surd_head']:
            if hasattr(self.model, name):
                head = getattr(self.model, name)
                params = sum(p.numel() for p in head.parameters())
                head_params += params
                logger.info(f"  {name}: {params/1e6:.1f}M")
        total_params += head_params
        
        # Neural Analyzers
        if hasattr(self.model, 'neural_analyzers') and self.model.neural_analyzers:
            analyzer_params = 0
            for name, analyzer in self.model.neural_analyzers.items():
                params = sum(p.numel() for p in analyzer.parameters())
                analyzer_params += params
            total_params += analyzer_params
            logger.info(f"  Neural Analyzers: {analyzer_params/1e6:.1f}M")
        
        # Advanced Wrappers
        if hasattr(self.model, 'advanced_wrappers') and self.model.advanced_wrappers:
            wrapper_params = 0
            for name, wrapper in self.model.advanced_wrappers.items():
                if hasattr(wrapper, 'parameters'):
                    params = sum(p.numel() for p in wrapper.parameters())
                    wrapper_params += params
            total_params += wrapper_params
            logger.info(f"  Advanced Wrappers: {wrapper_params/1e6:.1f}M")
        
        # Phase Networks
        phase_params = 0
        for name in ['phase0_net', 'phase2_net', 'hierarchical_integrator']:
            if hasattr(self.model, name) and getattr(self.model, name) is not None:
                net = getattr(self.model, name)
                params = sum(p.numel() for p in net.parameters())
                phase_params += params
        if phase_params > 0:
            total_params += phase_params
            logger.info(f"  Phase Networks: {phase_params/1e6:.1f}M")
        
        # DSP & Kalman
        dsp_kalman_params = 0
        if hasattr(self.model, 'dsp_simulator') and self.model.dsp_simulator:
            dsp_kalman_params += sum(p.numel() for p in self.model.dsp_simulator.parameters())
        if hasattr(self.model, 'kalman_filter') and self.model.kalman_filter:
            dsp_kalman_params += sum(p.numel() for p in self.model.kalman_filter.parameters())
        if dsp_kalman_params > 0:
            total_params += dsp_kalman_params
            logger.info(f"  DSP & Kalman: {dsp_kalman_params/1e6:.1f}M")
        
        logger.info(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: ì´ {total_params/1e6:.1f}M íŒŒë¼ë¯¸í„°")
        
        # 730M íƒ€ê²Ÿ í™•ì¸
        target_params = 730e6
        if abs(total_params - target_params) > 10e6:  # 10M ì´ìƒ ì°¨ì´ë‚˜ë©´ ê²½ê³ 
            logger.warning(f"âš ï¸ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ë¶ˆì¼ì¹˜!")
            logger.warning(f"   ëª©í‘œ: {target_params/1e6:.1f}M")
            logger.warning(f"   ì‹¤ì œ: {total_params/1e6:.1f}M")
            logger.warning(f"   ì°¨ì´: {(total_params - target_params)/1e6:.1f}M")
            
            # ìƒì„¸ ë¶„ì„
            logger.warning("ğŸ“Š ëª¨ë“ˆë³„ íŒŒë¼ë¯¸í„° ë¶„ì„:")
            all_params_dict = {}
            for name, module in self.model.named_children():
                if hasattr(module, 'parameters'):
                    params = sum(p.numel() for p in module.parameters())
                    if params > 0:
                        all_params_dict[name] = params/1e6
                        logger.warning(f"   - {name}: {params/1e6:.1f}M")
            
            # íŒŒë¼ë¯¸í„°ê°€ í•™ìŠµì— ì°¸ì—¬í•˜ëŠ”ì§€ í™•ì¸
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            logger.warning(f"   í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params/1e6:.1f}M")
            
            # íŒŒë¼ë¯¸í„°ê°€ optimizerì— ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸
            optimizer_params = sum(p.numel() for group in self.optimizer.param_groups for p in group['params'])
            logger.warning(f"   Optimizer íŒŒë¼ë¯¸í„°: {optimizer_params/1e6:.1f}M")
        else:
            logger.info(f"âœ… ëª©í‘œ íŒŒë¼ë¯¸í„° ìˆ˜ ë‹¬ì„±: {total_params/1e6:.1f}M â‰ˆ 730M")
    
    def _initialize_dataloaders(self):
        """ë°ì´í„° ë¡œë” ì´ˆê¸°í™”"""
        # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
        preprocessed_path = Path("claude_api_preprocessing/claude_preprocessed_complete.json")
        
        if not preprocessed_path.exists():
            # ëŒ€ì²´ ê²½ë¡œ ì‹œë„
            preprocessed_path = Path("for_learn_dataset/claude_preprocessed_complete.json")
            if not preprocessed_path.exists():
                logger.error(f"ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {preprocessed_path}")
                raise FileNotFoundError(f"ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ì„ë² ë”©ì´ í¬í•¨ëœ íŒŒì¼ì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
        embedded_path = Path(str(preprocessed_path).replace('.json', '.embedded.json'))
        if embedded_path.exists():
            logger.info(f"ğŸ¯ ì„ë² ë”© íŒŒì¼ ë°œê²¬: {embedded_path}")
            with open(embedded_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            # JSON ë°ì´í„° ë¡œë“œ
            logger.info(f"ğŸ“‚ ê¸°ë³¸ ë°ì´í„° ë¡œë“œ: {preprocessed_path}")
            with open(preprocessed_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œ)
        if hasattr(self.config, 'max_samples') and self.config.max_samples:
            data = data[:self.config.max_samples]
        
        # ë°ì´í„°ì…‹ í´ë˜ìŠ¤
        class RedHeartDataset(Dataset):
            def __init__(self, data_list, preprocessed_path=None):
                self.data = data_list
                self.preprocessed_path = preprocessed_path
                self.embedding_manager = None  # ì§€ì—° ì´ˆê¸°í™”
                self.embeddings_modified = False  # ì„ë² ë”© ìˆ˜ì • ì—¬ë¶€ ì¶”ì 
                
                # label ë§¤í•‘ (v2ì—ì„œ ì²˜ëŸ¼ TargetMapper ëŒ€ì‹  ì§ì ‘ ì²˜ë¦¬)
                self.label_to_idx = {
                    'AUTHOR': 0,
                    'EVERYBODY': 1,
                    'INFO': 2,
                    'NOBODY': 3,
                    'OTHER': 4
                }
                # ê°ì • ë§¤í•‘ (emotions dictì—ì„œ ì¶”ì¶œ)
                self.emotion_keys = ['joy', 'anger', 'surprise', 'disgust', 'sadness', 'shame', 'fear']
                
                # ì„ë² ë”© ìƒíƒœ í™•ì¸
                self._check_embeddings()
                
            def __len__(self):
                return len(self.data)
            
            def __getitem__(self, idx):
                item = self.data[idx]
                # ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = item.get('text', '') + ' ' + item.get('title', '')
                
                # í…ìŠ¤íŠ¸ ì„ë² ë”©ì€ ì‚¬ì „ ì²˜ë¦¬ëœ ë°ì´í„°ì—ì„œ ì‚¬ìš©
                # preprocessed ë°ì´í„°ì— ì´ë¯¸ embeddingì´ ìˆìœ¼ë©´ ì‚¬ìš©
                if 'embedding' in item:
                    text_embedding = torch.tensor(item['embedding'], dtype=torch.float32)
                    # 100x768 í¬ê¸°ë¡œ ì¡°ì •
                    if text_embedding.shape[0] < 100:
                        # íŒ¨ë”©
                        pad_size = 100 - text_embedding.shape[0]
                        text_embedding = torch.cat([text_embedding, torch.zeros(pad_size, 768)], dim=0)
                    elif text_embedding.shape[0] > 100:
                        # ìë¥´ê¸°
                        text_embedding = text_embedding[:100]
                else:
                    # ì„ë² ë”©ì´ ì—†ìœ¼ë©´ SentenceTransformerë¡œ ìƒì„±
                    if self.embedding_manager is None:
                        try:
                            self.embedding_manager = get_sentence_transformer(
                                'sentence-transformers/all-MiniLM-L6-v2',
                                device='cuda' if torch.cuda.is_available() else 'cpu'
                            )
                        except Exception as e:
                            logger.error(f"âŒ SentenceTransformer ë¡œë“œ ì‹¤íŒ¨: {e}")
                            logger.error("ì„ë² ë”© ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ ì¢…ë£Œ.")
                            raise RuntimeError(f"SentenceTransformer í•„ìˆ˜ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    
                    if self.embedding_manager:
                        try:
                            # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
                            embedding = self.embedding_manager.encode(text[:512])  # ìµœëŒ€ 512ì
                            text_embedding = torch.tensor(embedding, dtype=torch.float32)
                            
                            # 100x768 í˜•íƒœë¡œ í™•ì¥ (ë¬¸ì¥ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µ)
                            if text_embedding.dim() == 1:
                                text_embedding = text_embedding.unsqueeze(0)
                            
                            # 100ê°œ í† í°ìœ¼ë¡œ í™•ì¥
                            text_embedding = text_embedding.repeat(100, 1)
                            
                            # ìƒì„±ëœ ì„ë² ë”©ì„ ë°ì´í„°ì— ì €ì¥
                            self.data[idx]['embedding'] = text_embedding.numpy().tolist()
                            self.embeddings_modified = True
                            
                        except Exception as e:
                            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                            logger.error(f"í…ìŠ¤íŠ¸: {text[:50]}...")
                            raise RuntimeError(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                    else:
                        logger.error("âŒ SentenceTransformer ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        raise RuntimeError("SentenceTransformer ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
                
                # label ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜
                label_str = item.get('label', 'OTHER')
                label_idx = self.label_to_idx.get(label_str, 4)  # ê¸°ë³¸ê°’ 4 (OTHER)
                
                # emotions dictì—ì„œ ê°ì • ë²¡í„° ì¶”ì¶œ
                emotions = item.get('emotions', {})
                if isinstance(emotions, dict):
                    # 7ê°œ ê¸°ë³¸ ê°ì • ì¶”ì¶œ
                    emotion_vector = [emotions.get(key, 0.0) for key in self.emotion_keys]
                    # ê°€ì¥ ë†’ì€ ê°’ì˜ ì¸ë±ìŠ¤ë¥¼ ë¼ë²¨ë¡œ
                    emotion_label = torch.argmax(torch.tensor(emotion_vector)).item()
                else:
                    emotion_label = 0  # ê¸°ë³¸ê°’
                
                # bentham_scores ì²˜ë¦¬ (dict -> 10ì°¨ì› ë²¡í„°)
                bentham_keys = [
                    'intensity', 'duration', 'certainty', 'propinquity',
                    'purity', 'extent', 'fecundity', 'remoteness', 
                    'succession', 'utility'
                ]
                
                bentham_scores = item.get('bentham_scores', {})
                if isinstance(bentham_scores, dict):
                    # dictì—ì„œ ê°’ ì¶”ì¶œ (ì—†ìœ¼ë©´ 0.5 ê¸°ë³¸ê°’)
                    bentham_vector = [bentham_scores.get(key, 0.5) for key in bentham_keys]
                else:
                    # dictê°€ ì•„ë‹ˆë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                    bentham_vector = [0.5] * 10
                
                return {
                    'input': text_embedding,
                    'emotion_label': torch.tensor(emotion_label, dtype=torch.long),
                    'bentham_label': torch.tensor(bentham_vector, dtype=torch.float),
                    'regret_label': torch.tensor(item.get('regret_factor', 0.0), dtype=torch.float),
                    'surd_label': torch.tensor(label_idx, dtype=torch.long)  # labelì„ SURDì—ë„ ì‚¬ìš©
                }
            
            def _check_embeddings(self):
                """ì„ë² ë”© ìƒíƒœ í™•ì¸"""
                total_items = len(self.data)
                items_with_embedding = sum(1 for item in self.data if 'embedding' in item)
                items_without_embedding = total_items - items_with_embedding
                
                logger.info(f"ğŸ“Š ì„ë² ë”© ìƒíƒœ:")
                logger.info(f"  - ì „ì²´ ë°ì´í„°: {total_items}ê°œ")
                logger.info(f"  - ì„ë² ë”© ìˆìŒ: {items_with_embedding}ê°œ ({items_with_embedding/total_items*100:.1f}%)")
                logger.info(f"  - ì„ë² ë”© ì—†ìŒ: {items_without_embedding}ê°œ ({items_without_embedding/total_items*100:.1f}%)")
                
                if items_without_embedding > 0:
                    logger.warning(f"âš ï¸ {items_without_embedding}ê°œ í•­ëª©ì— ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ ìƒì„±ë©ë‹ˆë‹¤.")
            
            def save_embeddings(self):
                """ìƒì„±ëœ ì„ë² ë”©ì„ íŒŒì¼ì— ì €ì¥"""
                if not self.embeddings_modified:
                    return
                
                if self.preprocessed_path:
                    # ì›ë³¸ íŒŒì¼ëª…ì—ì„œ .embedded.json íŒŒì¼ ìƒì„±
                    embedded_path = Path(str(self.preprocessed_path).replace('.json', '.embedded.json'))
                    
                    try:
                        # ì „ì²´ ë°ì´í„° ì €ì¥
                        with open(embedded_path, 'w', encoding='utf-8') as f:
                            json.dump(self.data, f, ensure_ascii=False, indent=2)
                        logger.info(f"âœ… ì„ë² ë”©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {embedded_path}")
                        self.embeddings_modified = False
                    except Exception as e:
                        logger.error(f"ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
        val_size = int(len(data) * self.config.validation_split)
        train_data = data[val_size:]
        val_data = data[:val_size]
        
        # ë°ì´í„°ì…‹ ìƒì„± (preprocessed_path ì „ë‹¬)
        train_dataset = RedHeartDataset(train_data, preprocessed_path)
        val_dataset = RedHeartDataset(val_data, preprocessed_path)
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.micro_batch_size,
            shuffle=True,
            num_workers=self.config.num_workers,
            pin_memory=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.micro_batch_size,
            shuffle=False,
            num_workers=self.config.num_workers,
            pin_memory=True
        )
        
        logger.info(f"âœ… ë°ì´í„° ë¡œë” ì´ˆê¸°í™”: Train={train_size}, Val={val_size}")
    
    def _initialize_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”"""
        # LLRD ì‚¬ìš© ì‹œ
        if self.config.enable_llrd:
            self.optimizer = self.training_manager.get_optimizer(
                self.model,
                lr=self.config.base_lr,
                weight_decay=0.01
            )
        else:
            self.optimizer = torch.optim.AdamW(
                self.model.parameters(),
                lr=self.config.base_lr,
                weight_decay=0.01
            )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        total_steps = len(self.train_loader) * self.config.total_epochs
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=total_steps // 10,
            T_mult=2,
            eta_min=self.config.base_lr * 0.01
        )
        
        logger.info("âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def run_lr_sweep(self):
        """LR ìŠ¤ìœ• ì‹¤í–‰"""
        if not self.config.lr_sweep_enabled:
            return
        
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ” Learning Rate Sweep ì‹œì‘...")
        logger.info("=" * 70)
        
        # ê°„ë‹¨í•œ ì†ì‹¤ í•¨ìˆ˜
        criterion = nn.CrossEntropyLoss()
        
        # ìŠ¤ìœ• ì‹¤í–‰
        sweep_results = self.lr_sweep.run_sweep(
            model=self.model,
            train_loader=self.train_loader,
            val_loader=self.val_loader,
            criterion=criterion,
            device=self.device
        )
        
        # ìµœì  LRë¡œ ì˜µí‹°ë§ˆì´ì € ì¬ì´ˆê¸°í™”
        self.config.base_lr = self.lr_sweep.best_lr
        self._initialize_optimizer()
        
        logger.info(f"âœ… ìµœì  LR ì„ íƒ: {self.config.base_lr:.1e}")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        epoch_losses = []
        module_metrics = {}
        batch_gradients = {}  # ëª¨ë“  ë°°ì¹˜ì˜ gradient norm ì €ì¥
        total_batches = len(self.train_loader)
        completed_batches = 0
        
        for batch_idx, batch in enumerate(self.train_loader):
            # OOM í•¸ë“¤ë§
            if self.config.enable_oom_handler:
                self.oom_handler.log_memory_stats(self.global_step, 'train')
            
            # Forward pass
            try:
                loss, metrics = self._forward_step(batch)
            except RuntimeError as e:
                if 'out of memory' in str(e).lower():
                    if self.oom_handler.handle_oom(e):
                        # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì • í›„ ì¬ì‹œë„
                        self.train_loader = self.oom_handler.adjust_dataloader(self.train_loader)
                        continue
                    else:
                        raise
                else:
                    raise
            
            # Backward pass (Gradient Accumulation)
            loss = loss / self.config.gradient_accumulation
            loss.backward()
            
            # ë§¤ ë°°ì¹˜ë§ˆë‹¤ gradient norm ê³„ì‚° (accumulation ì—¬ë¶€ì™€ ë¬´ê´€)
            with torch.no_grad():
                # ì „ì²´ ëª¨ë¸ì˜ gradient norm
                total_grad_norm = 0.0
                for p in self.model.parameters():
                    if p.grad is not None:
                        param_norm = p.grad.data.norm(2).item()
                        total_grad_norm += param_norm ** 2
                total_grad_norm = total_grad_norm ** 0.5
                
                # ëª¨ë“ˆë³„ gradient norm ê³„ì‚°
                for name, module in self.model.named_children():
                    module_grad_norm = 0.0
                    for p in module.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2).item()
                            module_grad_norm += param_norm ** 2
                    module_grad_norm = module_grad_norm ** 0.5
                    
                    if module_grad_norm > 0:
                        if f'{name}_grad_norm' not in batch_gradients:
                            batch_gradients[f'{name}_grad_norm'] = []
                        batch_gradients[f'{name}_grad_norm'].append(module_grad_norm)
                        metrics[f'{name}_grad_norm'] = module_grad_norm
                
                if total_grad_norm > 0:
                    if 'total_grad_norm' not in batch_gradients:
                        batch_gradients['total_grad_norm'] = []
                    batch_gradients['total_grad_norm'].append(total_grad_norm)
                    metrics['total_grad_norm'] = total_grad_norm
            
            # Gradient Accumulation
            if (batch_idx + 1) % self.config.gradient_accumulation == 0:
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # Optimizer step (íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í”Œë˜ê·¸ í™•ì¸)
                if not self.no_param_update:
                    # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì „ ê°’ ì €ì¥ (ìƒ˜í”Œë§)
                    param_before = {}
                    if batch_idx == 0 or batch_idx % 100 == 0:  # ì²« ë°°ì¹˜ì™€ 100ë°°ì¹˜ë§ˆë‹¤ ì²´í¬
                        for name, module in self.model.named_children():
                            if hasattr(module, 'parameters'):
                                # ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°ë§Œ ìƒ˜í”Œë§
                                for p in module.parameters():
                                    if p.requires_grad and p.grad is not None:
                                        param_before[name] = p.data.clone().mean().item()
                                        break
                    
                    self.optimizer.step()
                    self.scheduler.step()
                    
                    # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í›„ í™•ì¸
                    if param_before:
                        param_updated = []
                        param_not_updated = []
                        for name, module in self.model.named_children():
                            if name in param_before:
                                for p in module.parameters():
                                    if p.requires_grad and p.grad is not None:
                                        param_after = p.data.mean().item()
                                        if abs(param_after - param_before[name]) > 1e-8:
                                            param_updated.append(name)
                                        else:
                                            param_not_updated.append(name)
                                        break
                        
                        if param_updated:
                            logger.debug(f"  âœ… íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ë¨ (batch {batch_idx}): {', '.join(param_updated)}")
                        if param_not_updated:
                            logger.warning(f"  âš ï¸ íŒŒë¼ë¯¸í„° ë¯¸ì—…ë°ì´íŠ¸ (batch {batch_idx}): {', '.join(param_not_updated)}")
                    
                    # EMA update
                    self.training_manager.step()
                else:
                    # ê²€ì¦ ëª¨ë“œ: ê·¸ë¼ë””ì–¸íŠ¸ë§Œ ê³„ì‚°í•˜ê³  ì—…ë°ì´íŠ¸ëŠ” ê±´ë„ˆëœ€
                    logger.debug("  [ê²€ì¦] íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€")
                
                self.optimizer.zero_grad()
                self.global_step += 1
            
            epoch_losses.append(loss.item() * self.config.gradient_accumulation)
            
            # ë¡œê¹…
            if batch_idx % self.config.log_interval == 0:
                avg_loss = np.mean(epoch_losses[-self.config.log_interval:])
                lr = self.optimizer.param_groups[0]['lr']
                logger.info(f"  [Epoch {epoch}][{batch_idx}/{len(self.train_loader)}] "
                          f"Loss: {avg_loss:.4f}, LR: {lr:.1e}")
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            for key, value in metrics.items():
                if key not in module_metrics:
                    module_metrics[key] = []
                module_metrics[key].append(value)
            
            completed_batches += 1
        
        # ë°°ì¹˜ ë£¨í”„ ì™„ë£Œ í™•ì¸
        if completed_batches != total_batches:
            logger.error(f"  âš ï¸ ì—í­ {epoch} ë¶ˆì™„ì „ ì¢…ë£Œ: {completed_batches}/{total_batches} ë°°ì¹˜ë§Œ ì²˜ë¦¬ë¨")
            logger.error(f"     ë§ˆì§€ë§‰ ì²˜ë¦¬ ë°°ì¹˜ ì¸ë±ìŠ¤: {completed_batches - 1}")
        else:
            logger.info(f"  âœ… ì—í­ {epoch} ì™„ë£Œ: {completed_batches}/{total_batches} ë°°ì¹˜ ëª¨ë‘ ì²˜ë¦¬")
        
        # gradient norm í‰ê·  ì¶”ê°€ (ëª¨ë“  ë°°ì¹˜ì˜ í‰ê· )
        for key, values in batch_gradients.items():
            module_metrics[key] = values
        
        # ì—í­ í‰ê· 
        avg_metrics = {k: np.mean(v) for k, v in module_metrics.items()}
        avg_metrics['loss'] = np.mean(epoch_losses)
        avg_metrics['completed_batches'] = completed_batches
        avg_metrics['total_batches'] = total_batches
        
        return avg_metrics
    
    def validate(self) -> Dict[str, float]:
        """ê²€ì¦"""
        self.model.eval()
        val_losses = []
        module_metrics = {}
        
        with torch.no_grad():
            # EMA ì ìš©
            if self.config.enable_ema:
                self.training_manager.apply_ema()
            
            for batch in self.val_loader:
                loss, metrics = self._forward_step(batch)
                val_losses.append(loss.item())
                
                for key, value in metrics.items():
                    if key not in module_metrics:
                        module_metrics[key] = []
                    module_metrics[key].append(value)
            
            # EMA ë³µì›
            if self.config.enable_ema:
                self.training_manager.restore_ema()
        
        # í‰ê· 
        avg_metrics = {k: np.mean(v) for k, v in module_metrics.items()}
        avg_metrics['val_loss'] = np.mean(val_losses)
        
        return avg_metrics
    
    def _forward_step(self, batch: Dict) -> Tuple[torch.Tensor, Dict]:
        """
        í•™ìŠµ ìŠ¤í… - V2ì˜ 3ë‹¨ê³„ ì›Œí¬í”Œë¡œìš° ë³µì›
        1. FORWARD: ë°ì´í„° â†’ ë°±ë³¸ â†’ í—¤ë“œ
        2. COMPUTE: ì†ì‹¤ ê³„ì‚° + Neural Analyzers + DSP/Kalman
        3. UPDATE: ì—­ì „íŒŒ + ìµœì í™”
        """
        batch_idx = self.global_step % 100  # ë¡œê¹…ìš©
        
        # ========== STAGE 1: FORWARD ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 1] Forward Pass ì‹œì‘")
        
        # ë°ì´í„° ì¤€ë¹„ - ì…ë ¥ ì¶”ì¶œ
        inputs = batch['input'].to(self.device)
        
        # ë°±ë³¸ í†µê³¼
        backbone_outputs = self.model.backbone(inputs, return_all_tasks=True)
        features = backbone_outputs.get('emotion', inputs)  # 896ì°¨ì›
        
        if self.verbose and batch_idx < 3:
            logger.info(f"      - ë°±ë³¸ ì¶œë ¥ shape: {features.shape}")
            logger.info(f"      - ë°±ë³¸ ì¶œë ¥ í‚¤: {list(backbone_outputs.keys())}")
        
        # ========== STAGE 2: COMPUTE LOSSES ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 2] Compute Loss")
        
        # í—¤ë“œ ì†ì‹¤ ê³„ì‚°
        head_losses = []
        individual_losses = {}  # ê°œë³„ ì†ì‹¤ ì €ì¥ìš©
        individual_accs = {}    # ê°œë³„ ì •í™•ë„ ì €ì¥ìš©
        
        # ê°ì • í—¤ë“œ
        if hasattr(self.model, 'emotion_head'):
            emotion_output = self.model.emotion_head(features)
            emotion_pred = emotion_output['emotions'] if isinstance(emotion_output, dict) else emotion_output
            emotion_target = batch['emotion_label'].to(self.device)
            emotion_loss = self.model.emotion_head.compute_loss(emotion_pred, emotion_target)
            head_losses.append(emotion_loss)
            individual_losses['emotion_loss'] = emotion_loss.item()
            # accuracy ê³„ì‚° (classification task)
            emotion_acc = (emotion_pred.argmax(dim=-1) == emotion_target).float().mean().item()
            individual_accs['emotion_acc'] = emotion_acc
            if self.verbose and batch_idx < 3:
                logger.info(f"      - ê°ì • ì†ì‹¤: {emotion_loss.item():.6f}, ì •í™•ë„: {emotion_acc:.4f}")
        
        # ë²¤ë‹´ í—¤ë“œ
        if hasattr(self.model, 'bentham_head'):
            bentham_output = self.model.bentham_head(features)
            bentham_pred = bentham_output['bentham_scores'] if isinstance(bentham_output, dict) else bentham_output
            bentham_target = batch['bentham_label'].to(self.device)
            bentham_loss = self.model.bentham_head.compute_loss(bentham_pred, bentham_target)
            head_losses.append(bentham_loss)
            individual_losses['bentham_loss'] = bentham_loss.item()
            # accuracy ê³„ì‚° (regression task - ë™ì  threshold ê¸°ë°˜)
            # í•™ìŠµ ì§„í–‰ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ì—„ê²©í•œ ê¸°ì¤€ ì ìš©
            dynamic_threshold = 0.5 if self.current_epoch <= 10 else 0.3 if self.current_epoch <= 30 else 0.2
            bentham_acc = ((bentham_pred - bentham_target).abs() < dynamic_threshold).float().mean().item()
            individual_accs['bentham_acc'] = bentham_acc
            if self.verbose and batch_idx < 3:
                logger.info(f"      - ë²¤ë‹´ ì†ì‹¤: {bentham_loss.item():.6f}, ì •í™•ë„: {bentham_acc:.4f}")
        
        # í›„íšŒ í—¤ë“œ
        if hasattr(self.model, 'regret_head'):
            regret_output = self.model.regret_head(features)
            regret_pred = regret_output['regret_score'] if isinstance(regret_output, dict) else regret_output
            regret_target = batch['regret_label'].to(self.device)
            regret_loss = self.model.regret_head.compute_loss(regret_pred, regret_target)
            head_losses.append(regret_loss)
            individual_losses['regret_loss'] = regret_loss.item()
            # accuracy ê³„ì‚° (regression task - ë™ì  threshold ê¸°ë°˜)
            # í•™ìŠµ ì§„í–‰ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ì—„ê²©í•œ ê¸°ì¤€ ì ìš©
            dynamic_threshold = 0.5 if self.current_epoch <= 10 else 0.3 if self.current_epoch <= 30 else 0.2
            regret_acc = ((regret_pred - regret_target).abs() < dynamic_threshold).float().mean().item()
            individual_accs['regret_acc'] = regret_acc
            if self.verbose and batch_idx < 3:
                logger.info(f"      - í›„íšŒ ì†ì‹¤: {regret_loss.item():.6f}, ì •í™•ë„: {regret_acc:.4f}")
        
        # SURD í—¤ë“œ
        if hasattr(self.model, 'surd_head'):
            surd_output = self.model.surd_head(features)
            surd_pred = surd_output['surd_values'] if isinstance(surd_output, dict) else surd_output
            
            # SURD íƒ€ê²Ÿì„ ì‹¤ì œ ë°ì´í„°ì—ì„œ ê³„ì‚°
            batch_size = surd_pred.shape[0]
            surd_target = torch.zeros((batch_size, 4), device=self.device)
            
            # Synergy: ê°ì • ë‹¤ì–‘ì„± (ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜)
            if 'emotion_label' in batch:
                emotion_probs = F.one_hot(batch['emotion_label'].to(self.device), num_classes=7).float()
                emotion_entropy = -(emotion_probs * (emotion_probs + 1e-10).log()).sum(dim=1)
                surd_target[:, 0] = emotion_entropy / np.log(7)  # ì •ê·œí™”
            
            # Unique: ë ˆì´ë¸” ê³ ìœ ì„± (one-hot ì¸ì½”ë”©)
            if 'surd_label' in batch:
                label_unique = F.one_hot(batch['surd_label'].to(self.device), num_classes=5).float()
                surd_target[:, 1] = label_unique.max(dim=1)[0]  # ìµœëŒ€ê°’ = 1.0
            
            # Redundant: ë²¤ë‹´ ìƒê´€ë„ (í‰ê· ê³¼ ë¶„ì‚°)
            if 'bentham_label' in batch:
                bentham = batch['bentham_label'].to(self.device)
                bentham_mean = bentham.mean(dim=1)
                bentham_std = bentham.std(dim=1) + 1e-10
                surd_target[:, 2] = 1.0 - (bentham_std / (bentham_mean + 1e-10)).clamp(0, 1)
            
            # Deterministic: í›„íšŒ ê²°ì •ì„± (ì ˆëŒ€ê°’)
            if 'regret_label' in batch:
                regret = batch['regret_label'].to(self.device)
                if regret.dim() == 1:
                    regret = regret.unsqueeze(1)
                surd_target[:, 3] = regret.abs().squeeze()
            
            surd_loss = self.model.surd_head.compute_loss(surd_pred, surd_target)
            head_losses.append(surd_loss)
            individual_losses['surd_loss'] = surd_loss.item()
            # accuracy ê³„ì‚° (multi-dimensional regression - ë™ì  threshold ê¸°ë°˜)
            # í•™ìŠµ ì§„í–‰ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ì—„ê²©í•œ ê¸°ì¤€ ì ìš©
            # SURDëŠ” 4ì°¨ì›ì´ë¯€ë¡œ ì•½ê°„ ë” ì™„í™”ëœ ê¸°ì¤€ ì ìš©
            dynamic_threshold = 0.5 if self.current_epoch <= 10 else 0.35 if self.current_epoch <= 30 else 0.25
            surd_acc = ((surd_pred - surd_target).abs() < dynamic_threshold).float().mean().item()
            individual_accs['surd_acc'] = surd_acc
            if self.verbose and batch_idx < 3:
                logger.info(f"      - SURD ì†ì‹¤: {surd_loss.item():.6f}, ì •í™•ë„: {surd_acc:.4f}")
        
        # ========== STAGE 2: NEURAL ANALYZERS ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 2] Neural Analyzer Processing")
        
        analyzer_losses = []
        dsp_output = None
        neural_emotion_output = None
        
        # Neural Emotion Analyzer ì²˜ë¦¬ (ë¨¼ì €)
        if hasattr(self.model, 'neural_analyzers') and 'emotion' in self.model.neural_analyzers:
            try:
                emotion_analyzer = self.model.neural_analyzers['emotion']
                neural_emotion_output = emotion_analyzer(features)
                
                if 'emotion_logits' in neural_emotion_output:
                    target = batch['emotion_label'].to(self.device)
                    if target.dim() == 1:
                        target = F.one_hot(target, num_classes=7).float()
                    emotion_loss = F.cross_entropy(neural_emotion_output['emotion_logits'], target)
                    analyzer_losses.append(emotion_loss)
                    if self.verbose and batch_idx < 3:
                        logger.info(f"      - neural_emotion ì†ì‹¤: {emotion_loss.item():.6f}")
            except Exception as e:
                logger.error(f"    âŒ neural_emotion ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # DSP Simulator ì²˜ë¦¬
        if hasattr(self.model, 'dsp_simulator') and self.model.dsp_simulator:
            try:
                # DSPëŠ” 384ì°¨ì› ì…ë ¥ í•„ìš”
                if not hasattr(self, 'dsp_projection'):
                    self.dsp_projection = torch.nn.Linear(features.shape[-1], 384).to(self.device)
                
                dsp_input = self.dsp_projection(features)
                dsp_output = self.model.dsp_simulator(dsp_input)
                
                if self.verbose and batch_idx < 3:
                    logger.info(f"      - DSP ì¶œë ¥ ì²˜ë¦¬ë¨")
            except Exception as e:
                logger.warning(f"    âš ï¸ DSP ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # Kalman Filter ì²˜ë¦¬ (neural_emotion + DSP í•„ìš”)
        if hasattr(self.model, 'kalman_filter') and self.model.kalman_filter and \
           dsp_output is not None and neural_emotion_output is not None:
            try:
                traditional_emotions = neural_emotion_output.get('emotion_logits', None)
                dsp_emotions = dsp_output.get('final_emotions', None) if isinstance(dsp_output, dict) else dsp_output
                
                if traditional_emotions is not None and dsp_emotions is not None:
                    kalman_output = self.model.kalman_filter(
                        traditional_emotions=traditional_emotions,
                        dsp_emotions=dsp_emotions
                    )
                    if self.verbose and batch_idx < 3:
                        logger.info(f"      - Kalman í•„í„° ì¶œë ¥ ì²˜ë¦¬ë¨")
            except Exception as e:
                logger.warning(f"    âš ï¸ Kalman ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ë‚˜ë¨¸ì§€ Neural Analyzers ì²˜ë¦¬
        if hasattr(self.model, 'neural_analyzers'):
            for name, analyzer in self.model.neural_analyzers.items():
                if name == 'emotion':  # ì´ë¯¸ ì²˜ë¦¬í•¨
                    continue
                    
                try:
                    analyzer_output = analyzer(features)
                    
                    # ê° analyzerë³„ ì†ì‹¤ ê³„ì‚°
                    if 'bentham' in name and 'bentham_scores' in analyzer_output:
                        target = batch['bentham_label'].to(self.device)
                        analyzer_loss = F.mse_loss(analyzer_output['bentham_scores'], target)
                        analyzer_losses.append(analyzer_loss)
                        
                        # Analyzer accuracy ê³„ì‚° (regression - ë™ì  ì„ê³€4ê°’)
                        with torch.no_grad():
                            # ì—í­ì— ë”°ë¼ ì„ê³€4ê°’ ì¡°ì ˆ
                            dynamic_threshold = 0.5 if self.current_epoch <= 10 else 0.3 if self.current_epoch <= 30 else 0.2
                            analyzer_acc = ((analyzer_output['bentham_scores'] - target).abs() < dynamic_threshold).float().mean().item()
                            analyzer_accuracies.append(analyzer_acc)
                        
                        if self.verbose and batch_idx < 3:
                            logger.info(f"      - {name} ì†ì‹¤: {analyzer_loss.item():.6f}, ì •í™•ë„: {analyzer_acc:.4f}")
                    
                    elif 'regret' in name and 'regret_score' in analyzer_output:
                        target = batch['regret_label'].to(self.device)
                        analyzer_loss = F.smooth_l1_loss(analyzer_output['regret_score'], target)
                        analyzer_losses.append(analyzer_loss)
                        
                        # Analyzer accuracy ê³„ì‚° (regression - ë™ì  ì„ê³€4ê°’)
                        with torch.no_grad():
                            dynamic_threshold = 0.5 if self.current_epoch <= 10 else 0.3 if self.current_epoch <= 30 else 0.2
                            analyzer_acc = ((analyzer_output['regret_score'] - target).abs() < dynamic_threshold).float().mean().item()
                            analyzer_accuracies.append(analyzer_acc)
                        
                        if self.verbose and batch_idx < 3:
                            logger.info(f"      - {name} ì†ì‹¤: {analyzer_loss.item():.6f}, ì •í™•ë„: {analyzer_acc:.4f}")
                    
                    elif 'surd' in name and 'surd_scores' in analyzer_output:
                        # SURD analyzerë„ 4ì°¨ì› íƒ€ê²Ÿ í•„ìš”
                        batch_size = analyzer_output['surd_scores'].shape[0]
                        target = torch.zeros((batch_size, 4), device=self.device)
                        
                        # ì‹¤ì œ ë°ì´í„°ë¡œ SURD ê³„ì‚° (ìœ„ì™€ ë™ì¼)
                        if 'emotion_label' in batch:
                            emotion_probs = F.one_hot(batch['emotion_label'].to(self.device), num_classes=7).float()
                            emotion_entropy = -(emotion_probs * (emotion_probs + 1e-10).log()).sum(dim=1)
                            target[:, 0] = emotion_entropy / np.log(7)
                        
                        if 'surd_label' in batch:
                            label_unique = F.one_hot(batch['surd_label'].to(self.device), num_classes=5).float()
                            target[:, 1] = label_unique.max(dim=1)[0]
                        
                        if 'bentham_label' in batch:
                            bentham = batch['bentham_label'].to(self.device)
                            bentham_mean = bentham.mean(dim=1)
                            bentham_std = bentham.std(dim=1) + 1e-10
                            target[:, 2] = 1.0 - (bentham_std / (bentham_mean + 1e-10)).clamp(0, 1)
                        
                        if 'regret_label' in batch:
                            regret = batch['regret_label'].to(self.device)
                            if regret.dim() == 1:
                                regret = regret.unsqueeze(1)
                            target[:, 3] = regret.abs().squeeze()
                        
                        analyzer_loss = F.mse_loss(analyzer_output['surd_scores'], target)
                        analyzer_losses.append(analyzer_loss)
                        if self.verbose and batch_idx < 3:
                            logger.info(f"      - {name} ì†ì‹¤: {analyzer_loss.item():.6f}")
                    
                except Exception as e:
                    if hasattr(self.config, 'debug') and self.config.debug:
                        logger.error(f"    {name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    else:
                        logger.error(f"    {name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # Advanced Wrappers ì²˜ë¦¬
        if hasattr(self.model, 'advanced_wrappers') and self.model.advanced_wrappers:
            for name, wrapper in self.model.advanced_wrappers.items():
                try:
                    wrapper_output = wrapper(features)
                    
                    # Advanced wrapper ì†ì‹¤ (ê°„ë‹¨íˆ ì²˜ë¦¬)
                    if isinstance(wrapper_output, dict) and any(key in wrapper_output for key in ['emotion', 'bentham', 'regret', 'surd']):
                        # ì ì ˆí•œ ì†ì‹¤ ê³„ì‚°
                        pass  # TODO: wrapperë³„ ì†ì‹¤ êµ¬í˜„ í•„ìš”
                        
                except Exception as e:
                    if self.config.debug:
                        logger.error(f"    {name} wrapper ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì „ì²´ ì†ì‹¤ í†µí•© (V2ì²˜ëŸ¼: í—¤ë“œ 70%, Analyzer 30%)
        all_losses = head_losses + analyzer_losses
        
        if all_losses:
            if head_losses and analyzer_losses:
                head_loss = sum(head_losses) / len(head_losses)
                analyzer_loss = sum(analyzer_losses) / len(analyzer_losses)
                loss = 0.7 * head_loss + 0.3 * analyzer_loss
                if self.verbose and batch_idx < 3:
                    logger.info(f"      - í—¤ë“œ ì†ì‹¤: {head_loss.item():.6f}")
                    logger.info(f"      - ë¶„ì„ê¸° ì†ì‹¤: {analyzer_loss.item():.6f}")
                    logger.info(f"      - ì „ì²´ ì†ì‹¤: {loss.item():.6f}")
            else:
                loss = sum(all_losses) / len(all_losses)
        else:
            # NO FALLBACK - ì†ì‹¤ì´ ì—†ìœ¼ë©´ ì—ëŸ¬
            raise RuntimeError("ì†ì‹¤ ê³„ì‚° ì‹¤íŒ¨: í—¤ë“œë‚˜ ë¶„ì„ê¸°ê°€ ì†ì‹¤ì„ ìƒì„±í•˜ì§€ ëª»í•¨")
        
        # ë©”íŠ¸ë¦­ - ê°œë³„ ëª¨ë“ˆ ì†ì‹¤ í¬í•¨
        metrics = {
            'loss': loss.item(),
            'train_loss': loss.item(),  # ì „ì²´ ì†ì‹¤ (backward í˜¸í™˜)
            'head_losses': len(head_losses),
            'analyzer_losses': len(analyzer_losses),
            'total_losses': len(all_losses)
        }
        
        # ê°œë³„ í—¤ë“œ ì†ì‹¤ ë° ì •í™•ë„ ì¶”ê°€
        metrics.update(individual_losses)
        metrics.update(individual_accs)
        
        # ë°±ë³¸ ì†ì‹¤ (ì „ì²´ ì†ì‹¤ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •)
        metrics['backbone_loss'] = loss.item()
        metrics['backbone_acc'] = 0.0  # ë°±ë³¸ì€ ë³„ë„ accuracy ì—†ìŒ
        
        # Neural Analyzer ì†ì‹¤ ë° ì •í™•ë„
        if analyzer_losses:
            metrics['analyzer_loss'] = sum(al.item() for al in analyzer_losses) / len(analyzer_losses)
            # ì‹¤ì œ analyzer accuracy ê³„ì‚° (í‰ê· )
            metrics['analyzer_acc'] = np.mean(analyzer_accuracies) if analyzer_accuracies else 0.0
        else:
            metrics['analyzer_loss'] = 0.0
            metrics['analyzer_acc'] = 0.0
        
        # ì „ì²´ accuracy ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        # ê° íƒœìŠ¤í¬ì˜ ì¤‘ìš”ë„: emotion(30%), bentham(25%), regret(20%), surd(15%), analyzer(10%)
        weighted_acc = 0.0
        weights_sum = 0.0
        
        task_weights = {
            'emotion_acc': 0.30,
            'bentham_acc': 0.25,
            'regret_acc': 0.20,
            'surd_acc': 0.15,
            'analyzer_acc': 0.10
        }
        
        for task, weight in task_weights.items():
            if task in metrics and metrics[task] > 0:
                weighted_acc += metrics[task] * weight
                weights_sum += weight
        
        # train/val ë©”íŠ¸ë¦­
        metrics['train_loss'] = loss.item()
        metrics['train_acc'] = weighted_acc / weights_sum if weights_sum > 0 else 0.0
        metrics['val_loss'] = loss.item()  # validate()ì—ì„œ ë®ì–´ì”Œì›Œì§
        metrics['val_acc'] = metrics['train_acc']  # validate()ì—ì„œ ë®ì–´ì”Œì›Œì§
        
        # ëª¨ë“ˆ ìƒí˜¸ì‘ìš© ë©”íŠ¸ë¦­ ê³„ì‚° (ì‹¤ì œ êµ¬í˜„)
        with torch.no_grad():
            # 1. ëª¨ë“ˆ ê°„ ì†ì‹¤ ìƒê´€ê´€ê³„
            if len(individual_losses) > 1:
                loss_values = list(individual_losses.values())
                if len(loss_values) >= 2:
                    # ì†ì‹¤ ê°„ ìƒê´€ì„± ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ - ë…ë¦½ì ì¸ í•™ìŠµ)
                    loss_tensor = torch.tensor(loss_values)
                    loss_std = loss_tensor.std().item()
                    loss_mean = loss_tensor.mean().item()
                    metrics['module_loss_variance'] = loss_std
                    metrics['module_loss_mean'] = loss_mean
                    # ë³€ë™ê³„ìˆ˜ (Coefficient of Variation)
                    metrics['module_loss_cv'] = loss_std / (loss_mean + 1e-10)
            
            # 2. ëª¨ë“ˆ ê°„ ì •í™•ë„ ì‹œë„ˆì§€
            if len(individual_accs) > 1:
                acc_values_list = list(individual_accs.values())
                if len(acc_values_list) >= 2:
                    acc_tensor = torch.tensor(acc_values_list)
                    # ì •í™•ë„ ê°„ ì¼ê´€ì„± (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                    acc_std = acc_tensor.std().item()
                    acc_mean = acc_tensor.mean().item()
                    metrics['module_acc_consistency'] = 1.0 - (acc_std / (acc_mean + 1e-10))
                    
                    # ì‹œë„ˆì§€ ì ìˆ˜: ì „ì²´ ì •í™•ë„ê°€ ê°œë³„ í‰ê· ë³´ë‹¤ ë†’ìœ¼ë©´ ì–‘ì˜ ì‹œë„ˆì§€
                    synergy_score = metrics['train_acc'] - acc_mean
                    metrics['module_synergy_score'] = synergy_score
            
            # 3. Head-Analyzer ìƒí˜¸ì‘ìš©
            if analyzer_accuracies and len(individual_accs) > 0:
                head_acc_mean = np.mean(list(individual_accs.values()))
                analyzer_acc_mean = np.mean(analyzer_accuracies)
                # Headì™€ Analyzer ê°„ ì„±ëŠ¥ ê²©ì°¨ (ì‘ì„ìˆ˜ë¡ ê· í˜•ì )
                metrics['head_analyzer_gap'] = abs(head_acc_mean - analyzer_acc_mean)
                # ìƒí˜¸ ë³´ì™„ ì§€ìˆ˜
                metrics['head_analyzer_complement'] = min(head_acc_mean, analyzer_acc_mean) / (max(head_acc_mean, analyzer_acc_mean) + 1e-10)
        
        return loss, metrics
    
    def train(self):
        """ì „ì²´ í•™ìŠµ ì‹¤í–‰"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸš€ í•™ìŠµ ì‹œì‘")
        logger.info("=" * 70)
        
        # LR ìŠ¤ìœ• ì‹¤í–‰
        self.run_lr_sweep()
        
        # 60 ì—í­ í•™ìŠµ
        for epoch in range(1, self.config.total_epochs + 1):
            self.current_epoch = epoch
            
            logger.info(f"\nğŸ“Œ Epoch {epoch}/{self.config.total_epochs}")
            
            # í•™ìŠµ
            try:
                train_metrics = self.train_epoch(epoch)
            except Exception as e:
                logger.error(f"  âŒ ì—í­ {epoch} í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
                train_metrics = {'train_loss': float('inf')}
            
            # ê²€ì¦ ì‹¤í–‰ ì¡°ê±´ (ì„¤ì • ê°€ëŠ¥)
            should_validate = False
            if hasattr(self.config, 'val_interval'):
                # val_intervalì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ í•´ë‹¹ ê°„ê²©ìœ¼ë¡œ ê²€ì¦
                should_validate = (epoch % self.config.val_interval == 0)
            else:
                # ê¸°ë³¸ ë¡œì§: í…ŒìŠ¤íŠ¸ ëª¨ë“œë‚˜ ì‘ì€ ì—í­ ìˆ˜ì¼ ë•ŒëŠ” ìì£¼ ê²€ì¦
                if self.config.total_epochs <= 5:
                    should_validate = True  # ëª¨ë“  ì—í­ì—ì„œ ê²€ì¦
                elif self.config.total_epochs <= 20:
                    should_validate = (epoch % 2 == 0)  # ì§ìˆ˜ ì—í­ë§ˆë‹¤
                else:
                    should_validate = (epoch % 5 == 0) or (epoch == self.config.total_epochs)  # 5 ì—í­ë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰
            
            if should_validate:
                val_metrics = self.validate()
                logger.info(f"  Validation Loss: {val_metrics['val_loss']:.4f}")
            else:
                val_metrics = {}
            
            # ë©”íŠ¸ë¦­ í†µí•© ë° ëª¨ë“ˆë³„ ê·¸ë£¹í™” (train/val ë¶„ë¦¬)
            all_metrics = {**train_metrics, **val_metrics}
            
            # í•™ìŠµ ëª¨ë“ˆë³„ ë©”íŠ¸ë¦­
            train_module_metrics = {
                'backbone': {
                    'loss': train_metrics.get('backbone_loss', train_metrics.get('train_loss', 0)),
                    'accuracy': train_metrics.get('backbone_acc', 0),
                    'gradient_norm': train_metrics.get('backbone_grad_norm', 0)
                },
                'emotion_head': {
                    'loss': train_metrics.get('emotion_loss', train_metrics.get('train_loss', 0)),
                    'accuracy': train_metrics.get('emotion_acc', 0),
                    'gradient_norm': train_metrics.get('emotion_grad_norm', 0)
                },
                'bentham_head': {
                    'loss': train_metrics.get('bentham_loss', train_metrics.get('train_loss', 0)),
                    'accuracy': train_metrics.get('bentham_acc', 0),
                    'gradient_norm': train_metrics.get('bentham_grad_norm', 0)
                },
                'regret_head': {
                    'loss': train_metrics.get('regret_loss', train_metrics.get('train_loss', 0)),
                    'accuracy': train_metrics.get('regret_acc', 0),
                    'gradient_norm': train_metrics.get('regret_grad_norm', 0)
                },
                'surd_head': {
                    'loss': train_metrics.get('surd_loss', train_metrics.get('train_loss', 0)),
                    'accuracy': train_metrics.get('surd_acc', 0),
                    'gradient_norm': train_metrics.get('surd_grad_norm', 0)
                },
                'neural_analyzers': {
                    'loss': train_metrics.get('analyzer_loss', train_metrics.get('train_loss', 0)),
                    'accuracy': train_metrics.get('analyzer_acc', 0),
                    'gradient_norm': train_metrics.get('analyzer_grad_norm', 0)
                },
                'system': {
                    'loss': train_metrics.get('train_loss', 0),
                    'accuracy': train_metrics.get('train_acc', 0),
                    'gradient_norm': train_metrics.get('total_grad_norm', 0)
                }
            }
            
            # ê²€ì¦ ëª¨ë“ˆë³„ ë©”íŠ¸ë¦­ (val_metricsê°€ ìˆì„ ë•Œë§Œ)
            if val_metrics:
                val_module_metrics = {
                    'backbone': {
                        'loss': val_metrics.get('backbone_loss', val_metrics.get('val_loss', 0)),
                        'accuracy': val_metrics.get('backbone_acc', val_metrics.get('val_acc', 0))
                    },
                    'emotion_head': {
                        'loss': val_metrics.get('emotion_loss', val_metrics.get('val_loss', 0)),
                        'accuracy': val_metrics.get('emotion_acc', val_metrics.get('val_acc', 0))
                    },
                    'bentham_head': {
                        'loss': val_metrics.get('bentham_loss', val_metrics.get('val_loss', 0)),
                        'accuracy': val_metrics.get('bentham_acc', val_metrics.get('val_acc', 0))
                    },
                    'regret_head': {
                        'loss': val_metrics.get('regret_loss', val_metrics.get('val_loss', 0)),
                        'accuracy': val_metrics.get('regret_acc', val_metrics.get('val_acc', 0))
                    },
                    'surd_head': {
                        'loss': val_metrics.get('surd_loss', val_metrics.get('val_loss', 0)),
                        'accuracy': val_metrics.get('surd_acc', val_metrics.get('val_acc', 0))
                    },
                    'neural_analyzers': {
                        'loss': val_metrics.get('analyzer_loss', val_metrics.get('val_loss', 0)),
                        'accuracy': val_metrics.get('analyzer_acc', val_metrics.get('val_acc', 0))
                    },
                    'system': {
                        'loss': val_metrics.get('val_loss', 0),
                        'accuracy': val_metrics.get('val_acc', 0)
                    }
                }
            else:
                # ê²€ì¦ì´ ì—†ëŠ” ì—í­ì€ train ë©”íŠ¸ë¦­ì„ ë³µì‚¬ (í˜¸í™˜ì„±)
                val_module_metrics = train_module_metrics.copy()
            
            # ë””ë²„ê·¸: ë©”íŠ¸ë¦­ ê²€ì¦
            if epoch == 1 and self.verbose:
                logger.info("\n  ğŸ“Š ë©”íŠ¸ë¦­ ê²€ì¦ (Epoch 1):")
                logger.info("  [Train]")
                for module_name, metrics in train_module_metrics.items():
                    logger.info(f"    - {module_name}: loss={metrics['loss']:.4f}, acc={metrics['accuracy']:.4f}")
                if val_metrics:
                    logger.info("  [Validation]")
                    for module_name, metrics in val_module_metrics.items():
                        logger.info(f"    - {module_name}: loss={metrics['loss']:.4f}, acc={metrics['accuracy']:.4f}")
            
            # Sweet Spot ì—…ë°ì´íŠ¸ (train/val ë¶„ë¦¬)
            if self.config.enable_sweet_spot:
                self.sweet_spot_detector.update(
                    epoch=epoch,
                    train_module_metrics=train_module_metrics,
                    val_module_metrics=val_module_metrics,
                    learning_rate=self.optimizer.param_groups[0]['lr']
                )
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            checkpoint_path = self.checkpoint_manager.save_checkpoint(
                epoch=epoch,
                model=self.model,
                optimizer=self.optimizer,
                scheduler=self.scheduler,
                metrics=all_metrics,
                lr=self.optimizer.param_groups[0]['lr']
            )
            
            # Crossover ì‹œìŠ¤í…œì— ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€
            if checkpoint_path and self.config.enable_crossover:
                self.crossover_system.add_checkpoint(
                    epoch=epoch,
                    checkpoint_path=checkpoint_path,
                    module_metrics=all_metrics
                )
            
            # ìµœê³  ì„±ëŠ¥ ê°±ì‹ 
            if 'loss' in all_metrics and all_metrics['loss'] < self.best_loss:
                self.best_loss = all_metrics['loss']
                logger.info(f"  ğŸ† ìµœê³  ì„±ëŠ¥ ê°±ì‹ : {self.best_loss:.4f}")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í™•ì¸ ë° ë§ˆì§€ë§‰ ì—í­ ê°•ì œ ì €ì¥
            if epoch == self.config.total_epochs:
                # ë§ˆì§€ë§‰ ì—í­ì€ ë¬´ì¡°ê±´ ì €ì¥
                if checkpoint_path is None:
                    logger.info("  ğŸ“Œ ë§ˆì§€ë§‰ ì—í­ ì²´í¬í¬ì¸íŠ¸ ê°•ì œ ì €ì¥...")
                    checkpoint_path = self.checkpoint_manager.save_checkpoint(
                        epoch=epoch,
                        model=self.model,
                        optimizer=self.optimizer,
                        scheduler=self.scheduler,
                        metrics=all_metrics,
                        lr=self.optimizer.param_groups[0]['lr']
                    )
                else:
                    logger.info(f"  âœ… ë§ˆì§€ë§‰ ì—í­ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨: {checkpoint_path}")
        
        logger.info("\n" + "=" * 70)
        logger.info(f"âœ… {self.config.total_epochs} ì—í­ í•™ìŠµ ì™„ë£Œ!")
        logger.info("=" * 70)
        
        # ìµœì¢… ì²˜ë¦¬
        self._finalize_training()
    
    def _finalize_training(self):
        """í•™ìŠµ ë§ˆë¬´ë¦¬ ì²˜ë¦¬"""
        logger.info("\nğŸ”§ ìµœì¢… ì²˜ë¦¬ ì‹œì‘...")
        
        # Sweet Spot ì¢…í•© ë¶„ì„ ì‹¤í–‰
        optimal_epochs = {}
        if self.config.enable_sweet_spot:
            logger.info("\nğŸ¯ Sweet Spot ì¢…í•© ë¶„ì„ ì‹œì‘...")
            try:
                # 5ê°€ì§€ ê³ ê¸‰ ë¶„ì„ ê¸°ë²• ì ìš©
                analysis_results = self.sweet_spot_detector.analyze_all(
                    output_dir='training/sweet_spot_analysis'
                )
                
                # analyze_allì˜ ì¶”ì²œ ì—í­ì„ ì§ì ‘ ì‚¬ìš©
                for module, result in analysis_results.items():
                    rec = result['recommendation']
                    optimal_epochs[module] = rec['epoch']
                    logger.info(f"    - {module}: Epoch {rec['epoch']} (ì‹ ë¢°ë„: {rec['confidence']:.1%})")
                
                logger.info(f"  ğŸ“Š ëª¨ë“ˆë³„ ìµœì  ì—í­: {optimal_epochs}")
                    
            except Exception as e:
                logger.error(f"Sweet Spot ë¶„ì„ ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ ë©”ì„œë“œ ì‚¬ìš© (fallback)
                optimal_epochs = self.sweet_spot_detector.get_optimal_epochs()
                logger.info(f"  ğŸ“Š ê¸°ë³¸ ë¶„ì„ ìµœì  ì—í­: {optimal_epochs}")
        
        # Parameter Crossover ì‹¤í–‰
        if self.config.enable_crossover and self.config.enable_sweet_spot:
            logger.info("\nğŸ§¬ Parameter Crossover ì‹¤í–‰...")
            
            crossover_model = self.crossover_system.perform_crossover(
                model=self.model,
                optimal_epochs=optimal_epochs
            )
            
            # Crossover ëª¨ë¸ ì €ì¥
            crossover_path = Path(self.config.checkpoint_dir) / "crossover_final.pth"
            self.crossover_system.save_crossover_result(
                model=crossover_model,
                save_path=str(crossover_path),
                metadata={'optimal_epochs': optimal_epochs}
            )
            logger.info(f"  ğŸ’¾ Crossover ëª¨ë¸ ì €ì¥: {crossover_path}")
        
        # í•™ìŠµ ê³¡ì„  ë‚´ë³´ë‚´ê¸°
        curves_file = self.checkpoint_manager.export_training_curves()
        logger.info(f"  ğŸ“ˆ í•™ìŠµ ê³¡ì„  ì €ì¥: {curves_file}")
        
        # OOM í†µê³„ ì €ì¥
        if self.config.enable_oom_handler:
            oom_stats = self.oom_handler.save_stats()
            logger.info(f"  ğŸ“Š OOM í†µê³„ ì €ì¥: {oom_stats}")
        
        # ìƒì„±ëœ ì„ë² ë”© ì €ì¥
        if hasattr(self.train_loader.dataset, 'save_embeddings'):
            logger.info("\nğŸ’¾ ìƒì„±ëœ ì„ë² ë”© ì €ì¥ ì¤‘...")
            self.train_loader.dataset.save_embeddings()
        if hasattr(self.val_loader.dataset, 'save_embeddings'):
            self.val_loader.dataset.save_embeddings()
        
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        logger.info("=" * 70)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Red Heart AI ìµœì¢… í†µí•© í•™ìŠµ")
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ')
    parser.add_argument('--epochs', type=int, default=60, help='í•™ìŠµ ì—í­')
    parser.add_argument('--batch-size', type=int, default=2, help='ë°°ì¹˜ ì‚¬ì´ì¦ˆ')
    parser.add_argument('--lr', type=float, default=1e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--resume', type=str, help='ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ')
    parser.add_argument('--no-param-update', action='store_true', help='íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê±´ë„ˆë›°ê¸° (ê²€ì¦ìš©)')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ë¡œê¹…')
    parser.add_argument('--samples', type=int, help='í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ìˆ˜ (ì—í­ ìˆ˜ë¡œ ì‚¬ìš©)')
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = UnifiedTrainingConfig()
    
    # argsì—ì„œ ì„¤ì • ì ìš©
    config.verbose = args.verbose
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    if args.test:
        if args.samples:
            config.total_epochs = args.samples
            logger.info(f"âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {args.samples} ì—í­ ì‹¤í–‰")
        else:
            config.total_epochs = 2
            logger.info("âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 2 ì—í­ë§Œ ì‹¤í–‰")
    else:
        config.total_epochs = args.epochs
    
    config.micro_batch_size = args.batch_size
    config.base_lr = args.lr
    
    # ë””ë²„ê·¸/ìƒì„¸ ë¡œê¹… ì„¤ì •
    if args.debug or args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        config.log_interval = 1  # ë§¤ ìŠ¤í…ë§ˆë‹¤ ë¡œê¹…
        config.val_interval = 10  # ë” ìì£¼ ê²€ì¦
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° ì‹¤í–‰
    trainer = UnifiedTrainer(config)
    trainer.no_param_update = args.no_param_update  # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í”Œë˜ê·¸
    
    if args.no_param_update:
        logger.warning("âš ï¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë¹„í™œì„±í™” - ê²€ì¦ ëª¨ë“œ")
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
    if args.resume:
        checkpoint = trainer.checkpoint_manager.load_checkpoint(args.resume)
        trainer.model.load_state_dict(checkpoint['model_state'])
        trainer.optimizer.load_state_dict(checkpoint['optimizer_state'])
        trainer.current_epoch = checkpoint['epoch']
        logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: Epoch {trainer.current_epoch}")
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train()


if __name__ == "__main__":
    main()