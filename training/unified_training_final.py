#!/usr/bin/env python3
"""
Red Heart AI ìµœì¢… í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ
730M íŒŒë¼ë¯¸í„° ëª¨ë¸ì˜ 60 ì—í­ í•™ìŠµ with Advanced Techniques
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import numpy as np
import logging
import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import gc
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸
from training.enhanced_checkpoint_manager import EnhancedCheckpointManager
from training.lr_sweep_optimizer import LRSweepOptimizer
from training.sweet_spot_detector import SweetSpotDetector
from training.parameter_crossover_system import ParameterCrossoverSystem
from training.oom_handler import OOMHandler
from training.advanced_training_techniques import AdvancedTrainingManager

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from config import ADVANCED_CONFIG, get_device
from data_loader import PreprocessedDataLoader

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('RedHeart.UnifiedTrainingFinal')


class UnifiedTrainingConfig:
    """í†µí•© í•™ìŠµ ì„¤ì •"""
    
    def __init__(self):
        # ëª¨ë¸ ì„¤ì • (730M íŒŒë¼ë¯¸í„°)
        self.model_params = 730_000_000
        self.hidden_dim = 1280
        self.num_layers = 18
        self.num_heads = 20
        
        # í•™ìŠµ ì„¤ì •
        self.total_epochs = 60
        self.micro_batch_size = 2  # ì•ˆì •ì„±ì„ ìœ„í•´ 2ë¡œ ì‹œì‘
        self.gradient_accumulation = 32  # ìœ íš¨ ë°°ì¹˜ = 64
        self.base_lr = 1e-4
        
        # LR ìŠ¤ìœ• ì„¤ì •
        self.lr_sweep_enabled = True
        self.lr_sweep_range = (1e-5, 1e-2)
        self.lr_sweep_points = 5
        
        # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
        self.checkpoint_interval = 2  # ì§ìˆ˜ ì—í­ë§ˆë‹¤ ì €ì¥ (30ê°œ)
        self.checkpoint_dir = "training/checkpoints_final"
        
        # Advanced Training
        self.enable_label_smoothing = True
        self.enable_rdrop = True
        self.enable_ema = True
        self.enable_llrd = True
        self.label_smoothing = 0.1
        self.rdrop_alpha = 1.0
        self.ema_decay = 0.999
        
        # OOM í•¸ë“¤ë§
        self.enable_oom_handler = True
        self.memory_threshold = 0.85
        self.min_batch_size = 1
        
        # Sweet Spot & Crossover
        self.enable_sweet_spot = True
        self.enable_crossover = True
        self.crossover_strategy = 'selective'
        
        # ë°ì´í„° ì„¤ì •
        self.data_dir = "for_learn_dataset"
        self.validation_split = 0.1
        self.num_workers = 4
        
        # ë¡œê¹…
        self.log_interval = 10
        self.val_interval = 100


class DummyModel(nn.Module):
    """í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ëª¨ë¸ (ì‹¤ì œ ëª¨ë¸ë¡œ êµì²´ í•„ìš”)"""
    
    def __init__(self, config: UnifiedTrainingConfig):
        super().__init__()
        self.config = config
        
        # ê°„ë‹¨í•œ íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°
        self.embedding = nn.Linear(1024, config.hidden_dim)
        self.layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=config.hidden_dim,
                nhead=config.num_heads,
                dim_feedforward=config.hidden_dim * 4,
                dropout=0.1
            ) for _ in range(config.num_layers)
        ])
        
        # ëª¨ë“ˆë³„ í—¤ë“œ
        self.emotion_head = nn.Linear(config.hidden_dim, 6)
        self.bentham_head = nn.Linear(config.hidden_dim, 100)
        self.regret_head = nn.Linear(config.hidden_dim, 10)
        self.surd_head = nn.Linear(config.hidden_dim, 4)
        
    def forward(self, x):
        # ì„ë² ë”©
        x = self.embedding(x)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´
        for layer in self.layers:
            x = layer(x)
        
        # í‰ê·  í’€ë§
        x = x.mean(dim=1)
        
        # í—¤ë“œ ì¶œë ¥
        outputs = {
            'emotion': self.emotion_head(x),
            'bentham': self.bentham_head(x),
            'regret': self.regret_head(x),
            'surd': self.surd_head(x)
        }
        
        return outputs


class UnifiedTrainer:
    """í†µí•© í•™ìŠµ ê´€ë¦¬ì"""
    
    def __init__(self, config: UnifiedTrainingConfig):
        self.config = config
        self.device = get_device()
        
        logger.info("=" * 70)
        logger.info("Red Heart AI ìµœì¢… í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info(f"  - ëª¨ë¸ í¬ê¸°: {config.model_params/1e6:.0f}M íŒŒë¼ë¯¸í„°")
        logger.info(f"  - ì´ ì—í­: {config.total_epochs}")
        logger.info(f"  - ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {config.micro_batch_size} (GA={config.gradient_accumulation})")
        logger.info(f"  - ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info("=" * 70)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_model()
        
        # ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
        self._initialize_dataloaders()
        
        # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
        self._initialize_optimizer()
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.global_step = 0
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.no_param_update = False  # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í”Œë˜ê·¸
        
    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì €
        self.checkpoint_manager = EnhancedCheckpointManager(
            checkpoint_dir=self.config.checkpoint_dir,
            max_checkpoints=30,
            save_interval=self.config.checkpoint_interval
        )
        
        # LR ìŠ¤ìœ• ì˜µí‹°ë§ˆì´ì €
        if self.config.lr_sweep_enabled:
            self.lr_sweep = LRSweepOptimizer(
                base_lr=self.config.base_lr,
                sweep_range=self.config.lr_sweep_range,
                num_sweep_points=self.config.lr_sweep_points
            )
        
        # Sweet Spot íƒì§€ê¸°
        if self.config.enable_sweet_spot:
            self.sweet_spot_detector = SweetSpotDetector(
                window_size=5,
                stability_threshold=0.01,
                patience=10
            )
        
        # Parameter Crossover
        if self.config.enable_crossover:
            self.crossover_system = ParameterCrossoverSystem(
                crossover_strategy=self.config.crossover_strategy,
                blend_ratio=0.7
            )
        
        # OOM í•¸ë“¤ëŸ¬
        if self.config.enable_oom_handler:
            self.oom_handler = OOMHandler(
                initial_batch_size=self.config.micro_batch_size,
                min_batch_size=self.config.min_batch_size,
                gradient_accumulation=self.config.gradient_accumulation,
                memory_threshold=self.config.memory_threshold
            )
        
        # Advanced Training Manager
        self.training_manager = AdvancedTrainingManager(
            enable_label_smoothing=self.config.enable_label_smoothing,
            enable_rdrop=self.config.enable_rdrop,
            enable_ema=self.config.enable_ema,
            enable_llrd=self.config.enable_llrd,
            label_smoothing=self.config.label_smoothing,
            rdrop_alpha=self.config.rdrop_alpha,
            ema_decay=self.config.ema_decay
        )
        
        logger.info("âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” unified_training_v2.pyì˜ ëª¨ë¸ ì‚¬ìš©
        self.model = DummyModel(self.config).to(self.device)
        
        # Advanced Training ì´ˆê¸°í™”
        self.training_manager.initialize(
            model=self.model,
            num_classes=6,
            base_lr=self.config.base_lr
        )
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {total_params/1e6:.1f}M íŒŒë¼ë¯¸í„°")
    
    def _initialize_dataloaders(self):
        """ë°ì´í„° ë¡œë” ì´ˆê¸°í™”"""
        # ë”ë¯¸ ë°ì´í„°ì…‹ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” PreprocessedDataLoader ì‚¬ìš©)
        class DummyDataset(Dataset):
            def __len__(self):
                return 10460  # ë¬¸ì„œ ê¸°ì¤€
            
            def __getitem__(self, idx):
                return {
                    'input': torch.randn(100, 1024),  # (seq_len, feature_dim)
                    'emotion_label': torch.tensor(np.random.randint(0, 6)),
                    'bentham_label': torch.randn(100),
                    'regret_label': torch.tensor(np.random.randint(0, 10)),
                    'surd_label': torch.tensor(np.random.randint(0, 4))
                }
        
        dataset = DummyDataset()
        val_size = int(len(dataset) * self.config.validation_split)
        train_size = len(dataset) - val_size
        
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.micro_batch_size,
            shuffle=True,
            num_workers=self.config.num_workers,
            pin_memory=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.micro_batch_size,
            shuffle=False,
            num_workers=self.config.num_workers,
            pin_memory=True
        )
        
        logger.info(f"âœ… ë°ì´í„° ë¡œë” ì´ˆê¸°í™”: Train={train_size}, Val={val_size}")
    
    def _initialize_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”"""
        # LLRD ì‚¬ìš© ì‹œ
        if self.config.enable_llrd:
            self.optimizer = self.training_manager.get_optimizer(
                self.model,
                lr=self.config.base_lr,
                weight_decay=0.01
            )
        else:
            self.optimizer = torch.optim.AdamW(
                self.model.parameters(),
                lr=self.config.base_lr,
                weight_decay=0.01
            )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        total_steps = len(self.train_loader) * self.config.total_epochs
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=total_steps // 10,
            T_mult=2,
            eta_min=self.config.base_lr * 0.01
        )
        
        logger.info("âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def run_lr_sweep(self):
        """LR ìŠ¤ìœ• ì‹¤í–‰"""
        if not self.config.lr_sweep_enabled:
            return
        
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ” Learning Rate Sweep ì‹œì‘...")
        logger.info("=" * 70)
        
        # ê°„ë‹¨í•œ ì†ì‹¤ í•¨ìˆ˜
        criterion = nn.CrossEntropyLoss()
        
        # ìŠ¤ìœ• ì‹¤í–‰
        sweep_results = self.lr_sweep.run_sweep(
            model=self.model,
            train_loader=self.train_loader,
            val_loader=self.val_loader,
            criterion=criterion,
            device=self.device
        )
        
        # ìµœì  LRë¡œ ì˜µí‹°ë§ˆì´ì € ì¬ì´ˆê¸°í™”
        self.config.base_lr = self.lr_sweep.best_lr
        self._initialize_optimizer()
        
        logger.info(f"âœ… ìµœì  LR ì„ íƒ: {self.config.base_lr:.1e}")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        epoch_losses = []
        module_metrics = {}
        
        for batch_idx, batch in enumerate(self.train_loader):
            # OOM í•¸ë“¤ë§
            if self.config.enable_oom_handler:
                self.oom_handler.log_memory_stats(self.global_step, 'train')
            
            # Forward pass
            try:
                loss, metrics = self._forward_step(batch)
            except RuntimeError as e:
                if 'out of memory' in str(e).lower():
                    if self.oom_handler.handle_oom(e):
                        # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì • í›„ ì¬ì‹œë„
                        self.train_loader = self.oom_handler.adjust_dataloader(self.train_loader)
                        continue
                    else:
                        raise
                else:
                    raise
            
            # Backward pass (Gradient Accumulation)
            loss = loss / self.config.gradient_accumulation
            loss.backward()
            
            # Gradient Accumulation
            if (batch_idx + 1) % self.config.gradient_accumulation == 0:
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # Optimizer step (íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í”Œë˜ê·¸ í™•ì¸)
                if not self.no_param_update:
                    self.optimizer.step()
                    self.scheduler.step()
                    
                    # EMA update
                    self.training_manager.step()
                else:
                    # ê²€ì¦ ëª¨ë“œ: ê·¸ë¼ë””ì–¸íŠ¸ë§Œ ê³„ì‚°í•˜ê³  ì—…ë°ì´íŠ¸ëŠ” ê±´ë„ˆëœ€
                    logger.debug("  [ê²€ì¦] íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€")
                
                self.optimizer.zero_grad()
                self.global_step += 1
            
            epoch_losses.append(loss.item() * self.config.gradient_accumulation)
            
            # ë¡œê¹…
            if batch_idx % self.config.log_interval == 0:
                avg_loss = np.mean(epoch_losses[-self.config.log_interval:])
                lr = self.optimizer.param_groups[0]['lr']
                logger.info(f"  [Epoch {epoch}][{batch_idx}/{len(self.train_loader)}] "
                          f"Loss: {avg_loss:.4f}, LR: {lr:.1e}")
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            for key, value in metrics.items():
                if key not in module_metrics:
                    module_metrics[key] = []
                module_metrics[key].append(value)
        
        # ì—í­ í‰ê· 
        avg_metrics = {k: np.mean(v) for k, v in module_metrics.items()}
        avg_metrics['loss'] = np.mean(epoch_losses)
        
        return avg_metrics
    
    def validate(self) -> Dict[str, float]:
        """ê²€ì¦"""
        self.model.eval()
        val_losses = []
        module_metrics = {}
        
        with torch.no_grad():
            # EMA ì ìš©
            if self.config.enable_ema:
                self.training_manager.apply_ema()
            
            for batch in self.val_loader:
                loss, metrics = self._forward_step(batch)
                val_losses.append(loss.item())
                
                for key, value in metrics.items():
                    if key not in module_metrics:
                        module_metrics[key] = []
                    module_metrics[key].append(value)
            
            # EMA ë³µì›
            if self.config.enable_ema:
                self.training_manager.restore_ema()
        
        # í‰ê· 
        avg_metrics = {k: np.mean(v) for k, v in module_metrics.items()}
        avg_metrics['val_loss'] = np.mean(val_losses)
        
        return avg_metrics
    
    def _forward_step(self, batch: Dict) -> Tuple[torch.Tensor, Dict]:
        """Forward step"""
        # ë°ì´í„° ì¤€ë¹„
        inputs = batch['input'].to(self.device)
        emotion_labels = batch['emotion_label'].to(self.device)
        
        # Forward pass
        outputs = self.model(inputs)
        
        # ì†ì‹¤ ê³„ì‚° (Advanced Training ì‚¬ìš©)
        loss = self.training_manager.compute_loss(
            model=self.model,
            inputs=inputs,
            labels=emotion_labels
        )
        
        # ëª¨ë“ˆë³„ ë©”íŠ¸ë¦­
        metrics = {
            'emotion_loss': loss.item(),
            'bentham_loss': 0.1,  # ë”ë¯¸
            'regret_loss': 0.1,   # ë”ë¯¸
            'surd_loss': 0.1      # ë”ë¯¸
        }
        
        return loss, metrics
    
    def train(self):
        """ì „ì²´ í•™ìŠµ ì‹¤í–‰"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸš€ í•™ìŠµ ì‹œì‘")
        logger.info("=" * 70)
        
        # LR ìŠ¤ìœ• ì‹¤í–‰
        self.run_lr_sweep()
        
        # 60 ì—í­ í•™ìŠµ
        for epoch in range(1, self.config.total_epochs + 1):
            self.current_epoch = epoch
            
            logger.info(f"\nğŸ“Œ Epoch {epoch}/{self.config.total_epochs}")
            
            # í•™ìŠµ
            train_metrics = self.train_epoch(epoch)
            
            # ê²€ì¦
            if epoch % 2 == 0:  # ì§ìˆ˜ ì—í­ë§ˆë‹¤
                val_metrics = self.validate()
                logger.info(f"  Validation Loss: {val_metrics['val_loss']:.4f}")
            else:
                val_metrics = {}
            
            # ë©”íŠ¸ë¦­ í†µí•©
            all_metrics = {**train_metrics, **val_metrics}
            
            # Sweet Spot ì—…ë°ì´íŠ¸
            if self.config.enable_sweet_spot:
                self.sweet_spot_detector.update(
                    epoch=epoch,
                    module_metrics=all_metrics,
                    learning_rate=self.optimizer.param_groups[0]['lr']
                )
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            checkpoint_path = self.checkpoint_manager.save_checkpoint(
                epoch=epoch,
                model=self.model,
                optimizer=self.optimizer,
                scheduler=self.scheduler,
                metrics=all_metrics,
                lr=self.optimizer.param_groups[0]['lr']
            )
            
            # Crossover ì‹œìŠ¤í…œì— ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€
            if checkpoint_path and self.config.enable_crossover:
                self.crossover_system.add_checkpoint(
                    epoch=epoch,
                    checkpoint_path=checkpoint_path,
                    module_metrics=all_metrics
                )
            
            # ìµœê³  ì„±ëŠ¥ ê°±ì‹ 
            if 'loss' in all_metrics and all_metrics['loss'] < self.best_loss:
                self.best_loss = all_metrics['loss']
                logger.info(f"  ğŸ† ìµœê³  ì„±ëŠ¥ ê°±ì‹ : {self.best_loss:.4f}")
        
        logger.info("\n" + "=" * 70)
        logger.info("âœ… 60 ì—í­ í•™ìŠµ ì™„ë£Œ!")
        logger.info("=" * 70)
        
        # ìµœì¢… ì²˜ë¦¬
        self._finalize_training()
    
    def _finalize_training(self):
        """í•™ìŠµ ë§ˆë¬´ë¦¬ ì²˜ë¦¬"""
        logger.info("\nğŸ”§ ìµœì¢… ì²˜ë¦¬ ì‹œì‘...")
        
        # Sweet Spot ë¶„ì„ ê²°ê³¼ ì €ì¥
        if self.config.enable_sweet_spot:
            optimal_epochs = self.sweet_spot_detector.get_optimal_epochs()
            logger.info(f"  ğŸ¯ ëª¨ë“ˆë³„ ìµœì  ì—í­: {optimal_epochs}")
            
            analysis_results = self.sweet_spot_detector.export_analysis()
            logger.info(f"  ğŸ“Š Sweet Spot ë¶„ì„ ì €ì¥: {analysis_results['json_file']}")
        
        # Parameter Crossover ì‹¤í–‰
        if self.config.enable_crossover and self.config.enable_sweet_spot:
            logger.info("\nğŸ§¬ Parameter Crossover ì‹¤í–‰...")
            
            crossover_model = self.crossover_system.perform_crossover(
                model=self.model,
                optimal_epochs=optimal_epochs
            )
            
            # Crossover ëª¨ë¸ ì €ì¥
            crossover_path = Path(self.config.checkpoint_dir) / "crossover_final.pth"
            self.crossover_system.save_crossover_result(
                model=crossover_model,
                save_path=str(crossover_path),
                metadata={'optimal_epochs': optimal_epochs}
            )
            logger.info(f"  ğŸ’¾ Crossover ëª¨ë¸ ì €ì¥: {crossover_path}")
        
        # í•™ìŠµ ê³¡ì„  ë‚´ë³´ë‚´ê¸°
        curves_file = self.checkpoint_manager.export_training_curves()
        logger.info(f"  ğŸ“ˆ í•™ìŠµ ê³¡ì„  ì €ì¥: {curves_file}")
        
        # OOM í†µê³„ ì €ì¥
        if self.config.enable_oom_handler:
            oom_stats = self.oom_handler.save_stats()
            logger.info(f"  ğŸ“Š OOM í†µê³„ ì €ì¥: {oom_stats}")
        
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        logger.info("=" * 70)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Red Heart AI ìµœì¢… í†µí•© í•™ìŠµ")
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ')
    parser.add_argument('--epochs', type=int, default=60, help='í•™ìŠµ ì—í­')
    parser.add_argument('--batch-size', type=int, default=2, help='ë°°ì¹˜ ì‚¬ì´ì¦ˆ')
    parser.add_argument('--lr', type=float, default=1e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--resume', type=str, help='ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ')
    parser.add_argument('--no-param-update', action='store_true', help='íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê±´ë„ˆë›°ê¸° (ê²€ì¦ìš©)')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ë¡œê¹…')
    parser.add_argument('--samples', type=int, help='í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ìˆ˜ (ì—í­ ìˆ˜ë¡œ ì‚¬ìš©)')
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = UnifiedTrainingConfig()
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    if args.test:
        if args.samples:
            config.total_epochs = args.samples
            logger.info(f"âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {args.samples} ì—í­ ì‹¤í–‰")
        else:
            config.total_epochs = 2
            logger.info("âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 2 ì—í­ë§Œ ì‹¤í–‰")
    else:
        config.total_epochs = args.epochs
    
    config.micro_batch_size = args.batch_size
    config.base_lr = args.lr
    
    # ë””ë²„ê·¸/ìƒì„¸ ë¡œê¹… ì„¤ì •
    if args.debug or args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        config.log_interval = 1  # ë§¤ ìŠ¤í…ë§ˆë‹¤ ë¡œê¹…
        config.val_interval = 10  # ë” ìì£¼ ê²€ì¦
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° ì‹¤í–‰
    trainer = UnifiedTrainer(config)
    trainer.no_param_update = args.no_param_update  # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í”Œë˜ê·¸
    
    if args.no_param_update:
        logger.warning("âš ï¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë¹„í™œì„±í™” - ê²€ì¦ ëª¨ë“œ")
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
    if args.resume:
        checkpoint = trainer.checkpoint_manager.load_checkpoint(args.resume)
        trainer.model.load_state_dict(checkpoint['model_state'])
        trainer.optimizer.load_state_dict(checkpoint['optimizer_state'])
        trainer.current_epoch = checkpoint['epoch']
        logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: Epoch {trainer.current_epoch}")
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train()


if __name__ == "__main__":
    main()