#!/usr/bin/env python3
"""
Red Heart AI ìµœì¢… í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ
730M íŒŒë¼ë¯¸í„° ëª¨ë¸ì˜ 60 ì—í­ í•™ìŠµ with Advanced Techniques
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import numpy as np
import logging
import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import gc
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸
from training.enhanced_checkpoint_manager import EnhancedCheckpointManager
from training.lr_sweep_optimizer import LRSweepOptimizer
from training.sweet_spot_detector import SweetSpotDetector
from training.parameter_crossover_system import ParameterCrossoverSystem
from training.oom_handler import OOMHandler
from training.advanced_training_techniques import AdvancedTrainingManager

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from config import ADVANCED_CONFIG, get_device
from data_loader import PreprocessedDataLoader

# ì‹¤ì œ ëª¨ë¸ ëª¨ë“ˆë“¤
from unified_backbone import RedHeartUnifiedBackbone
from unified_heads import EmotionHead, BenthamHead, RegretHead, SURDHead
from analyzer_neural_modules import create_neural_analyzers
from advanced_analyzer_wrappers import create_advanced_analyzer_wrappers
from phase_neural_networks import Phase0ProjectionNet, Phase2CommunityNet, HierarchicalEmotionIntegrator
try:
    from emotion_dsp_simulator import EmotionDSPSimulator, DynamicKalmanFilter
except ImportError:
    EmotionDSPSimulator = None
    DynamicKalmanFilter = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('RedHeart.UnifiedTrainingFinal')


class UnifiedTrainingConfig:
    """í†µí•© í•™ìŠµ ì„¤ì •"""
    
    def __init__(self):
        # ëª¨ë¸ ì„¤ì • (730M íŒŒë¼ë¯¸í„°)
        self.model_params = 730_000_000
        self.hidden_dim = 1280
        self.num_layers = 18
        self.num_heads = 20
        
        # í•™ìŠµ ì„¤ì •
        self.total_epochs = 60
        self.micro_batch_size = 2  # ì•ˆì •ì„±ì„ ìœ„í•´ 2ë¡œ ì‹œì‘
        self.gradient_accumulation = 32  # ìœ íš¨ ë°°ì¹˜ = 64
        self.base_lr = 1e-4
        
        # LR ìŠ¤ìœ• ì„¤ì •
        self.lr_sweep_enabled = True
        self.lr_sweep_range = (1e-5, 1e-2)
        self.lr_sweep_points = 5
        
        # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
        self.checkpoint_interval = 2  # ì§ìˆ˜ ì—í­ë§ˆë‹¤ ì €ì¥ (30ê°œ)
        self.checkpoint_dir = "training/checkpoints_final"
        
        # Advanced Training
        self.enable_label_smoothing = True
        self.enable_rdrop = True
        self.enable_ema = True
        self.enable_llrd = True
        self.label_smoothing = 0.1
        self.rdrop_alpha = 1.0
        self.ema_decay = 0.999
        
        # OOM í•¸ë“¤ë§
        self.enable_oom_handler = True
        self.memory_threshold = 0.85
        self.min_batch_size = 1
        
        # Sweet Spot & Crossover
        self.enable_sweet_spot = True
        self.enable_crossover = True
        self.crossover_strategy = 'selective'
        
        # ë°ì´í„° ì„¤ì •
        self.data_dir = "for_learn_dataset"
        self.validation_split = 0.1
        self.num_workers = 4
        
        # ë¡œê¹…
        self.log_interval = 10
        self.verbose = False  # ìƒì„¸ ì¶œë ¥ ì„¤ì •
        self.val_interval = 100


class UnifiedModel(nn.Module):
    """Red Heart AI 730M í†µí•© ëª¨ë¸"""
    
    def __init__(self, config: UnifiedTrainingConfig):
        super().__init__()
        self.config = config
        
        # ë°±ë³¸ ì„¤ì •
        backbone_config = {
            'input_dim': 768,
            'd_model': 896,
            'num_layers': 8,
            'num_heads': 14,
            'feedforward_dim': 3584,
            'dropout': 0.1,
            'task_dim': 896
        }
        
        # ë°±ë³¸ ì´ˆê¸°í™” (90.6M)
        self.backbone = RedHeartUnifiedBackbone(backbone_config)
        
        # í—¤ë“œ ì´ˆê¸°í™” (153M)
        self.emotion_head = EmotionHead(input_dim=896)
        self.bentham_head = BenthamHead(input_dim=896)
        self.regret_head = RegretHead(input_dim=896) 
        self.surd_head = SURDHead(input_dim=896)
        
        # ì‹ ê²½ë§ ë¶„ì„ê¸° (368M)
        self.neural_analyzers = create_neural_analyzers(input_dim=896)
        
        # Advanced ë¶„ì„ê¸° ë˜í¼ (112M) - translator ì´ˆê¸°í™” í›„ ìƒì„±
        self.advanced_wrappers = None  # ë‚˜ì¤‘ì— ì´ˆê¸°í™”
        
        # Phase ë„¤íŠ¸ì›Œí¬ (4.3M)
        self.phase0_net = Phase0ProjectionNet()
        self.phase2_net = Phase2CommunityNet()
        self.hierarchical_integrator = HierarchicalEmotionIntegrator()
        
        # DSP & ì¹¼ë§Œ í•„í„° (2.3M)
        if EmotionDSPSimulator is not None:
            self.dsp_simulator = EmotionDSPSimulator({'hidden_dim': 384})
            self.kalman_filter = DynamicKalmanFilter(state_dim=7)
        else:
            self.dsp_simulator = None
            self.kalman_filter = None
        
    def forward(self, x, task='emotion'):
        """ìˆœì „íŒŒ - GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
        
        Args:
            x: ì…ë ¥ í…ì„œ
            task: í˜„ì¬ í•™ìŠµ ì¤‘ì¸ íƒœìŠ¤í¬
            
        Returns:
            í•´ë‹¹ íƒœìŠ¤í¬ì˜ ì¶œë ¥ í…ì„œ (dictê°€ ì•„ë‹Œ tensor)
        """
        # ì…ë ¥ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (í•„ìš”ì‹œ)
        if x.device != self.backbone.parameters().__next__().device:
            x = x.to(self.backbone.parameters().__next__().device)
        
        # ë°±ë³¸ ì²˜ë¦¬ (ì´ë¯¸ GPUì— ìˆìŒ)
        backbone_outputs = self.backbone(x, task=task)
        
        # íƒœìŠ¤í¬ë³„ íŠ¹ì§• ì¶”ì¶œ
        if task in backbone_outputs:
            features = backbone_outputs[task]
        else:
            # ëª¨ë“  íƒœìŠ¤í¬ ì¶œë ¥ì˜ í‰ê·  ì‚¬ìš©
            features = torch.stack(list(backbone_outputs.values())).mean(dim=0)
        
        # íƒœìŠ¤í¬ë³„ í—¤ë“œ ì ìš© (ëª¨ë‘ GPUì— ìˆìŒ)
        if task == 'emotion':
            # EmotionHeadê°€ dictë¥¼ ë°˜í™˜í•˜ë©´ 'emotions' í‚¤ ì¶”ì¶œ
            output = self.emotion_head(features)
            if isinstance(output, dict):
                output = output.get('emotions', output.get('emotion_logits', list(output.values())[0]))
        elif task == 'bentham':
            output = self.bentham_head(features)
            if isinstance(output, dict):
                output = output.get('bentham_scores', list(output.values())[0])
        elif task == 'regret':
            output = self.regret_head(features)
            if isinstance(output, dict):
                output = output.get('regret_score', list(output.values())[0])
        elif task == 'surd':
            output = self.surd_head(features)
            if isinstance(output, dict):
                output = output.get('surd_values', output.get('surd_scores', list(output.values())[0]))
        else:
            # ê¸°ë³¸ê°’: emotion
            output = self.emotion_head(features)
            if isinstance(output, dict):
                output = output.get('emotions', output.get('emotion_logits', list(output.values())[0]))
        
        return output


class UnifiedTrainer:
    """í†µí•© í•™ìŠµ ê´€ë¦¬ì"""
    
    def __init__(self, config: UnifiedTrainingConfig):
        self.config = config
        self.device = get_device()
        self.verbose = config.verbose  # V2ì™€ ë™ì¼í•˜ê²Œ verbose ì„¤ì •
        
        logger.info("=" * 70)
        logger.info("Red Heart AI ìµœì¢… í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info(f"  - ëª¨ë¸ í¬ê¸°: {config.model_params/1e6:.0f}M íŒŒë¼ë¯¸í„°")
        logger.info(f"  - ì´ ì—í­: {config.total_epochs}")
        logger.info(f"  - ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {config.micro_batch_size} (GA={config.gradient_accumulation})")
        logger.info(f"  - ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info("=" * 70)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_model()
        
        # ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
        self._initialize_dataloaders()
        
        # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
        self._initialize_optimizer()
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.global_step = 0
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.no_param_update = False  # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í”Œë˜ê·¸
        
    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì €
        self.checkpoint_manager = EnhancedCheckpointManager(
            checkpoint_dir=self.config.checkpoint_dir,
            max_checkpoints=30,
            save_interval=self.config.checkpoint_interval
        )
        
        # LR ìŠ¤ìœ• ì˜µí‹°ë§ˆì´ì €
        if self.config.lr_sweep_enabled:
            self.lr_sweep = LRSweepOptimizer(
                base_lr=self.config.base_lr,
                sweep_range=self.config.lr_sweep_range,
                num_sweep_points=self.config.lr_sweep_points
            )
        
        # Sweet Spot íƒì§€ê¸°
        if self.config.enable_sweet_spot:
            self.sweet_spot_detector = SweetSpotDetector(
                window_size=5,
                stability_threshold=0.01,
                patience=10
            )
        
        # Parameter Crossover
        if self.config.enable_crossover:
            self.crossover_system = ParameterCrossoverSystem(
                crossover_strategy=self.config.crossover_strategy,
                blend_ratio=0.7
            )
        
        # OOM í•¸ë“¤ëŸ¬
        if self.config.enable_oom_handler:
            self.oom_handler = OOMHandler(
                initial_batch_size=self.config.micro_batch_size,
                min_batch_size=self.config.min_batch_size,
                gradient_accumulation=self.config.gradient_accumulation,
                memory_threshold=self.config.memory_threshold
            )
        
        # Advanced Training Manager
        self.training_manager = AdvancedTrainingManager(
            enable_label_smoothing=self.config.enable_label_smoothing,
            enable_rdrop=self.config.enable_rdrop,
            enable_ema=self.config.enable_ema,
            enable_llrd=self.config.enable_llrd,
            label_smoothing=self.config.label_smoothing,
            rdrop_alpha=self.config.rdrop_alpha,
            ema_decay=self.config.ema_decay
        )
        
        logger.info("âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™” - v2 ë°©ì‹ ì°¨ìš© (ìˆœì°¨ì  GPU ë¡œë“œ)"""
        logger.info("ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘ (ìˆœì°¨ì  GPU ë¡œë“œ ë°©ì‹)...")
        
        # ì‹¤ì œ 730M ëª¨ë¸ ì´ˆê¸°í™” (CPUì—ì„œ ìƒì„±)
        self.model = UnifiedModel(self.config)
        
        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        if self.device.type == 'cuda':
            gpu_mem_before = torch.cuda.memory_allocated() / (1024**3)
            logger.info(f"  ì´ˆê¸° GPU ë©”ëª¨ë¦¬: {gpu_mem_before:.2f}GB")
        
        # ìˆœì°¨ì ìœ¼ë¡œ ì»´í¬ë„ŒíŠ¸ë¥¼ GPUë¡œ ì´ë™ (7GBê¹Œì§€ í™œìš©)
        # 1. ë°±ë³¸ (90.6M) - í•­ìƒ GPU
        self.model.backbone = self.model.backbone.to(self.device)
        logger.info(f"  âœ… ë°±ë³¸ GPU ë¡œë“œ (90.6M)")
        
        # 2. ëª¨ë“  í—¤ë“œ (153M) - GPUì— ìœ ì§€ (ì´ì „ì—” í•„ìš”ì‹œë§Œ ë¡œë“œ)
        self.model.emotion_head = self.model.emotion_head.to(self.device)
        logger.info(f"  âœ… ê°ì • í—¤ë“œ GPU ë¡œë“œ (38.3M)")
        
        self.model.bentham_head = self.model.bentham_head.to(self.device)
        logger.info(f"  âœ… ë²¤ë‹´ í—¤ë“œ GPU ë¡œë“œ (38.3M)")
        
        self.model.regret_head = self.model.regret_head.to(self.device)
        logger.info(f"  âœ… í›„íšŒ í—¤ë“œ GPU ë¡œë“œ (38.3M)")
        
        self.model.surd_head = self.model.surd_head.to(self.device)
        logger.info(f"  âœ… SURD í—¤ë“œ GPU ë¡œë“œ (38.3M)")
        
        # 3. Translator ëª¨ë“ˆ ì´ˆê¸°í™” (Advanced ë¶„ì„ê¸° ì˜ì¡´ì„±)
        logger.info("  ğŸ”„ Translator ëª¨ë“ˆ ì´ˆê¸°í™” ì¤‘...")
        try:
            from config import get_system_module, register_system_module
            existing_translator = get_system_module('translator')
            if existing_translator is None:
                from local_translator import LocalTranslator
                translator = LocalTranslator()
                register_system_module('translator', translator)
                logger.info("  âœ… LocalTranslator ì´ˆê¸°í™” ë° ì „ì—­ ë“±ë¡ ì™„ë£Œ")
            else:
                logger.info("  â„¹ï¸ Translatorê°€ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        except Exception as e:
            logger.error(f"  âŒ Translator ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.warning("     Advanced Emotion Wrapperê°€ ì œí•œë©ë‹ˆë‹¤")
        
        # Translator ì´ˆê¸°í™” í›„ Advanced Wrappers ìƒì„±
        if self.model.advanced_wrappers is None:
            logger.info("  ğŸ”§ Advanced Wrappers ìƒì„± ì¤‘...")
            from advanced_analyzer_wrappers import create_advanced_analyzer_wrappers
            self.model.advanced_wrappers = create_advanced_analyzer_wrappers()
            
            # Advanced Wrappers íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
            wrapper_params = 0
            if self.model.advanced_wrappers:
                for name, wrapper in self.model.advanced_wrappers.items():
                    if hasattr(wrapper, 'parameters'):
                        params = sum(p.numel() for p in wrapper.parameters())
                        wrapper_params += params
            logger.info(f"  âœ… Advanced Wrappers ìƒì„± ì™„ë£Œ ({wrapper_params/1e6:.1f}M)")
        
        # 4. Neural Analyzers (368M) - GPU ì—¬ìœ  ìˆìœ¼ë©´ ë¡œë“œ  
        if hasattr(self.model, 'neural_analyzers') and self.model.neural_analyzers:
            try:
                for name, analyzer in self.model.neural_analyzers.items():
                    analyzer.to(self.device)
                    logger.info(f"  âœ… {name} ë¶„ì„ê¸° GPU ë¡œë“œ")
            except RuntimeError as e:
                if 'out of memory' in str(e).lower():
                    logger.warning(f"  âš ï¸ Neural AnalyzersëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ CPU ìœ ì§€")
                else:
                    raise
        
        # 5. Advanced Wrappers (112M) - GPU ì—¬ìœ  ìˆìœ¼ë©´ ë¡œë“œ
        if hasattr(self.model, 'advanced_wrappers') and self.model.advanced_wrappers:
            try:
                for name, wrapper in self.model.advanced_wrappers.items():
                    wrapper.to(self.device)
                    logger.info(f"  âœ… {name} Wrapper GPU ë¡œë“œ")
            except RuntimeError as e:
                if 'out of memory' in str(e).lower():
                    logger.warning(f"  âš ï¸ Advanced WrappersëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ CPU ìœ ì§€")
                else:
                    raise
        
        # 6. Phase Networks (4.3M) - ì‘ìœ¼ë‹ˆê¹Œ GPUë¡œ
        if hasattr(self.model, 'phase0_net') and self.model.phase0_net:
            self.model.phase0_net = self.model.phase0_net.to(self.device)
            logger.info(f"  âœ… Phase0 ë„¤íŠ¸ì›Œí¬ GPU ë¡œë“œ")
        
        if hasattr(self.model, 'phase2_net') and self.model.phase2_net:
            self.model.phase2_net = self.model.phase2_net.to(self.device)
            logger.info(f"  âœ… Phase2 ë„¤íŠ¸ì›Œí¬ GPU ë¡œë“œ")
        
        if hasattr(self.model, 'hierarchical_integrator') and self.model.hierarchical_integrator:
            self.model.hierarchical_integrator = self.model.hierarchical_integrator.to(self.device)
            logger.info(f"  âœ… Hierarchical Integrator GPU ë¡œë“œ")
        
        # 7. DSP & Kalman (2.3M) - ì‘ìœ¼ë‹ˆê¹Œ GPUë¡œ
        if hasattr(self.model, 'dsp_simulator') and self.model.dsp_simulator:
            self.model.dsp_simulator = self.model.dsp_simulator.to(self.device)
            logger.info(f"  âœ… DSP Simulator GPU ë¡œë“œ")
        
        if hasattr(self.model, 'kalman_filter') and self.model.kalman_filter:
            self.model.kalman_filter = self.model.kalman_filter.to(self.device)
            logger.info(f"  âœ… Kalman Filter GPU ë¡œë“œ")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if self.device.type == 'cuda':
            gpu_mem_after = torch.cuda.memory_allocated() / (1024**3)
            logger.info(f"  ìµœì¢… GPU ë©”ëª¨ë¦¬: {gpu_mem_after:.2f}GB (ì¦ê°€: {gpu_mem_after - gpu_mem_before:.2f}GB)")
            
            # ì „ì²´ GPU ë©”ëª¨ë¦¬ ì •ë³´
            total_gpu = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            logger.info(f"  GPU ì‚¬ìš©ë¥ : {gpu_mem_after/total_gpu*100:.1f}% / {total_gpu:.1f}GB")
        
        # Advanced Training ì´ˆê¸°í™”
        self.training_manager.initialize(
            model=self.model,
            num_classes=6,
            base_lr=self.config.base_lr
        )
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸ (v2ì²˜ëŸ¼ ê° ì»´í¬ë„ŒíŠ¸ë³„ë¡œ ê³„ì‚°)
        total_params = 0
        
        # ë°±ë³¸
        backbone_params = sum(p.numel() for p in self.model.backbone.parameters())
        total_params += backbone_params
        logger.info(f"  ë°±ë³¸: {backbone_params/1e6:.1f}M")
        
        # í—¤ë“œë“¤
        head_params = 0
        for name in ['emotion_head', 'bentham_head', 'regret_head', 'surd_head']:
            if hasattr(self.model, name):
                head = getattr(self.model, name)
                params = sum(p.numel() for p in head.parameters())
                head_params += params
                logger.info(f"  {name}: {params/1e6:.1f}M")
        total_params += head_params
        
        # Neural Analyzers
        if hasattr(self.model, 'neural_analyzers') and self.model.neural_analyzers:
            analyzer_params = 0
            for name, analyzer in self.model.neural_analyzers.items():
                params = sum(p.numel() for p in analyzer.parameters())
                analyzer_params += params
            total_params += analyzer_params
            logger.info(f"  Neural Analyzers: {analyzer_params/1e6:.1f}M")
        
        # Advanced Wrappers
        if hasattr(self.model, 'advanced_wrappers') and self.model.advanced_wrappers:
            wrapper_params = 0
            for name, wrapper in self.model.advanced_wrappers.items():
                if hasattr(wrapper, 'parameters'):
                    params = sum(p.numel() for p in wrapper.parameters())
                    wrapper_params += params
            total_params += wrapper_params
            logger.info(f"  Advanced Wrappers: {wrapper_params/1e6:.1f}M")
        
        # Phase Networks
        phase_params = 0
        for name in ['phase0_net', 'phase2_net', 'hierarchical_integrator']:
            if hasattr(self.model, name) and getattr(self.model, name) is not None:
                net = getattr(self.model, name)
                params = sum(p.numel() for p in net.parameters())
                phase_params += params
        if phase_params > 0:
            total_params += phase_params
            logger.info(f"  Phase Networks: {phase_params/1e6:.1f}M")
        
        # DSP & Kalman
        dsp_kalman_params = 0
        if hasattr(self.model, 'dsp_simulator') and self.model.dsp_simulator:
            dsp_kalman_params += sum(p.numel() for p in self.model.dsp_simulator.parameters())
        if hasattr(self.model, 'kalman_filter') and self.model.kalman_filter:
            dsp_kalman_params += sum(p.numel() for p in self.model.kalman_filter.parameters())
        if dsp_kalman_params > 0:
            total_params += dsp_kalman_params
            logger.info(f"  DSP & Kalman: {dsp_kalman_params/1e6:.1f}M")
        
        logger.info(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: ì´ {total_params/1e6:.1f}M íŒŒë¼ë¯¸í„°")
    
    def _initialize_dataloaders(self):
        """ë°ì´í„° ë¡œë” ì´ˆê¸°í™”"""
        # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
        preprocessed_path = Path("claude_api_preprocessing/claude_preprocessed_complete.json")
        
        if not preprocessed_path.exists():
            # ëŒ€ì²´ ê²½ë¡œ ì‹œë„
            preprocessed_path = Path("for_learn_dataset/claude_preprocessed_complete.json")
            if not preprocessed_path.exists():
                logger.error(f"ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {preprocessed_path}")
                raise FileNotFoundError(f"ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # JSON ë°ì´í„° ë¡œë“œ
        with open(preprocessed_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œ)
        if hasattr(self.config, 'max_samples') and self.config.max_samples:
            data = data[:self.config.max_samples]
        
        # ë°ì´í„°ì…‹ í´ë˜ìŠ¤
        class RedHeartDataset(Dataset):
            def __init__(self, data_list):
                self.data = data_list
                # label ë§¤í•‘ (v2ì—ì„œ ì²˜ëŸ¼ TargetMapper ëŒ€ì‹  ì§ì ‘ ì²˜ë¦¬)
                self.label_to_idx = {
                    'AUTHOR': 0,
                    'EVERYBODY': 1,
                    'INFO': 2,
                    'NOBODY': 3,
                    'OTHER': 4
                }
                # ê°ì • ë§¤í•‘ (emotions dictì—ì„œ ì¶”ì¶œ)
                self.emotion_keys = ['joy', 'anger', 'surprise', 'disgust', 'sadness', 'shame', 'fear']
                
            def __len__(self):
                return len(self.data)
            
            def __getitem__(self, idx):
                item = self.data[idx]
                # í…ì„œ ë³€í™˜
                text_embedding = torch.randn(100, 768)  # ì‹¤ì œë¡œëŠ” í…ìŠ¤íŠ¸ ì„ë² ë”© ì‚¬ìš©
                
                # label ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜
                label_str = item.get('label', 'OTHER')
                label_idx = self.label_to_idx.get(label_str, 4)  # ê¸°ë³¸ê°’ 4 (OTHER)
                
                # emotions dictì—ì„œ ê°ì • ë²¡í„° ì¶”ì¶œ
                emotions = item.get('emotions', {})
                if isinstance(emotions, dict):
                    # 7ê°œ ê¸°ë³¸ ê°ì • ì¶”ì¶œ
                    emotion_vector = [emotions.get(key, 0.0) for key in self.emotion_keys]
                    # ê°€ì¥ ë†’ì€ ê°’ì˜ ì¸ë±ìŠ¤ë¥¼ ë¼ë²¨ë¡œ
                    emotion_label = torch.argmax(torch.tensor(emotion_vector)).item()
                else:
                    emotion_label = 0  # ê¸°ë³¸ê°’
                
                # bentham_scores ì²˜ë¦¬ (dict -> 10ì°¨ì› ë²¡í„°)
                bentham_keys = [
                    'intensity', 'duration', 'certainty', 'propinquity',
                    'purity', 'extent', 'fecundity', 'remoteness', 
                    'succession', 'utility'
                ]
                
                bentham_scores = item.get('bentham_scores', {})
                if isinstance(bentham_scores, dict):
                    # dictì—ì„œ ê°’ ì¶”ì¶œ (ì—†ìœ¼ë©´ 0.5 ê¸°ë³¸ê°’)
                    bentham_vector = [bentham_scores.get(key, 0.5) for key in bentham_keys]
                else:
                    # dictê°€ ì•„ë‹ˆë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                    bentham_vector = [0.5] * 10
                
                return {
                    'input': text_embedding,
                    'emotion_label': torch.tensor(emotion_label, dtype=torch.long),
                    'bentham_label': torch.tensor(bentham_vector, dtype=torch.float),
                    'regret_label': torch.tensor(item.get('regret_factor', 0.0), dtype=torch.float),
                    'surd_label': torch.tensor(label_idx, dtype=torch.long)  # labelì„ SURDì—ë„ ì‚¬ìš©
                }
        
        # ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
        dataset = RedHeartDataset(data)
        val_size = int(len(dataset) * self.config.validation_split)
        train_size = len(dataset) - val_size
        
        # í•™ìŠµ/ê²€ì¦ ë¶„í•   
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.micro_batch_size,
            shuffle=True,
            num_workers=self.config.num_workers,
            pin_memory=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.micro_batch_size,
            shuffle=False,
            num_workers=self.config.num_workers,
            pin_memory=True
        )
        
        logger.info(f"âœ… ë°ì´í„° ë¡œë” ì´ˆê¸°í™”: Train={train_size}, Val={val_size}")
    
    def _initialize_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”"""
        # LLRD ì‚¬ìš© ì‹œ
        if self.config.enable_llrd:
            self.optimizer = self.training_manager.get_optimizer(
                self.model,
                lr=self.config.base_lr,
                weight_decay=0.01
            )
        else:
            self.optimizer = torch.optim.AdamW(
                self.model.parameters(),
                lr=self.config.base_lr,
                weight_decay=0.01
            )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        total_steps = len(self.train_loader) * self.config.total_epochs
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=total_steps // 10,
            T_mult=2,
            eta_min=self.config.base_lr * 0.01
        )
        
        logger.info("âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def run_lr_sweep(self):
        """LR ìŠ¤ìœ• ì‹¤í–‰"""
        if not self.config.lr_sweep_enabled:
            return
        
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ” Learning Rate Sweep ì‹œì‘...")
        logger.info("=" * 70)
        
        # ê°„ë‹¨í•œ ì†ì‹¤ í•¨ìˆ˜
        criterion = nn.CrossEntropyLoss()
        
        # ìŠ¤ìœ• ì‹¤í–‰
        sweep_results = self.lr_sweep.run_sweep(
            model=self.model,
            train_loader=self.train_loader,
            val_loader=self.val_loader,
            criterion=criterion,
            device=self.device
        )
        
        # ìµœì  LRë¡œ ì˜µí‹°ë§ˆì´ì € ì¬ì´ˆê¸°í™”
        self.config.base_lr = self.lr_sweep.best_lr
        self._initialize_optimizer()
        
        logger.info(f"âœ… ìµœì  LR ì„ íƒ: {self.config.base_lr:.1e}")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        epoch_losses = []
        module_metrics = {}
        
        for batch_idx, batch in enumerate(self.train_loader):
            # OOM í•¸ë“¤ë§
            if self.config.enable_oom_handler:
                self.oom_handler.log_memory_stats(self.global_step, 'train')
            
            # Forward pass
            try:
                loss, metrics = self._forward_step(batch)
            except RuntimeError as e:
                if 'out of memory' in str(e).lower():
                    if self.oom_handler.handle_oom(e):
                        # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì • í›„ ì¬ì‹œë„
                        self.train_loader = self.oom_handler.adjust_dataloader(self.train_loader)
                        continue
                    else:
                        raise
                else:
                    raise
            
            # Backward pass (Gradient Accumulation)
            loss = loss / self.config.gradient_accumulation
            loss.backward()
            
            # Gradient Accumulation
            if (batch_idx + 1) % self.config.gradient_accumulation == 0:
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # Optimizer step (íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í”Œë˜ê·¸ í™•ì¸)
                if not self.no_param_update:
                    self.optimizer.step()
                    self.scheduler.step()
                    
                    # EMA update
                    self.training_manager.step()
                else:
                    # ê²€ì¦ ëª¨ë“œ: ê·¸ë¼ë””ì–¸íŠ¸ë§Œ ê³„ì‚°í•˜ê³  ì—…ë°ì´íŠ¸ëŠ” ê±´ë„ˆëœ€
                    logger.debug("  [ê²€ì¦] íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€")
                
                self.optimizer.zero_grad()
                self.global_step += 1
            
            epoch_losses.append(loss.item() * self.config.gradient_accumulation)
            
            # ë¡œê¹…
            if batch_idx % self.config.log_interval == 0:
                avg_loss = np.mean(epoch_losses[-self.config.log_interval:])
                lr = self.optimizer.param_groups[0]['lr']
                logger.info(f"  [Epoch {epoch}][{batch_idx}/{len(self.train_loader)}] "
                          f"Loss: {avg_loss:.4f}, LR: {lr:.1e}")
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            for key, value in metrics.items():
                if key not in module_metrics:
                    module_metrics[key] = []
                module_metrics[key].append(value)
        
        # ì—í­ í‰ê· 
        avg_metrics = {k: np.mean(v) for k, v in module_metrics.items()}
        avg_metrics['loss'] = np.mean(epoch_losses)
        
        return avg_metrics
    
    def validate(self) -> Dict[str, float]:
        """ê²€ì¦"""
        self.model.eval()
        val_losses = []
        module_metrics = {}
        
        with torch.no_grad():
            # EMA ì ìš©
            if self.config.enable_ema:
                self.training_manager.apply_ema()
            
            for batch in self.val_loader:
                loss, metrics = self._forward_step(batch)
                val_losses.append(loss.item())
                
                for key, value in metrics.items():
                    if key not in module_metrics:
                        module_metrics[key] = []
                    module_metrics[key].append(value)
            
            # EMA ë³µì›
            if self.config.enable_ema:
                self.training_manager.restore_ema()
        
        # í‰ê· 
        avg_metrics = {k: np.mean(v) for k, v in module_metrics.items()}
        avg_metrics['val_loss'] = np.mean(val_losses)
        
        return avg_metrics
    
    def _forward_step(self, batch: Dict) -> Tuple[torch.Tensor, Dict]:
        """
        í•™ìŠµ ìŠ¤í… - V2ì˜ 3ë‹¨ê³„ ì›Œí¬í”Œë¡œìš° ë³µì›
        1. FORWARD: ë°ì´í„° â†’ ë°±ë³¸ â†’ í—¤ë“œ
        2. COMPUTE: ì†ì‹¤ ê³„ì‚° + Neural Analyzers + DSP/Kalman
        3. UPDATE: ì—­ì „íŒŒ + ìµœì í™”
        """
        batch_idx = self.global_step % 100  # ë¡œê¹…ìš©
        
        # ========== STAGE 1: FORWARD ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 1] Forward Pass ì‹œì‘")
        
        # ë°ì´í„° ì¤€ë¹„ - ì…ë ¥ ì¶”ì¶œ
        inputs = batch['input'].to(self.device)
        
        # ë°±ë³¸ í†µê³¼
        backbone_outputs = self.model.backbone(inputs, return_all_tasks=True)
        features = backbone_outputs.get('emotion', inputs)  # 896ì°¨ì›
        
        if self.verbose and batch_idx < 3:
            logger.info(f"      - ë°±ë³¸ ì¶œë ¥ shape: {features.shape}")
            logger.info(f"      - ë°±ë³¸ ì¶œë ¥ í‚¤: {list(backbone_outputs.keys())}")
        
        # ========== STAGE 2: COMPUTE LOSSES ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 2] Compute Loss")
        
        # í—¤ë“œ ì†ì‹¤ ê³„ì‚°
        head_losses = []
        individual_losses = {}  # ê°œë³„ ì†ì‹¤ ì €ì¥ìš©
        individual_accs = {}    # ê°œë³„ ì •í™•ë„ ì €ì¥ìš©
        
        # ê°ì • í—¤ë“œ
        if hasattr(self.model, 'emotion_head'):
            emotion_output = self.model.emotion_head(features)
            emotion_pred = emotion_output['emotions'] if isinstance(emotion_output, dict) else emotion_output
            emotion_target = batch['emotion_label'].to(self.device)
            emotion_loss = self.model.emotion_head.compute_loss(emotion_pred, emotion_target)
            head_losses.append(emotion_loss)
            individual_losses['emotion_loss'] = emotion_loss.item()
            # accuracy ê³„ì‚° (classification task)
            emotion_acc = (emotion_pred.argmax(dim=-1) == emotion_target).float().mean().item()
            individual_accs['emotion_acc'] = emotion_acc
            if self.verbose and batch_idx < 3:
                logger.info(f"      - ê°ì • ì†ì‹¤: {emotion_loss.item():.6f}, ì •í™•ë„: {emotion_acc:.4f}")
        
        # ë²¤ë‹´ í—¤ë“œ
        if hasattr(self.model, 'bentham_head'):
            bentham_output = self.model.bentham_head(features)
            bentham_pred = bentham_output['bentham_scores'] if isinstance(bentham_output, dict) else bentham_output
            bentham_target = batch['bentham_label'].to(self.device)
            bentham_loss = self.model.bentham_head.compute_loss(bentham_pred, bentham_target)
            head_losses.append(bentham_loss)
            individual_losses['bentham_loss'] = bentham_loss.item()
            # accuracy ê³„ì‚° (regression task - threshold ê¸°ë°˜)
            bentham_acc = ((bentham_pred - bentham_target).abs() < 0.1).float().mean().item()
            individual_accs['bentham_acc'] = bentham_acc
            if self.verbose and batch_idx < 3:
                logger.info(f"      - ë²¤ë‹´ ì†ì‹¤: {bentham_loss.item():.6f}, ì •í™•ë„: {bentham_acc:.4f}")
        
        # í›„íšŒ í—¤ë“œ
        if hasattr(self.model, 'regret_head'):
            regret_output = self.model.regret_head(features)
            regret_pred = regret_output['regret_score'] if isinstance(regret_output, dict) else regret_output
            regret_target = batch['regret_label'].to(self.device)
            regret_loss = self.model.regret_head.compute_loss(regret_pred, regret_target)
            head_losses.append(regret_loss)
            individual_losses['regret_loss'] = regret_loss.item()
            # accuracy ê³„ì‚° (regression task)
            regret_acc = ((regret_pred - regret_target).abs() < 0.1).float().mean().item()
            individual_accs['regret_acc'] = regret_acc
            if self.verbose and batch_idx < 3:
                logger.info(f"      - í›„íšŒ ì†ì‹¤: {regret_loss.item():.6f}, ì •í™•ë„: {regret_acc:.4f}")
        
        # SURD í—¤ë“œ
        if hasattr(self.model, 'surd_head'):
            surd_output = self.model.surd_head(features)
            surd_pred = surd_output['surd_values'] if isinstance(surd_output, dict) else surd_output
            
            # SURD íƒ€ê²Ÿì„ ì‹¤ì œ ë°ì´í„°ì—ì„œ ê³„ì‚°
            batch_size = surd_pred.shape[0]
            surd_target = torch.zeros((batch_size, 4), device=self.device)
            
            # Synergy: ê°ì • ë‹¤ì–‘ì„± (ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜)
            if 'emotion_label' in batch:
                emotion_probs = F.one_hot(batch['emotion_label'].to(self.device), num_classes=7).float()
                emotion_entropy = -(emotion_probs * (emotion_probs + 1e-10).log()).sum(dim=1)
                surd_target[:, 0] = emotion_entropy / np.log(7)  # ì •ê·œí™”
            
            # Unique: ë ˆì´ë¸” ê³ ìœ ì„± (one-hot ì¸ì½”ë”©)
            if 'surd_label' in batch:
                label_unique = F.one_hot(batch['surd_label'].to(self.device), num_classes=5).float()
                surd_target[:, 1] = label_unique.max(dim=1)[0]  # ìµœëŒ€ê°’ = 1.0
            
            # Redundant: ë²¤ë‹´ ìƒê´€ë„ (í‰ê· ê³¼ ë¶„ì‚°)
            if 'bentham_label' in batch:
                bentham = batch['bentham_label'].to(self.device)
                bentham_mean = bentham.mean(dim=1)
                bentham_std = bentham.std(dim=1) + 1e-10
                surd_target[:, 2] = 1.0 - (bentham_std / (bentham_mean + 1e-10)).clamp(0, 1)
            
            # Deterministic: í›„íšŒ ê²°ì •ì„± (ì ˆëŒ€ê°’)
            if 'regret_label' in batch:
                regret = batch['regret_label'].to(self.device)
                if regret.dim() == 1:
                    regret = regret.unsqueeze(1)
                surd_target[:, 3] = regret.abs().squeeze()
            
            surd_loss = self.model.surd_head.compute_loss(surd_pred, surd_target)
            head_losses.append(surd_loss)
            individual_losses['surd_loss'] = surd_loss.item()
            # accuracy ê³„ì‚° (multi-dimensional regression)
            surd_acc = ((surd_pred - surd_target).abs() < 0.1).float().mean().item()
            individual_accs['surd_acc'] = surd_acc
            if self.verbose and batch_idx < 3:
                logger.info(f"      - SURD ì†ì‹¤: {surd_loss.item():.6f}, ì •í™•ë„: {surd_acc:.4f}")
        
        # ========== STAGE 2: NEURAL ANALYZERS ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 2] Neural Analyzer Processing")
        
        analyzer_losses = []
        dsp_output = None
        neural_emotion_output = None
        
        # Neural Emotion Analyzer ì²˜ë¦¬ (ë¨¼ì €)
        if hasattr(self.model, 'neural_analyzers') and 'emotion' in self.model.neural_analyzers:
            try:
                emotion_analyzer = self.model.neural_analyzers['emotion']
                neural_emotion_output = emotion_analyzer(features)
                
                if 'emotion_logits' in neural_emotion_output:
                    target = batch['emotion_label'].to(self.device)
                    if target.dim() == 1:
                        target = F.one_hot(target, num_classes=7).float()
                    emotion_loss = F.cross_entropy(neural_emotion_output['emotion_logits'], target)
                    analyzer_losses.append(emotion_loss)
                    if self.verbose and batch_idx < 3:
                        logger.info(f"      - neural_emotion ì†ì‹¤: {emotion_loss.item():.6f}")
            except Exception as e:
                logger.error(f"    âŒ neural_emotion ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # DSP Simulator ì²˜ë¦¬
        if hasattr(self.model, 'dsp_simulator') and self.model.dsp_simulator:
            try:
                # DSPëŠ” 384ì°¨ì› ì…ë ¥ í•„ìš”
                if not hasattr(self, 'dsp_projection'):
                    self.dsp_projection = torch.nn.Linear(features.shape[-1], 384).to(self.device)
                
                dsp_input = self.dsp_projection(features)
                dsp_output = self.model.dsp_simulator(dsp_input)
                
                if self.verbose and batch_idx < 3:
                    logger.info(f"      - DSP ì¶œë ¥ ì²˜ë¦¬ë¨")
            except Exception as e:
                logger.warning(f"    âš ï¸ DSP ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # Kalman Filter ì²˜ë¦¬ (neural_emotion + DSP í•„ìš”)
        if hasattr(self.model, 'kalman_filter') and self.model.kalman_filter and \
           dsp_output is not None and neural_emotion_output is not None:
            try:
                traditional_emotions = neural_emotion_output.get('emotion_logits', None)
                dsp_emotions = dsp_output.get('final_emotions', None) if isinstance(dsp_output, dict) else dsp_output
                
                if traditional_emotions is not None and dsp_emotions is not None:
                    kalman_output = self.model.kalman_filter(
                        traditional_emotions=traditional_emotions,
                        dsp_emotions=dsp_emotions
                    )
                    if self.verbose and batch_idx < 3:
                        logger.info(f"      - Kalman í•„í„° ì¶œë ¥ ì²˜ë¦¬ë¨")
            except Exception as e:
                logger.warning(f"    âš ï¸ Kalman ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ë‚˜ë¨¸ì§€ Neural Analyzers ì²˜ë¦¬
        if hasattr(self.model, 'neural_analyzers'):
            for name, analyzer in self.model.neural_analyzers.items():
                if name == 'emotion':  # ì´ë¯¸ ì²˜ë¦¬í•¨
                    continue
                    
                try:
                    analyzer_output = analyzer(features)
                    
                    # ê° analyzerë³„ ì†ì‹¤ ê³„ì‚°
                    if 'bentham' in name and 'bentham_scores' in analyzer_output:
                        target = batch['bentham_label'].to(self.device)
                        analyzer_loss = F.mse_loss(analyzer_output['bentham_scores'], target)
                        analyzer_losses.append(analyzer_loss)
                        if self.verbose and batch_idx < 3:
                            logger.info(f"      - {name} ì†ì‹¤: {analyzer_loss.item():.6f}")
                    
                    elif 'regret' in name and 'regret_score' in analyzer_output:
                        target = batch['regret_label'].to(self.device)
                        analyzer_loss = F.smooth_l1_loss(analyzer_output['regret_score'], target)
                        analyzer_losses.append(analyzer_loss)
                        if self.verbose and batch_idx < 3:
                            logger.info(f"      - {name} ì†ì‹¤: {analyzer_loss.item():.6f}")
                    
                    elif 'surd' in name and 'surd_scores' in analyzer_output:
                        # SURD analyzerë„ 4ì°¨ì› íƒ€ê²Ÿ í•„ìš”
                        batch_size = analyzer_output['surd_scores'].shape[0]
                        target = torch.zeros((batch_size, 4), device=self.device)
                        
                        # ì‹¤ì œ ë°ì´í„°ë¡œ SURD ê³„ì‚° (ìœ„ì™€ ë™ì¼)
                        if 'emotion_label' in batch:
                            emotion_probs = F.one_hot(batch['emotion_label'].to(self.device), num_classes=7).float()
                            emotion_entropy = -(emotion_probs * (emotion_probs + 1e-10).log()).sum(dim=1)
                            target[:, 0] = emotion_entropy / np.log(7)
                        
                        if 'surd_label' in batch:
                            label_unique = F.one_hot(batch['surd_label'].to(self.device), num_classes=5).float()
                            target[:, 1] = label_unique.max(dim=1)[0]
                        
                        if 'bentham_label' in batch:
                            bentham = batch['bentham_label'].to(self.device)
                            bentham_mean = bentham.mean(dim=1)
                            bentham_std = bentham.std(dim=1) + 1e-10
                            target[:, 2] = 1.0 - (bentham_std / (bentham_mean + 1e-10)).clamp(0, 1)
                        
                        if 'regret_label' in batch:
                            regret = batch['regret_label'].to(self.device)
                            if regret.dim() == 1:
                                regret = regret.unsqueeze(1)
                            target[:, 3] = regret.abs().squeeze()
                        
                        analyzer_loss = F.mse_loss(analyzer_output['surd_scores'], target)
                        analyzer_losses.append(analyzer_loss)
                        if self.verbose and batch_idx < 3:
                            logger.info(f"      - {name} ì†ì‹¤: {analyzer_loss.item():.6f}")
                    
                except Exception as e:
                    if hasattr(self.config, 'debug') and self.config.debug:
                        logger.error(f"    {name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    else:
                        logger.error(f"    {name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # Advanced Wrappers ì²˜ë¦¬
        if hasattr(self.model, 'advanced_wrappers') and self.model.advanced_wrappers:
            for name, wrapper in self.model.advanced_wrappers.items():
                try:
                    wrapper_output = wrapper(features)
                    
                    # Advanced wrapper ì†ì‹¤ (ê°„ë‹¨íˆ ì²˜ë¦¬)
                    if isinstance(wrapper_output, dict) and any(key in wrapper_output for key in ['emotion', 'bentham', 'regret', 'surd']):
                        # ì ì ˆí•œ ì†ì‹¤ ê³„ì‚°
                        pass  # TODO: wrapperë³„ ì†ì‹¤ êµ¬í˜„ í•„ìš”
                        
                except Exception as e:
                    if self.config.debug:
                        logger.error(f"    {name} wrapper ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì „ì²´ ì†ì‹¤ í†µí•© (V2ì²˜ëŸ¼: í—¤ë“œ 70%, Analyzer 30%)
        all_losses = head_losses + analyzer_losses
        
        if all_losses:
            if head_losses and analyzer_losses:
                head_loss = sum(head_losses) / len(head_losses)
                analyzer_loss = sum(analyzer_losses) / len(analyzer_losses)
                loss = 0.7 * head_loss + 0.3 * analyzer_loss
                if self.verbose and batch_idx < 3:
                    logger.info(f"      - í—¤ë“œ ì†ì‹¤: {head_loss.item():.6f}")
                    logger.info(f"      - ë¶„ì„ê¸° ì†ì‹¤: {analyzer_loss.item():.6f}")
                    logger.info(f"      - ì „ì²´ ì†ì‹¤: {loss.item():.6f}")
            else:
                loss = sum(all_losses) / len(all_losses)
        else:
            # NO FALLBACK - ì†ì‹¤ì´ ì—†ìœ¼ë©´ ì—ëŸ¬
            raise RuntimeError("ì†ì‹¤ ê³„ì‚° ì‹¤íŒ¨: í—¤ë“œë‚˜ ë¶„ì„ê¸°ê°€ ì†ì‹¤ì„ ìƒì„±í•˜ì§€ ëª»í•¨")
        
        # ë©”íŠ¸ë¦­ - ê°œë³„ ëª¨ë“ˆ ì†ì‹¤ í¬í•¨
        metrics = {
            'loss': loss.item(),
            'train_loss': loss.item(),  # ì „ì²´ ì†ì‹¤ (backward í˜¸í™˜)
            'head_losses': len(head_losses),
            'analyzer_losses': len(analyzer_losses),
            'total_losses': len(all_losses)
        }
        
        # ê°œë³„ í—¤ë“œ ì†ì‹¤ ë° ì •í™•ë„ ì¶”ê°€
        metrics.update(individual_losses)
        metrics.update(individual_accs)
        
        # ë°±ë³¸ ì†ì‹¤ (ì „ì²´ ì†ì‹¤ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •)
        metrics['backbone_loss'] = loss.item()
        metrics['backbone_acc'] = 0.0  # ë°±ë³¸ì€ ë³„ë„ accuracy ì—†ìŒ
        
        # Neural Analyzer ì†ì‹¤
        if analyzer_losses:
            metrics['analyzer_loss'] = sum(al.item() for al in analyzer_losses) / len(analyzer_losses)
            metrics['analyzer_acc'] = 0.0  # AnalyzerëŠ” accuracy ê³„ì‚°ì´ ë³µì¡í•˜ë¯€ë¡œ ì¼ë‹¨ 0
        else:
            metrics['analyzer_loss'] = 0.0
            metrics['analyzer_acc'] = 0.0
        
        # validation ë©”íŠ¸ë¦­ (ê°™ì€ ê°’ìœ¼ë¡œ ì„¤ì • - ë‚˜ì¤‘ì— validate()ì—ì„œ ë®ì–´ì”Œì›Œì§)
        metrics['val_loss'] = loss.item()
        metrics['val_acc'] = metrics.get('emotion_acc', 0.0)  # ëŒ€í‘œë¡œ emotion accuracy ì‚¬ìš©
        
        return loss, metrics
    
    def train(self):
        """ì „ì²´ í•™ìŠµ ì‹¤í–‰"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸš€ í•™ìŠµ ì‹œì‘")
        logger.info("=" * 70)
        
        # LR ìŠ¤ìœ• ì‹¤í–‰
        self.run_lr_sweep()
        
        # 60 ì—í­ í•™ìŠµ
        for epoch in range(1, self.config.total_epochs + 1):
            self.current_epoch = epoch
            
            logger.info(f"\nğŸ“Œ Epoch {epoch}/{self.config.total_epochs}")
            
            # í•™ìŠµ
            train_metrics = self.train_epoch(epoch)
            
            # ê²€ì¦
            if epoch % 2 == 0:  # ì§ìˆ˜ ì—í­ë§ˆë‹¤
                val_metrics = self.validate()
                logger.info(f"  Validation Loss: {val_metrics['val_loss']:.4f}")
            else:
                val_metrics = {}
            
            # ë©”íŠ¸ë¦­ í†µí•© ë° ëª¨ë“ˆë³„ ê·¸ë£¹í™”
            all_metrics = {**train_metrics, **val_metrics}
            
            # ëª¨ë“ˆë³„ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì¬êµ¬ì„± (sweet_spot_detector í˜¸í™˜)
            module_metrics = {
                'backbone': {
                    'loss': all_metrics.get('backbone_loss', all_metrics.get('train_loss', 0)),
                    'accuracy': all_metrics.get('backbone_acc', 0)
                },
                'emotion_head': {
                    'loss': all_metrics.get('emotion_loss', 0),
                    'accuracy': all_metrics.get('emotion_acc', 0)
                },
                'bentham_head': {
                    'loss': all_metrics.get('bentham_loss', 0),
                    'accuracy': all_metrics.get('bentham_acc', 0)
                },
                'regret_head': {
                    'loss': all_metrics.get('regret_loss', 0),
                    'accuracy': all_metrics.get('regret_acc', 0)
                },
                'surd_head': {
                    'loss': all_metrics.get('surd_loss', 0),
                    'accuracy': all_metrics.get('surd_acc', 0)
                },
                'neural_analyzers': {
                    'loss': all_metrics.get('analyzer_loss', 0),
                    'accuracy': all_metrics.get('analyzer_acc', 0)
                },
                'system': {
                    'loss': all_metrics.get('val_loss', all_metrics.get('train_loss', 0)),
                    'accuracy': all_metrics.get('val_acc', 0)
                }
            }
            
            # ë””ë²„ê·¸: ë©”íŠ¸ë¦­ ê²€ì¦
            if epoch == 1 and self.verbose:
                logger.info("\n  ğŸ“Š ë©”íŠ¸ë¦­ ê²€ì¦ (Epoch 1):")
                for module_name, metrics in module_metrics.items():
                    logger.info(f"    - {module_name}: loss={metrics['loss']:.4f}, acc={metrics['accuracy']:.4f}")
            
            # Sweet Spot ì—…ë°ì´íŠ¸
            if self.config.enable_sweet_spot:
                self.sweet_spot_detector.update(
                    epoch=epoch,
                    module_metrics=module_metrics,
                    learning_rate=self.optimizer.param_groups[0]['lr']
                )
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            checkpoint_path = self.checkpoint_manager.save_checkpoint(
                epoch=epoch,
                model=self.model,
                optimizer=self.optimizer,
                scheduler=self.scheduler,
                metrics=all_metrics,
                lr=self.optimizer.param_groups[0]['lr']
            )
            
            # Crossover ì‹œìŠ¤í…œì— ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€
            if checkpoint_path and self.config.enable_crossover:
                self.crossover_system.add_checkpoint(
                    epoch=epoch,
                    checkpoint_path=checkpoint_path,
                    module_metrics=all_metrics
                )
            
            # ìµœê³  ì„±ëŠ¥ ê°±ì‹ 
            if 'loss' in all_metrics and all_metrics['loss'] < self.best_loss:
                self.best_loss = all_metrics['loss']
                logger.info(f"  ğŸ† ìµœê³  ì„±ëŠ¥ ê°±ì‹ : {self.best_loss:.4f}")
        
        logger.info("\n" + "=" * 70)
        logger.info("âœ… 60 ì—í­ í•™ìŠµ ì™„ë£Œ!")
        logger.info("=" * 70)
        
        # ìµœì¢… ì²˜ë¦¬
        self._finalize_training()
    
    def _finalize_training(self):
        """í•™ìŠµ ë§ˆë¬´ë¦¬ ì²˜ë¦¬"""
        logger.info("\nğŸ”§ ìµœì¢… ì²˜ë¦¬ ì‹œì‘...")
        
        # Sweet Spot ì¢…í•© ë¶„ì„ ì‹¤í–‰
        if self.config.enable_sweet_spot:
            logger.info("\nğŸ¯ Sweet Spot ì¢…í•© ë¶„ì„ ì‹œì‘...")
            try:
                # 5ê°€ì§€ ê³ ê¸‰ ë¶„ì„ ê¸°ë²• ì ìš©
                analysis_results = self.sweet_spot_detector.analyze_all(
                    output_dir='training/sweet_spot_analysis'
                )
                
                # ê¸°ì¡´ ë©”ì„œë“œë„ í˜¸ì¶œ (í˜¸í™˜ì„± ìœ ì§€)
                optimal_epochs = self.sweet_spot_detector.get_optimal_epochs()
                logger.info(f"  ğŸ“Š ëª¨ë“ˆë³„ ìµœì  ì—í­: {optimal_epochs}")
                
                # ì¶”ê°€ ë¶„ì„ ê²°ê³¼ ë¡œê¹…
                for module, result in analysis_results.items():
                    rec = result['recommendation']
                    logger.info(f"    - {module}: Epoch {rec['epoch']} (ì‹ ë¢°ë„: {rec['confidence']:.1%})")
                    
            except Exception as e:
                logger.error(f"Sweet Spot ë¶„ì„ ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰
                analysis_results = self.sweet_spot_detector.export_analysis()
                logger.info(f"  ğŸ“Š ê¸°ë³¸ ë¶„ì„ ì €ì¥: {analysis_results['json_file']}")
        
        # Parameter Crossover ì‹¤í–‰
        if self.config.enable_crossover and self.config.enable_sweet_spot:
            logger.info("\nğŸ§¬ Parameter Crossover ì‹¤í–‰...")
            
            crossover_model = self.crossover_system.perform_crossover(
                model=self.model,
                optimal_epochs=optimal_epochs
            )
            
            # Crossover ëª¨ë¸ ì €ì¥
            crossover_path = Path(self.config.checkpoint_dir) / "crossover_final.pth"
            self.crossover_system.save_crossover_result(
                model=crossover_model,
                save_path=str(crossover_path),
                metadata={'optimal_epochs': optimal_epochs}
            )
            logger.info(f"  ğŸ’¾ Crossover ëª¨ë¸ ì €ì¥: {crossover_path}")
        
        # í•™ìŠµ ê³¡ì„  ë‚´ë³´ë‚´ê¸°
        curves_file = self.checkpoint_manager.export_training_curves()
        logger.info(f"  ğŸ“ˆ í•™ìŠµ ê³¡ì„  ì €ì¥: {curves_file}")
        
        # OOM í†µê³„ ì €ì¥
        if self.config.enable_oom_handler:
            oom_stats = self.oom_handler.save_stats()
            logger.info(f"  ğŸ“Š OOM í†µê³„ ì €ì¥: {oom_stats}")
        
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        logger.info("=" * 70)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Red Heart AI ìµœì¢… í†µí•© í•™ìŠµ")
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ')
    parser.add_argument('--epochs', type=int, default=60, help='í•™ìŠµ ì—í­')
    parser.add_argument('--batch-size', type=int, default=2, help='ë°°ì¹˜ ì‚¬ì´ì¦ˆ')
    parser.add_argument('--lr', type=float, default=1e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--resume', type=str, help='ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ')
    parser.add_argument('--no-param-update', action='store_true', help='íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê±´ë„ˆë›°ê¸° (ê²€ì¦ìš©)')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ë¡œê¹…')
    parser.add_argument('--samples', type=int, help='í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ìˆ˜ (ì—í­ ìˆ˜ë¡œ ì‚¬ìš©)')
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = UnifiedTrainingConfig()
    
    # argsì—ì„œ ì„¤ì • ì ìš©
    config.verbose = args.verbose
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    if args.test:
        if args.samples:
            config.total_epochs = args.samples
            logger.info(f"âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {args.samples} ì—í­ ì‹¤í–‰")
        else:
            config.total_epochs = 2
            logger.info("âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 2 ì—í­ë§Œ ì‹¤í–‰")
    else:
        config.total_epochs = args.epochs
    
    config.micro_batch_size = args.batch_size
    config.base_lr = args.lr
    
    # ë””ë²„ê·¸/ìƒì„¸ ë¡œê¹… ì„¤ì •
    if args.debug or args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        config.log_interval = 1  # ë§¤ ìŠ¤í…ë§ˆë‹¤ ë¡œê¹…
        config.val_interval = 10  # ë” ìì£¼ ê²€ì¦
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° ì‹¤í–‰
    trainer = UnifiedTrainer(config)
    trainer.no_param_update = args.no_param_update  # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í”Œë˜ê·¸
    
    if args.no_param_update:
        logger.warning("âš ï¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë¹„í™œì„±í™” - ê²€ì¦ ëª¨ë“œ")
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
    if args.resume:
        checkpoint = trainer.checkpoint_manager.load_checkpoint(args.resume)
        trainer.model.load_state_dict(checkpoint['model_state'])
        trainer.optimizer.load_state_dict(checkpoint['optimizer_state'])
        trainer.current_epoch = checkpoint['epoch']
        logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: Epoch {trainer.current_epoch}")
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train()


if __name__ == "__main__":
    main()