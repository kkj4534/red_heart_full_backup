"""
í–¥ìƒëœ ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì €
60 ì—í­ í•™ìŠµì—ì„œ 30ê°œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë° Sweet Spot íƒì§€ ì§€ì›
"""

import torch
import os
import json
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import logging
from datetime import datetime
import numpy as np

logger = logging.getLogger(__name__)


class EnhancedCheckpointManager:
    """
    í–¥ìƒëœ ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì €
    - 60 ì—í­ ì¤‘ 30ê°œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì§ìˆ˜ ì—í­ë§ˆë‹¤)
    - ëª¨ë“ˆë³„ ìµœì  ì„±ëŠ¥ ì¶”ì 
    - Sweet Spot ìë™ íƒì§€
    - Parameter Crossover ì§€ì›
    """
    
    def __init__(self, 
                 checkpoint_dir: str = "training/checkpoints",
                 max_checkpoints: int = 30,
                 save_interval: int = 1):
        """
        Args:
            checkpoint_dir: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬
            max_checkpoints: ìµœëŒ€ ì²´í¬í¬ì¸íŠ¸ ê°œìˆ˜ (ê¸°ë³¸ 30ê°œ)
            save_interval: ì €ì¥ ê°„ê²© (ê¸°ë³¸ 1 ì—í­ë§ˆë‹¤ - ëª¨ë“  ì—í­ ì €ì¥)
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        self.max_checkpoints = max_checkpoints
        self.save_interval = save_interval
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.metrics_history = {
            'global': [],  # ì „ì²´ ë©”íŠ¸ë¦­
            'modules': {}  # ëª¨ë“ˆë³„ ë©”íŠ¸ë¦­
        }
        
        # Sweet Spot ì¶”ì 
        self.sweet_spots = {}
        
        # ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„°
        self.checkpoint_metadata = []
        self.load_metadata()
        
        logger.info(f"âœ… Enhanced CheckpointManager ì´ˆê¸°í™”")
        logger.info(f"  - ì €ì¥ ë””ë ‰í† ë¦¬: {self.checkpoint_dir}")
        logger.info(f"  - ìµœëŒ€ ì²´í¬í¬ì¸íŠ¸: {self.max_checkpoints}ê°œ")
        logger.info(f"  - ì €ì¥ ê°„ê²©: {self.save_interval} ì—í­ë§ˆë‹¤")
    
    def load_metadata(self):
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        metadata_file = self.checkpoint_dir / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, 'r') as f:
                self.checkpoint_metadata = json.load(f)
                logger.info(f"  - ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ: {len(self.checkpoint_metadata)}ê°œ ì²´í¬í¬ì¸íŠ¸")
    
    def save_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata_file = self.checkpoint_dir / "metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(self.checkpoint_metadata, f, indent=2)
    
    def should_save_checkpoint(self, epoch: int) -> bool:
        """í˜„ì¬ ì—í­ì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸"""
        # save_interval=1ì¼ ë•Œ ëª¨ë“  ì—í­ ì €ì¥
        return epoch % self.save_interval == 0
    
    def should_keep_optimizer(self, epoch: int) -> bool:
        """optimizer_stateë¥¼ ìœ ì§€í•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •
        
        50 ì—í­ ì „ëµ:
        - 10, 20, 30, 40, 50: ë§ˆì¼ìŠ¤í†¤ ìœ ì§€ (ì¬ê°œ ê°€ëŠ¥)
        - ë‚˜ë¨¸ì§€: ì œê±° (ê³µê°„ ì ˆì•½, í¬ë¡œìŠ¤ì˜¤ë²„ë§Œ ê°€ëŠ¥)
        """
        # 10 ì—í­ ë‹¨ìœ„ë¡œ optimizer_state ì €ì¥
        if epoch % 10 == 0:
            return True
        return False
    
    def save_checkpoint(self,
                       epoch: int,
                       model: Any,
                       optimizer: Any,
                       scheduler: Any,
                       metrics: Dict[str, Any],
                       lr: float) -> Optional[str]:
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        
        Args:
            epoch: í˜„ì¬ ì—í­
            model: ëª¨ë¸ ê°ì²´
            optimizer: ì˜µí‹°ë§ˆì´ì €
            scheduler: ìŠ¤ì¼€ì¤„ëŸ¬
            metrics: í˜„ì¬ ë©”íŠ¸ë¦­
            lr: í˜„ì¬ í•™ìŠµë¥ 
            
        Returns:
            ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì €ì¥í•˜ì§€ ì•Šìœ¼ë©´ None)
        """
        if not self.should_save_checkpoint(epoch):
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_name = f"checkpoint_epoch_{epoch:04d}_lr_{lr:.6f}_{timestamp}.pt"
        checkpoint_path = self.checkpoint_dir / checkpoint_name
        
        # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° êµ¬ì„± (CPUë¡œ ì´ë™í•˜ì—¬ GPU ë©”ëª¨ë¦¬ ì ˆì•½)
        # Optimizer stateë¥¼ CPUë¡œ ì´ë™ (ì¤‘ì²©ëœ êµ¬ì¡° ì²˜ë¦¬)
        optimizer_state_cpu = {}
        opt_state = optimizer.state_dict()
        
        # stateì™€ param_groups ì²˜ë¦¬
        if 'state' in opt_state:
            optimizer_state_cpu['state'] = {}
            for key, state in opt_state['state'].items():
                optimizer_state_cpu['state'][key] = {
                    k: v.cpu() if torch.is_tensor(v) else v
                    for k, v in state.items()
                }
        
        if 'param_groups' in opt_state:
            optimizer_state_cpu['param_groups'] = opt_state['param_groups']
        
        # optimizer_state ì €ì¥ ì—¬ë¶€ ê²°ì •
        keep_optimizer = self.should_keep_optimizer(epoch)
        
        checkpoint_data = {
            'epoch': epoch,
            'lr': lr,
            'timestamp': timestamp,
            'model_state': self._extract_modular_states(model),  # ì´ë¯¸ CPUë¡œ ì´ë™ë¨
            'scheduler_state': scheduler.state_dict() if scheduler else None,
            'metrics': metrics,  # í˜„ì¬ ì—í­ì˜ ë©”íŠ¸ë¦­ë§Œ
            # sweet_spots ì œê±° - ëˆ„ì  ë°©ì§€
        }
        
        # optimizer_stateëŠ” ì¡°ê±´ë¶€ë¡œ ì¶”ê°€
        if keep_optimizer:
            checkpoint_data['optimizer_state'] = optimizer_state_cpu
            logger.info(f"   - Optimizer state ìœ ì§€ (ì—í­ {epoch})")
        else:
            logger.info(f"   - Optimizer state ì œê±° (ê³µê°„ ì ˆì•½)")
        
        # ì €ì¥
        torch.save(checkpoint_data, checkpoint_path)
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        metadata_entry = {
            'epoch': epoch,
            'lr': lr,
            'timestamp': timestamp,
            'path': str(checkpoint_path),
            'metrics': metrics,
            'file_size_mb': checkpoint_path.stat().st_size / (1024 * 1024)
        }
        self.checkpoint_metadata.append(metadata_entry)
        self.save_metadata()
        
        # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self._update_metrics_history(epoch, metrics)
        
        # Sweet Spot íƒì§€
        self._detect_sweet_spots(epoch, metrics)
        
        logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
        logger.info(f"   - ì—í­: {epoch}, LR: {lr:.6f}")
        # optimizer ì €ì¥ ì •ë³´ëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì¶œë ¥ë¨
        # loss ê°’ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì ì ˆí•œ í¬ë§· ì ìš©
        loss_val = metrics.get('loss', 'N/A')
        if isinstance(loss_val, (int, float)) and loss_val != float('inf'):
            logger.info(f"   - ë©”íŠ¸ë¦­: loss={loss_val:.4f}")
        else:
            logger.info(f"   - ë©”íŠ¸ë¦­: loss={loss_val}")
        
        # ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
        self._cleanup_old_checkpoints()
        
        return str(checkpoint_path)
    
    def _extract_modular_states(self, model: Any) -> Dict[str, Any]:
        """ëª¨ë¸ì„ ëª¨ë“ˆë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ state_dict ì¶”ì¶œ (GPUâ†’CPU ì´ë™)"""
        modular_states = {}
        
        # Group A: Backbone + Heads
        group_a_modules = ['backbone', 'emotion_head', 'bentham_head', 
                          'regret_head', 'surd_head']
        for module_name in group_a_modules:
            if hasattr(model, module_name):
                module = getattr(model, module_name)
                if module is not None:
                    # GPU â†’ CPU ì´ë™í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
                    modular_states[module_name] = {
                        k: v.cpu() for k, v in module.state_dict().items()
                    }
        
        # Neural Analyzers Dict ì²˜ë¦¬ (368M íŒŒë¼ë¯¸í„°)
        if hasattr(model, 'neural_analyzers'):
            neural_analyzers = getattr(model, 'neural_analyzers')
            # nn.ModuleDictë„ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •
            if hasattr(neural_analyzers, 'items'):  # dict-like ê°ì²´ì¸ì§€ í™•ì¸
                # dict ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ëª¨ë“ˆë¡œ ì €ì¥
                neural_states = {}
                for analyzer_name, analyzer_module in neural_analyzers.items():
                    if analyzer_module is not None:
                        # ê° analyzerì˜ stateë¥¼ nested dictë¡œ ì €ì¥
                        neural_states[analyzer_name] = {
                            k: v.cpu() for k, v in analyzer_module.state_dict().items()
                        }
                if neural_states:
                    modular_states['neural_analyzers'] = neural_states
                    logger.debug(f"  âœ“ neural_analyzers dict ì €ì¥: {len(neural_states)}ê°œ ë¶„ì„ê¸°")
            else:
                # dictê°€ ì•„ë‹Œ ê²½ìš° ê¸°ì¡´ ë°©ì‹ (fallback)
                group_b_modules = ['neural_emotion', 'neural_bentham', 
                                  'neural_regret', 'neural_surd']
                for module_name in group_b_modules:
                    if hasattr(model, module_name):
                        module = getattr(model, module_name)
                        if module is not None:
                            modular_states[module_name] = {
                                k: v.cpu() for k, v in module.state_dict().items()
                            }
        
        # Group C: Phase Networks (ì¤‘ìš”: í¬ë¡œìŠ¤ì˜¤ë²„ í•„ìˆ˜)
        phase_modules = ['phase0_net', 'phase2_net', 'hierarchical_integrator']
        for module_name in phase_modules:
            if hasattr(model, module_name):
                module = getattr(model, module_name)
                if module is not None:
                    modular_states[module_name] = {
                        k: v.cpu() for k, v in module.state_dict().items()
                    }
                    logger.debug(f"  âœ“ {module_name} ì €ì¥ ì™„ë£Œ")
        
        # Group D: DSP + Kalman
        group_d_modules = ['dsp_simulator', 'kalman_filter']
        for module_name in group_d_modules:
            if hasattr(model, module_name):
                module = getattr(model, module_name)
                if module is not None:
                    # GPU â†’ CPU ì´ë™í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
                    modular_states[module_name] = {
                        k: v.cpu() for k, v in module.state_dict().items()
                    }
        
        # Advanced Wrappers Dict ì²˜ë¦¬ (ì¤‘ìš”: í¬ë¡œìŠ¤ì˜¤ë²„ í•„ìˆ˜)
        if hasattr(model, 'advanced_wrappers'):
            advanced_wrappers = getattr(model, 'advanced_wrappers')
            if advanced_wrappers is not None and hasattr(advanced_wrappers, 'items'):
                wrapper_states = {}
                for wrapper_name, wrapper_module in advanced_wrappers.items():
                    if wrapper_module is not None:
                        wrapper_states[wrapper_name] = {
                            k: v.cpu() for k, v in wrapper_module.state_dict().items()
                        }
                if wrapper_states:
                    modular_states['advanced_wrappers'] = wrapper_states
                    logger.debug(f"  âœ“ advanced_wrappers dict ì €ì¥: {len(wrapper_states)}ê°œ ë˜í¼")
        
        # System: ì „ì²´ ì‹œìŠ¤í…œ í†µí•© íŒŒë¼ë¯¸í„°
        # ì „ì²´ ëª¨ë¸ì˜ í†µí•© ì„±ëŠ¥ì„ ìœ„í•œ ì™„ì „í•œ state_dict ì €ì¥
        # ì´ë¥¼ í†µí•´ ëª¨ë“ˆë³„ ìµœì í™”ì™€ ì „ì²´ ì‹œìŠ¤í…œ ìµœì í™”ë¥¼ ëª¨ë‘ ì¶”ì 
        if hasattr(model, 'state_dict'):
            # ì „ì²´ ëª¨ë¸ state_dictë¥¼ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ì„ íƒì ìœ¼ë¡œ)
            # í•µì‹¬ í†µí•© íŒŒë¼ë¯¸í„°ë§Œ ì €ì¥ (ë°±ë³¸ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ ë“±)
            system_state = {}
            
            # ë°±ë³¸ì˜ í†µí•© ë ˆì´ì–´ (ë§ˆì§€ë§‰ ë ˆì´ì–´ë“¤)
            if hasattr(model, 'backbone'):
                backbone = getattr(model, 'backbone')
                if hasattr(backbone, 'final_norm'):
                    final_norm = getattr(backbone, 'final_norm')
                    system_state['backbone_final_norm'] = {
                        k: v.cpu() for k, v in final_norm.state_dict().items()
                    }
                if hasattr(backbone, 'output_projection'):
                    output_proj = getattr(backbone, 'output_projection')
                    system_state['backbone_output_projection'] = {
                        k: v.cpu() for k, v in output_proj.state_dict().items()
                    }
            
            # í†µí•© ë©”íŠ¸ë¦­ ë° ë©”íƒ€ë°ì´í„°
            system_state['meta'] = {
                'total_modules': len(modular_states),
                'timestamp': datetime.now().isoformat(),
                'module_names': list(modular_states.keys()),
                'total_parameters': sum(p.numel() for p in model.parameters()),
                'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)
            }
            
            modular_states['system'] = system_state
        
        return modular_states
    
    def _update_metrics_history(self, epoch: int, metrics: Dict[str, Any]):
        """ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ - ë³„ë„ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ëˆ„ì  ë°©ì§€"""
        # ì „ì²´ ë©”íŠ¸ë¦­
        self.metrics_history['global'].append({
            'epoch': epoch,
            'metrics': metrics.copy()
        })
        
        # ëª¨ë“ˆë³„ ë©”íŠ¸ë¦­
        for key, value in metrics.items():
            if '_' in key:  # ëª¨ë“ˆ ì´ë¦„ì´ í¬í•¨ëœ ë©”íŠ¸ë¦­
                module_name = key.split('_')[0]
                if module_name not in self.metrics_history['modules']:
                    self.metrics_history['modules'][module_name] = []
                self.metrics_history['modules'][module_name].append({
                    'epoch': epoch,
                    'value': value
                })
        
        # ë³„ë„ íŒŒì¼ë¡œ ì €ì¥ (ì²´í¬í¬ì¸íŠ¸ì™€ ë¶„ë¦¬)
        history_file = self.checkpoint_dir / "metrics_history.json"
        with open(history_file, 'w') as f:
            json.dump(self.metrics_history, f, indent=2)
    
    def _detect_sweet_spots(self, epoch: int, metrics: Dict[str, Any]):
        """Sweet Spot ìë™ íƒì§€"""
        for module_name in self.metrics_history['modules']:
            history = self.metrics_history['modules'][module_name]
            if len(history) >= 5:  # ìµœì†Œ 5ê°œ ë°ì´í„°í¬ì¸íŠ¸ í•„ìš”
                recent_values = [h['value'] for h in history[-5:]]
                
                # Sweet Spot ì¡°ê±´: ìµœê·¼ 5 ì—í­ ì¤‘ ë³€ë™ì´ ì‘ê³  ì„±ëŠ¥ì´ ì¢‹ìŒ
                std_dev = np.std(recent_values)
                mean_value = np.mean(recent_values)
                
                # ì´ì „ Sweet Spotê³¼ ë¹„êµ
                if module_name not in self.sweet_spots or \
                   mean_value < self.sweet_spots[module_name]['value']:
                    if std_dev < 0.01:  # ì•ˆì •ì„± ì¡°ê±´
                        self.sweet_spots[module_name] = {
                            'epoch': epoch,
                            'value': mean_value,
                            'std': std_dev
                        }
                        logger.info(f"  ğŸ¯ Sweet Spot ë°œê²¬: {module_name} @ epoch {epoch}")
                        
                        # Sweet spotsë„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
                        sweet_spots_file = self.checkpoint_dir / "sweet_spots.json"
                        with open(sweet_spots_file, 'w') as f:
                            json.dump(self.sweet_spots, f, indent=2)
    
    def _cleanup_old_checkpoints(self):
        """ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
        if len(self.checkpoint_metadata) > self.max_checkpoints:
            # ê°€ì¥ ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
            oldest = self.checkpoint_metadata[0]
            checkpoint_path = Path(oldest['path'])
            if checkpoint_path.exists():
                checkpoint_path.unlink()
                logger.info(f"  ğŸ—‘ï¸ ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ: {checkpoint_path.name}")
            self.checkpoint_metadata.pop(0)
            self.save_metadata()
    
    def load_checkpoint(self, checkpoint_path: Optional[str] = None) -> Dict[str, Any]:
        """
        ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        
        Args:
            checkpoint_path: ë¡œë“œí•  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (Noneì´ë©´ ìµœì‹ )
            
        Returns:
            ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°
        """
        if checkpoint_path is None:
            # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
            if not self.checkpoint_metadata:
                raise ValueError("ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            checkpoint_path = self.checkpoint_metadata[-1]['path']
        
        checkpoint_path = Path(checkpoint_path)
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        
        checkpoint_data = torch.load(checkpoint_path, map_location='cpu')
        logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
        logger.info(f"   - ì—í­: {checkpoint_data['epoch']}")
        logger.info(f"   - LR: {checkpoint_data.get('lr', 'N/A')}")
        
        # optimizer_state ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if 'optimizer_state' in checkpoint_data:
            logger.info(f"   - Optimizer state: í¬í•¨ (í•™ìŠµ ì¬ê°œ ê°€ëŠ¥)")
        else:
            logger.info(f"   - Optimizer state: ì—†ìŒ (íŒŒë¼ë¯¸í„° í¬ë¡œìŠ¤ì˜¤ë²„ë§Œ ê°€ëŠ¥)")
        
        return checkpoint_data
    
    def get_best_checkpoint(self, metric_name: str = 'loss') -> Optional[str]:
        """
        íŠ¹ì • ë©”íŠ¸ë¦­ ê¸°ì¤€ ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        
        Args:
            metric_name: í‰ê°€ ë©”íŠ¸ë¦­ ì´ë¦„
            
        Returns:
            ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        """
        if not self.checkpoint_metadata:
            return None
        
        best_checkpoint = None
        best_value = float('inf')
        
        for metadata in self.checkpoint_metadata:
            if metric_name in metadata['metrics']:
                value = metadata['metrics'][metric_name]
                if value < best_value:
                    best_value = value
                    best_checkpoint = metadata['path']
        
        if best_checkpoint:
            logger.info(f"ğŸ† ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸: {best_checkpoint}")
            logger.info(f"   - {metric_name}: {best_value:.4f}")
        
        return best_checkpoint
    
    def get_sweet_spot_summary(self) -> Dict[str, Any]:
        """Sweet Spot ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        summary = {
            'detected_spots': len(self.sweet_spots),
            'modules': {}
        }
        
        for module_name, spot_info in self.sweet_spots.items():
            summary['modules'][module_name] = {
                'optimal_epoch': spot_info['epoch'],
                'metric_value': spot_info['value'],
                'stability': spot_info['std']
            }
        
        return summary
    
    def export_training_curves(self, output_dir: str = "training/plots"):
        """í•™ìŠµ ê³¡ì„  ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì „ì²´ ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì €ì¥
        curves_data = {
            'global_metrics': self.metrics_history['global'],
            'module_metrics': self.metrics_history['modules'],
            'sweet_spots': self.sweet_spots,
            'checkpoint_metadata': self.checkpoint_metadata
        }
        
        output_file = output_dir / f"training_curves_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w') as f:
            json.dump(curves_data, f, indent=2)
        
        logger.info(f"ğŸ“Š í•™ìŠµ ê³¡ì„  ë°ì´í„° ë‚´ë³´ë‚´ê¸°: {output_file}")
        
        return str(output_file)