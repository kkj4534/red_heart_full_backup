"""
ν–¥μƒλ μ²΄ν¬ν¬μΈνΈ λ§¤λ‹μ €
60 μ—ν­ ν•™μµμ—μ„ 30κ° μ²΄ν¬ν¬μΈνΈ μ €μ¥ λ° Sweet Spot νƒμ§€ μ§€μ›
"""

import torch
import os
import json
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import logging
from datetime import datetime
import numpy as np

logger = logging.getLogger(__name__)


class EnhancedCheckpointManager:
    """
    ν–¥μƒλ μ²΄ν¬ν¬μΈνΈ λ§¤λ‹μ €
    - 60 μ—ν­ μ¤‘ 30κ° μ²΄ν¬ν¬μΈνΈ μ €μ¥ (μ§μ μ—ν­λ§λ‹¤)
    - λ¨λ“λ³„ μµμ  μ„±λ¥ μ¶”μ 
    - Sweet Spot μλ™ νƒμ§€
    - Parameter Crossover μ§€μ›
    """
    
    def __init__(self, 
                 checkpoint_dir: str = "training/checkpoints",
                 max_checkpoints: int = 30,
                 save_interval: int = 2):
        """
        Args:
            checkpoint_dir: μ²΄ν¬ν¬μΈνΈ μ €μ¥ λ””λ ‰ν† λ¦¬
            max_checkpoints: μµλ€ μ²΄ν¬ν¬μΈνΈ κ°μ (κΈ°λ³Έ 30κ°)
            save_interval: μ €μ¥ κ°„κ²© (κΈ°λ³Έ 2 μ—ν­λ§λ‹¤)
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        self.max_checkpoints = max_checkpoints
        self.save_interval = save_interval
        
        # λ©”νΈλ¦­ μ¶”μ 
        self.metrics_history = {
            'global': [],  # μ „μ²΄ λ©”νΈλ¦­
            'modules': {}  # λ¨λ“λ³„ λ©”νΈλ¦­
        }
        
        # Sweet Spot μ¶”μ 
        self.sweet_spots = {}
        
        # μ²΄ν¬ν¬μΈνΈ λ©”νƒ€λ°μ΄ν„°
        self.checkpoint_metadata = []
        self.load_metadata()
        
        logger.info(f"β… Enhanced CheckpointManager μ΄κΈ°ν™”")
        logger.info(f"  - μ €μ¥ λ””λ ‰ν† λ¦¬: {self.checkpoint_dir}")
        logger.info(f"  - μµλ€ μ²΄ν¬ν¬μΈνΈ: {self.max_checkpoints}κ°")
        logger.info(f"  - μ €μ¥ κ°„κ²©: {self.save_interval} μ—ν­λ§λ‹¤")
    
    def load_metadata(self):
        """λ©”νƒ€λ°μ΄ν„° λ΅λ“"""
        metadata_file = self.checkpoint_dir / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, 'r') as f:
                self.checkpoint_metadata = json.load(f)
                logger.info(f"  - κΈ°μ΅΄ λ©”νƒ€λ°μ΄ν„° λ΅λ“: {len(self.checkpoint_metadata)}κ° μ²΄ν¬ν¬μΈνΈ")
    
    def save_metadata(self):
        """λ©”νƒ€λ°μ΄ν„° μ €μ¥"""
        metadata_file = self.checkpoint_dir / "metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(self.checkpoint_metadata, f, indent=2)
    
    def should_save_checkpoint(self, epoch: int) -> bool:
        """ν„μ¬ μ—ν­μ—μ„ μ²΄ν¬ν¬μΈνΈλ¥Ό μ €μ¥ν•΄μ•Ό ν•λ”μ§€ ν™•μΈ"""
        # μ§μ μ—ν­λ§λ‹¤ μ €μ¥ (60 μ—ν­ μ¤‘ 30κ°)
        return epoch % self.save_interval == 0
    
    def save_checkpoint(self,
                       epoch: int,
                       model: Any,
                       optimizer: Any,
                       scheduler: Any,
                       metrics: Dict[str, Any],
                       lr: float) -> Optional[str]:
        """
        μ²΄ν¬ν¬μΈνΈ μ €μ¥
        
        Args:
            epoch: ν„μ¬ μ—ν­
            model: λ¨λΈ κ°μ²΄
            optimizer: μµν‹°λ§μ΄μ €
            scheduler: μ¤μΌ€μ¤„λ¬
            metrics: ν„μ¬ λ©”νΈλ¦­
            lr: ν„μ¬ ν•™μµλ¥ 
            
        Returns:
            μ €μ¥λ μ²΄ν¬ν¬μΈνΈ κ²½λ΅ (μ €μ¥ν•μ§€ μ•μΌλ©΄ None)
        """
        if not self.should_save_checkpoint(epoch):
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_name = f"checkpoint_epoch_{epoch:04d}_lr_{lr:.6f}_{timestamp}.pt"
        checkpoint_path = self.checkpoint_dir / checkpoint_name
        
        # μ²΄ν¬ν¬μΈνΈ λ°μ΄ν„° κµ¬μ„±
        checkpoint_data = {
            'epoch': epoch,
            'lr': lr,
            'timestamp': timestamp,
            'model_state': self._extract_modular_states(model),
            'optimizer_state': optimizer.state_dict(),
            'scheduler_state': scheduler.state_dict() if scheduler else None,
            'metrics': metrics,
            'sweet_spots': self.sweet_spots.copy()
        }
        
        # μ €μ¥
        torch.save(checkpoint_data, checkpoint_path)
        
        # λ©”νƒ€λ°μ΄ν„° μ—…λ°μ΄νΈ
        metadata_entry = {
            'epoch': epoch,
            'lr': lr,
            'timestamp': timestamp,
            'path': str(checkpoint_path),
            'metrics': metrics,
            'file_size_mb': checkpoint_path.stat().st_size / (1024 * 1024)
        }
        self.checkpoint_metadata.append(metadata_entry)
        self.save_metadata()
        
        # λ©”νΈλ¦­ νμ¤ν† λ¦¬ μ—…λ°μ΄νΈ
        self._update_metrics_history(epoch, metrics)
        
        # Sweet Spot νƒμ§€
        self._detect_sweet_spots(epoch, metrics)
        
        logger.info(f"π’Ύ μ²΄ν¬ν¬μΈνΈ μ €μ¥: {checkpoint_path}")
        logger.info(f"   - μ—ν­: {epoch}, LR: {lr:.6f}")
        logger.info(f"   - λ©”νΈλ¦­: loss={metrics.get('loss', 'N/A'):.4f}")
        
        # μ¤λλ μ²΄ν¬ν¬μΈνΈ μ •λ¦¬
        self._cleanup_old_checkpoints()
        
        return str(checkpoint_path)
    
    def _extract_modular_states(self, model: Any) -> Dict[str, Any]:
        """λ¨λΈμ„ λ¨λ“λ³„λ΅ λ¶„λ¦¬ν•μ—¬ state_dict μ¶”μ¶"""
        modular_states = {}
        
        # Group A: Backbone + Heads
        group_a_modules = ['backbone', 'emotion_head', 'bentham_head', 
                          'regret_head', 'surd_head']
        for module_name in group_a_modules:
            if hasattr(model, module_name):
                module = getattr(model, module_name)
                if module is not None:
                    modular_states[module_name] = module.state_dict()
        
        # Group B: Neural Analyzers
        group_b_modules = ['neural_emotion', 'neural_bentham', 
                          'neural_regret', 'neural_surd']
        for module_name in group_b_modules:
            if hasattr(model, module_name):
                module = getattr(model, module_name)
                if module is not None:
                    modular_states[module_name] = module.state_dict()
        
        # Group C: DSP + Kalman
        group_c_modules = ['emotion_dsp', 'kalman_filter']
        for module_name in group_c_modules:
            if hasattr(model, module_name):
                module = getattr(model, module_name)
                if module is not None:
                    modular_states[module_name] = module.state_dict()
        
        # Independent: Advanced Analyzers
        independent_modules = ['advanced_emotion', 'advanced_regret', 
                              'advanced_surd', 'advanced_bentham']
        for module_name in independent_modules:
            if hasattr(model, module_name):
                module = getattr(model, module_name)
                if module is not None:
                    modular_states[module_name] = module.state_dict()
        
        return modular_states
    
    def _update_metrics_history(self, epoch: int, metrics: Dict[str, Any]):
        """λ©”νΈλ¦­ νμ¤ν† λ¦¬ μ—…λ°μ΄νΈ"""
        # μ „μ²΄ λ©”νΈλ¦­
        self.metrics_history['global'].append({
            'epoch': epoch,
            'metrics': metrics.copy()
        })
        
        # λ¨λ“λ³„ λ©”νΈλ¦­
        for key, value in metrics.items():
            if '_' in key:  # λ¨λ“ μ΄λ¦„μ΄ ν¬ν•¨λ λ©”νΈλ¦­
                module_name = key.split('_')[0]
                if module_name not in self.metrics_history['modules']:
                    self.metrics_history['modules'][module_name] = []
                self.metrics_history['modules'][module_name].append({
                    'epoch': epoch,
                    'value': value
                })
    
    def _detect_sweet_spots(self, epoch: int, metrics: Dict[str, Any]):
        """Sweet Spot μλ™ νƒμ§€"""
        for module_name in self.metrics_history['modules']:
            history = self.metrics_history['modules'][module_name]
            if len(history) >= 5:  # μµμ† 5κ° λ°μ΄ν„°ν¬μΈνΈ ν•„μ”
                recent_values = [h['value'] for h in history[-5:]]
                
                # Sweet Spot μ΅°κ±΄: μµκ·Ό 5 μ—ν­ μ¤‘ λ³€λ™μ΄ μ‘κ³  μ„±λ¥μ΄ μΆ‹μ
                std_dev = np.std(recent_values)
                mean_value = np.mean(recent_values)
                
                # μ΄μ „ Sweet Spotκ³Ό λΉ„κµ
                if module_name not in self.sweet_spots or \
                   mean_value < self.sweet_spots[module_name]['value']:
                    if std_dev < 0.01:  # μ•μ •μ„± μ΅°κ±΄
                        self.sweet_spots[module_name] = {
                            'epoch': epoch,
                            'value': mean_value,
                            'std': std_dev
                        }
                        logger.info(f"  π― Sweet Spot λ°κ²¬: {module_name} @ epoch {epoch}")
    
    def _cleanup_old_checkpoints(self):
        """μ¤λλ μ²΄ν¬ν¬μΈνΈ μ •λ¦¬"""
        if len(self.checkpoint_metadata) > self.max_checkpoints:
            # κ°€μ¥ μ¤λλ μ²΄ν¬ν¬μΈνΈ μ‚­μ 
            oldest = self.checkpoint_metadata[0]
            checkpoint_path = Path(oldest['path'])
            if checkpoint_path.exists():
                checkpoint_path.unlink()
                logger.info(f"  π—‘οΈ μ¤λλ μ²΄ν¬ν¬μΈνΈ μ‚­μ : {checkpoint_path.name}")
            self.checkpoint_metadata.pop(0)
            self.save_metadata()
    
    def load_checkpoint(self, checkpoint_path: Optional[str] = None) -> Dict[str, Any]:
        """
        μ²΄ν¬ν¬μΈνΈ λ΅λ“
        
        Args:
            checkpoint_path: λ΅λ“ν•  μ²΄ν¬ν¬μΈνΈ κ²½λ΅ (Noneμ΄λ©΄ μµμ‹ )
            
        Returns:
            μ²΄ν¬ν¬μΈνΈ λ°μ΄ν„°
        """
        if checkpoint_path is None:
            # μµμ‹  μ²΄ν¬ν¬μΈνΈ μ°ΎκΈ°
            if not self.checkpoint_metadata:
                raise ValueError("μ €μ¥λ μ²΄ν¬ν¬μΈνΈκ°€ μ—†μµλ‹λ‹¤")
            checkpoint_path = self.checkpoint_metadata[-1]['path']
        
        checkpoint_path = Path(checkpoint_path)
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"μ²΄ν¬ν¬μΈνΈλ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {checkpoint_path}")
        
        checkpoint_data = torch.load(checkpoint_path, map_location='cpu')
        logger.info(f"β… μ²΄ν¬ν¬μΈνΈ λ΅λ“: {checkpoint_path}")
        logger.info(f"   - μ—ν­: {checkpoint_data['epoch']}")
        logger.info(f"   - LR: {checkpoint_data['lr']:.6f}")
        
        return checkpoint_data
    
    def get_best_checkpoint(self, metric_name: str = 'loss') -> Optional[str]:
        """
        νΉμ • λ©”νΈλ¦­ κΈ°μ¤€ μµκ³  μ„±λ¥ μ²΄ν¬ν¬μΈνΈ μ°ΎκΈ°
        
        Args:
            metric_name: ν‰κ°€ λ©”νΈλ¦­ μ΄λ¦„
            
        Returns:
            μµκ³  μ„±λ¥ μ²΄ν¬ν¬μΈνΈ κ²½λ΅
        """
        if not self.checkpoint_metadata:
            return None
        
        best_checkpoint = None
        best_value = float('inf')
        
        for metadata in self.checkpoint_metadata:
            if metric_name in metadata['metrics']:
                value = metadata['metrics'][metric_name]
                if value < best_value:
                    best_value = value
                    best_checkpoint = metadata['path']
        
        if best_checkpoint:
            logger.info(f"π† μµκ³  μ„±λ¥ μ²΄ν¬ν¬μΈνΈ: {best_checkpoint}")
            logger.info(f"   - {metric_name}: {best_value:.4f}")
        
        return best_checkpoint
    
    def get_sweet_spot_summary(self) -> Dict[str, Any]:
        """Sweet Spot μ”μ•½ μ •λ³΄ λ°ν™"""
        summary = {
            'detected_spots': len(self.sweet_spots),
            'modules': {}
        }
        
        for module_name, spot_info in self.sweet_spots.items():
            summary['modules'][module_name] = {
                'optimal_epoch': spot_info['epoch'],
                'metric_value': spot_info['value'],
                'stability': spot_info['std']
            }
        
        return summary
    
    def export_training_curves(self, output_dir: str = "training/plots"):
        """ν•™μµ κ³΅μ„  λ°μ΄ν„° λ‚΄λ³΄λ‚΄κΈ°"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # μ „μ²΄ λ©”νΈλ¦­ νμ¤ν† λ¦¬ μ €μ¥
        curves_data = {
            'global_metrics': self.metrics_history['global'],
            'module_metrics': self.metrics_history['modules'],
            'sweet_spots': self.sweet_spots,
            'checkpoint_metadata': self.checkpoint_metadata
        }
        
        output_file = output_dir / f"training_curves_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w') as f:
            json.dump(curves_data, f, indent=2)
        
        logger.info(f"π“ ν•™μµ κ³΅μ„  λ°μ΄ν„° λ‚΄λ³΄λ‚΄κΈ°: {output_file}")
        
        return str(output_file)