#!/usr/bin/env python3
"""
í›„íšŒ ê¸°ë°˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ (7íšŒ í›„íšŒ/ìŠ¤í…, 3ë²ˆ ì„ íšŒ)
Regret-Based Training Pipeline (7 regrets per step, 3 epochs)
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict
import math
import gc
from collections import defaultdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ëª¨ë“ˆ imports
from models.mega_scale_models.scalable_xai_model import create_mega_scale_model, optimize_model_for_inference
from models.hierarchical_emotion.emotion_phase_models import HierarchicalEmotionModel
from xai_core.xai_logging_system import xai_logger, xai_trace
from llm_module import llm_tracker, register_llm, ask_llm

@dataclass
class RegretTrainingConfig:
    """í›„íšŒ ê¸°ë°˜ í•™ìŠµ ì„¤ì •"""
    # í›„íšŒ ì„¤ì •
    regrets_per_step: int = 7
    bentham_calculations_per_regret: int = 3  # ì´ 21ë²ˆì˜ ë²¤ë‹´ ê³„ì‚°
    epochs: int = 3
    
    # í•™ìŠµ ì„¤ì •
    batch_size: int = 16
    learning_rate: float = 1e-4
    warmup_steps: int = 100
    max_gradient_norm: float = 1.0
    
    # ë¡œê¹… ì„¤ì •
    log_every_n_steps: int = 20
    save_checkpoint_every: int = 100
    
    # ìŠ¤í† ë¦¬ì§€ ì„¤ì •
    max_storage_gb: float = 200.0
    cleanup_old_logs: bool = True
    
    # ëª¨ë¸ ì„¤ì •
    model_params: int = 200_000_000
    sequence_length: int = 128
    
    def __post_init__(self):
        """ê³„ì‚°ëœ ê°’ë“¤"""
        self.total_bentham_per_step = self.regrets_per_step * self.bentham_calculations_per_regret

class RegretCalculator:
    """í›„íšŒ ê³„ì‚° ëª¨ë“ˆ"""
    
    def __init__(self, config: RegretTrainingConfig):
        self.config = config
        
        # í›„íšŒ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
        self.regret_weights = {
            'counterfactual': 0.3,    # "ë§Œì•½ ~í–ˆë‹¤ë©´"
            'temporal': 0.2,          # "ê·¸ë•Œ ~í–ˆì–´ì•¼ í–ˆëŠ”ë°"
            'moral': 0.25,            # "ì˜³ì€ ì¼ì„ í•˜ì§€ ëª»í–ˆë‹¤"
            'opportunity': 0.15,      # "ê¸°íšŒë¥¼ ë†“ì³¤ë‹¤"
            'social': 0.1             # "ë‹¤ë¥¸ ì‚¬ëŒì„ ì‹¤ë§ì‹œì¼°ë‹¤"
        }
        
        # ë²¤ë‹´ ì¾Œë½ ê³„ì‚° ìš”ì†Œ
        self.bentham_factors = {
            'intensity': 1.0,    # ê°•ë„
            'duration': 0.8,     # ì§€ì†ì„±
            'certainty': 0.9,    # í™•ì‹¤ì„±
            'propinquity': 0.7,  # ê·¼ì ‘ì„±
            'fecundity': 0.6,    # ë‹¤ì‚°ì„±
            'purity': 0.8,       # ìˆœìˆ˜ì„±
            'extent': 0.5        # ë²”ìœ„
        }
    
    def calculate_regret_scenarios(self, original_decision: torch.Tensor, 
                                 context: Dict[str, Any]) -> List[Dict[str, torch.Tensor]]:
        """7ê°€ì§€ í›„íšŒ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        scenarios = []
        
        for i, regret_type in enumerate(self.regret_weights.keys()):
            if i >= self.config.regrets_per_step:
                break
                
            # ê° í›„íšŒ ìœ í˜•ë³„ ë³€í˜•ëœ ê²°ì • ìƒì„±
            regret_decision = self._generate_regret_decision(
                original_decision, regret_type, context
            )
            
            # ë²¤ë‹´ ì¾Œë½ ê³„ì‚°
            bentham_scores = self._calculate_bentham_pleasure(
                original_decision, regret_decision, regret_type
            )
            
            scenarios.append({
                'regret_type': regret_type,
                'original_decision': original_decision,
                'regret_decision': regret_decision,
                'bentham_scores': bentham_scores,
                'regret_weight': self.regret_weights[regret_type]
            })
        
        # ë¶€ì¡±í•œ ê²½ìš° ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        while len(scenarios) < self.config.regrets_per_step:
            scenarios.append(self._generate_additional_scenario(original_decision, context))
        
        return scenarios[:self.config.regrets_per_step]
    
    def _generate_regret_decision(self, original: torch.Tensor, 
                                regret_type: str, context: Dict[str, Any]) -> torch.Tensor:
        """í›„íšŒ ìœ í˜•ë³„ ë³€í˜•ëœ ê²°ì • ìƒì„±"""
        batch_size = original.shape[0]
        
        if regret_type == 'counterfactual':
            # ë°˜ëŒ€ ê²°ì •
            return -original + torch.randn_like(original) * 0.1
        elif regret_type == 'temporal':
            # ì‹œê°„ ì§€ì—°ëœ ê²°ì •
            return original * 0.7 + torch.randn_like(original) * 0.2
        elif regret_type == 'moral':
            # ë„ë•ì ìœ¼ë¡œ ë” ì˜¬ë°”ë¥¸ ê²°ì •
            moral_bias = torch.ones_like(original) * 0.3
            return original + moral_bias + torch.randn_like(original) * 0.1
        elif regret_type == 'opportunity':
            # ë” ì ê·¹ì ì¸ ê²°ì •
            return original * 1.3 + torch.randn_like(original) * 0.15
        elif regret_type == 'social':
            # ì‚¬íšŒì ìœ¼ë¡œ ë” ë°”ëŒì§í•œ ê²°ì •
            social_bias = torch.ones_like(original) * 0.2
            return original + social_bias + torch.randn_like(original) * 0.1
        else:
            return original + torch.randn_like(original) * 0.1
    
    def _calculate_bentham_pleasure(self, original: torch.Tensor, 
                                  regret: torch.Tensor, regret_type: str) -> Dict[str, torch.Tensor]:
        """ë²¤ë‹´ ì¾Œë½ ê³„ì‚° (7ê°€ì§€ ìš”ì†Œ)"""
        batch_size = original.shape[0]
        scores = {}
        
        for factor, weight in self.bentham_factors.items():
            if factor == 'intensity':
                # ê°ì • ê°•ë„
                scores[factor] = torch.abs(regret - original).mean(dim=-1) * weight
            elif factor == 'duration':
                # ì§€ì†ì„± (ê²°ì •ì˜ ì˜í–¥ ì§€ì†ë„)
                scores[factor] = torch.sigmoid(torch.norm(regret, dim=-1)) * weight
            elif factor == 'certainty':
                # í™•ì‹¤ì„± (ê²°ì •ì˜ í™•ì‹ ë„)
                scores[factor] = (1.0 - torch.std(regret, dim=-1)) * weight
            elif factor == 'propinquity':
                # ê·¼ì ‘ì„± (ì¦‰ì‹œì„±)
                scores[factor] = torch.exp(-torch.norm(regret - original, dim=-1)) * weight
            elif factor == 'fecundity':
                # ë‹¤ì‚°ì„± (ì¶”ê°€ ì¦ê±°ì›€ ìƒì„±)
                scores[factor] = torch.relu(regret.mean(dim=-1)) * weight
            elif factor == 'purity':
                # ìˆœìˆ˜ì„± (ê³ í†µ ì—†ëŠ” ì¦ê±°ì›€)
                scores[factor] = torch.sigmoid(regret.mean(dim=-1)) * weight
            elif factor == 'extent':
                # ë²”ìœ„ (ì˜í–¥ë°›ëŠ” ì‚¬ëŒ ìˆ˜)
                scores[factor] = torch.tanh(torch.norm(regret, dim=-1)) * weight
        
        return scores
    
    def _generate_additional_scenario(self, original: torch.Tensor, 
                                    context: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        regret_decision = original + torch.randn_like(original) * 0.2
        bentham_scores = self._calculate_bentham_pleasure(original, regret_decision, 'additional')
        
        return {
            'regret_type': 'additional',
            'original_decision': original,
            'regret_decision': regret_decision,
            'bentham_scores': bentham_scores,
            'regret_weight': 0.1
        }

class RegretDataset(Dataset):
    """í›„íšŒ ê¸°ë°˜ ë°ì´í„°ì…‹"""
    
    def __init__(self, data_files: List[Path], config: RegretTrainingConfig):
        self.config = config
        self.scenarios = []
        
        # ë°ì´í„° ë¡œë“œ
        for file_path in data_files:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    self.scenarios.extend(data)
                else:
                    self.scenarios.append(data)
        
        print(f"âœ… ì´ {len(self.scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œë¨")
    
    def __len__(self):
        return len(self.scenarios)
    
    def __getitem__(self, idx):
        scenario = self.scenarios[idx]
        
        # í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ë‹¨ìˆœí™”)
        description = scenario.get('description', '')
        
        # ì„ë² ë”© ì‹œë®¬ë ˆì´ì…˜ (ë©”ê°€ìŠ¤ì¼€ì¼ ëª¨ë¸ ì…ë ¥ ì°¨ì›ì— ë§ì¶¤)
        embedding = torch.randn(1024)  # ë©”ê°€ìŠ¤ì¼€ì¼ ëª¨ë¸ ì…ë ¥ ì°¨ì›
        
        # ë¼ë²¨ ì¤€ë¹„
        options = scenario.get('options', [])
        if len(options) >= 3:
            # approve, disapprove, neutral
            labels = torch.tensor([0.5, 0.3, 0.2])  # ì˜ˆì‹œ ë¶„í¬
        else:
            labels = torch.tensor([1.0, 0.0, 0.0])
        
        return {
            'text_embedding': embedding,
            'labels': labels,
            'scenario_id': scenario.get('id', f'scenario_{idx}'),
            'category': scenario.get('category', 'general'),
            'complexity': scenario.get('complexity_score', 0.5)
        }

class StorageMonitor:
    """ìŠ¤í† ë¦¬ì§€ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, max_gb: float, base_dir: Path):
        self.max_bytes = max_gb * 1024 * 1024 * 1024
        self.base_dir = base_dir
        
    def get_directory_size(self, directory: Path) -> int:
        """ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°"""
        total = 0
        for path in directory.rglob('*'):
            if path.is_file():
                total += path.stat().st_size
        return total
    
    def cleanup_if_needed(self):
        """í•„ìš”ì‹œ ì •ë¦¬"""
        current_size = self.get_directory_size(self.base_dir)
        
        if current_size > self.max_bytes:
            print(f"âš ï¸ ìŠ¤í† ë¦¬ì§€ í•œê³„ ì´ˆê³¼: {current_size / 1024**3:.2f}GB")
            
            # ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì‚­ì œ
            log_files = list(self.base_dir.glob('**/*.log'))
            log_files.sort(key=lambda x: x.stat().st_mtime)
            
            for log_file in log_files[:len(log_files)//2]:
                log_file.unlink()
                print(f"ğŸ—‘ï¸ ì‚­ì œë¨: {log_file}")
    
    def get_size_gb(self) -> float:
        """í˜„ì¬ í¬ê¸° GB ë°˜í™˜"""
        return self.get_directory_size(self.base_dir) / (1024**3)

class RegretTrainer:
    """í›„íšŒ ê¸°ë°˜ í•™ìŠµê¸°"""
    
    def __init__(self, config: RegretTrainingConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = project_root / 'training' / 'outputs'
        self.logs_dir = self.output_dir / 'logs'
        self.checkpoints_dir = self.output_dir / 'checkpoints'
        self.reports_dir = self.output_dir / 'reports'
        
        for dir_path in [self.output_dir, self.logs_dir, self.checkpoints_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ìŠ¤í† ë¦¬ì§€ ëª¨ë‹ˆí„°
        self.storage_monitor = StorageMonitor(config.max_storage_gb, self.output_dir)
        
        # í›„íšŒ ê³„ì‚°ê¸°
        self.regret_calculator = RegretCalculator(config)
        
        # í•™ìŠµ í†µê³„
        self.training_stats = defaultdict(list)
        self.step_count = 0
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = self.logs_dir / f'regret_training_{int(time.time())}.log'
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('RegretTrainer')
    
    def prepare_models(self):
        """ëª¨ë¸ ì¤€ë¹„"""
        self.logger.info("ğŸ¤– ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
        
        # ë©”ê°€ ìŠ¤ì¼€ì¼ ëª¨ë¸
        self.main_model = create_mega_scale_model(target_params=self.config.model_params)
        self.main_model = optimize_model_for_inference(self.main_model)
        self.main_model.to(self.device)
        
        # ê°ì • ëª¨ë¸
        self.emotion_model = HierarchicalEmotionModel()
        self.emotion_model.to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.AdamW(
            list(self.main_model.parameters()) + list(self.emotion_model.parameters()),
            lr=self.config.learning_rate,
            weight_decay=0.01
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=1000, eta_min=1e-6
        )
        
        self.logger.info(f"âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ: {self.main_model.get_parameter_count():,}ê°œ íŒŒë¼ë¯¸í„°")
    
    def prepare_data(self) -> DataLoader:
        """ë°ì´í„° ì¤€ë¹„"""
        self.logger.info("ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        data_dir = project_root / 'processed_datasets'
        batch_files = list(data_dir.glob('full_scenarios_batch_*.json'))
        
        if not batch_files:
            raise FileNotFoundError("ë°°ì¹˜ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = RegretDataset(batch_files, self.config)
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        dataloader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=True
        )
        
        self.logger.info(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(dataset)}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
        return dataloader
    
    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """ë‹¨ì¼ í•™ìŠµ ìŠ¤í…"""
        self.main_model.train()
        self.emotion_model.train()
        
        text_embeddings = batch['text_embedding'].to(self.device)
        labels = batch['labels'].to(self.device)
        
        # ë©”ì¸ ëª¨ë¸ ìˆœì „íŒŒ
        main_outputs = self.main_model(text_embeddings.unsqueeze(1))
        
        # ê°ì • ëª¨ë¸ ìˆœì „íŒŒ (768ì°¨ì›ìœ¼ë¡œ ë³€í™˜)
        emotion_embeddings = text_embeddings[:, :768] if text_embeddings.shape[1] > 768 else F.pad(text_embeddings, (0, 768 - text_embeddings.shape[1]))
        emotion_outputs = self.emotion_model(emotion_embeddings)
        
        # í›„íšŒ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (7ê°œ)
        regret_scenarios = self.regret_calculator.calculate_regret_scenarios(
            text_embeddings, {
                'categories': batch['category'],
                'complexity': batch['complexity']
            }
        )
        
        total_loss = 0.0
        regret_losses = []
        bentham_scores = []
        
        # ê° í›„íšŒ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì†ì‹¤ ê³„ì‚°
        for scenario in regret_scenarios:
            regret_decision = scenario['regret_decision'].to(self.device)
            
            # í›„íšŒ ê²°ì •ì— ëŒ€í•œ ëª¨ë¸ ì¶œë ¥
            regret_outputs = self.main_model(regret_decision.unsqueeze(1))
            
            # ë²¤ë‹´ ì¾Œë½ ê³„ì‚° (3ë²ˆì”©)
            for i in range(self.config.bentham_calculations_per_regret):
                bentham_loss = self._calculate_bentham_loss(
                    main_outputs, regret_outputs, scenario['bentham_scores']
                )
                total_loss += bentham_loss * scenario['regret_weight']
                bentham_scores.append(bentham_loss.item())
        
        # ê¸°ë³¸ ë¶„ë¥˜ ì†ì‹¤ (ì°¨ì› ë§ì¶¤)
        emotion_predictions = emotion_outputs['final_emotion']  # [batch_size, 6]
        emotion_avg = emotion_predictions.mean(dim=1, keepdim=True).expand(-1, 3)  # [batch_size, 3]ìœ¼ë¡œ í™•ì¥
        classification_loss = nn.MSELoss()(emotion_avg, labels)
        total_loss += classification_loss
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        total_loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(
            list(self.main_model.parameters()) + list(self.emotion_model.parameters()),
            self.config.max_gradient_norm
        )
        
        self.optimizer.step()
        self.scheduler.step()
        
        return {
            'total_loss': total_loss.item(),
            'classification_loss': classification_loss.item(),
            'regret_count': len(regret_scenarios),
            'bentham_count': len(bentham_scores),
            'avg_bentham_score': np.mean(bentham_scores) if bentham_scores else 0.0,
            'learning_rate': self.scheduler.get_last_lr()[0]
        }
    
    def _calculate_bentham_loss(self, original_outputs: Dict[str, torch.Tensor],
                              regret_outputs: Dict[str, torch.Tensor],
                              bentham_scores: Dict[str, torch.Tensor]) -> torch.Tensor:
        """ë²¤ë‹´ ê¸°ë°˜ ì†ì‹¤ ê³„ì‚° (NaN ë°©ì§€ ì•ˆì •ì„± ê°œì„ )"""
        # ê°ì • ì˜ˆì¸¡ ì°¨ì´ (NaN ê²€ì‚¬ ì¶”ê°€)
        original_emotion = original_outputs.get('emotion_predictions', torch.zeros(1))
        regret_emotion = regret_outputs.get('emotion_predictions', torch.zeros(1))
        
        if torch.isnan(original_emotion).any() or torch.isnan(regret_emotion).any():
            logger.warning("ê°ì • ì˜ˆì¸¡ì˜ NaN ê°’ ë°œê²¬, ê¸°ë³¸ê°’ ì‚¬ìš©")
            emotion_diff = torch.tensor(0.1, dtype=torch.float32, requires_grad=True)
        else:
            emotion_diff = torch.abs(original_emotion - regret_emotion).mean()
            emotion_diff = torch.clamp(emotion_diff, min=0.0, max=10.0)  # ë²”ìœ„ ì œí•œ
        
        # ì˜ë¯¸ ì˜ˆì¸¡ ì°¨ì´ (NaN ê²€ì‚¬ ì¶”ê°€)
        original_semantic = original_outputs.get('semantic_predictions', torch.zeros(1))
        regret_semantic = regret_outputs.get('semantic_predictions', torch.zeros(1))
        
        if torch.isnan(original_semantic).any() or torch.isnan(regret_semantic).any():
            logger.warning("ì˜ë¯¸ ì˜ˆì¸¡ì˜ NaN ê°’ ë°œê²¬, ê¸°ë³¸ê°’ ì‚¬ìš©")
            semantic_diff = torch.tensor(0.1, dtype=torch.float32, requires_grad=True)
        else:
            semantic_diff = torch.abs(original_semantic - regret_semantic).mean()
            semantic_diff = torch.clamp(semantic_diff, min=0.0, max=10.0)  # ë²”ìœ„ ì œí•œ
        
        # ë²¤ë‹´ ì ìˆ˜ ê°€ì¤‘í•© (NaN ë°©ì§€)
        try:
            bentham_values = list(bentham_scores.values())
            if bentham_values and not any(torch.isnan(v).any() if torch.is_tensor(v) else math.isnan(v) for v in bentham_values):
                bentham_weight = torch.stack([torch.tensor(v) if not torch.is_tensor(v) else v for v in bentham_values]).mean()
                bentham_weight = torch.clamp(bentham_weight, min=0.1, max=5.0)  # ë²”ìœ„ ì œí•œ
            else:
                bentham_weight = torch.tensor(1.0, dtype=torch.float32)
        except Exception as e:
            logger.warning(f"ë²¤ë‹´ ê°€ì¤‘ì¹˜ ê³„ì‚° ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            bentham_weight = torch.tensor(1.0, dtype=torch.float32)
        
        # ìµœì¢… ì†ì‹¤ ê³„ì‚° ë° NaN ë°©ì§€
        final_loss = (emotion_diff + semantic_diff) * bentham_weight
        final_loss = torch.clamp(final_loss, min=0.0, max=100.0)  # ë²”ìœ„ ì œí•œ
        
        if torch.isnan(final_loss) or torch.isinf(final_loss):
            logger.error(f"ë²¤ë‹´ ì†ì‹¤ ê³„ì‚° ê²°ê³¼ê°€ ë¹„ì •ìƒ: {final_loss}")
            return torch.tensor(1.0, dtype=torch.float32, requires_grad=True)
            
        return final_loss
    
    def train_epoch(self, dataloader: DataLoader, epoch: int):
        """ì—í¬í¬ í•™ìŠµ"""
        self.logger.info(f"ğŸ¯ ì—í¬í¬ {epoch+1}/{self.config.epochs} ì‹œì‘")
        
        epoch_stats = defaultdict(list)
        
        for batch_idx, batch in enumerate(dataloader):
            step_stats = self.train_step(batch)
            
            # í†µê³„ ìˆ˜ì§‘
            for key, value in step_stats.items():
                epoch_stats[key].append(value)
                self.training_stats[key].append(value)
            
            self.step_count += 1
            
            # ì£¼ê¸°ì  ë¡œê¹…
            if self.step_count % self.config.log_every_n_steps == 0:
                avg_loss = np.mean(epoch_stats['total_loss'][-self.config.log_every_n_steps:])
                avg_regret = np.mean(epoch_stats['regret_count'][-self.config.log_every_n_steps:])
                avg_bentham = np.mean(epoch_stats['avg_bentham_score'][-self.config.log_every_n_steps:])
                
                self.logger.info(
                    f"ìŠ¤í… {self.step_count}: ì†ì‹¤={avg_loss:.4f}, "
                    f"í›„íšŒ={avg_regret:.1f}, ë²¤ë‹´={avg_bentham:.4f}, "
                    f"ìŠ¤í† ë¦¬ì§€={self.storage_monitor.get_size_gb():.1f}GB"
                )
                
                # XAI ë¡œê¹…
                with xai_logger.trace_operation("regret_training", f"step_{self.step_count}") as op_id:
                    xai_logger.log_llm_interaction(
                        operation_id=op_id,
                        prompt=f"Step {self.step_count} training metrics",
                        response=f"Loss: {avg_loss:.4f}, Regrets: {avg_regret}",
                        model_name="regret_trainer",
                        tokens_used=len(str(step_stats))
                    )
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if self.step_count % self.config.save_checkpoint_every == 0:
                self.save_checkpoint(epoch, batch_idx)
            
            # ìŠ¤í† ë¦¬ì§€ ëª¨ë‹ˆí„°ë§
            if self.step_count % 50 == 0:
                self.storage_monitor.cleanup_if_needed()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if batch_idx % 10 == 0:
                torch.cuda.empty_cache() if torch.cuda.is_available() else gc.collect()
        
        # ì—í¬í¬ ìš”ì•½
        avg_epoch_loss = np.mean(epoch_stats['total_loss'])
        total_regrets = sum(epoch_stats['regret_count'])
        total_benthams = sum(epoch_stats['bentham_count'])
        
        self.logger.info(
            f"âœ… ì—í¬í¬ {epoch+1} ì™„ë£Œ: í‰ê·  ì†ì‹¤={avg_epoch_loss:.4f}, "
            f"ì´ í›„íšŒ={total_regrets}, ì´ ë²¤ë‹´ ê³„ì‚°={total_benthams}"
        )
    
    def save_checkpoint(self, epoch: int, batch_idx: int):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = self.checkpoints_dir / f'regret_model_epoch_{epoch}_step_{self.step_count}.pth'
        
        checkpoint = {
            'epoch': epoch,
            'step': self.step_count,
            'batch_idx': batch_idx,
            'main_model_state_dict': self.main_model.state_dict(),
            'emotion_model_state_dict': self.emotion_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'config': asdict(self.config),
            'training_stats': dict(self.training_stats),
            'timestamp': datetime.now().isoformat()
        }
        
        torch.save(checkpoint, checkpoint_path)
        self.logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    def generate_training_report(self) -> Dict[str, Any]:
        """í•™ìŠµ ë¦¬í¬íŠ¸ ìƒì„±"""
        total_steps = len(self.training_stats['total_loss'])
        total_regrets = sum(self.training_stats['regret_count'])
        total_benthams = sum(self.training_stats['bentham_count'])
        
        report = {
            'training_summary': {
                'total_steps': total_steps,
                'total_regrets': total_regrets,
                'total_bentham_calculations': total_benthams,
                'average_regrets_per_step': total_regrets / total_steps if total_steps > 0 else 0,
                'average_benthams_per_step': total_benthams / total_steps if total_steps > 0 else 0,
                'final_loss': self.training_stats['total_loss'][-1] if self.training_stats['total_loss'] else 0,
                'training_duration': time.time()
            },
            'model_info': {
                'main_model_parameters': self.main_model.get_parameter_count(),
                'target_parameters': self.config.model_params,
                'device': str(self.device)
            },
            'configuration': asdict(self.config),
            'storage_usage': {
                'final_size_gb': self.storage_monitor.get_size_gb(),
                'max_allowed_gb': self.config.max_storage_gb
            },
            'xai_integration': {
                'xai_logs_generated': len(xai_logger.logs),
                'session_id': xai_logger.session_id
            }
        }
        
        return report
    
    def train(self):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
        start_time = time.time()
        
        self.logger.info("ğŸš€ í›„íšŒ ê¸°ë°˜ í•™ìŠµ ì‹œì‘")
        self.logger.info(f"ğŸ“Š ì„¤ì •: {self.config.regrets_per_step}íšŒ í›„íšŒ/ìŠ¤í…, {self.config.epochs}ë²ˆ ì„ íšŒ")
        
        # ëª¨ë¸ ë° ë°ì´í„° ì¤€ë¹„
        self.prepare_models()
        dataloader = self.prepare_data()
        
        # í•™ìŠµ ì‹¤í–‰
        for epoch in range(self.config.epochs):
            self.train_epoch(dataloader, epoch)
            
            # ì—í¬í¬ë³„ ì²´í¬í¬ì¸íŠ¸
            self.save_checkpoint(epoch, -1)
        
        # ìµœì¢… ì €ì¥
        final_checkpoint_path = self.checkpoints_dir / 'final_regret_model.pth'
        torch.save({
            'main_model_state_dict': self.main_model.state_dict(),
            'emotion_model_state_dict': self.emotion_model.state_dict(),
            'config': asdict(self.config),
            'training_stats': dict(self.training_stats),
            'timestamp': datetime.now().isoformat()
        }, final_checkpoint_path)
        
        # í•™ìŠµ ë¦¬í¬íŠ¸ ìƒì„±
        training_time = time.time() - start_time
        report = self.generate_training_report()
        report['training_summary']['training_duration'] = training_time
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = self.reports_dir / f'regret_training_report_{int(time.time())}.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ‰ í•™ìŠµ ì™„ë£Œ! ì´ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
        self.logger.info(f"ğŸ“Š ì´ í›„íšŒ: {sum(self.training_stats['regret_count'])}")
        self.logger.info(f"ğŸ“Š ì´ ë²¤ë‹´ ê³„ì‚°: {sum(self.training_stats['bentham_count'])}")
        self.logger.info(f"ğŸ“‹ ë¦¬í¬íŠ¸: {report_path}")
        
        return report, final_checkpoint_path

def create_training_config(**kwargs) -> RegretTrainingConfig:
    """í•™ìŠµ ì„¤ì • ìƒì„± í—¬í¼"""
    return RegretTrainingConfig(**kwargs)

if __name__ == "__main__":
    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    config = RegretTrainingConfig()
    trainer = RegretTrainer(config)
    
    print("ğŸ§ª í›„íšŒ ê¸°ë°˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ")
    print(f"ğŸ“Š ì„¤ì •: {config.regrets_per_step}íšŒ í›„íšŒ/ìŠ¤í…, {config.epochs}ë²ˆ ì„ íšŒ")
    print(f"ğŸ’¾ ìŠ¤í† ë¦¬ì§€ í•œê³„: {config.max_storage_gb}GB")
    print("ì¤€ë¹„ëœ í•™ìŠµì„ ì‹œì‘í•˜ë ¤ë©´ trainer.train()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")