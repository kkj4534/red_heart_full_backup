#!/usr/bin/env python3
"""
ë…ë¦½ì ì¸ ê³„ì¸µì  LR ìŠ¤ìœ• ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
5-5-5-5 ì „ëµìœ¼ë¡œ ì´ 25ê°œ í¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
ê° LRë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ì´ˆê¸° ê°€ì¤‘ì¹˜ì—ì„œ ì‹œì‘
"""

import sys
import os
sys.path.append('/mnt/c/large_project/linux_red_heart')
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import logging
from datetime import datetime
import time
import json
from pathlib import Path
import numpy as np
from sentence_transformer_singleton import get_sentence_transformer, SentenceTransformerManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'training/lr_sweep_results/hierarchical_sweep_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger(__name__)

# ëª¨ë“ˆ ì„í¬íŠ¸
from training.unified_training_final import UnifiedModel, UnifiedTrainingConfig
from training.hierarchical_lr_sweep import HierarchicalLRSweep


class RedHeartDataset(Dataset):
    """ì‹¤ì œ Red Heart ë°ì´í„°ì…‹"""
    def __init__(self, data_list, preprocessed_path=None):
        self.data = data_list
        self.preprocessed_path = preprocessed_path
        self.embedding_manager = None  # ì§€ì—° ì´ˆê¸°í™”
        self.embeddings_modified = False
        
        # label ë§¤í•‘
        self.label_to_idx = {
            'AUTHOR': 0,
            'EVERYBODY': 1,
            'INFO': 2,
            'NOBODY': 3,
            'OTHER': 4
        }
        # ê°ì • ë§¤í•‘
        self.emotion_keys = ['joy', 'anger', 'surprise', 'disgust', 'sadness', 'shame', 'fear']
        
        # ì„ë² ë”© ìƒíƒœ í™•ì¸
        self._check_embeddings()
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        """ì‹¤ì œ ë°ì´í„° ë°˜í™˜"""
        item = self.data[idx]
        text = item.get('text', '') + ' ' + item.get('title', '')
        
        # ì„ë² ë”© ì²˜ë¦¬
        if 'embedding' in item:
            text_embedding = torch.tensor(item['embedding'], dtype=torch.float32)
            # 100x768 í¬ê¸°ë¡œ ì¡°ì •
            if text_embedding.shape[0] < 100:
                pad_size = 100 - text_embedding.shape[0]
                text_embedding = torch.cat([text_embedding, torch.zeros(pad_size, 768)], dim=0)
            elif text_embedding.shape[0] > 100:
                text_embedding = text_embedding[:100]
        else:
            # ì„ë² ë”©ì´ ì—†ìœ¼ë©´ SentenceTransformerë¡œ ìƒì„±
            if self.embedding_manager is None:
                try:
                    self.embedding_manager = get_sentence_transformer(
                        'sentence-transformers/all-MiniLM-L6-v2',
                        device='cuda' if torch.cuda.is_available() else 'cpu',
                        cache_folder=os.path.expanduser('~/.cache/huggingface/hub')
                    )
                except Exception as e:
                    logger.error(f"âŒ SentenceTransformer ë¡œë“œ ì‹¤íŒ¨: {e}")
                    logger.error("LR ìŠ¤ìœ•ì— í•„ìˆ˜ì ì¸ ì„ë² ë”© ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨")
                    raise RuntimeError(f"SentenceTransformer í•„ìˆ˜ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            if self.embedding_manager:
                try:
                    embedding = self.embedding_manager.encode(text[:512])
                    text_embedding = torch.tensor(embedding, dtype=torch.float32)
                    if text_embedding.dim() == 1:
                        text_embedding = text_embedding.unsqueeze(0)
                    
                    # 384ì°¨ì›ì„ 768ì°¨ì›ìœ¼ë¡œ íŒ¨ë”© (all-MiniLM-L6-v2ëŠ” 384ì°¨ì› ì¶œë ¥)
                    if text_embedding.shape[-1] == 384:
                        padding = torch.zeros(text_embedding.shape[0], 384, dtype=torch.float32)
                        text_embedding = torch.cat([text_embedding, padding], dim=-1)  # (1, 768)
                    
                    text_embedding = text_embedding.repeat(100, 1)
                    self.data[idx]['embedding'] = text_embedding.numpy().tolist()
                    self.embeddings_modified = True
                except Exception as e:
                    logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                    raise RuntimeError(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            else:
                logger.error("âŒ SentenceTransformer ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                raise RuntimeError("SentenceTransformer ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
        
        # label ì²˜ë¦¬
        label_str = item.get('label', 'OTHER')
        label_idx = self.label_to_idx.get(label_str, 4)
        
        # emotions ì²˜ë¦¬
        emotions = item.get('emotions', {})
        if isinstance(emotions, dict):
            emotion_vector = [emotions.get(key, 0.0) for key in self.emotion_keys]
            emotion_label = torch.argmax(torch.tensor(emotion_vector)).item()
        else:
            emotion_label = 0
        
        # bentham_scores ì²˜ë¦¬
        bentham_keys = ['intensity', 'duration', 'certainty', 'propinquity',
                        'purity', 'extent', 'fecundity', 'remoteness', 
                        'succession', 'utility']
        bentham_scores = item.get('bentham_scores', {})
        if isinstance(bentham_scores, dict):
            bentham_vector = [bentham_scores.get(key, 0.5) for key in bentham_keys]
        else:
            bentham_vector = [0.5] * 10
        
        return {
            'input': text_embedding,
            'emotion_label': torch.tensor(emotion_label, dtype=torch.long),
            'bentham_label': torch.tensor(bentham_vector, dtype=torch.float),
            'regret_label': torch.tensor(item.get('regret_factor', 0.0), dtype=torch.float),
            'surd_label': torch.tensor(label_idx, dtype=torch.long)
        }
    
    def _check_embeddings(self):
        """ì„ë² ë”© ìƒíƒœ í™•ì¸"""
        total_items = len(self.data)
        items_with_embedding = sum(1 for item in self.data if 'embedding' in item)
        items_without_embedding = total_items - items_with_embedding
        
        logger.info(f"ğŸ“Š ì„ë² ë”© ìƒíƒœ:")
        logger.info(f"  - ì „ì²´ ë°ì´í„°: {total_items}ê°œ")
        logger.info(f"  - ì„ë² ë”© ìˆìŒ: {items_with_embedding}ê°œ ({items_with_embedding/total_items*100:.1f}%)")
        logger.info(f"  - ì„ë² ë”© ì—†ìŒ: {items_without_embedding}ê°œ ({items_without_embedding/total_items*100:.1f}%)")
        
        if items_without_embedding > 0:
            logger.warning(f"âš ï¸ {items_without_embedding}ê°œ í•­ëª©ì— ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ ìƒì„±ë©ë‹ˆë‹¤.")
    
    def save_embeddings(self):
        """ìƒì„±ëœ ì„ë² ë”©ì„ íŒŒì¼ì— ì €ì¥"""
        if not self.embeddings_modified:
            return
        
        if self.preprocessed_path:
            embedded_path = Path(str(self.preprocessed_path).replace('.json', '.embedded.json'))
            try:
                with open(embedded_path, 'w', encoding='utf-8') as f:
                    json.dump(self.data, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… ì„ë² ë”©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {embedded_path}")
                self.embeddings_modified = False
            except Exception as e:
                logger.error(f"ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    logger.info("=" * 80)
    logger.info("ğŸš€ ë…ë¦½ì ì¸ Hierarchical Learning Rate Sweep ì‹œì‘")
    logger.info("=" * 80)
    
    # GPU ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Device: {device}")
    
    if device.type == 'cuda':
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        # ì´ˆê¸° GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
    
    # ëª¨ë¸ ì„¤ì •
    config = UnifiedTrainingConfig()
    config.d_model = 896
    config.num_heads = 16
    config.num_layers = 6
    config.d_ff = 3584
    config.dropout = 0.1
    config.vocab_size = 50000
    config.max_length = 512
    config.micro_batch_size = 2
    
    logger.info("\nğŸ“Š ëª¨ë¸ ì„¤ì •:")
    logger.info(f"  - d_model: {config.d_model}")
    logger.info(f"  - num_heads: {config.num_heads}")
    logger.info(f"  - num_layers: {config.num_layers}")
    logger.info(f"  - dropout: {config.dropout}")
    
    # ëª¨ë¸ ìƒì„±
    logger.info("\nğŸ”§ ëª¨ë¸ ìƒì„± ì¤‘...")
    model = UnifiedModel(config, device=device).to(device)
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"  - ì´ íŒŒë¼ë¯¸í„°: {total_params/1e6:.1f}M")
    logger.info(f"  - í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params/1e6:.1f}M")
    
    # ì´ˆê¸° ê°€ì¤‘ì¹˜ ì €ì¥ (ê° LR í…ŒìŠ¤íŠ¸ë§ˆë‹¤ ì´ ê°€ì¤‘ì¹˜ë¡œ ë¦¬ì…‹)
    initial_state = model.state_dict()
    torch.save(initial_state, 'training/lr_sweep_results/initial_weights.pth')
    logger.info("  - ì´ˆê¸° ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    logger.info("\nğŸ“ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    
    # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
    preprocessed_path = Path("claude_api_preprocessing/claude_preprocessed_complete.json")
    
    if not preprocessed_path.exists():
        preprocessed_path = Path("for_learn_dataset/claude_preprocessed_complete.json")
        if not preprocessed_path.exists():
            logger.error(f"ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {preprocessed_path}")
            raise FileNotFoundError(f"ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    
    # ì„ë² ë”©ì´ í¬í•¨ëœ íŒŒì¼ì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
    embedded_path = Path(str(preprocessed_path).replace('.json', '.embedded.json'))
    if embedded_path.exists():
        logger.info(f"ğŸ¯ ì„ë² ë”© íŒŒì¼ ë°œê²¬: {embedded_path}")
        with open(embedded_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    else:
        logger.info(f"ğŸ“‚ ê¸°ë³¸ ë°ì´í„° ë¡œë“œ: {preprocessed_path}")
        with open(preprocessed_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    
    # ì „ì²´ ë°ì´í„° ì„ë² ë”© ìƒì„± (ë‚˜ì¤‘ì— í•™ìŠµìš©ìœ¼ë¡œ ì¼ë¶€ë§Œ ì‚¬ìš©)
    total_data_count = len(data)
    logger.info(f"  - ì „ì²´ ë°ì´í„°: {total_data_count}ê°œ")
    
    # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í•  (90:10) - ì„ë² ë”©ì€ ì „ì²´ì— ëŒ€í•´ ìƒì„±
    val_size = int(len(data) * 0.1)
    all_train_data = data[val_size:]  # ì „ì²´ í•™ìŠµ ë°ì´í„°
    all_val_data = data[:val_size]    # ì „ì²´ ê²€ì¦ ë°ì´í„°
    
    # LR ìŠ¤ìœ•ìš©ìœ¼ë¡œ ì¼ë¶€ë§Œ ìƒ˜í”Œë§ (ì„ë² ë”©ì€ ì „ì²´ì— ëŒ€í•´ ì´ë¯¸ ìƒì„±ë¨)
    train_data = all_train_data[:1000]  # LR ìŠ¤ìœ•ìš© í•™ìŠµ ë°ì´í„°
    val_data = all_val_data[:100]       # LR ìŠ¤ìœ•ìš© ê²€ì¦ ë°ì´í„°
    
    logger.info(f"  - ì „ì²´ ë°ì´í„°: {len(data)}ê°œ")
    logger.info(f"  - LR ìŠ¤ìœ•ìš© í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ")
    logger.info(f"  - LR ìŠ¤ìœ•ìš© ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")
    
    # ì „ì²´ ë°ì´í„°ì…‹ ìƒì„± (ì„ë² ë”© ìƒì„±ìš©)
    logger.info("\nğŸ”„ ì „ì²´ ë°ì´í„° ì„ë² ë”© ìƒì„± ì¤‘...")
    full_dataset = RedHeartDataset(data, preprocessed_path)
    
    # ì„ë² ë”©ì´ ì—†ëŠ” ë°ì´í„° í™•ì¸ ë° ìƒì„±
    missing_embeddings = 0
    for idx, item in enumerate(data):
        if 'embedding' not in item or item.get('embedding') is None:
            missing_embeddings += 1
    
    if missing_embeddings > 0:
        logger.info(f"  - ì„ë² ë”© ìƒì„± í•„ìš”: {missing_embeddings}ê°œ / {len(data)}ê°œ")
        logger.info("  - ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (30ê°œì”©, ê° ë°°ì¹˜ íƒ€ì„ì•„ì›ƒ 13ë¶„)")
        
        # ì§„í–‰ ìƒí™© ë¡œê·¸ íŒŒì¼
        progress_log_path = Path(f'training/lr_sweep_results/embedding_progress_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
        
        batch_size = 30  # 30ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
        total_batches = (len(full_dataset) + batch_size - 1) // batch_size
        successful_count = 0
        failed_count = 0
        total_retry_count = 0  # ì „ì²´ í”„ë¡œì„¸ìŠ¤ì˜ ëˆ„ì  ì¬ì‹œë„ íšŸìˆ˜
        max_total_retries = 3  # ìµœëŒ€ í—ˆìš© ì¬ì‹œë„ íšŸìˆ˜
        total_skip_count = 0  # ì „ì²´ ìŠ¤í‚µ íšŸìˆ˜
        max_total_skips = 5  # ìµœëŒ€ í—ˆìš© ìŠ¤í‚µ íšŸìˆ˜
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for batch_idx in range(total_batches):
            batch_start = batch_idx * batch_size
            batch_end = min(batch_start + batch_size, len(full_dataset))
            batch_items = batch_end - batch_start
            
            # ì§„í–‰ë¥  ê³„ì‚°
            progress = (batch_start / len(full_dataset)) * 100
            eta_batches = total_batches - batch_idx
            eta_minutes = eta_batches * 0.5  # ë°°ì¹˜ë‹¹ í‰ê·  30ì´ˆ ì˜ˆìƒ
            
            logger.info(f"\n  ğŸ“¦ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘...")
            logger.info(f"    - ë²”ìœ„: {batch_start} ~ {batch_end-1} ({batch_items}ê°œ)")
            logger.info(f"    - ì§„í–‰ë¥ : {progress:.1f}%")
            logger.info(f"    - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {eta_minutes:.1f}ë¶„")
            
            # ì§„í–‰ ìƒí™© ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡
            with open(progress_log_path, 'a') as f:
                f.write(f"[{datetime.now().isoformat()}] ë°°ì¹˜ {batch_idx + 1}/{total_batches}: {batch_start}-{batch_end-1} (ì§„í–‰ë¥ : {progress:.1f}%)\n")
            
            batch_success = 0
            batch_fail = 0
            batch_start_time = time.time()
            
            # í˜„ì¬ ë°°ì¹˜ì˜ ì•„ì´í…œë“¤ ì²˜ë¦¬
            for idx in range(batch_start, batch_end):
                success = False
                for retry in range(2):  # ìµœëŒ€ 2ë²ˆ ì‹œë„ (ì›ë³¸ + ì¬ì‹œë„ 1ë²ˆ)
                    try:
                        _ = full_dataset[idx]  # ì´ ê³¼ì •ì—ì„œ ì„ë² ë”©ì´ ìƒì„±ë¨
                        batch_success += 1
                        successful_count += 1
                        success = True
                        break
                    except Exception as e:
                        logger.error(f"    âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (idx={idx}, ì‹œë„ {retry+1}/2): {e}")
                        
                        if retry == 0:  # ì²« ë²ˆì§¸ ì‹¤íŒ¨ ì‹œ ì„œë²„ ì¬ì‹œì‘
                            # ëˆ„ì  ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸
                            if total_retry_count >= max_total_retries:
                                logger.error(f"\nğŸ”´ ëˆ„ì  ì¬ì‹œë„ íšŸìˆ˜ {total_retry_count}íšŒ ì´ˆê³¼. í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ.")
                                logger.error(f"   - ì„±ê³µ: {successful_count}ê°œ")
                                logger.error(f"   - ì‹¤íŒ¨: {failed_count}ê°œ")
                                with open(progress_log_path, 'a') as f:
                                    f.write(f"[{datetime.now().isoformat()}] ëˆ„ì  ì¬ì‹œë„ ì´ˆê³¼\n")
                                    f.write(f"ìµœì¢… ìƒíƒœ - ì„±ê³µ: {successful_count}, ì‹¤íŒ¨: {failed_count}\n")
                                raise RuntimeError(f"ëˆ„ì  ì¬ì‹œë„ {total_retry_count}íšŒ ì´ˆê³¼ë¡œ ì¢…ë£Œ")
                            
                            logger.info(f"    ğŸ”„ ì„œë²„ ì¬ì‹œì‘ ì‹œë„ ì¤‘... (ëˆ„ì  {total_retry_count+1}/{max_total_retries})")
                            total_retry_count += 1  # ì‹œë„ ìì²´ë¥¼ ì¹´ìš´íŠ¸
                            
                            try:
                                # SentenceTransformer ì„œë²„ ì¬ì‹œì‘
                                manager = SentenceTransformerManager()
                                manager.restart_server(
                                    'sentence-transformers/all-MiniLM-L6-v2',
                                    device='cuda' if torch.cuda.is_available() else 'cpu'
                                )
                                
                                # embedding_manager ì¬ì´ˆê¸°í™”
                                full_dataset.embedding_manager = None
                                logger.info(f"    âœ… ì„œë²„ ì¬ì‹œì‘ ì™„ë£Œ, ì¬ì‹œë„ ì¤‘...")
                                time.sleep(2)  # ì„œë²„ ì•ˆì •í™” ëŒ€ê¸°
                            except Exception as restart_error:
                                logger.error(f"    âŒ ì„œë²„ ì¬ì‹œì‘ ì‹¤íŒ¨: {restart_error}")
                                # break ì œê±° - ë‘ ë²ˆì§¸ ì‹œë„ ì§„í–‰
                        else:  # ë‘ ë²ˆì§¸ ì‹¤íŒ¨ ì‹œ
                            batch_fail += 1
                            failed_count += 1
                            logger.error(f"    âŒâŒ ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨ (idx={idx})")
                            
                            # ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡
                            with open(progress_log_path, 'a') as f:
                                f.write(f"[{datetime.now().isoformat()}] ìµœì¢… ì‹¤íŒ¨! idx={idx}, ì—ëŸ¬: {e}\n")
                                f.write(f"í˜„ì¬ ìƒíƒœ - ì„±ê³µ: {successful_count}, ì‹¤íŒ¨: {failed_count}\n")
                            
                            break
                
                if not success:
                    # ìŠ¤í‚µ íšŸìˆ˜ í™•ì¸
                    total_skip_count += 1
                    if total_skip_count >= max_total_skips:
                        logger.error(f"\nğŸ”´ ëˆ„ì  ìŠ¤í‚µ íšŸìˆ˜ {total_skip_count}ê°œ ì´ˆê³¼. í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ.")
                        logger.error(f"   - ì„±ê³µ: {successful_count}ê°œ")
                        logger.error(f"   - ì‹¤íŒ¨: {failed_count}ê°œ")
                        logger.error(f"   - ìŠ¤í‚µ: {total_skip_count}ê°œ")
                        with open(progress_log_path, 'a') as f:
                            f.write(f"[{datetime.now().isoformat()}] ëˆ„ì  ìŠ¤í‚µ ì´ˆê³¼\n")
                            f.write(f"ìµœì¢… ìƒíƒœ - ì„±ê³µ: {successful_count}, ì‹¤íŒ¨: {failed_count}, ìŠ¤í‚µ: {total_skip_count}\n")
                        raise RuntimeError(f"ëˆ„ì  ìŠ¤í‚µ {total_skip_count}ê°œ ì´ˆê³¼ë¡œ ì¢…ë£Œ")
                    
                    # ì‹¤íŒ¨í•œ í•­ëª© ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰
                    logger.warning(f"    âš ï¸ ì¸ë±ìŠ¤ {idx} ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰ (ëˆ„ì  ìŠ¤í‚µ: {total_skip_count}/{max_total_skips})")
            
            batch_elapsed = time.time() - batch_start_time
            logger.info(f"    âœ… ë°°ì¹˜ ì™„ë£Œ: ì„±ê³µ {batch_success}ê°œ (ì†Œìš”ì‹œê°„: {batch_elapsed:.1f}ì´ˆ)")
            
            # ë¡œê·¸ íŒŒì¼ì— ë°°ì¹˜ ì™„ë£Œ ê¸°ë¡
            with open(progress_log_path, 'a') as f:
                f.write(f"[{datetime.now().isoformat()}] ë°°ì¹˜ {batch_idx + 1} ì™„ë£Œ: ì„±ê³µ {batch_success}ê°œ, ì‹œê°„ {batch_elapsed:.1f}ì´ˆ\n")
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ì„ë² ë”© ì €ì¥ (10ë°°ì¹˜ë§ˆë‹¤)
            if (batch_idx + 1) % 10 == 0:
                logger.info(f"  ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì¤‘... (ë°°ì¹˜ {batch_idx + 1})")
                full_dataset.save_embeddings()
                logger.info(f"     ì €ì¥ ì™„ë£Œ")
        
        # ìµœì¢… ì„ë² ë”© ì €ì¥
        full_dataset.save_embeddings()
        
        # ìµœì¢… í†µê³„
        logger.info(f"\n  âœ… ì „ì²´ ë°ì´í„° ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        logger.info(f"     - ì„±ê³µ: {successful_count}ê°œ")
        logger.info(f"     - ì‹¤íŒ¨: {failed_count}ê°œ")
        logger.info(f"     - ì§„í–‰ ë¡œê·¸: {progress_log_path}")
        
        with open(progress_log_path, 'a') as f:
            f.write(f"\n[{datetime.now().isoformat()}] === ì„ë² ë”© ìƒì„± ì™„ë£Œ ===\n")
            f.write(f"ì´ ì„±ê³µ: {successful_count}ê°œ\n")
            f.write(f"ì´ ì‹¤íŒ¨: {failed_count}ê°œ\n")
    else:
        logger.info("  âœ… ëª¨ë“  ë°ì´í„°ì— ì„ë² ë”©ì´ ì´ë¯¸ ì¡´ì¬")
    
    # LR ìŠ¤ìœ•ìš© ë°ì´í„°ì…‹ (ì´ë¯¸ ìƒì„±ëœ ì„ë² ë”© ì‚¬ìš©)
    train_dataset = RedHeartDataset(train_data, preprocessed_path)
    val_dataset = RedHeartDataset(val_data, preprocessed_path)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=2,
        shuffle=True,
        num_workers=0,
        pin_memory=True if device.type == 'cuda' else False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=2,
        shuffle=False,
        num_workers=0,
        pin_memory=True if device.type == 'cuda' else False
    )
    
    logger.info(f"  - Train batches: {len(train_loader)}")
    logger.info(f"  - Val batches: {len(val_loader)}")
    
    # ì†ì‹¤ í•¨ìˆ˜
    criterion = nn.CrossEntropyLoss()
    
    # Hierarchical LR Sweep ì„¤ì •
    sweep_config = {
        'test_epochs': 3,      # ê° LRë‹¹ 3 ì—í­ í…ŒìŠ¤íŠ¸
        'test_steps': 50,      # ê° ì—í­ë‹¹ 50 ìŠ¤í…
        'warmup_steps': 10,    # 10 ìŠ¤í… ì›Œë°ì—…
        'output_dir': 'training/lr_sweep_results'
    }
    
    logger.info("\nâš™ï¸ Sweep ì„¤ì •:")
    logger.info(f"  - í…ŒìŠ¤íŠ¸ ì—í­: {sweep_config['test_epochs']}")
    logger.info(f"  - í…ŒìŠ¤íŠ¸ ìŠ¤í…/ì—í­: {sweep_config['test_steps']}")
    logger.info(f"  - ì˜ˆìƒ ì´ í¬ì¸íŠ¸: 25ê°œ (5-5-5-5 ì „ëµ)")
    logger.info(f"  - ê° LRì€ ë…ë¦½ì ìœ¼ë¡œ ì´ˆê¸° ê°€ì¤‘ì¹˜ì—ì„œ ì‹œì‘")
    
    # Hierarchical LR Sweep ì‹¤í–‰
    sweep = HierarchicalLRSweep(**sweep_config)
    
    try:
        # ìŠ¤ìœ• ì‹¤í–‰
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Œ ê° LR í…ŒìŠ¤íŠ¸ëŠ” ë™ì¼í•œ ì´ˆê¸° ê°€ì¤‘ì¹˜ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ì‹œì‘ë©ë‹ˆë‹¤")
        logger.info("=" * 80)
        
        results = sweep.run_hierarchical_sweep(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            criterion=criterion,
            device=device
        )
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 80)
        logger.info(f"âœ… ìµœì  Learning Rate: {results['best_lr']:.1e}")
        logger.info(f"âœ… ìµœì  Validation Loss: {results['best_loss']:.4f}")
        logger.info(f"âœ… ì´ í…ŒìŠ¤íŠ¸ í¬ì¸íŠ¸: {results['total_points_tested']}")
        logger.info(f"âœ… íš¨ìœ¨ì„±: {results['efficiency_gain']['vs_grid_search']}")
        logger.info(f"âœ… ì ˆì•½ëœ í¬ì¸íŠ¸: {results['efficiency_gain']['points_saved']}")
        
        # Stageë³„ ìµœê³  ì„±ëŠ¥
        logger.info("\nğŸ“ˆ Stageë³„ ìµœê³  ì„±ëŠ¥:")
        for stage, info in results['stage_results'].items():
            logger.info(f"  - {stage}: LR={info['lr']:.1e}, Loss={info['val_loss']:.4f}, Acc={info['accuracy']:.4f}")
        
        # ìµœì  LRì„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
        optimal_lr_path = Path('training/lr_sweep_results/optimal_lr.json')
        with open(optimal_lr_path, 'w') as f:
            json.dump({
                'optimal_lr': results['best_lr'],
                'optimal_loss': results['best_loss'],
                'timestamp': datetime.now().isoformat(),
                'strategy': '5-5-5-5 Hierarchical',
                'total_points': results['total_points_tested'],
                'stage_results': results['stage_results']
            }, f, indent=2)
        
        logger.info(f"\nğŸ’¾ ìµœì  LR ì €ì¥: {optimal_lr_path}")
        
        # Stageë³„ íŒŒì¼ í™•ì¸
        logger.info("\nğŸ“ ìƒì„±ëœ íŒŒì¼ í™•ì¸:")
        lr_sweep_dir = Path('training/lr_sweep_results')
        pattern = f"hierarchical_lr_sweep_stage*_{datetime.now().strftime('%Y%m%d')}*.json"
        stage_files = list(lr_sweep_dir.glob(pattern))
        for f in sorted(stage_files):
            logger.info(f"  - {f.name}")
        
        pattern = f"hierarchical_lr_sweep_stage*_{datetime.now().strftime('%Y%m%d')}*.png"
        stage_plots = list(lr_sweep_dir.glob(pattern))
        for f in sorted(stage_plots):
            logger.info(f"  - {f.name}")
        
        # í•™ìŠµ ê¶Œì¥ì‚¬í•­
        logger.info("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info(f"  1. ìµœì  LR ({results['best_lr']:.1e})ë¡œ ë³¸ê²© í•™ìŠµ ì‹œì‘")
        logger.info(f"  2. unified_training_final.pyì˜ base_lrì„ {results['best_lr']:.1e}ë¡œ ì„¤ì •")
        logger.info(f"  3. 60 ì—í­ ì „ì²´ í•™ìŠµ ì‹¤í–‰")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ ìŠ¤ìœ• ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise
    
    finally:
        # ìƒì„±ëœ ì„ë² ë”© ì €ì¥ (ì „ì²´ ë°ì´í„°ì…‹)
        if 'full_dataset' in locals() and hasattr(full_dataset, 'save_embeddings'):
            logger.info("\nğŸ’¾ ì „ì²´ ë°ì´í„°ì…‹ ì„ë² ë”© ì €ì¥ ì¤‘...")
            full_dataset.save_embeddings()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            logger.info(f"\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        
        # ì´ˆê¸° ê°€ì¤‘ì¹˜ íŒŒì¼ ì‚­ì œ
        initial_weights_path = Path('training/lr_sweep_results/initial_weights.pth')
        if initial_weights_path.exists():
            initial_weights_path.unlink()
            logger.info("  - ì„ì‹œ ì´ˆê¸° ê°€ì¤‘ì¹˜ íŒŒì¼ ì‚­ì œ")


if __name__ == "__main__":
    results = main()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ‰ Hierarchical LR Sweep ì™„ë£Œ!")
    print(f"   ìµœì  LR: {results['best_lr']:.1e}")
    print(f"   ìµœì  Loss: {results['best_loss']:.4f}")
    print(f"   ê° Stageë³„ JSON/PNG íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    print("=" * 80)