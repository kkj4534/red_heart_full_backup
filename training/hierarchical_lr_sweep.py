"""
ê³„ì¸µì  Learning Rate ìŠ¤ìœ• êµ¬í˜„
5-5-5-5 Coarse-to-Fine ìµœì í™” ì „ëµ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import matplotlib.pyplot as plt
from dataclasses import dataclass
import time

logger = logging.getLogger(__name__)


@dataclass
class LRTestResult:
    """LR í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    lr: float
    stage: int
    epoch_losses: List[float]
    val_loss: float
    train_loss: float
    gradient_norm: float
    convergence_rate: float
    time_taken: float
    accuracy: float = 0.0
    

class HierarchicalLRSweep:
    """
    ê³„ì¸µì  í•™ìŠµë¥  íƒìƒ‰ (5-5-5-5 ì „ëµ)
    ì´ 25ê°œ í¬ì¸íŠ¸ë¡œ ìµœì  LR íƒìƒ‰
    """
    
    def __init__(self, 
                 test_epochs: int = 3,
                 test_steps: int = 50,
                 warmup_steps: int = 10,
                 output_dir: str = "training/lr_sweep_results"):
        """
        Args:
            test_epochs: ê° LR í…ŒìŠ¤íŠ¸ ì—í­ ìˆ˜
            test_steps: ê° ì—í­ë‹¹ ìŠ¤í… ìˆ˜ 
            warmup_steps: ì›Œë°ì—… ìŠ¤í…
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.test_epochs = test_epochs
        self.test_steps = test_steps
        self.warmup_steps = warmup_steps
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        self.all_results: Dict[int, List[LRTestResult]] = {
            0: [],  # Stage 0
            1: [],  # Stage 1
            2: [],  # Stage 2
            3: [],  # Stage 3
            4: []   # Stage 4
        }
        
        # ìµœì  LR ì¶”ì 
        self.best_lr = None
        self.best_loss = float('inf')
        
        # Stageë³„ íƒìƒ‰ êµ¬ê°„
        self.search_intervals: Dict[int, List[Tuple[float, float]]] = {}
        
        # ëˆ„ì  ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        self.cumulative_results_path = self.output_dir / "lr_sweep_cumulative.json"
        self.tested_lrs_history = self._load_cumulative_results()
        
        logger.info("=" * 70)
        logger.info("ğŸ¯ Hierarchical LR Sweep ì´ˆê¸°í™”")
        logger.info(f"  - ì „ëµ: 5-5-5-5 (ì´ 25ê°œ í¬ì¸íŠ¸)")
        logger.info(f"  - í…ŒìŠ¤íŠ¸ ì—í­: {test_epochs}")
        logger.info(f"  - í…ŒìŠ¤íŠ¸ ìŠ¤í…: {test_steps}/epoch")
        logger.info(f"  - ê¸°ì¡´ í…ŒìŠ¤íŠ¸ LR: {len(self.tested_lrs_history)}ê°œ")
        logger.info("=" * 70)
    
    def _load_cumulative_results(self) -> Dict[float, Dict[str, Any]]:
        """ê¸°ì¡´ ëˆ„ì  ê²°ê³¼ ë¡œë“œ"""
        if self.cumulative_results_path.exists():
            try:
                with open(self.cumulative_results_path, 'r') as f:
                    data = json.load(f)
                    logger.info(f"ğŸ“‚ ê¸°ì¡´ LR í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œë“œ: {len(data)} ê°œ LR ê¸°ë¡")
                    # float í‚¤ë¡œ ë³€í™˜
                    return {float(k): v for k, v in data.items()}
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {}
        return {}
    
    def _save_cumulative_results(self):
        """ëˆ„ì  ê²°ê³¼ ì €ì¥"""
        try:
            # ë¬¸ìì—´ í‚¤ë¡œ ë³€í™˜ (JSON í˜¸í™˜ì„±)
            save_data = {str(k): v for k, v in self.tested_lrs_history.items()}
            with open(self.cumulative_results_path, 'w') as f:
                json.dump(save_data, f, indent=2)
            logger.info(f"ğŸ’¾ ëˆ„ì  ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(self.tested_lrs_history)} ê°œ LR")
        except Exception as e:
            logger.error(f"ëˆ„ì  ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _adjust_lr_if_duplicate(self, lr: float, tolerance: float = 0.1) -> float:
        """ì¤‘ë³µ LR ì²´í¬ ë° ì¡°ì • (10% ê°„ê²© ì¶”ê°€)"""
        if lr in self.tested_lrs_history:
            # ì´ë¯¸ í…ŒìŠ¤íŠ¸í•œ LRì´ë©´ 10% ê°„ê²© ì¶”ê°€
            adjusted_lr = lr * (1 + tolerance)
            logger.warning(f"âš ï¸ LR {lr:.1e} ì´ë¯¸ í…ŒìŠ¤íŠ¸ë¨. {adjusted_lr:.1e}ë¡œ ì¡°ì •")
            # ì¬ê·€ì ìœ¼ë¡œ ì²´í¬ (ì¡°ì •ëœ ê°’ë„ ì¤‘ë³µì¼ ìˆ˜ ìˆìŒ)
            return self._adjust_lr_if_duplicate(adjusted_lr, tolerance)
        return lr
    
    def run_hierarchical_sweep(self,
                               model: nn.Module,
                               train_loader: Any,
                               val_loader: Any,
                               criterion: nn.Module,
                               device: torch.device) -> Dict[str, Any]:
        """
        ê³„ì¸µì  LR ìŠ¤ìœ• ì‹¤í–‰
        
        Returns:
            ìµœì¢… ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("\nğŸš€ Hierarchical LR Sweep ì‹œì‘...")
        start_time = time.time()
        
        # Stage 0: ì´ˆê¸° íƒìƒ‰ (5ê°œ í¬ì¸íŠ¸)
        stage0_lrs = [1e-5, 5.6e-5, 3.2e-4, 1.8e-3, 1e-2]
        # ì¤‘ë³µ ì²´í¬ ë° ì¡°ì •
        stage0_lrs = [self._adjust_lr_if_duplicate(lr) for lr in stage0_lrs]
        logger.info(f"\nğŸ“Š [Stage 0] ì´ˆê¸° íƒìƒ‰: {stage0_lrs}")
        
        for lr in stage0_lrs:
            result = self._test_single_lr(
                model, train_loader, val_loader, criterion, device, 
                lr, stage=0
            )
            self.all_results[0].append(result)
            
            # ìµœì  LR ì—…ë°ì´íŠ¸
            if result.val_loss < self.best_loss:
                self.best_loss = result.val_loss
                self.best_lr = lr
                logger.info(f"  ğŸ† ìƒˆë¡œìš´ ìµœì  LR: {lr:.1e} (loss: {result.val_loss:.4f})")
        
        # Stage 0 ë¶„ì„ ë° ìƒìœ„ 2ê°œ êµ¬ê°„ ì„ íƒ
        top_intervals = self._analyze_stage_results(0)
        logger.info(f"\nâœ… Stage 0 ì™„ë£Œ. ì„ íƒëœ êµ¬ê°„: {top_intervals}")
        
        # Stage 1-4: ì ì§„ì  ì„¸ë¶„í™”
        for stage in range(1, 5):
            logger.info(f"\nğŸ“Š [Stage {stage}] ì„¸ë¶„í™” íƒìƒ‰")
            
            # ì´ì „ stageì˜ ìƒìœ„ êµ¬ê°„ì—ì„œ ìƒˆë¡œìš´ í¬ì¸íŠ¸ ìƒì„±
            stage_lrs = self._generate_stage_points(top_intervals, stage)
            logger.info(f"  - íƒìƒ‰ í¬ì¸íŠ¸: {[f'{lr:.1e}' for lr in stage_lrs]}")
            
            # ê° LR í…ŒìŠ¤íŠ¸
            for lr in stage_lrs:
                # ì´ë¯¸ í…ŒìŠ¤íŠ¸í•œ LRì€ ê±´ë„ˆë›°ê¸°
                if self._already_tested(lr):
                    logger.info(f"  â­ï¸ {lr:.1e} ì´ë¯¸ í…ŒìŠ¤íŠ¸ë¨, ê±´ë„ˆë›°ê¸°")
                    continue
                    
                result = self._test_single_lr(
                    model, train_loader, val_loader, criterion, device,
                    lr, stage=stage
                )
                self.all_results[stage].append(result)
                
                # ìµœì  LR ì—…ë°ì´íŠ¸
                if result.val_loss < self.best_loss:
                    self.best_loss = result.val_loss
                    self.best_lr = lr
                    logger.info(f"  ğŸ† ìƒˆë¡œìš´ ìµœì  LR: {lr:.1e} (loss: {result.val_loss:.4f})")
            
            # ë‹¤ìŒ stageë¥¼ ìœ„í•œ ìƒìœ„ êµ¬ê°„ ì„ íƒ
            if stage < 4:
                top_intervals = self._analyze_stage_results(stage)
                logger.info(f"  âœ… Stage {stage} ì™„ë£Œ. ë‹¤ìŒ êµ¬ê°„: {top_intervals}")
        
        # ëˆ„ì  ê²°ê³¼ì— ì¶”ê°€
        for stage_results in self.all_results.values():
            for result in stage_results:
                self.tested_lrs_history[result.lr] = {
                    'val_loss': result.val_loss,
                    'train_loss': result.train_loss,
                    'accuracy': result.accuracy,
                    'timestamp': datetime.now().isoformat(),
                    'stage': result.stage
                }
        
        # ëˆ„ì  ê²°ê³¼ ì €ì¥
        self._save_cumulative_results()
        
        # ìµœì¢… ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±
        total_time = time.time() - start_time
        final_report = self._generate_final_report(total_time)
        
        # ê²°ê³¼ ì €ì¥
        self._save_results(final_report)
        
        # ì‹œê°í™”
        self._visualize_results()
        
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ‰ Hierarchical LR Sweep ì™„ë£Œ!")
        logger.info(f"  - ìµœì  LR: {self.best_lr:.1e}")
        logger.info(f"  - ìµœì  Loss: {self.best_loss:.4f}")
        logger.info(f"  - ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
        logger.info(f"  - í…ŒìŠ¤íŠ¸ëœ í¬ì¸íŠ¸: {self._count_total_tests()}ê°œ")
        logger.info("=" * 70)
        
        return final_report
    
    def _test_single_lr(self,
                       model: nn.Module,
                       train_loader: Any,
                       val_loader: Any,
                       criterion: nn.Module,
                       device: torch.device,
                       lr: float,
                       stage: int) -> LRTestResult:
        """ë‹¨ì¼ LR í…ŒìŠ¤íŠ¸"""
        logger.info(f"\n  ğŸ“Œ LR {lr:.1e} í…ŒìŠ¤íŠ¸ ì‹œì‘ (Stage {stage})")
        
        # ëª¨ë¸ ë³µì‚¬ (ì›ë³¸ ë³´ì¡´)
        test_model = self._copy_model(model, device)
        optimizer = torch.optim.AdamW(test_model.parameters(), lr=lr)
        
        epoch_losses = []
        start_time = time.time()
        
        # í…ŒìŠ¤íŠ¸ ì—í­ ì‹¤í–‰
        for epoch in range(self.test_epochs):
            test_model.train()
            batch_losses = []
            grad_norms = []
            
            # ì œí•œëœ ìŠ¤í…ë§Œ ì‹¤í–‰
            for step, batch in enumerate(train_loader):
                if step >= self.test_steps:
                    break
                
                # Forward - ì‹¤ì œ ëª¨ë¸ êµ¬ì¡°ì— ë§ëŠ” ì „ì²´ ì†ì‹¤ ê³„ì‚°
                inputs = batch['input'].to(device)
                
                # ë°±ë³¸ í†µê³¼
                backbone_outputs = test_model.backbone(inputs, return_all_tasks=True)
                features = backbone_outputs.get('emotion', inputs)
                
                # ëª¨ë“  í—¤ë“œì˜ ì†ì‹¤ ê³„ì‚°
                head_losses = []
                
                # 1. Emotion Head
                if hasattr(test_model, 'emotion_head') and 'emotion_label' in batch:
                    emotion_output = test_model.emotion_head(features)
                    emotion_pred = emotion_output['emotions'] if isinstance(emotion_output, dict) else emotion_output
                    emotion_target = batch['emotion_label'].to(device)
                    emotion_loss = test_model.emotion_head.compute_loss(emotion_pred, emotion_target)
                    head_losses.append(emotion_loss)
                
                # 2. Bentham Head
                if hasattr(test_model, 'bentham_head') and 'bentham_label' in batch:
                    bentham_output = test_model.bentham_head(features)
                    bentham_pred = bentham_output['bentham_scores'] if isinstance(bentham_output, dict) else bentham_output
                    bentham_target = batch['bentham_label'].to(device)
                    bentham_loss = test_model.bentham_head.compute_loss(bentham_pred, bentham_target)
                    head_losses.append(bentham_loss)
                
                # 3. Regret Head
                if hasattr(test_model, 'regret_head') and 'regret_label' in batch:
                    regret_output = test_model.regret_head(features)
                    regret_pred = regret_output['regret_score'] if isinstance(regret_output, dict) else regret_output
                    regret_target = batch['regret_label'].to(device)
                    regret_loss = test_model.regret_head.compute_loss(regret_pred, regret_target)
                    head_losses.append(regret_loss)
                
                # 4. SURD Head
                if hasattr(test_model, 'surd_head') and 'surd_label' in batch:
                    surd_output = test_model.surd_head(features)
                    surd_pred = surd_output['surd_values'] if isinstance(surd_output, dict) else surd_output
                    
                    # SURD íƒ€ê²Ÿ ê³„ì‚° (unified_training_final.pyì˜ ì‹¤ì œ êµ¬í˜„ê³¼ ë™ì¼)
                    batch_size = surd_pred.shape[0]
                    surd_target = torch.zeros((batch_size, 4), device=device)
                    
                    # Synergy: ê°ì • ë‹¤ì–‘ì„± (ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜)
                    if 'emotion_label' in batch:
                        emotion_probs = torch.nn.functional.one_hot(batch['emotion_label'].to(device), num_classes=7).float()
                        emotion_entropy = -(emotion_probs * (emotion_probs + 1e-10).log()).sum(dim=1)
                        surd_target[:, 0] = emotion_entropy / np.log(7)  # ì •ê·œí™”
                    
                    # Unique: ë ˆì´ë¸” ê³ ìœ ì„±
                    if 'surd_label' in batch:
                        label_unique = torch.nn.functional.one_hot(batch['surd_label'].to(device), num_classes=5).float()
                        surd_target[:, 1] = label_unique.max(dim=1)[0]
                    
                    # Redundant: ë²¤ë‹´ ìƒê´€ë„
                    if 'bentham_label' in batch:
                        bentham = batch['bentham_label'].to(device)
                        bentham_mean = bentham.mean(dim=1)
                        bentham_std = bentham.std(dim=1) + 1e-10
                        surd_target[:, 2] = 1.0 - (bentham_std / (bentham_mean + 1e-10)).clamp(0, 1)
                    
                    # Deterministic: í›„íšŒ ê²°ì •ì„±
                    if 'regret_label' in batch:
                        regret = batch['regret_label'].to(device)
                        if regret.dim() == 1:
                            regret = regret.unsqueeze(1)
                        surd_target[:, 3] = regret.abs().squeeze()
                    
                    surd_loss = test_model.surd_head.compute_loss(surd_pred, surd_target)
                    head_losses.append(surd_loss)
                
                # ì „ì²´ ì†ì‹¤ = ëª¨ë“  í—¤ë“œ ì†ì‹¤ì˜ í‰ê· 
                if head_losses:
                    loss = sum(head_losses) / len(head_losses)
                else:
                    # fallback ì—†ìŒ - ì—ëŸ¬ ë°œìƒ
                    raise RuntimeError("No head losses computed - model structure error")
                
                # Backward
                optimizer.zero_grad()
                loss.backward()
                
                # Gradient norm ê³„ì‚°
                total_norm = 0.0
                for p in test_model.parameters():
                    if p.grad is not None:
                        param_norm = p.grad.data.norm(2).item()
                        total_norm += param_norm ** 2
                total_norm = total_norm ** 0.5
                grad_norms.append(total_norm)
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(test_model.parameters(), max_norm=1.0)
                
                optimizer.step()
                batch_losses.append(loss.item())
            
            epoch_loss = np.mean(batch_losses)
            epoch_losses.append(epoch_loss)
            avg_grad_norm = np.mean(grad_norms)
            
            logger.info(f"    Epoch {epoch+1}: loss={epoch_loss:.4f}, grad_norm={avg_grad_norm:.4f}")
        
        # Validation
        test_model.eval()
        val_losses = []
        val_accs = []
        
        with torch.no_grad():
            for step, batch in enumerate(val_loader):
                if step >= 20:  # ë¹ ë¥¸ ê²€ì¦
                    break
                    
                # Forward - ì‹¤ì œ ëª¨ë¸ êµ¬ì¡°ì— ë§ëŠ” ì „ì²´ ì†ì‹¤ ê³„ì‚°
                inputs = batch['input'].to(device)
                
                # ë°±ë³¸ í†µê³¼
                backbone_outputs = test_model.backbone(inputs, return_all_tasks=True)
                features = backbone_outputs.get('emotion', inputs)
                
                # ëª¨ë“  í—¤ë“œì˜ ì†ì‹¤ ê³„ì‚°
                head_losses = []
                head_accuracies = []
                
                # 1. Emotion Head
                if hasattr(test_model, 'emotion_head') and 'emotion_label' in batch:
                    emotion_output = test_model.emotion_head(features)
                    emotion_pred = emotion_output['emotions'] if isinstance(emotion_output, dict) else emotion_output
                    emotion_target = batch['emotion_label'].to(device)
                    emotion_loss = test_model.emotion_head.compute_loss(emotion_pred, emotion_target)
                    head_losses.append(emotion_loss)
                    
                    # Accuracy ê³„ì‚°
                    preds = emotion_pred.argmax(dim=-1)
                    acc = (preds == emotion_target).float().mean().item()
                    head_accuracies.append(acc)
                
                # 2. Bentham Head
                if hasattr(test_model, 'bentham_head') and 'bentham_label' in batch:
                    bentham_output = test_model.bentham_head(features)
                    bentham_pred = bentham_output['bentham_scores'] if isinstance(bentham_output, dict) else bentham_output
                    bentham_target = batch['bentham_label'].to(device)
                    bentham_loss = test_model.bentham_head.compute_loss(bentham_pred, bentham_target)
                    head_losses.append(bentham_loss)
                    
                    # Regression accuracy (threshold-based)
                    acc = ((bentham_pred - bentham_target).abs() < 0.5).float().mean().item()
                    head_accuracies.append(acc)
                
                # 3. Regret Head
                if hasattr(test_model, 'regret_head') and 'regret_label' in batch:
                    regret_output = test_model.regret_head(features)
                    regret_pred = regret_output['regret_score'] if isinstance(regret_output, dict) else regret_output
                    regret_target = batch['regret_label'].to(device)
                    regret_loss = test_model.regret_head.compute_loss(regret_pred, regret_target)
                    head_losses.append(regret_loss)
                    
                    # Regression accuracy
                    acc = ((regret_pred - regret_target).abs() < 0.5).float().mean().item()
                    head_accuracies.append(acc)
                
                # 4. SURD Head (ì •ë³´ì´ë¡  ê¸°ë°˜ 4ì°¨ì› íƒ€ê²Ÿ)
                if hasattr(test_model, 'surd_head') and 'surd_label' in batch:
                    surd_output = test_model.surd_head(features)
                    surd_pred = surd_output['surd_values'] if isinstance(surd_output, dict) else surd_output
                    
                    # SURD íƒ€ê²Ÿì„ ì‹¤ì œ ë°ì´í„°ì—ì„œ ê³„ì‚° (4ì°¨ì›)
                    batch_size = surd_pred.shape[0]
                    surd_target = torch.zeros((batch_size, 4), device=device)
                    
                    # Synergy: ê°ì • ë‹¤ì–‘ì„± (ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜)
                    if 'emotion_label' in batch:
                        emotion_probs = F.one_hot(batch['emotion_label'].to(device), num_classes=7).float()
                        emotion_entropy = -(emotion_probs * (emotion_probs + 1e-10).log()).sum(dim=1)
                        surd_target[:, 0] = emotion_entropy / np.log(7)  # ì •ê·œí™”
                    
                    # Unique: ë ˆì´ë¸” ê³ ìœ ì„± (one-hot ì¸ì½”ë”©)
                    if 'surd_label' in batch:
                        label_unique = F.one_hot(batch['surd_label'].to(device), num_classes=5).float()
                        surd_target[:, 1] = label_unique.max(dim=1)[0]  # ìµœëŒ€ê°’ = 1.0
                    
                    # Redundant: ë²¤ë‹´ ìƒê´€ë„ (í‰ê· ê³¼ ë¶„ì‚°)
                    if 'bentham_label' in batch:
                        bentham = batch['bentham_label'].to(device)
                        bentham_mean = bentham.mean(dim=1)
                        bentham_std = bentham.std(dim=1) + 1e-10
                        surd_target[:, 2] = 1.0 - (bentham_std / (bentham_mean + 1e-10)).clamp(0, 1)
                    
                    # Deterministic: í›„íšŒ ê²°ì •ì„± (ì ˆëŒ€ê°’)
                    if 'regret_label' in batch:
                        regret = batch['regret_label'].to(device)
                        if regret.dim() == 1:
                            regret = regret.unsqueeze(1)
                        surd_target[:, 3] = regret.abs().squeeze()
                    
                    surd_loss = test_model.surd_head.compute_loss(surd_pred, surd_target)
                    head_losses.append(surd_loss)
                    
                    # Multi-dimensional regression accuracy (threshold ê¸°ë°˜)
                    acc = ((surd_pred - surd_target).abs() < 0.3).float().mean().item()
                    head_accuracies.append(acc)
                
                # ì „ì²´ ì†ì‹¤ = ëª¨ë“  í—¤ë“œ ì†ì‹¤ì˜ í‰ê· 
                if head_losses:
                    loss = sum(head_losses) / len(head_losses)
                    val_losses.append(loss.item())
                    
                    # ì „ì²´ accuracy = í—¤ë“œë³„ accuracyì˜ í‰ê· 
                    if head_accuracies:
                        overall_acc = np.mean(head_accuracies)
                        val_accs.append(overall_acc)
                else:
                    raise RuntimeError("No head losses computed in validation")
        
        val_loss = np.mean(val_losses) if val_losses else float('inf')
        val_acc = np.mean(val_accs) if val_accs else 0.0
        
        # Convergence rate ê³„ì‚°
        if len(epoch_losses) >= 2:
            convergence_rate = (epoch_losses[0] - epoch_losses[-1]) / epoch_losses[0]
        else:
            convergence_rate = 0.0
        
        time_taken = time.time() - start_time
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del test_model
        torch.cuda.empty_cache()
        
        result = LRTestResult(
            lr=lr,
            stage=stage,
            epoch_losses=epoch_losses,
            val_loss=val_loss,
            train_loss=epoch_losses[-1] if epoch_losses else float('inf'),
            gradient_norm=avg_grad_norm,
            convergence_rate=convergence_rate,
            time_taken=time_taken,
            accuracy=val_acc
        )
        
        logger.info(f"    âœ… ì™„ë£Œ: val_loss={val_loss:.4f}, acc={val_acc:.4f}, time={time_taken:.1f}s")
        
        return result
    
    def _copy_model(self, model: nn.Module, device: torch.device) -> nn.Module:
        """ëª¨ë¸ ë³µì‚¬ - ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        # deepcopyëŠ” thread lock ë¬¸ì œê°€ ìˆìœ¼ë¯€ë¡œ 
        # ëª¨ë¸ì˜ state_dictë¥¼ ì €ì¥í•˜ê³  ìƒˆ ëª¨ë¸ì— ë¡œë“œí•˜ëŠ” ë°©ì‹ ì‚¬ìš©
        
        # ì›ë³¸ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì €ì¥
        original_state = model.state_dict()
        
        # ìƒˆ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (UnifiedModel ê°€ì •)
        from training.unified_training_final import UnifiedModel
        
        # config ì¶”ì¶œ (UnifiedModelì€ configë¥¼ ê°€ì§€ê³  ìˆìŒ)
        if hasattr(model, 'config'):
            config = model.config
        else:
            # ê¸°ë³¸ config ìƒì„±
            from training.unified_training_final import UnifiedTrainingConfig
            config = UnifiedTrainingConfig()
        
        # ìƒˆ ëª¨ë¸ ìƒì„±
        model_copy = UnifiedModel(config, device=device)
        
        # ì›ë³¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model_copy.load_state_dict(original_state)
        model_copy.to(device)
        
        return model_copy
    
    def _analyze_stage_results(self, stage: int) -> List[Tuple[float, float]]:
        """
        Stage ê²°ê³¼ ë¶„ì„ ë° ìƒìœ„ 2ê°œ êµ¬ê°„ ì„ íƒ
        """
        results = self.all_results[stage]
        if not results:
            return []
        
        # ê²°ê³¼ë¥¼ loss ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_results = sorted(results, key=lambda x: x.val_loss)
        
        # LR ê°’ë“¤ì„ ì •ë ¬
        all_lrs = sorted([r.lr for r in results])
        
        # ìƒìœ„ 2ê°œ êµ¬ê°„ ì°¾ê¸°
        intervals = []
        
        # ìµœê³  ì„±ëŠ¥ LR ì£¼ë³€ êµ¬ê°„
        best_lr = sorted_results[0].lr
        best_idx = all_lrs.index(best_lr)
        
        # êµ¬ê°„ 1: ìµœê³  ì„±ëŠ¥ LR ì£¼ë³€
        if best_idx > 0:
            intervals.append((all_lrs[best_idx-1], all_lrs[best_idx]))
        if best_idx < len(all_lrs) - 1:
            intervals.append((all_lrs[best_idx], all_lrs[best_idx+1]))
        
        # êµ¬ê°„ì´ ë¶€ì¡±í•˜ë©´ ë‘ ë²ˆì§¸ ì¢‹ì€ LR ì£¼ë³€ë„ ê³ ë ¤
        if len(intervals) < 2 and len(sorted_results) > 1:
            second_lr = sorted_results[1].lr
            second_idx = all_lrs.index(second_lr)
            
            if second_idx > 0 and (all_lrs[second_idx-1], all_lrs[second_idx]) not in intervals:
                intervals.append((all_lrs[second_idx-1], all_lrs[second_idx]))
            elif second_idx < len(all_lrs) - 1 and (all_lrs[second_idx], all_lrs[second_idx+1]) not in intervals:
                intervals.append((all_lrs[second_idx], all_lrs[second_idx+1]))
        
        # ìµœëŒ€ 2ê°œ êµ¬ê°„ë§Œ ë°˜í™˜
        return intervals[:2]
    
    def _generate_stage_points(self, intervals: List[Tuple[float, float]], stage: int) -> List[float]:
        """
        ì£¼ì–´ì§„ êµ¬ê°„ë“¤ì—ì„œ ìƒˆë¡œìš´ íƒìƒ‰ í¬ì¸íŠ¸ ìƒì„±
        """
        points = []
        points_per_interval = 5 // len(intervals) if intervals else 5
        extra_points = 5 % len(intervals) if intervals else 0
        
        for i, (low, high) in enumerate(intervals):
            # ì´ êµ¬ê°„ì— í• ë‹¹í•  í¬ì¸íŠ¸ ìˆ˜
            n_points = points_per_interval + (1 if i < extra_points else 0)
            
            # ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ê· ë“± ë¶„í¬
            interval_points = np.logspace(
                np.log10(low),
                np.log10(high),
                n_points + 2  # ê²½ê³„ í¬í•¨
            )[1:-1]  # ê²½ê³„ ì œì™¸
            
            points.extend(interval_points)
        
        # ì •í™•íˆ 5ê°œë¡œ ì¡°ì •
        if len(points) > 5:
            points = points[:5]
        elif len(points) < 5:
            # ë¶€ì¡±í•˜ë©´ ê¸°ì¡´ êµ¬ê°„ì„ ë” ì„¸ë°€í•˜ê²Œ
            while len(points) < 5:
                if intervals:
                    mid_lr = np.sqrt(intervals[0][0] * intervals[0][1])
                    if mid_lr not in points:
                        points.append(mid_lr)
                else:
                    break
        
        return sorted(points)
    
    def _already_tested(self, lr: float, tolerance: float = 1e-10) -> bool:
        """ì´ë¯¸ í…ŒìŠ¤íŠ¸ëœ LRì¸ì§€ í™•ì¸"""
        for stage_results in self.all_results.values():
            for result in stage_results:
                if abs(result.lr - lr) < tolerance:
                    return True
        return False
    
    def _count_total_tests(self) -> int:
        """ì´ í…ŒìŠ¤íŠ¸ ìˆ˜ ê³„ì‚°"""
        return sum(len(results) for results in self.all_results.values())
    
    def _generate_final_report(self, total_time: float) -> Dict[str, Any]:
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        
        # ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘
        all_results_flat = []
        for stage, results in self.all_results.items():
            for r in results:
                all_results_flat.append({
                    'lr': r.lr,
                    'stage': r.stage,
                    'val_loss': r.val_loss,
                    'train_loss': r.train_loss,
                    'convergence_rate': r.convergence_rate,
                    'gradient_norm': r.gradient_norm,
                    'accuracy': r.accuracy,
                    'time': r.time_taken
                })
        
        # Stageë³„ ìµœê³  ì„±ëŠ¥
        stage_best = {}
        for stage, results in self.all_results.items():
            if results:
                best = min(results, key=lambda x: x.val_loss)
                stage_best[f'stage_{stage}'] = {
                    'lr': best.lr,
                    'val_loss': best.val_loss,
                    'accuracy': best.accuracy
                }
        
        report = {
            'strategy': '5-5-5-5 Hierarchical',
            'total_points_tested': self._count_total_tests(),
            'total_time_minutes': total_time / 60,
            'best_lr': self.best_lr,
            'best_loss': self.best_loss,
            'stage_results': stage_best,
            'all_results': all_results_flat,
            'efficiency_gain': {
                'vs_grid_search': f"{(500 / self._count_total_tests()):.1f}x faster",
                'points_saved': 500 - self._count_total_tests()
            },
            'timestamp': datetime.now().isoformat()
        }
        
        return report
    
    def _save_results(self, report: Dict[str, Any]):
        """ê²°ê³¼ ì €ì¥ (ì „ì²´ ë° ê° Stageë³„)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì „ì²´ JSON ì €ì¥
        json_path = self.output_dir / f"hierarchical_lr_sweep_{timestamp}.json"
        with open(json_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        # ê° Stageë³„ JSON ì €ì¥
        for stage, results in self.all_results.items():
            if results:
                stage_data = {
                    'stage': stage,
                    'results': [{
                        'lr': r.lr,
                        'val_loss': r.val_loss,
                        'train_loss': r.train_loss,
                        'accuracy': r.accuracy,
                        'convergence_rate': r.convergence_rate
                    } for r in results]
                }
                stage_json_path = self.output_dir / f"hierarchical_lr_sweep_stage{stage}_{timestamp}.json"
                with open(stage_json_path, 'w') as f:
                    json.dump(stage_data, f, indent=2)
                logger.info(f"  ğŸ“ Stage {stage} ì €ì¥: {stage_json_path}")
        
        logger.info(f"\nğŸ“ ì „ì²´ ê²°ê³¼ ì €ì¥: {json_path}")
    
    def _visualize_results(self):
        """ê²°ê³¼ ì‹œê°í™” (ì „ì²´ ë° ê° Stageë³„)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ê° Stageë³„ ê°œë³„ PNG ìƒì„±
        for stage in range(5):
            if self.all_results[stage]:
                fig, ax = plt.subplots(figsize=(8, 6))
                
                lrs = [r.lr for r in self.all_results[stage]]
                losses = [r.val_loss for r in self.all_results[stage]]
                
                ax.semilogx(lrs, losses, 'bo-', markersize=10, linewidth=2)
                ax.set_xlabel('Learning Rate', fontsize=12)
                ax.set_ylabel('Validation Loss', fontsize=12)
                ax.set_title(f'Stage {stage} LR Sweep Results', fontsize=14)
                ax.grid(True, alpha=0.3)
                
                # ìµœê³  ì„±ëŠ¥ í‘œì‹œ
                best_idx = np.argmin(losses)
                ax.plot(lrs[best_idx], losses[best_idx], 'r*', markersize=20)
                ax.annotate(f'Best: {lrs[best_idx]:.1e}\nLoss: {losses[best_idx]:.4f}',
                           xy=(lrs[best_idx], losses[best_idx]),
                           xytext=(10, 10), textcoords='offset points',
                           bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))
                
                # Stageë³„ PNG ì €ì¥
                stage_plot_path = self.output_dir / f"hierarchical_lr_sweep_stage{stage}_{timestamp}.png"
                plt.tight_layout()
                plt.savefig(stage_plot_path, dpi=100, bbox_inches='tight')
                plt.close()
                logger.info(f"  ğŸ“Š Stage {stage} ì‹œê°í™”: {stage_plot_path}")
        
        # ì „ì²´ í†µí•© ì‹œê°í™”
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle('Hierarchical LR Sweep Results (5-5-5-5 Strategy)', fontsize=16)
        
        # Stageë³„ ê²°ê³¼ í”Œë¡¯
        for stage in range(5):
            ax = axes[stage // 3, stage % 3]
            
            if self.all_results[stage]:
                lrs = [r.lr for r in self.all_results[stage]]
                losses = [r.val_loss for r in self.all_results[stage]]
                
                ax.semilogx(lrs, losses, 'bo-', markersize=8)
                ax.set_xlabel('Learning Rate')
                ax.set_ylabel('Validation Loss')
                ax.set_title(f'Stage {stage}')
                ax.grid(True, alpha=0.3)
                
                # ìµœê³  ì„±ëŠ¥ í‘œì‹œ
                best_idx = np.argmin(losses)
                ax.plot(lrs[best_idx], losses[best_idx], 'r*', markersize=15)
                ax.annotate(f'Best: {lrs[best_idx]:.1e}',
                           xy=(lrs[best_idx], losses[best_idx]),
                           xytext=(5, 5), textcoords='offset points')
        
        # ì „ì²´ ê²°ê³¼ ì¢…í•©
        ax = axes[1, 2]
        all_lrs = []
        all_losses = []
        all_stages = []
        
        for stage, results in self.all_results.items():
            for r in results:
                all_lrs.append(r.lr)
                all_losses.append(r.val_loss)
                all_stages.append(stage)
        
        if all_lrs:
            scatter = ax.scatter(all_lrs, all_losses, c=all_stages, 
                               cmap='viridis', s=100, alpha=0.6)
            ax.set_xscale('log')
            ax.set_xlabel('Learning Rate')
            ax.set_ylabel('Validation Loss')
            ax.set_title('All Stages Combined')
            ax.grid(True, alpha=0.3)
            plt.colorbar(scatter, ax=ax, label='Stage')
            
            # ìµœì  LR í‘œì‹œ
            ax.plot(self.best_lr, self.best_loss, 'r*', markersize=20)
            ax.annotate(f'Optimal: {self.best_lr:.1e}',
                       xy=(self.best_lr, self.best_loss),
                       xytext=(5, 5), textcoords='offset points',
                       fontweight='bold')
        
        plt.tight_layout()
        
        # ì €ì¥
        plot_path = self.output_dir / f"hierarchical_lr_sweep_plot_{timestamp}.png"
        plt.savefig(plot_path, dpi=100, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ğŸ“Š ì‹œê°í™” ì €ì¥: {plot_path}")


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    import sys
    sys.path.append('/mnt/c/large_project/linux_red_heart')
    
    from models.red_heart_model_final import RedHeartModel
    from training.data_loader import create_data_loaders
    
    # ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ëª¨ë¸ ìƒì„±
    model = RedHeartModel(config={
        'vocab_size': 50000,
        'max_length': 512,
        'd_model': 896,
        'num_heads': 16,
        'num_layers': 6,
        'd_ff': 3584,
        'dropout': 0.1
    }).to(device)
    
    # ë°ì´í„° ë¡œë” ìƒì„±  
    train_loader, val_loader = create_data_loaders(
        batch_size=2,
        num_workers=0
    )
    
    # ì†ì‹¤ í•¨ìˆ˜
    criterion = nn.CrossEntropyLoss()
    
    # Hierarchical LR Sweep ì‹¤í–‰
    sweep = HierarchicalLRSweep(
        test_epochs=3,
        test_steps=50,
        warmup_steps=10
    )
    
    results = sweep.run_hierarchical_sweep(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        device=device
    )
    
    print(f"\nìµœì  LR: {results['best_lr']:.1e}")
    print(f"ìµœì  Loss: {results['best_loss']:.4f}")
    print(f"ì´ í…ŒìŠ¤íŠ¸ í¬ì¸íŠ¸: {results['total_points_tested']}")


if __name__ == "__main__":
    main()