#!/usr/bin/env python3
"""
í•™ìŠµ ê²°ê³¼ ë¶„ì„ê¸° ë° docs ìë™ ìƒì„±
Training Results Analyzer and Automatic Documentation Generator
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional
import markdown
from jinja2 import Template
import os

class TrainingResultsAnalyzer:
    """í•™ìŠµ ê²°ê³¼ ë¶„ì„ ë° ë¬¸ì„œ ìƒì„±ê¸°"""
    
    def __init__(self, project_root: Path):
        self.project_root = Path(project_root)
        self.training_dir = self.project_root / 'training' / 'outputs'
        self.docs_dir = self.project_root / 'docs'
        self.docs_dir.mkdir(exist_ok=True)
        
        # í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
        plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
    def analyze_training_report(self, report_path: Path) -> Dict[str, Any]:
        """í•™ìŠµ ë¦¬í¬íŠ¸ ë¶„ì„"""
        with open(report_path, 'r', encoding='utf-8') as f:
            report = json.load(f)
        
        analysis = {
            'report_path': str(report_path),
            'timestamp': datetime.now().isoformat(),
            'basic_metrics': self._extract_basic_metrics(report),
            'regret_analysis': self._analyze_regret_patterns(report),
            'performance_trends': self._analyze_performance_trends(report),
            'efficiency_metrics': self._calculate_efficiency_metrics(report),
            'recommendations': self._generate_recommendations(report)
        }
        
        return analysis
    
    def _extract_basic_metrics(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        training_summary = report.get('training_summary', {})
        model_info = report.get('model_info', {})
        
        return {
            'total_training_steps': training_summary.get('total_steps', 0),
            'total_regrets_generated': training_summary.get('total_regrets', 0),
            'total_bentham_calculations': training_summary.get('total_bentham_calculations', 0),
            'final_loss': training_summary.get('final_loss', 0),
            'training_duration_hours': training_summary.get('training_duration', 0) / 3600,
            'model_parameters': model_info.get('main_model_parameters', 0),
            'regrets_per_step': training_summary.get('average_regrets_per_step', 0),
            'benthams_per_step': training_summary.get('average_benthams_per_step', 0)
        }
    
    def _analyze_regret_patterns(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """í›„íšŒ íŒ¨í„´ ë¶„ì„"""
        config = report.get('configuration', {})
        training_summary = report.get('training_summary', {})
        
        target_regrets_per_step = config.get('regrets_per_step', 7)
        actual_regrets_per_step = training_summary.get('average_regrets_per_step', 0)
        
        regret_efficiency = (actual_regrets_per_step / target_regrets_per_step * 100) if target_regrets_per_step > 0 else 0
        
        bentham_per_regret = (
            training_summary.get('total_bentham_calculations', 0) / 
            training_summary.get('total_regrets', 1)
        )
        
        return {
            'target_regrets_per_step': target_regrets_per_step,
            'actual_regrets_per_step': actual_regrets_per_step,
            'regret_generation_efficiency': regret_efficiency,
            'bentham_calculations_per_regret': bentham_per_regret,
            'expected_bentham_per_regret': config.get('bentham_calculations_per_regret', 3),
            'bentham_efficiency': (bentham_per_regret / 3 * 100) if bentham_per_regret > 0 else 0
        }
    
    def _analyze_performance_trends(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
        training_stats = report.get('training_stats', {})
        
        if not training_stats:
            return {'trend_analysis': 'No training statistics available'}
        
        # ì†ì‹¤ íŠ¸ë Œë“œ
        losses = training_stats.get('total_loss', [])
        if len(losses) > 10:
            early_loss = np.mean(losses[:len(losses)//4])
            late_loss = np.mean(losses[-len(losses)//4:])
            improvement = ((early_loss - late_loss) / early_loss * 100) if early_loss > 0 else 0
        else:
            improvement = 0
        
        return {
            'total_training_steps': len(losses),
            'loss_improvement_percentage': improvement,
            'convergence_analysis': 'Converged' if improvement > 10 else 'Partially converged' if improvement > 0 else 'No clear convergence',
            'training_stability': 'Stable' if len(losses) > 100 else 'Short training'
        }
    
    def _calculate_efficiency_metrics(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """íš¨ìœ¨ì„± ë©”íŠ¸ë¦­ ê³„ì‚°"""
        training_summary = report.get('training_summary', {})
        storage_usage = report.get('storage_usage', {})
        model_info = report.get('model_info', {})
        
        duration_hours = training_summary.get('training_duration', 0) / 3600
        parameters = model_info.get('main_model_parameters', 1)
        storage_gb = storage_usage.get('final_size_gb', 0)
        
        return {
            'parameters_per_hour': parameters / duration_hours if duration_hours > 0 else 0,
            'regrets_per_hour': training_summary.get('total_regrets', 0) / duration_hours if duration_hours > 0 else 0,
            'storage_efficiency_mb_per_parameter': (storage_gb * 1024) / parameters if parameters > 0 else 0,
            'training_speed_steps_per_hour': training_summary.get('total_steps', 0) / duration_hours if duration_hours > 0 else 0,
            'bentham_calculations_per_hour': training_summary.get('total_bentham_calculations', 0) / duration_hours if duration_hours > 0 else 0
        }
    
    def _generate_recommendations(self, report: Dict[str, Any]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        training_summary = report.get('training_summary', {})
        regret_efficiency = self._analyze_regret_patterns(report).get('regret_generation_efficiency', 0)
        
        # í›„íšŒ ìƒì„± íš¨ìœ¨ì„± ê²€í† 
        if regret_efficiency < 90:
            recommendations.append("í›„íšŒ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± íš¨ìœ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. í›„íšŒ ê³„ì‚° ë¡œì§ì„ ìµœì í™”í•´ë³´ì„¸ìš”.")
        
        # í•™ìŠµ ì•ˆì •ì„± ê²€í† 
        final_loss = training_summary.get('final_loss', 0)
        if final_loss > 1.0:
            recommendations.append("ìµœì¢… ì†ì‹¤ì´ ë†’ìŠµë‹ˆë‹¤. í•™ìŠµë¥  ì¡°ì •ì´ë‚˜ ì¶”ê°€ í•™ìŠµì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        # ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ ê²€í† 
        storage_usage = report.get('storage_usage', {})
        if storage_usage.get('final_size_gb', 0) > 180:
            recommendations.append("ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ì´ í•œê³„ì— ê°€ê¹ìŠµë‹ˆë‹¤. ë¡œê·¸ ì •ë¦¬ ì£¼ê¸°ë¥¼ ë‹¨ì¶•í•´ë³´ì„¸ìš”.")
        
        # ë²¤ë‹´ ê³„ì‚° íš¨ìœ¨ì„±
        bentham_efficiency = self._analyze_regret_patterns(report).get('bentham_efficiency', 0)
        if bentham_efficiency < 95:
            recommendations.append("ë²¤ë‹´ ì¾Œë½ ê³„ì‚° íš¨ìœ¨ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        if not recommendations:
            recommendations.append("í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë“  ë©”íŠ¸ë¦­ì´ ëª©í‘œ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.")
        
        return recommendations
    
    def create_visualizations(self, analysis: Dict[str, Any], report: Dict[str, Any]) -> Dict[str, Path]:
        """ì‹œê°í™” ìƒì„±"""
        viz_dir = self.docs_dir / 'visualizations'
        viz_dir.mkdir(exist_ok=True)
        
        plots = {}
        
        # 1. ê¸°ë³¸ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Red Heart XAI í•™ìŠµ ê²°ê³¼ ëŒ€ì‹œë³´ë“œ', fontsize=16, fontweight='bold')
        
        # í›„íšŒ ë° ë²¤ë‹´ ê³„ì‚° í†µê³„
        metrics = analysis['basic_metrics']
        labels = ['ì´ í›„íšŒ', 'ì´ ë²¤ë‹´ ê³„ì‚°', 'ì´ í•™ìŠµ ìŠ¤í…']
        values = [
            metrics['total_regrets_generated'],
            metrics['total_bentham_calculations'],
            metrics['total_training_steps']
        ]
        
        axes[0, 0].bar(labels, values, color=['#ff7f0e', '#2ca02c', '#1f77b4'])
        axes[0, 0].set_title('í•™ìŠµ í†µê³„')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # íš¨ìœ¨ì„± ë©”íŠ¸ë¦­
        efficiency = analysis['efficiency_metrics']
        eff_labels = ['í›„íšŒ/ì‹œê°„', 'ë²¤ë‹´/ì‹œê°„', 'ìŠ¤í…/ì‹œê°„']
        eff_values = [
            efficiency['regrets_per_hour'],
            efficiency['bentham_calculations_per_hour'],
            efficiency['training_speed_steps_per_hour']
        ]
        
        axes[0, 1].bar(eff_labels, eff_values, color=['#d62728', '#9467bd', '#8c564b'])
        axes[0, 1].set_title('ì‹œê°„ë‹¹ íš¨ìœ¨ì„±')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # ëª¨ë¸ ì •ë³´
        model_data = [
            ['íŒŒë¼ë¯¸í„° ìˆ˜', f"{metrics['model_parameters']:,}"],
            ['í•™ìŠµ ì‹œê°„', f"{metrics['training_duration_hours']:.2f}h"],
            ['ìµœì¢… ì†ì‹¤', f"{metrics['final_loss']:.4f}"]
        ]
        
        axes[1, 0].axis('tight')
        axes[1, 0].axis('off')
        table = axes[1, 0].table(cellText=model_data, colLabels=['ë©”íŠ¸ë¦­', 'ê°’'], 
                                cellLoc='center', loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        axes[1, 0].set_title('ëª¨ë¸ ì •ë³´')
        
        # í›„íšŒ íŒ¨í„´ ë¶„ì„
        regret_analysis = analysis['regret_analysis']
        regret_data = [
            ['ëª©í‘œ í›„íšŒ/ìŠ¤í…', regret_analysis['target_regrets_per_step']],
            ['ì‹¤ì œ í›„íšŒ/ìŠ¤í…', f"{regret_analysis['actual_regrets_per_step']:.2f}"],
            ['í›„íšŒ íš¨ìœ¨ì„±', f"{regret_analysis['regret_generation_efficiency']:.1f}%"],
            ['ë²¤ë‹´ íš¨ìœ¨ì„±', f"{regret_analysis['bentham_efficiency']:.1f}%"]
        ]
        
        axes[1, 1].axis('tight')
        axes[1, 1].axis('off')
        regret_table = axes[1, 1].table(cellText=regret_data, colLabels=['ë©”íŠ¸ë¦­', 'ê°’'],
                                       cellLoc='center', loc='center')
        regret_table.auto_set_font_size(False)
        regret_table.set_fontsize(10)
        axes[1, 1].set_title('í›„íšŒ ë¶„ì„')
        
        plt.tight_layout()
        dashboard_path = viz_dir / 'training_dashboard.png'
        plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')
        plt.close()
        plots['dashboard'] = dashboard_path
        
        # 2. ì†ì‹¤ íŠ¸ë Œë“œ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        training_stats = report.get('training_stats', {})
        if 'total_loss' in training_stats and len(training_stats['total_loss']) > 10:
            plt.figure(figsize=(12, 6))
            losses = training_stats['total_loss']
            plt.plot(losses, linewidth=2, color='#1f77b4')
            plt.title('í•™ìŠµ ì†ì‹¤ íŠ¸ë Œë“œ', fontsize=14, fontweight='bold')
            plt.xlabel('í•™ìŠµ ìŠ¤í…')
            plt.ylabel('ì†ì‹¤')
            plt.grid(True, alpha=0.3)
            
            # ì´ë™ í‰ê·  ì¶”ê°€
            if len(losses) > 50:
                window = min(50, len(losses) // 10)
                moving_avg = pd.Series(losses).rolling(window=window).mean()
                plt.plot(moving_avg, linewidth=3, color='#ff7f0e', label=f'{window}-step ì´ë™í‰ê· ')
                plt.legend()
            
            loss_trend_path = viz_dir / 'loss_trend.png'
            plt.savefig(loss_trend_path, dpi=300, bbox_inches='tight')
            plt.close()
            plots['loss_trend'] = loss_trend_path
        
        return plots
    
    def generate_markdown_report(self, analysis: Dict[str, Any], 
                               report: Dict[str, Any], 
                               plots: Dict[str, Path]) -> Path:
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        template_str = """
# Red Heart XAI í•™ìŠµ ê²°ê³¼ ë¦¬í¬íŠ¸

**ìƒì„±ì¼ì‹œ**: {{ analysis.timestamp }}  
**í•™ìŠµ ë°ì´í„°**: {{ analysis.report_path }}

## ğŸ“Š í•™ìŠµ ìš”ì•½

### ê¸°ë³¸ í†µê³„
- **ì´ í•™ìŠµ ìŠ¤í…**: {{ "{:,}".format(analysis.basic_metrics.total_training_steps) }}
- **ì´ í›„íšŒ ìƒì„±**: {{ "{:,}".format(analysis.basic_metrics.total_regrets_generated) }}
- **ì´ ë²¤ë‹´ ê³„ì‚°**: {{ "{:,}".format(analysis.basic_metrics.total_bentham_calculations) }}
- **í•™ìŠµ ì‹œê°„**: {{ "%.2f"|format(analysis.basic_metrics.training_duration_hours) }}ì‹œê°„
- **ëª¨ë¸ íŒŒë¼ë¯¸í„°**: {{ "{:,}".format(analysis.basic_metrics.model_parameters) }}ê°œ
- **ìµœì¢… ì†ì‹¤**: {{ "%.6f"|format(analysis.basic_metrics.final_loss) }}

### ğŸ¯ í›„íšŒ ì‹œìŠ¤í…œ ë¶„ì„

#### í›„íšŒ ìƒì„± ì„±ëŠ¥
- **ëª©í‘œ í›„íšŒ/ìŠ¤í…**: {{ analysis.regret_analysis.target_regrets_per_step }}íšŒ
- **ì‹¤ì œ í›„íšŒ/ìŠ¤í…**: {{ "%.2f"|format(analysis.regret_analysis.actual_regrets_per_step) }}íšŒ
- **í›„íšŒ ìƒì„± íš¨ìœ¨ì„±**: {{ "%.1f"|format(analysis.regret_analysis.regret_generation_efficiency) }}%

#### ë²¤ë‹´ ì¾Œë½ ê³„ì‚°
- **ëª©í‘œ ë²¤ë‹´/í›„íšŒ**: {{ analysis.regret_analysis.expected_bentham_per_regret }}íšŒ
- **ì‹¤ì œ ë²¤ë‹´/í›„íšŒ**: {{ "%.2f"|format(analysis.regret_analysis.bentham_calculations_per_regret) }}íšŒ
- **ë²¤ë‹´ ê³„ì‚° íš¨ìœ¨ì„±**: {{ "%.1f"|format(analysis.regret_analysis.bentham_efficiency) }}%

### âš¡ ì„±ëŠ¥ íš¨ìœ¨ì„±

- **ì‹œê°„ë‹¹ í›„íšŒ ìƒì„±**: {{ "{:,.0f}".format(analysis.efficiency_metrics.regrets_per_hour) }}ê°œ
- **ì‹œê°„ë‹¹ ë²¤ë‹´ ê³„ì‚°**: {{ "{:,.0f}".format(analysis.efficiency_metrics.bentham_calculations_per_hour) }}ê°œ  
- **ì‹œê°„ë‹¹ í•™ìŠµ ìŠ¤í…**: {{ "{:,.0f}".format(analysis.efficiency_metrics.training_speed_steps_per_hour) }}ê°œ

### ğŸ“ˆ í•™ìŠµ íŠ¸ë Œë“œ

{{ analysis.performance_trends.convergence_analysis }}

{% if analysis.performance_trends.loss_improvement_percentage > 0 %}
- **ì†ì‹¤ ê°œì„ ìœ¨**: {{ "%.1f"|format(analysis.performance_trends.loss_improvement_percentage) }}%
{% endif %}

## ğŸ¨ ì‹œê°í™”

{% if plots.dashboard %}
### í•™ìŠµ ëŒ€ì‹œë³´ë“œ
![í•™ìŠµ ëŒ€ì‹œë³´ë“œ](visualizations/training_dashboard.png)
{% endif %}

{% if plots.loss_trend %}
### ì†ì‹¤ íŠ¸ë Œë“œ
![ì†ì‹¤ íŠ¸ë Œë“œ](visualizations/loss_trend.png)
{% endif %}

## ğŸ’¡ ê¶Œì¥ì‚¬í•­

{% for recommendation in analysis.recommendations %}
- {{ recommendation }}
{% endfor %}

## ğŸ”§ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

### í•™ìŠµ ì„¤ì •
- **ì—í¬í¬ ìˆ˜**: {{ report.configuration.epochs }}
- **ë°°ì¹˜ í¬ê¸°**: {{ report.configuration.batch_size }}
- **í•™ìŠµë¥ **: {{ report.configuration.learning_rate }}
- **í›„íšŒ/ìŠ¤í…**: {{ report.configuration.regrets_per_step }}
- **ë²¤ë‹´/í›„íšŒ**: {{ report.configuration.bentham_calculations_per_regret }}

### ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰
- **ìµœì¢… ì‚¬ìš©ëŸ‰**: {{ "%.2f"|format(report.storage_usage.final_size_gb) }}GB
- **í—ˆìš© í•œê³„**: {{ "%.0f"|format(report.storage_usage.max_allowed_gb) }}GB
- **ì‚¬ìš©ë¥ **: {{ "%.1f"|format((report.storage_usage.final_size_gb / report.storage_usage.max_allowed_gb) * 100) }}%

### XAI í†µí•©
- **XAI ë¡œê·¸ ìƒì„±**: {{ report.xai_integration.xai_logs_generated }}ê°œ
- **ì„¸ì…˜ ID**: `{{ report.xai_integration.session_id }}`

---

*ì´ ë¦¬í¬íŠ¸ëŠ” Red Heart XAI ì‹œìŠ¤í…œì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*  
*ìƒì„± ì‹œê°: {{ analysis.timestamp }}*
"""

        template = Template(template_str)
        content = template.render(analysis=analysis, report=report, plots=plots)
        
        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
        report_path = self.docs_dir / f'training_report_{int(datetime.now().timestamp())}.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return report_path
    
    def generate_html_report(self, markdown_path: Path) -> Path:
        """HTML ë¦¬í¬íŠ¸ ìƒì„±"""
        with open(markdown_path, 'r', encoding='utf-8') as f:
            md_content = f.read()
        
        # CSS ìŠ¤íƒ€ì¼ ì¶”ê°€
        html_template = """
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Red Heart XAI í•™ìŠµ ê²°ê³¼ ë¦¬í¬íŠ¸</title>
    <style>
        body {
            font-family: 'Segoe UI', 'Malgun Gothic', sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1, h2, h3 { color: #2c3e50; }
        h1 { border-bottom: 3px solid #3498db; padding-bottom: 10px; }
        h2 { border-bottom: 2px solid #ecf0f1; padding-bottom: 5px; }
        .metric { 
            background: #f8f9fa; 
            padding: 10px; 
            margin: 5px 0; 
            border-left: 4px solid #3498db; 
        }
        .recommendation {
            background: #e8f5e8;
            padding: 10px;
            margin: 5px 0;
            border-left: 4px solid #27ae60;
        }
        img { max-width: 100%; height: auto; margin: 10px 0; }
        code { background: #f1f2f6; padding: 2px 4px; border-radius: 3px; }
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        .stat-card {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #e74c3c;
        }
    </style>
</head>
<body>
    <div class="container">
        {{ content }}
    </div>
    <script>
        // ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì •
        document.querySelectorAll('img').forEach(img => {
            if (img.src.includes('visualizations/')) {
                img.src = img.src.replace('visualizations/', 'visualizations/');
            }
        });
    </script>
</body>
</html>
"""
        
        # ë§ˆí¬ë‹¤ìš´ì„ HTMLë¡œ ë³€í™˜
        html_content = markdown.markdown(md_content, extensions=['tables', 'fenced_code'])
        
        # í…œí”Œë¦¿ì— ì‚½ì…
        final_html = html_template.replace('{{ content }}', html_content)
        
        # HTML íŒŒì¼ ì €ì¥
        html_path = markdown_path.with_suffix('.html')
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(final_html)
        
        return html_path
    
    def analyze_and_generate_docs(self, report_path: Optional[Path] = None) -> Dict[str, Path]:
        """ì „ì²´ ë¶„ì„ ë° ë¬¸ì„œ ìƒì„± í”„ë¡œì„¸ìŠ¤"""
        
        # ê°€ì¥ ìµœê·¼ ë¦¬í¬íŠ¸ ì°¾ê¸°
        if report_path is None:
            reports_dir = self.training_dir / 'reports'
            if not reports_dir.exists():
                raise FileNotFoundError("í•™ìŠµ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            report_files = list(reports_dir.glob('regret_training_report_*.json'))
            if not report_files:
                raise FileNotFoundError("í•™ìŠµ ë¦¬í¬íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            report_path = max(report_files, key=lambda x: x.stat().st_mtime)
        
        print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ: {report_path}")
        
        # 1. ë¦¬í¬íŠ¸ ë¶„ì„
        with open(report_path, 'r', encoding='utf-8') as f:
            report = json.load(f)
        
        analysis = self.analyze_training_report(report_path)
        
        # 2. ì‹œê°í™” ìƒì„±
        plots = self.create_visualizations(analysis, report)
        
        # 3. ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        markdown_path = self.generate_markdown_report(analysis, report, plots)
        
        # 4. HTML ë¦¬í¬íŠ¸ ìƒì„±
        html_path = self.generate_html_report(markdown_path)
        
        # 5. ìš”ì•½ JSON ìƒì„±
        summary_path = self.docs_dir / 'latest_training_summary.json'
        summary = {
            'analysis_timestamp': analysis['timestamp'],
            'source_report': str(report_path),
            'key_metrics': analysis['basic_metrics'],
            'recommendations': analysis['recommendations'],
            'files_generated': {
                'markdown_report': str(markdown_path),
                'html_report': str(html_path),
                'visualizations': {k: str(v) for k, v in plots.items()}
            }
        }
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        result = {
            'analysis': analysis,
            'markdown_report': markdown_path,
            'html_report': html_path,
            'summary': summary_path,
            'visualizations': plots
        }
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“„ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸: {markdown_path}")
        print(f"ğŸŒ HTML ë¦¬í¬íŠ¸: {html_path}")
        print(f"ğŸ“Š ì‹œê°í™”: {len(plots)}ê°œ ìƒì„±")
        
        return result

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    project_root = Path(__file__).parent.parent
    analyzer = TrainingResultsAnalyzer(project_root)
    
    print("ğŸ” í•™ìŠµ ê²°ê³¼ ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ")
    print("analyze_and_generate_docs()ë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì‹  í•™ìŠµ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì„¸ìš”.")