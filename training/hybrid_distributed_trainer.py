#!/usr/bin/env python3
"""
í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° í•™ìŠµ ì‹œìŠ¤í…œ (CPU 128GB + GPU RTX 2070S)
Hybrid Distributed Training System (CPU 128GB + GPU RTX 2070S)
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict, field
import math
import gc
from collections import defaultdict
import threading
import queue
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, as_completed

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ëª¨ë“ˆ imports
from xai_core.xai_logging_system import xai_logger, xai_trace
from llm_module import llm_tracker, register_llm, ask_llm

# ì´ˆê¸°í™” ë° ì˜µí‹°ë§ˆì´ì € ê°œì„ ì„ ìœ„í•œ ì„í¬íŠ¸
from torch.nn import init
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR

@dataclass
class HybridConfig:
    """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì„¤ì •"""
    # ëª¨ë¸ ë¶„í•  ì„¤ì • (ê³µê²©ì  ìµœì í™” 70GB)
    gpu_memory_gb: float = 8.0          # RTX 2070S ë©”ëª¨ë¦¬
    cpu_memory_gb: float = 70.0         # WSL ê³µê²©ì  ë©”ëª¨ë¦¬ í™œìš©
    target_params: int = 800_000_000     # 800M íŒŒë¼ë¯¸í„° (300M ë°±ë³¸ + 500M í—¤ë“œ)
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì •
    test_mode: bool = False             # í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”
    test_samples: int = 10              # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ìˆ˜
    
    # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
    max_safe_batch_size: int = 8        # GPU í™œìš©ìœ¼ë¡œ ë°°ì¹˜ ì¦ê°€ ê°€ëŠ¥
    gradient_accumulation_steps: int = 8 # íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° ì¦ëŒ€
    use_gradient_checkpointing: bool = True  # í™œì„±í™” ë©”ëª¨ë¦¬ ì ˆì•½
    use_parameter_sharing: bool = True   # ë ˆì´ì–´ê°„ íŒŒë¼ë¯¸í„° ê³µìœ 
    
    # í•™ìŠµ ì„¤ì • (ê³µê²©ì  ê³ ì„±ëŠ¥)
    regrets_per_step: int = 7           # ì›ë˜ ì„¤ì • ë³µì› (ìµœê³  í’ˆì§ˆ)
    bentham_calculations_per_regret: int = 3  # ì›ë˜ ì„¤ì • ë³µì› (ì •í™•ë„ ê·¹ëŒ€í™”)
    epochs: int = 3
    batch_size: int = 12                # ëŒ€í˜• ë°°ì¹˜ (ì²˜ë¦¬ëŸ‰ ì¦ê°€)
    micro_batch_size: int = 3           # ë©”ëª¨ë¦¬ ì—¬ìœ ë¡œ ì¦ê°€
    
    # ë¶„ì‚° ì„¤ì • (ê³µê²©ì  ë³‘ë ¬í™”)
    num_workers: int = 8                # ìµœëŒ€ CPU í™œìš©
    gpu_layers_ratio: float = 0.6       # ê· í˜•ì¡íŒ GPU/CPU ë¶„í• 
    overlap_computation: bool = True     # ê³„ì‚° ì˜¤ë²„ë©
    use_cpu_offload: bool = True        # CPU ì˜¤í”„ë¡œë“œ í™œì„±í™”
    enable_memory_monitoring: bool = True  # ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
    
    # ìµœì í™” ì„¤ì • (ê·¸ë˜ë””ì–¸íŠ¸ ì•ˆì •í™”)
    use_mixed_precision: bool = True    # Mixed Precision ìœ ì§€ (íš¨ìœ¨ì„±)
    gradient_accumulation_steps: int = 8 # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ ì¦ê°€
    max_grad_norm: float = 0.5          # ì ë‹¹í•œ ìˆ˜ì¤€ì˜ í´ë¦¬í•‘
    
    # ì´ˆê¸°í™” ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ê·¸ë˜ë””ì–¸íŠ¸ ì•ˆì •í™”)
    initialization_method: str = 'xavier'  # Xavier ì´ˆê¸°í™”ë¡œ ë³€ê²½ (ë” ë³´ìˆ˜ì )
    optimizer_type: str = 'adamw'          # AdamW ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©
    learning_rate: float = 1e-4            # ì ì ˆí•œ í•™ìŠµë¥  (Adam ê¸°ë³¸ê°’)
    weight_decay: float = 0.1              # ê°€ì¤‘ì¹˜ ê°ì‡  ê°•í™”
    scheduler_type: str = 'cosine'         # 'cosine' ë˜ëŠ” 'linear' ìŠ¤ì¼€ì¤„ëŸ¬
    
    # ë°ì´í„° ìƒ˜í”Œë§ ì„¤ì • (Phase 2 ê°œì„ )
    enable_balanced_sampling: bool = True  # í´ë”ë³„ ê· ë“± ìƒ˜í”Œë§
    data_folder_weights: dict = field(default_factory=lambda: {
        'scruples': 0.4,              # ìœ¤ë¦¬ì  ë”œë ˆë§ˆ (í•µì‹¬)
        'classic_literature': 0.3,    # ë¬¸í•™ì  ê°ì • ë³µì¡ì„±
        'ai_generated_scenarios.json': 0.2,  # ì¼ê´€ëœ íŒ¨í„´
        'ebs_korean_literature': 0.1  # êµìœ¡ì  ì²´ê³„ì„±
    })
    validation_frequency: int = 50        # NìŠ¤í…ë§ˆë‹¤ ê²€ì¦
    enable_continuous_monitoring: bool = True  # ì§€ì†ì  ëª¨ë‹ˆí„°ë§
    
    # ë¡œê¹…/ì²´í¬í¬ì¸íŠ¸
    log_every_n_steps: int = 5          # ë” ìì£¼ ë¡œê¹…
    save_checkpoint_every: int = 20     # ë” ìì£¼ ì €ì¥
    max_storage_gb: float = 50.0        # ìŠ¤í† ë¦¬ì§€ ì ˆì•½

class MemoryOptimizedModel(nn.Module):
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ëª¨ë¸ (ê°œì„ ëœ ì´ˆê¸°í™” ë° ì˜µí‹°ë§ˆì´ì € ì ìš©)"""
    
    def __init__(self, config: HybridConfig):
        super().__init__()
        self.config = config
        
        # 800M íŒŒë¼ë¯¸í„° ì„¤ê³„ (config.py ì„¤ì •ê³¼ ë™ì¼)
        self.hidden_dim = 1280      # configì˜ d_modelê³¼ ì¼ì¹˜
        self.num_layers = 18        # configì˜ num_layersì™€ ì¼ì¹˜
        self.num_heads = 20         # configì˜ num_headsì™€ ì¼ì¹˜
        self.intermediate_size = 5120  # configì˜ feedforward_dimê³¼ ì¼ì¹˜
        
        # ì…ë ¥ ë ˆì´ì–´ (CPU)
        self.input_projection = nn.Linear(1024, self.hidden_dim)
        self.input_norm = nn.LayerNorm(self.hidden_dim, eps=1e-6)  # Loss NaN ë°©ì§€ ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ 
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤ (ë¶„í•  ë°°ì¹˜)
        gpu_layers = int(self.num_layers * config.gpu_layers_ratio)
        cpu_layers = self.num_layers - gpu_layers
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŠ¸ í™œì„±í™”
        use_checkpointing = getattr(config, 'use_gradient_checkpointing', True)
        
        self.gpu_layers = nn.ModuleList([
            OptimizedTransformerLayer(self.hidden_dim, self.num_heads, self.intermediate_size, use_checkpointing)
            for _ in range(gpu_layers)
        ])
        
        self.cpu_layers = nn.ModuleList([
            OptimizedTransformerLayer(self.hidden_dim, self.num_heads, self.intermediate_size, use_checkpointing)
            for _ in range(cpu_layers)
        ])
        
        # ì¶œë ¥ í—¤ë“œë“¤ (GPU) - ë©”ëª¨ë¦¬ ìµœì í™”
        # SwiGLU ê¸°ë°˜ ê°ì • í—¤ë“œ (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        self.emotion_swiglu = SwiGLU(self.hidden_dim, self.hidden_dim // 2)
        self.emotion_out = nn.Linear(self.hidden_dim // 2, 6)  # 6ì°¨ì› ê°ì •
        self.emotion_dropout = nn.Dropout(0.02)
        self.emotion_activation = nn.Tanh()
        
        # SwiGLU ê¸°ë°˜ ì˜ë¯¸ í—¤ë“œ (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        self.semantic_swiglu = SwiGLU(self.hidden_dim, self.hidden_dim // 2)
        self.semantic_out = nn.Linear(self.hidden_dim // 2, 512)  # 1000â†’512 ì¶•ì†Œ
        self.semantic_dropout = nn.Dropout(0.02)
        self.semantic_activation = nn.Softmax(dim=-1)
        
        # SwiGLU ê¸°ë°˜ ì¶”ë¡  í—¤ë“œ (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        self.reasoning_swiglu = SwiGLU(self.hidden_dim, self.hidden_dim // 4)
        self.reasoning_out = nn.Linear(self.hidden_dim // 4, 128)  # ì¶”ë¡  íŠ¹ì§•
        self.reasoning_dropout = nn.Dropout(0.05)
        
        # SwiGLU ê¸°ë°˜ í†µí•© í—¤ë“œ (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        self.integration_swiglu = SwiGLU(self.hidden_dim, self.hidden_dim)
        self.integration_out = nn.Linear(self.hidden_dim, 512)  # í†µí•© íŠ¹ì§•
        self.integration_dropout = nn.Dropout(0.05)
        self.integration_activation = nn.Tanh()
        
        # ì´ˆê¸°í™” ì „ëµ ì ìš© (He vs Xavier ë¹„êµ ì‹¤í—˜)
        self._initialize_weights()
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.gpu_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.cpu_device = torch.device('cpu')
        
        self._setup_devices()
        
    def _setup_devices(self):
        """ë””ë°”ì´ìŠ¤ë³„ ëª¨ë¸ ë°°ì¹˜"""
        # CPU ë ˆì´ì–´ë“¤
        self.input_projection = self.input_projection.to(self.cpu_device)
        self.input_norm = self.input_norm.to(self.cpu_device)
        self.cpu_layers = self.cpu_layers.to(self.cpu_device)
        
        # GPU ë ˆì´ì–´ë“¤ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if torch.cuda.is_available():
            self.gpu_layers = self.gpu_layers.to(self.gpu_device)
            # SwiGLU ê¸°ë°˜ í—¤ë“œë“¤ GPUë¡œ ì´ë™
            self.emotion_swiglu = self.emotion_swiglu.to(self.gpu_device)
            self.emotion_out = self.emotion_out.to(self.gpu_device)
            self.emotion_dropout = self.emotion_dropout.to(self.gpu_device)
            self.emotion_activation = self.emotion_activation.to(self.gpu_device)
            
            self.semantic_swiglu = self.semantic_swiglu.to(self.gpu_device)
            self.semantic_out = self.semantic_out.to(self.gpu_device)
            self.semantic_dropout = self.semantic_dropout.to(self.gpu_device)
            self.semantic_activation = self.semantic_activation.to(self.gpu_device)
            
            self.reasoning_swiglu = self.reasoning_swiglu.to(self.gpu_device)
            self.reasoning_out = self.reasoning_out.to(self.gpu_device)
            self.reasoning_dropout = self.reasoning_dropout.to(self.gpu_device)
            
            self.integration_swiglu = self.integration_swiglu.to(self.gpu_device)
            self.integration_out = self.integration_out.to(self.gpu_device)
            self.integration_dropout = self.integration_dropout.to(self.gpu_device)
            self.integration_activation = self.integration_activation.to(self.gpu_device)
        else:
            # GPU ì—†ìœ¼ë©´ CPUì— ë°°ì¹˜
            self.gpu_layers = self.gpu_layers.to(self.cpu_device)
            # SwiGLU ê¸°ë°˜ í—¤ë“œë“¤ CPUë¡œ ì´ë™
            self.emotion_swiglu = self.emotion_swiglu.to(self.cpu_device)
            self.emotion_out = self.emotion_out.to(self.cpu_device)
            self.emotion_dropout = self.emotion_dropout.to(self.cpu_device)
            self.emotion_activation = self.emotion_activation.to(self.cpu_device)
            
            self.semantic_swiglu = self.semantic_swiglu.to(self.cpu_device)
            self.semantic_out = self.semantic_out.to(self.cpu_device)
            self.semantic_dropout = self.semantic_dropout.to(self.cpu_device)
            self.semantic_activation = self.semantic_activation.to(self.cpu_device)
            
            self.reasoning_swiglu = self.reasoning_swiglu.to(self.cpu_device)
            self.reasoning_out = self.reasoning_out.to(self.cpu_device)
            self.reasoning_dropout = self.reasoning_dropout.to(self.cpu_device)
            
            self.integration_swiglu = self.integration_swiglu.to(self.cpu_device)
            self.integration_out = self.integration_out.to(self.cpu_device)
            self.integration_dropout = self.integration_dropout.to(self.cpu_device)
            self.integration_activation = self.integration_activation.to(self.cpu_device)
            self.gpu_device = self.cpu_device
    
    def _initialize_weights(self):
        """ì´ˆê¸°í™” ì „ëµ ì ìš© - He vs Xavier ë¹„êµ ì‹¤í—˜"""
        initialization_method = getattr(self.config, 'initialization_method', 'he')  # ê¸°ë³¸ê°’: He
        
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                if initialization_method == 'he':
                    # He ì´ˆê¸°í™”: ReLU ê³„ì—´ í™œì„±í™” í•¨ìˆ˜ì— ì í•© (SwiGLU í¬í•¨, ìŠ¤ì¼€ì¼ ì¶•ì†Œ)
                    init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
                    module.weight.data *= 0.5  # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ ìŠ¤ì¼€ì¼ ì¶•ì†Œ
                elif initialization_method == 'xavier':
                    # Xavier ì´ˆê¸°í™”: Tanh ê³„ì—´ í™œì„±í™” í•¨ìˆ˜ì— ì í•© (ìŠ¤ì¼€ì¼ ì¶•ì†Œ)
                    init.xavier_normal_(module.weight)
                    module.weight.data *= 0.7  # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ ìŠ¤ì¼€ì¼ ì¶•ì†Œ
                
                # biasëŠ” ê³µí†µì ìœ¼ë¡œ 0ìœ¼ë¡œ ì´ˆê¸°í™”
                if module.bias is not None:
                    init.constant_(module.bias, 0)
            
            elif isinstance(module, nn.LayerNorm):
                # LayerNormì€ í‘œì¤€ ì´ˆê¸°í™” ìœ ì§€
                init.constant_(module.weight, 1)
                init.constant_(module.bias, 0)
        
        print(f"ì´ˆê¸°í™” ë°©ë²• ì ìš©: {initialization_method}")
    
    def get_parameter_count(self) -> int:
        """íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        return sum(p.numel() for p in self.parameters())
    
    def forward(self, input_embeddings: torch.Tensor) -> Dict[str, torch.Tensor]:
        batch_size, seq_len, input_dim = input_embeddings.shape
        
        # CPUì—ì„œ ì…ë ¥ ì²˜ë¦¬
        hidden_states = input_embeddings.to(self.cpu_device)
        hidden_states = self.input_projection(hidden_states)
        hidden_states = self.input_norm(hidden_states)
        
        # CPU ë ˆì´ì–´ë“¤ í†µê³¼
        for layer in self.cpu_layers:
            hidden_states = layer(hidden_states)
        
        # GPUë¡œ ì´ë™ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if torch.cuda.is_available():
            hidden_states = hidden_states.to(self.gpu_device)
        
        # GPU ë ˆì´ì–´ë“¤ í†µê³¼
        for layer in self.gpu_layers:
            hidden_states = layer(hidden_states)
        
        # í‰ê·  í’€ë§
        pooled_output = hidden_states.mean(dim=1)
        
        # SwiGLU ê¸°ë°˜ ì¶œë ¥ í—¤ë“œë“¤ (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        emotion_swiglu = self.emotion_swiglu(pooled_output)
        emotion_output = self.emotion_activation(self.emotion_out(self.emotion_dropout(emotion_swiglu)))
        
        semantic_swiglu = self.semantic_swiglu(pooled_output)
        semantic_output = self.semantic_activation(self.semantic_out(self.semantic_dropout(semantic_swiglu)))
        
        reasoning_swiglu = self.reasoning_swiglu(pooled_output)
        reasoning_output = self.reasoning_out(self.reasoning_dropout(reasoning_swiglu))
        
        integration_swiglu = self.integration_swiglu(pooled_output)
        integration_output = self.integration_activation(self.integration_out(self.integration_dropout(integration_swiglu)))
        
        return {
            'emotion_predictions': emotion_output,
            'semantic_predictions': semantic_output,
            'reasoning_features': reasoning_output,
            'integration_features': integration_output,
            'pooled_output': pooled_output
        }

class SwiGLU(nn.Module):
    """
    SwiGLU í™œì„±í™” í•¨ìˆ˜ - ìˆ˜ì¹˜ì  ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì´ GELUë³´ë‹¤ ìš°ìˆ˜
    SwiGLU(x) = Swish(xW + b) âŠ— (xV + c)
    """
    def __init__(self, dim: int, hidden_dim: int):
        super().__init__()
        self.dim = dim
        self.hidden_dim = hidden_dim
        # SwiGLUëŠ” ë‘ ê°œì˜ ì„ í˜• ë³€í™˜ì„ í•„ìš”ë¡œ í•¨
        self.w = nn.Linear(dim, hidden_dim, bias=True)
        self.v = nn.Linear(dim, hidden_dim, bias=True)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ì…ë ¥ ì•ˆì „ì„± ê²€ì‚¬
        if torch.isnan(x).any() or torch.isinf(x).any():
            print(f"SwiGLU ì…ë ¥ì— NaN/Inf ë°œê²¬: {x.shape}")
            return torch.zeros_like(x)
        
        # Swish(x) = x * sigmoid(x), ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •ì 
        w_out = self.w(x)
        
        # ì¤‘ê°„ ì¶œë ¥ ì•ˆì „ì„± ê²€ì‚¬
        if torch.isnan(w_out).any() or torch.isinf(w_out).any():
            print(f"SwiGLU w_outì— NaN/Inf ë°œê²¬")
            return torch.zeros_like(x)
            
        swish_w = w_out * torch.sigmoid(w_out)
        v_out = self.v(x)
        
        # ìµœì¢… ì¶œë ¥ ì•ˆì „ì„± ê²€ì‚¬
        result = swish_w * v_out
        if torch.isnan(result).any() or torch.isinf(result).any():
            print(f"SwiGLU ì¶œë ¥ì— NaN/Inf ë°œê²¬")
            return torch.zeros_like(x)
            
        return result

class OptimizedTransformerLayer(nn.Module):
    """ì„±ëŠ¥ ìœ ì§€ ë©”ëª¨ë¦¬ ìµœì í™” íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ (SwiGLU ì ìš©)"""
    
    def __init__(self, hidden_dim: int, num_heads: int, intermediate_size: int, use_gradient_checkpointing: bool = True):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        self.use_gradient_checkpointing = use_gradient_checkpointing
        
        # ê³ íš¨ìœ¨ ì–´í…ì…˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=0.02,  # Dropout ì¶”ê°€ ê°ì†Œ (Loss NaN ë°©ì§€)
            batch_first=True
        )
        self.attention_norm = nn.LayerNorm(hidden_dim, eps=1e-6)  # Loss NaN ë°©ì§€ ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ 
        
        # SwiGLU ê¸°ë°˜ FFN (ìˆ˜ì¹˜ ì•ˆì •ì„± ë° ì„±ëŠ¥ ê°œì„ )
        self.swiglu = SwiGLU(hidden_dim, intermediate_size)
        self.ffn_out = nn.Linear(intermediate_size, hidden_dim)
        self.ffn_dropout = nn.Dropout(0.02)  # Dropout ì¶”ê°€ ê°ì†Œ (Loss NaN ë°©ì§€)
        self.ffn_norm = nn.LayerNorm(hidden_dim, eps=1e-6)  # Loss NaN ë°©ì§€ ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ 
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
        if self.use_gradient_checkpointing and self.training:
            from torch.utils.checkpoint import checkpoint
            return checkpoint(self._forward_impl, hidden_states, use_reentrant=False)
        else:
            return self._forward_impl(hidden_states)
    
    def _forward_impl(self, hidden_states: torch.Tensor) -> torch.Tensor:
        # ì–´í…ì…˜ ë¸”ë¡ (Post-LayerNorm ì•ˆì •ì„± ê°œì„ )
        attention_output, _ = self.attention(hidden_states, hidden_states, hidden_states)
        hidden_states = hidden_states + attention_output
        hidden_states = self.attention_norm(hidden_states)
        
        # SwiGLU ê¸°ë°˜ FFN ë¸”ë¡ (Post-LayerNorm ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        swiglu_output = self.swiglu(hidden_states)
        ffn_output = self.ffn_dropout(self.ffn_out(swiglu_output))
        hidden_states = hidden_states + ffn_output
        hidden_states = self.ffn_norm(hidden_states)
        
        return hidden_states

class AsyncRegretCalculator:
    """ë¹„ë™ê¸° í›„íšŒ ê³„ì‚°ê¸°"""
    
    def __init__(self, config: HybridConfig):
        self.config = config
        self.regret_queue = queue.Queue(maxsize=100)
        self.result_queue = queue.Queue(maxsize=100)
        self.workers = []
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œë“¤ ì‹œì‘
        for i in range(config.num_workers):
            worker = threading.Thread(target=self._worker_process, daemon=True)
            worker.start()
            self.workers.append(worker)
    
    def _worker_process(self):
        """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤"""
        while True:
            try:
                task = self.regret_queue.get(timeout=1)
                if task is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                
                original_decision, task_id = task
                regret_scenarios = self._calculate_regret_scenarios(original_decision)
                self.result_queue.put((task_id, regret_scenarios))
                self.regret_queue.task_done()
            except queue.Empty:
                continue
            except Exception as e:
                print(f"Worker error: {e}")
    
    def _calculate_regret_scenarios(self, original_decision: torch.Tensor) -> List[Dict[str, torch.Tensor]]:
        """7ê°€ì§€ í›„íšŒ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        scenarios = []
        regret_types = ['counterfactual', 'temporal', 'moral', 'opportunity', 'social']
        
        for i, regret_type in enumerate(regret_types):
            if i >= self.config.regrets_per_step:
                break
            
            # ê° í›„íšŒ ìœ í˜•ë³„ ë³€í˜•ëœ ê²°ì • ìƒì„±
            regret_decision = self._generate_regret_decision(original_decision, regret_type)
            
            # ë²¤ë‹´ ì¾Œë½ ê³„ì‚° (3íšŒ)
            bentham_scores = []
            for _ in range(self.config.bentham_calculations_per_regret):
                score = self._calculate_bentham_score(original_decision, regret_decision)
                bentham_scores.append(score)
            
            scenarios.append({
                'regret_type': regret_type,
                'regret_decision': regret_decision,
                'bentham_scores': torch.tensor(bentham_scores).mean(),
                'regret_weight': 0.2
            })
        
        # ë¶€ì¡±í•œ ê²½ìš° ì¶”ê°€
        while len(scenarios) < self.config.regrets_per_step:
            scenarios.append(scenarios[0])  # ì²« ë²ˆì§¸ ì‹œë‚˜ë¦¬ì˜¤ ë³µì‚¬
        
        return scenarios[:self.config.regrets_per_step]
    
    def _generate_regret_decision(self, original: torch.Tensor, regret_type: str) -> torch.Tensor:
        """í›„íšŒ ìœ í˜•ë³„ ë³€í˜•ëœ ê²°ì • ìƒì„±"""
        if regret_type == 'counterfactual':
            return -original + torch.randn_like(original) * 0.1
        elif regret_type == 'temporal':
            return original * 0.7 + torch.randn_like(original) * 0.2
        elif regret_type == 'moral':
            return original + torch.ones_like(original) * 0.3
        elif regret_type == 'opportunity':
            return original * 1.3 + torch.randn_like(original) * 0.15
        elif regret_type == 'social':
            return original + torch.ones_like(original) * 0.2
        else:
            return original + torch.randn_like(original) * 0.1
    
    def _calculate_bentham_score(self, original: torch.Tensor, regret: torch.Tensor) -> float:
        """ë²¤ë‹´ ì¾Œë½ ì ìˆ˜ ê³„ì‚°"""
        diff = torch.abs(regret - original).mean()
        intensity = diff.item()
        return intensity * 0.8  # ê°„ì†Œí™”ëœ ë²¤ë‹´ ê³„ì‚°
    
    def calculate_async(self, original_decision: torch.Tensor, task_id: int):
        """ë¹„ë™ê¸° í›„íšŒ ê³„ì‚° ìš”ì²­"""
        self.regret_queue.put((original_decision.cpu().clone(), task_id))
    
    def get_result(self, timeout: float = 0.1) -> Optional[Tuple[int, List[Dict]]]:
        """ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None

class OptimizedDataset(Dataset):
    """ìµœì í™”ëœ ë°ì´í„°ì…‹"""
    
    def __init__(self, data_files: List[Path], config: HybridConfig):
        self.config = config
        self.scenarios = []
        
        # ë°ì´í„° ë¡œë“œ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì§€ì›)
        print("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
        total_loaded = 0
        for file_path in data_files:
            if hasattr(config, 'test_mode') and config.test_mode and total_loaded >= config.test_samples:
                break
                
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    if hasattr(config, 'test_mode') and config.test_mode:
                        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì •í™•íˆ test_samplesê°œë§Œ ë¡œë“œ
                        remaining = config.test_samples - total_loaded
                        sample_size = min(len(data), remaining)
                        self.scenarios.extend(data[:sample_size])
                        total_loaded += sample_size
                    else:
                        # ì¼ë°˜ ëª¨ë“œ: íŒŒì¼ë‹¹ ìµœëŒ€ 5000ê°œ
                        sample_size = min(len(data), 5000)
                        self.scenarios.extend(data[:sample_size])
        
        print(f"âœ… ì´ {len(self.scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œë¨")
    
    def __len__(self):
        return len(self.scenarios)
    
    def __getitem__(self, idx):
        scenario = self.scenarios[idx]
        
        # í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ìµœì í™”ëœ ì°¨ì›)
        embedding = torch.randn(1024, dtype=torch.float32)
        
        # ë¼ë²¨ ì¤€ë¹„
        options = scenario.get('options', [])
        if len(options) >= 3:
            labels = torch.tensor([0.5, 0.3, 0.2], dtype=torch.float32)
        else:
            labels = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float32)
        
        return {
            'text_embedding': embedding,
            'labels': labels,
            'scenario_id': scenario.get('id', f'scenario_{idx}'),
            'category': scenario.get('category', 'general')
        }

class BalancedDataset(Dataset):
    """í´ë”ë³„ ê· ë“± ìƒ˜í”Œë§ ë°ì´í„°ì…‹ (Phase 2 ê°œì„ )"""
    
    def __init__(self, config: HybridConfig):
        self.config = config
        self.scenarios = []
        self.folder_data = {}  # í´ë”ë³„ ë°ì´í„° ì €ì¥
        self.validation_stats = {'total_batches': 0, 'folder_counts': {}}
        
        # ë°ì´í„° ë¡œë“œ ë° ë¶„ë¥˜
        self._load_and_categorize_data()
        
        # ê· ë“± ìƒ˜í”Œë§ ì„¤ì •
        if config.enable_balanced_sampling:
            self._setup_balanced_sampling()
        
        print(f"ğŸ“ˆ ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì™„ë£Œ: ì´ {len(self.scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
        for folder, data in self.folder_data.items():
            print(f"   {folder}: {len(data)}ê°œ")
    
    def _load_and_categorize_data(self):
        """ë°ì´í„° ë¡œë“œ ë° í´ë”ë³„ ë¶„ë¥˜"""
        base_path = Path('/mnt/c/large_project/linux_red_heart/processed_datasets')
        
        # ê¸°ì¡´ full_scenarios_batch íŒŒì¼ë“¤ ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
        self._load_full_batch_files(base_path)
    
    def _load_full_batch_files(self, base_path: Path):
        """ê¸°ì¡´ ë°°ì¹˜ íŒŒì¼ë“¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        self.folder_data = {
            'scruples': [],
            'classic_literature': [],
            'ai_generated_scenarios.json': [],
            'ebs_korean_literature': []
        }
        
        limit = self.config.test_samples if hasattr(self.config, 'test_mode') and self.config.test_mode else 1000
        total_loaded = 0
        
        # full_scenarios_batch íŒŒì¼ë“¤ ë¡œë“œ
        for file_path in base_path.glob('full_scenarios_batch_*.json'):
            if total_loaded >= limit:
                break
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    batch_data = json.load(f)
                    if isinstance(batch_data, list):
                        for item in batch_data:
                            if total_loaded >= limit:
                                break
                            # ê°„ë‹¨í•œ ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
                            scenario = {
                                'text': str(item.get('title', '')) + ' ' + str(item.get('description', '')),
                                'source': 'mixed_data',
                                'labels': [0.3, 0.5, 0.2]
                            }
                            # ê· ë“±í•˜ê²Œ ë¶„ë°°
                            folder_idx = total_loaded % 4
                            folder_names = list(self.folder_data.keys())
                            self.folder_data[folder_names[folder_idx]].append(scenario)
                            total_loaded += 1
                            
                print(f"ğŸ“‚ {file_path.name}: {len(batch_data) if isinstance(batch_data, list) else 1}ê°œ ì²˜ë¦¬")
            except Exception as e:
                print(f"âš ï¸  {file_path} ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        # ë¡œë“œ ê²°ê³¼ ì¶œë ¥
        for folder_name, data in self.folder_data.items():
            print(f"ğŸ“‚ {folder_name}: {len(data)}ê°œ ë¡œë“œ")
    
    def _load_scruples_data(self, folder_path: Path) -> List[Dict]:
        """ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë°ì´í„° ë¡œë“œ"""
        data = []
        limit = self.config.test_samples // 4 if hasattr(self.config, 'test_mode') and self.config.test_mode else 1000
        
        # JSON ë°°ì¹˜ íŒŒì¼ë“¤ ë¡œë“œ (ì‹¤ì œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì— ë§ì¶¤)
        for file_path in folder_path.glob('scruples_batch_*.json'):
            if len(data) < limit:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        batch_data = json.load(f)
                        if isinstance(batch_data, list):
                            for item in batch_data:
                                if len(data) >= limit:
                                    break
                                scenario = self._process_scruples_item(item, 'scruples')
                                if scenario:
                                    data.append(scenario)
                except Exception as e:
                    print(f"âš ï¸  {file_path} ë¡œë“œ ì˜¤ë¥˜: {e}")
        return data
    
    def _load_book_data(self, folder_path: Path) -> List[Dict]:
        """ë¬¸í•™ ì‘í’ˆ ë°ì´í„° ë¡œë“œ"""
        data = []
        limit = self.config.test_samples // 4 if hasattr(self.config, 'test_mode') and self.config.test_mode else 500
        
        for file_path in folder_path.glob('*.txt'):
            if len(data) >= limit:
                break
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()[:5000]  # ì²˜ìŒ 5000ìë§Œ
                    sentences = content.split('. ')
                    
                    for sentence in sentences:
                        if len(data) >= limit:
                            break
                        if len(sentence.strip()) > 20:
                            scenario = {
                                'text': sentence.strip(),
                                'source': 'book',
                                'labels': [0.5, 0.3, 0.2]
                            }
                            data.append(scenario)
            except Exception as e:
                print(f"âš ï¸  {file_path} ë¡œë“œ ì˜¤ë¥˜: {e}")
        return data
    
    def _load_ai_generated_data(self, folder_path: Path) -> List[Dict]:
        """AI ìƒì„± ë°ì´í„° ë¡œë“œ"""
        data = []
        limit = self.config.test_samples // 4 if hasattr(self.config, 'test_mode') and self.config.test_mode else 300
        
        for file_path in folder_path.glob('*.txt'):
            if len(data) >= limit:
                break
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()[:100]  # ì²˜ìŒ 100ì¤„ë§Œ
                    for line in lines:
                        if len(data) >= limit:
                            break
                        if len(line.strip()) > 10:
                            scenario = {
                                'text': line.strip(),
                                'source': 'ai_generated',
                                'labels': [0.4, 0.4, 0.2]
                            }
                            data.append(scenario)
            except Exception as e:
                print(f"âš ï¸  {file_path} ë¡œë“œ ì˜¤ë¥˜: {e}")
        return data
    
    def _load_ebs_data(self, folder_path: Path) -> List[Dict]:
        """êµìœ¡ ì½˜í…ì¸  ë°ì´í„° ë¡œë“œ"""
        data = []
        limit = self.config.test_samples // 4 if hasattr(self.config, 'test_mode') and self.config.test_mode else 200
        
        for file_path in folder_path.glob('*.txt'):
            if len(data) >= limit:
                break
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()[:3000]  # ì²˜ìŒ 3000ìë§Œ
                    paragraphs = content.split('\n\n')
                    for paragraph in paragraphs:
                        if len(data) >= limit:
                            break
                        if len(paragraph.strip()) > 30:
                            scenario = {
                                'text': paragraph.strip(),
                                'source': 'ai_ebs',
                                'labels': [0.6, 0.2, 0.2]
                            }
                            data.append(scenario)
            except Exception as e:
                print(f"âš ï¸  {file_path} ë¡œë“œ ì˜¤ë¥˜: {e}")
        return data
    
    def _process_scruples_item(self, item: Dict, subfolder: str) -> Dict:
        """ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ì•„ì´í…œ ì²˜ë¦¬"""
        text = ''
        if 'title' in item and 'text' in item:
            text = f"{item['title']} {item['text']}"
        elif 'text' in item:
            text = item['text']
        elif 'action' in item:
            text = item['action']
        
        if not text or len(text.strip()) < 10:
            return None
        
        return {
            'text': text.strip()[:500],  # ìµœëŒ€ 500ì
            'source': 'scruples_real_data',
            'labels': [0.3, 0.5, 0.2]
        }
    
    def _setup_balanced_sampling(self):
        """ê· ë“± ìƒ˜í”Œë§ ì„¤ì •"""
        self.scenarios = []
        
        for folder_name, data in self.folder_data.items():
            if data and folder_name in self.config.data_folder_weights:
                weight = self.config.data_folder_weights[folder_name]
                # ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
                target_samples = int(len(data) * weight * 1.5)  # 1.5ë°° ì˜¤ë²„ìƒ˜í”Œë§
                target_samples = min(target_samples, len(data))  # ë°ì´í„° ìˆ˜ë¥¼ ì´ˆê³¼í•˜ì§€ ì•ŠìŒ
                
                if target_samples > 0:
                    sampled_data = np.random.choice(data, target_samples, replace=False).tolist()
                    self.scenarios.extend(sampled_data)
        
        # ì „ì²´ ë°ì´í„° ì…”í”Œ
        np.random.shuffle(self.scenarios)
        
        print(f"ğŸ¯ ê· ë“± ìƒ˜í”Œë§ ì™„ë£Œ: {len(self.scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
    
    def __len__(self):
        return len(self.scenarios)
    
    def __getitem__(self, idx):
        scenario = self.scenarios[idx]
        
        # í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ìµœì í™”ëœ ì°¨ì›)
        embedding = torch.randn(1024, dtype=torch.float32)
        labels = torch.tensor(scenario['labels'], dtype=torch.float32)
        
        return {
            'text_embedding': embedding,
            'labels': labels,
            'scenario_id': f"balanced_{idx}",
            'source': scenario['source']
        }
    
    def get_batch_distribution(self, batch_sources: List[str]) -> Dict[str, float]:
        """ë°°ì¹˜ ë‚´ ë°ì´í„° ë¶„í¬ ê³„ì‚°"""
        from collections import Counter
        counter = Counter(batch_sources)
        total = len(batch_sources)
        return {source: count/total for source, count in counter.items()}

class HybridDistributedTrainer:
    """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° í•™ìŠµê¸°"""
    
    def __init__(self, config: HybridConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = project_root / 'training' / 'hybrid_outputs'
        self.logs_dir = self.output_dir / 'logs'
        self.checkpoints_dir = self.output_dir / 'checkpoints'
        self.reports_dir = self.output_dir / 'reports'
        
        for dir_path in [self.output_dir, self.logs_dir, self.checkpoints_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ë¹„ë™ê¸° í›„íšŒ ê³„ì‚°ê¸°
        self.regret_calculator = AsyncRegretCalculator(config)
        
        # í•™ìŠµ í†µê³„
        self.training_stats = defaultdict(list)
        self.step_count = 0
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
        # ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” (ë‚˜ì¤‘ì— ì„¤ì •)
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = None
        
        print(f"ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        if torch.cuda.is_available():
            print(f"   - GPU ê°€ì†: âœ… CUDA í™œì„±í™”")
            print(f"   - GPU ë©”ëª¨ë¦¬: {config.gpu_memory_gb}GB")
        else:
            print(f"   - CPU ìµœì í™”: âœ… ê³ ì„±ëŠ¥ CPU ëª¨ë“œ")
            print(f"   - CPU ë©”ëª¨ë¦¬: {config.cpu_memory_gb}GB (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬)")
        print(f"   - ë³‘ë ¬ ì›Œì»¤: {config.num_workers}ê°œ")
        print(f"   - ë¶„ì‚° ì²˜ë¦¬: CPU+ë©”ëª¨ë¦¬ ìµœì í™”")
    
    def setup_model_and_optimizer(self):
        """ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ë‹¨ê³„ë³„ ê°œì„  ì ìš©)"""
        
        # ëª¨ë¸ ìƒì„±
        self.model = MemoryOptimizedModel(self.config)
        self.logger.info(f"ëª¨ë¸ ìƒì„± ì™„ë£Œ: {self.model.get_parameter_count():,}ê°œ íŒŒë¼ë¯¸í„°")
        
        # ê°œì„ ëœ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        if self.config.optimizer_type == 'adamw':
            self.optimizer = AdamW(
                self.model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay,
                eps=1e-6,  # ìˆ˜ì¹˜ ì•ˆì •ì„± í–¥ìƒ
                betas=(0.9, 0.98),  # ë” ë³´ìˆ˜ì ì¸ ë² íƒ€ ê°’
                amsgrad=True  # AMSGrad í™œì„±í™”ë¡œ ì•ˆì •ì„± í–¥ìƒ
            )
            self.logger.info(f"AdamW ì˜µí‹°ë§ˆì´ì € ì„¤ì •: lr={self.config.learning_rate}, wd={self.config.weight_decay}")
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì˜µí‹°ë§ˆì´ì €: {self.config.optimizer_type}")
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        if self.config.scheduler_type == 'cosine':
            # Cosine Annealing: ì£¼ê¸°ì  í•™ìŠµë¥  ê°ì†Œ
            self.scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=200,  # ìµœëŒ€ ì—í¬í¬ ìˆ˜
                eta_min=self.config.learning_rate * 0.01  # ìµœì†Œ í•™ìŠµë¥ 
            )
            self.logger.info("ì½”ì‚¬ì¸ ì–´ë‹ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •")
        elif self.config.scheduler_type == 'linear':
            # Linear Decay
            self.scheduler = LinearLR(
                self.optimizer,
                start_factor=1.0,
                end_factor=0.1,
                total_iters=100
            )
            self.logger.info("ì„ í˜• ê°ì†Œ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •")
        
        # Mixed Precision ìŠ¤ì¼€ì¼ëŸ¬
        if self.config.use_mixed_precision and torch.cuda.is_available():
            self.scaler = torch.cuda.amp.GradScaler()
            self.logger.info("Mixed Precision ìŠ¤ì¼€ì¼ëŸ¬ í™œì„±í™”")
        
        # ìˆ˜ì¹˜ ì•ˆì •ì„± ê²€ì¦
        self._validate_model_stability()
    
    def _validate_model_stability(self):
        """ëª¨ë¸ ìˆ˜ì¹˜ ì•ˆì •ì„± ê²€ì¦"""
        self.logger.info("ëª¨ë¸ ì•ˆì •ì„± ê²€ì¦ ì‹œì‘...")
        
        # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        dummy_input = torch.randn(2, 16, 1024, device=self.model.cpu_device)
        self.model.eval()
        
        with torch.no_grad():
            outputs = self.model(dummy_input)
            
        # ì¶œë ¥ ì•ˆì •ì„± ê²€ì‚¬
        all_stable = True
        for key, value in outputs.items():
            has_nan = torch.isnan(value).any()
            has_inf = torch.isinf(value).any()
            if has_nan or has_inf:
                all_stable = False
                self.logger.warning(f"{key} ì¶œë ¥ì— ë¹„ì •ìƒ ê°’: NaN={has_nan}, Inf={has_inf}")
            else:
                self.logger.debug(f"{key}: ì•ˆì •ì  ({value.min():.3f} ~ {value.max():.3f})")
        
        if all_stable:
            self.logger.info("âœ… ëª¨ë¸ ì•ˆì •ì„± ê²€ì¦ í†µê³¼")
        else:
            self.logger.error("âŒ ëª¨ë¸ ì•ˆì •ì„± ë¬¸ì œ ë°œê²¬")
            
        self.model.train()  # í•™ìŠµ ëª¨ë“œë¡œ ë³µê·€
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = self.logs_dir / f'hybrid_training_{int(time.time())}.log'
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('HybridTrainer')
    
    def prepare_model(self):
        """ëª¨ë¸ ì¤€ë¹„"""
        self.logger.info("ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”ëœ ëª¨ë¸
        self.model = MemoryOptimizedModel(self.config)
        
        # Mixed Precision ì„¤ì •
        if self.config.use_mixed_precision and torch.cuda.is_available():
            self.scaler = torch.cuda.amp.GradScaler()
        else:
            self.scaler = None
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=1e-4,
            weight_decay=0.01
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=1000, eta_min=1e-6
        )
        
        actual_params = self.model.get_parameter_count()
        self.logger.info(f"âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ: {actual_params:,}ê°œ íŒŒë¼ë¯¸í„°")
        
        return actual_params
    
    def prepare_data(self) -> DataLoader:
        """ë°ì´í„° ì¤€ë¹„"""
        self.logger.info("ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        data_dir = project_root / 'processed_datasets'
        batch_files = list(data_dir.glob('full_scenarios_batch_*.json'))
        
        if not batch_files:
            raise FileNotFoundError("ë°°ì¹˜ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # Phase 2: ê· ë“± ìƒ˜í”Œë§ ë˜ëŠ” ê¸°ì¡´ ë°ì´í„°ì…‹
        if self.config.enable_balanced_sampling:
            self.logger.info("ğŸ¯ ê· ë“± ìƒ˜í”Œë§ ë°ì´í„°ì…‹ ì‚¬ìš©")
            dataset = BalancedDataset(self.config)
        else:
            self.logger.info("ğŸ“‹ ê¸°ì¡´ ë°ì´í„°ì…‹ ì‚¬ìš©")
            dataset = OptimizedDataset(batch_files, self.config)
        
        # ë°ì´í„°ë¡œë” (ìµœì í™”ëœ ì„¤ì •)
        dataloader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=torch.cuda.is_available(),
            persistent_workers=True
        )
        
        self.logger.info(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(dataset)}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
        
        # ë°ì´í„° ë¶„í¬ ê²€ì¦ (ê· ë“± ìƒ˜í”Œë§ì¸ ê²½ìš°)
        if isinstance(dataset, BalancedDataset) and len(dataset) > 0:
            sample_batch = [dataset[i] for i in range(min(10, len(dataset)))]
            sources = [item['source'] for item in sample_batch]
            distribution = dataset.get_batch_distribution(sources)
            self.logger.info(f"ğŸ“ˆ ìƒ˜í”Œ ë¶„í¬: {distribution}")
        
        return dataloader
    
    def vad_feedback_loop(self, emotion_predictions: torch.Tensor, step_idx: int) -> Dict[str, torch.Tensor]:
        """VAD ë²¡í„° ì‹¤ì‹œê°„ í”¼ë“œë°± ë£¨í”„ (Phase 3 - ì‹¬ì¸µ ì—°ë™ ê°•í™”)"""
        
        # VAD ë²¡í„° ì¶”ì¶œ (Valence-Arousal-Dominance) - ì°¨ì› ì•ˆì „ì„± ë³´ì¥
        # emotion_predictions: [batch_size, 6] -> [valence, arousal, dominance, certainty, surprise, anticipation]
        if emotion_predictions.shape[1] >= 3:
            vad_vector = emotion_predictions[:, :3]  # [batch_size, 3] - VAD ì°¨ì›ë§Œ ì¶”ì¶œ
        else:
            # ì°¨ì›ì´ ë¶€ì¡±í•œ ê²½ìš° ì œë¡œ íŒ¨ë”©
            batch_size = emotion_predictions.shape[0]
            vad_vector = torch.zeros(batch_size, 3, device=emotion_predictions.device)
            min_dim = min(emotion_predictions.shape[1], 3)
            vad_vector[:, :min_dim] = emotion_predictions[:, :min_dim]
            print(f"emotion_predictions ì°¨ì› ë¶€ì¡±: {emotion_predictions.shape}, VADëŠ” {vad_vector.shape}ë¡œ íŒ¨ë”©")
        
        # 1. VAD ê¸°ë°˜ ìœ¤ë¦¬ íŒë‹¨ ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •
        ethics_weights = self._calculate_ethics_weights_from_vad(vad_vector)
        
        # 2. ì‹¤ì‹œê°„ ê°ì •-ìœ¤ë¦¬ í”¼ë“œë°± ë§¤í•‘
        ethics_feedback = self._apply_vad_to_ethics_mapping(vad_vector, ethics_weights)
        
        # 3. ìœ¤ë¦¬ íŒë‹¨ ê²°ê³¼ê°€ ê°ì • ì‹œìŠ¤í…œì— ì—­í”¼ë“œë°±
        emotion_adjustment = self._ethics_to_emotion_feedback(ethics_feedback, vad_vector)
        
        # 4. í›„íšŒ ê¸°ë°˜ VAD ì—…ë°ì´íŠ¸ (í•™ìŠµì  ì‘ìš©)
        regret_adjusted_vad = self._regret_based_vad_adjustment(vad_vector, step_idx)
        
        # 5. ìƒˆë¡œìš´ ê¸°ëŠ¥: regret_scoreë¥¼ VAD ì¡°ì • ì¸ìë¡œ ëª…ì‹œì  ë„ì…
        regret_score_adjustment = self._calculate_regret_score_vad_adjustment(vad_vector, step_idx)
        
        # 6. í†µí•©ëœ VAD ê³„ì‚° (ëª¨ë“  ì¡°ì • ìš”ì†Œ ë°˜ì˜)
        integrated_vad = regret_adjusted_vad + emotion_adjustment * 0.1 + regret_score_adjustment
        
        return {
            'original_vad': vad_vector,
            'ethics_weights': ethics_weights,
            'ethics_feedback': ethics_feedback,
            'emotion_adjustment': emotion_adjustment,
            'regret_adjusted_vad': regret_adjusted_vad,
            'regret_score_adjustment': regret_score_adjustment,  # ìƒˆë¡œìš´ ì¡°ì • ìš”ì†Œ
            'integrated_vad': integrated_vad
        }
    
    def _calculate_ethics_weights_from_vad(self, vad_vector: torch.Tensor) -> torch.Tensor:
        """VAD ë²¡í„°ë¡œë¶€í„° ìœ¤ë¦¬ íŒë‹¨ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        # Valence-Arousal-Dominance â†’ ìœ¤ë¦¬ì  ì„ íƒì§€ ê°€ì¤‘ì¹˜ ë³€í™˜
        valence = vad_vector[:, 0]    # ê°ì • ê·¹ì„± (-1~1)
        arousal = vad_vector[:, 1]    # ê°ì„±ë„ (0~1) 
        dominance = vad_vector[:, 2]  # ì§€ë°°ê° (0~1)
        
        # ìœ¤ë¦¬ì  ì„ íƒì— ëŒ€í•œ ê°ì • ê¸°ë°˜ ê°€ì¤‘ì¹˜
        # ë†’ì€ valence + ë‚®ì€ arousal = ì°¨ë¶„í•œ íŒë‹¨ (ê·œì¹™ ê¸°ë°˜ ê°•í™”)
        rule_based_weight = torch.sigmoid(valence - arousal)
        
        # ë†’ì€ arousal + ë†’ì€ dominance = ì§ê´€ì  íŒë‹¨ (ê²°ê³¼ ê¸°ë°˜ ê°•í™”)  
        consequence_weight = torch.sigmoid(arousal + dominance - 1.0)
        
        # ê· í˜•ì¡íŒ ìƒíƒœ = ë•ìœ¤ë¦¬ ê¸°ë°˜
        virtue_weight = 1.0 - torch.abs(valence) * torch.abs(arousal - 0.5)
        
        return torch.stack([rule_based_weight, consequence_weight, virtue_weight], dim=1)
    
    def _apply_vad_to_ethics_mapping(self, vad_vector: torch.Tensor, ethics_weights: torch.Tensor) -> torch.Tensor:
        """VAD â†’ ìœ¤ë¦¬ íŒë‹¨ ì‹¤ì‹œê°„ ë§¤í•‘ (í¼ì§€ ê°ì • ë§¤í•‘)"""
        batch_size = vad_vector.shape[0]
        
        # ê°ì • ìƒíƒœì— ë”°ë¥¸ ìœ¤ë¦¬ íŒë‹¨ ì¡°ì •
        valence = vad_vector[:, 0]
        arousal = vad_vector[:, 1] 
        dominance = vad_vector[:, 2]
        
        # í¼ì§€ ë¡œì§ ê¸°ë°˜ ì—°ì†ì  ë§¤í•‘
        # ìŠ¬í”” ì£¼ë„ ìƒíƒœ (valence < 0, arousal < 0.5) â†’ ìœ„ë¡œ/ë°°ë ¤ ìš°ì„ 
        sadness_driven = torch.relu(-valence) * torch.relu(0.5 - arousal)
        
        # ë¶„ë…¸ ì£¼ë„ ìƒíƒœ (valence < 0, arousal > 0.5) â†’ ì •ì˜/ê³µì •ì„± ìš°ì„   
        anger_driven = torch.relu(-valence) * torch.relu(arousal - 0.5)
        
        # ê¸°ì¨ ì£¼ë„ ìƒíƒœ (valence > 0, arousal > 0.5) â†’ ê³µë™ì²´/í˜‘ë ¥ ìš°ì„ 
        joy_driven = torch.relu(valence) * torch.relu(arousal - 0.5)
        
        # í‰ì˜¨ ì£¼ë„ ìƒíƒœ (valence > 0, arousal < 0.5) â†’ ì´ì„±/ìˆ™ê³  ìš°ì„ 
        calm_driven = torch.relu(valence) * torch.relu(0.5 - arousal)
        
        # í†µí•©ëœ ìœ¤ë¦¬ ì¡°ì • ë²¡í„°
        ethics_adjustment = torch.stack([
            sadness_driven,   # ë°°ë ¤ ì°¨ì›
            anger_driven,     # ì •ì˜ ì°¨ì›  
            joy_driven,       # í˜‘ë ¥ ì°¨ì›
            calm_driven       # ì´ì„± ì°¨ì›
        ], dim=1)
        
        return ethics_adjustment * ethics_weights[:, :1]  # ì²« ë²ˆì§¸ ê°€ì¤‘ì¹˜ë¡œ ìŠ¤ì¼€ì¼ë§
    
    def _ethics_to_emotion_feedback(self, ethics_feedback: torch.Tensor, current_vad: torch.Tensor) -> torch.Tensor:
        """ìœ¤ë¦¬ íŒë‹¨ ê²°ê³¼ â†’ ê°ì • ì‹œìŠ¤í…œ ì—­í”¼ë“œë°±"""
        # ìœ¤ë¦¬ì  ê²°ì •ì´ ê°ì • ìƒíƒœì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ëª¨ë¸ë§
        
        # ì¥ì¹˜ ì¼ì¹˜ ë³´ì¥
        device = ethics_feedback.device
        
        # ë°°ë ¤ì  ê²°ì • â†’ ë”°ëœ»í•¨/ë§Œì¡±ê° ì¦ê°€
        care_influence = ethics_feedback[:, 0:1] * torch.tensor([0.3, -0.1, 0.1], device=device)  # valence+, arousal-, dominance+
        
        # ì •ì˜ë¡œìš´ ê²°ì • â†’ ì˜ê¸°ì–‘ì–‘í•¨ ì¦ê°€
        justice_influence = ethics_feedback[:, 1:2] * torch.tensor([0.2, 0.2, 0.3], device=device)  # valence+, arousal+, dominance+
        
        # í˜‘ë ¥ì  ê²°ì • â†’ ê¸°ì¨/í™œë ¥ ì¦ê°€  
        coop_influence = ethics_feedback[:, 2:3] * torch.tensor([0.4, 0.3, 0.0], device=device)   # valence+, arousal+, dominance=
        
        # ì´ì„±ì  ê²°ì • â†’ í‰ì˜¨/í™•ì‹  ì¦ê°€
        rational_influence = ethics_feedback[:, 3:4] * torch.tensor([0.1, -0.2, 0.2], device=device)  # valence+, arousal-, dominance+
        
        # í†µí•©ëœ ê°ì • ì¡°ì •
        total_influence = care_influence + justice_influence + coop_influence + rational_influence
        
        return total_influence
    
    def _regret_based_vad_adjustment(self, vad_vector: torch.Tensor, step_idx: int) -> torch.Tensor:
        """í›„íšŒ â†’ VAD ì¡°ì • (í•™ìŠµì  ì‘ìš©, ë°˜ì„± ê¸°ë°˜ ìœ¤ë¦¬ í•™ìŠµ)"""
        
        # ê³¼ê±° í›„íšŒ íŒ¨í„´ ê¸°ë°˜ VAD ì¡°ì • (ë‹¨ìˆœí™”ëœ ë²„ì „)
        # ì‹¤ì œë¡œëŠ” Experience DBì—ì„œ ìœ ì‚¬ ìƒí™©ì˜ í›„íšŒ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì•¼ í•¨
        
        # ì£¼ê¸°ì ì¸ í›„íšŒ ë°˜ì„± (ë§¤ 10ìŠ¤í…ë§ˆë‹¤)
        if step_idx % 10 == 0:
            # í›„íšŒê°€ ë†’ì•˜ë˜ ê²½ìš°ì˜ VAD íŒ¨í„´ íšŒí”¼
            # ì˜ˆ: ê³¼ë„í•œ arousal + ë‚®ì€ valence ì¡°í•© íšŒí”¼
            high_arousal_low_valence = (vad_vector[:, 1] > 0.7) & (vad_vector[:, 0] < -0.3)
            
            regret_adjustment = torch.zeros_like(vad_vector)
            regret_adjustment[high_arousal_low_valence, 1] -= 0.1  # arousal ê°ì†Œ
            regret_adjustment[high_arousal_low_valence, 0] += 0.05  # valence ì¦ê°€
            
            return vad_vector + regret_adjustment
        
        return vad_vector  # ì¡°ì • ì—†ìŒ
    
    def regret_based_ethics_adjustment(self, step_idx: int, recent_regret_patterns: List[Dict]) -> Dict[str, torch.Tensor]:
        """Phase 4: í›„íšŒ ê¸°ë°˜ ìœ¤ë¦¬ ê¸°ì¤€ ë™ì  ì¡°ì •"""
        
        # í›„íšŒ íŒ¨í„´ ë¶„ì„ ë° ìœ¤ë¦¬ ê¸°ì¤€ ì¬ì¡°ì •
        ethics_priority_adjustment = {
            'rule_based_priority': torch.tensor(1.0),      # ê·œì¹™ ê¸°ë°˜ ìœ¤ë¦¬ ìš°ì„ ë„
            'consequence_priority': torch.tensor(1.0),     # ê²°ê³¼ ê¸°ë°˜ ìœ¤ë¦¬ ìš°ì„ ë„
            'virtue_priority': torch.tensor(1.0),          # ë•ìœ¤ë¦¬ ìš°ì„ ë„
            'care_priority': torch.tensor(1.0)             # ë°°ë ¤ ìœ¤ë¦¬ ìš°ì„ ë„
        }
        
        # ìµœê·¼ í›„íšŒ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° ë¶„ì„
        if recent_regret_patterns:
            # í›„íšŒ ìœ í˜•ë³„ ë¹ˆë„ ë¶„ì„
            regret_counts = {
                'rule_violation': 0,    # ê·œì¹™ ìœ„ë°˜ìœ¼ë¡œ ì¸í•œ í›„íšŒ
                'bad_outcome': 0,       # ë‚˜ìœ ê²°ê³¼ë¡œ ì¸í•œ í›„íšŒ  
                'character_flaw': 0,    # í’ˆì„± ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ í›„íšŒ
                'lack_of_care': 0       # ë°°ë ¤ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ í›„íšŒ
            }
            
            # ìµœê·¼ 10ê°œ í›„íšŒ íŒ¨í„´ ë¶„ì„
            for pattern in recent_regret_patterns[-10:]:
                regret_type = pattern.get('type', 'unknown')
                if regret_type in regret_counts:
                    regret_counts[regret_type] += 1
            
            # í›„íšŒê°€ ë§ì€ ì˜ì—­ì˜ ìš°ì„ ìˆœìœ„ ê°•í™”
            total_regrets = sum(regret_counts.values())
            if total_regrets > 0:
                # ê·œì¹™ ìœ„ë°˜ í›„íšŒê°€ ë§ìœ¼ë©´ â†’ ê·œì¹™ ê¸°ë°˜ ìœ¤ë¦¬ ê°•í™”
                if regret_counts['rule_violation'] / total_regrets > 0.3:
                    ethics_priority_adjustment['rule_based_priority'] += 0.2
                
                # ë‚˜ìœ ê²°ê³¼ í›„íšŒê°€ ë§ìœ¼ë©´ â†’ ê²°ê³¼ ê¸°ë°˜ ìœ¤ë¦¬ ê°•í™”  
                if regret_counts['bad_outcome'] / total_regrets > 0.3:
                    ethics_priority_adjustment['consequence_priority'] += 0.2
                
                # í’ˆì„± ë¶€ì¡± í›„íšŒê°€ ë§ìœ¼ë©´ â†’ ë•ìœ¤ë¦¬ ê°•í™”
                if regret_counts['character_flaw'] / total_regrets > 0.3:
                    ethics_priority_adjustment['virtue_priority'] += 0.2
                
                # ë°°ë ¤ ë¶€ì¡± í›„íšŒê°€ ë§ìœ¼ë©´ â†’ ë°°ë ¤ ìœ¤ë¦¬ ê°•í™”
                if regret_counts['lack_of_care'] / total_regrets > 0.3:
                    ethics_priority_adjustment['care_priority'] += 0.2
        
        # ì‹œê°„ì— ë”°ë¥¸ ì ì§„ì  ì¡°ì • (í•™ìŠµ íš¨ê³¼)
        time_factor = min(step_idx / 1000.0, 1.0)  # 1000ìŠ¤í…ì— ê±¸ì³ ì ì§„ì  ì ìš©
        
        for key in ethics_priority_adjustment:
            base_value = ethics_priority_adjustment[key]
            adjustment = (base_value - 1.0) * time_factor
            ethics_priority_adjustment[key] = 1.0 + adjustment
        
        return ethics_priority_adjustment
    
    def _calculate_regret_score_vad_adjustment(self, vad_vector: torch.Tensor, step_idx: int) -> torch.Tensor:
        """regret_scoreë¥¼ VAD ê°’ ì¡°ì • ì¸ìë¡œ ëª…ì‹œì  ë„ì… (docs ê°œì„ ì‚¬í•­)"""
        
        # í˜„ì¬ ë‹¨ê³„ì—ì„œì˜ í›„íšŒ ì ìˆ˜ ì¶”ì • (step_idx ê¸°ë°˜)
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” regret_calculatorì—ì„œ ì‹¤ì‹œê°„ ì ìˆ˜ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
        base_regret_score = min(step_idx * 0.01, 1.0)  # ë‹¨ê³„ë³„ ëˆ„ì  í›„íšŒ
        
        batch_size = vad_vector.shape[0]
        device = vad_vector.device
        
        # VAD ê° ì°¨ì›ë³„ë¡œ í›„íšŒ ì ìˆ˜ì˜ ì˜í–¥ ê³„ì‚°
        valence_adjustment = -base_regret_score * 0.2  # í›„íšŒ ì¦ê°€ ì‹œ valence ê°ì†Œ
        arousal_adjustment = base_regret_score * 0.1   # í›„íšŒ ì¦ê°€ ì‹œ ê°ì„±ë„ ì•½ê°„ ì¦ê°€
        dominance_adjustment = -base_regret_score * 0.15  # í›„íšŒ ì¦ê°€ ì‹œ ì§€ë°°ê° ê°ì†Œ
        
        # ë°°ì¹˜ í¬ê¸°ì— ë§ì¶° ì¡°ì • ë²¡í„° ìƒì„±
        regret_adjustment = torch.tensor([
            [valence_adjustment, arousal_adjustment, dominance_adjustment]
        ], device=device).expand(batch_size, -1)
        
        # í˜„ì¬ VAD ìƒíƒœì— ë”°ë¥¸ ì ì‘ì  ì¡°ì •
        current_valence = vad_vector[:, 0]
        
        # ì´ë¯¸ ë¶€ì •ì ì¸ ê°ì • ìƒíƒœì—ì„œëŠ” í›„íšŒ ì˜í–¥ì„ ì™„í™”
        valence_mask = (current_valence < -0.5).float().unsqueeze(1)
        regret_adjustment = regret_adjustment * (1.0 - valence_mask * 0.5)
        
        return regret_adjustment
    
    def _calculate_individual_community_balance(self, vad_vector: torch.Tensor) -> torch.Tensor:
        """ê°œì¸-ê³µë™ì²´ ê· í˜• ê³„ìˆ˜ ê³„ì‚° (docs ê°œì„ ì‚¬í•­: ì² í•™ì  ê¸°ì¤€ ìˆ˜ì¹˜í™”)"""
        
        valence = vad_vector[:, 0]    # ê°ì • ê·¹ì„±
        arousal = vad_vector[:, 1]    # ê°ì„±ë„ 
        dominance = vad_vector[:, 2]  # ì§€ë°°ê°
        
        # ê°œì¸ ì¤‘ì‹¬ ì„±í–¥ ê³„ì‚°
        # ë†’ì€ ì§€ë°°ê° + ë‚®ì€ ê°ì„±ë„ = ê°œì¸ ì¤‘ì‹¬ì  ì‚¬ê³ 
        individual_tendency = dominance * (1.0 - arousal)
        
        # ê³µë™ì²´ ì¤‘ì‹¬ ì„±í–¥ ê³„ì‚°  
        # ê¸ì •ì  ê°ì • + ë†’ì€ ê°ì„±ë„ = ê³µë™ì²´ ì§€í–¥ì  ì‚¬ê³ 
        community_tendency = torch.clamp(valence, 0, 1) * arousal
        
        # ê· í˜• ê³„ìˆ˜ (0.5: ì™„ì „ ê°œì¸ ì¤‘ì‹¬, 1.5: ì™„ì „ ê³µë™ì²´ ì¤‘ì‹¬)
        balance_coefficient = 0.5 + community_tendency - individual_tendency * 0.5
        balance_coefficient = torch.clamp(balance_coefficient, 0.3, 1.7)
        
        return balance_coefficient
    
    def post_decision_emotional_response(self, ethics_decision: Dict[str, torch.Tensor], 
                                       original_vad: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Phase 5: ì‚¬í›„ ìœ¤ë¦¬ íŒë‹¨ì— ë”°ë¥¸ ê°ì • ë³€í™”"""
        
        # ìœ¤ë¦¬ì  ê²°ì •ì— ë”°ë¥¸ ê°ì •ì  ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
        decision_type = self._classify_ethics_decision(ethics_decision)
        
        # ì¥ì¹˜ ì¼ì¹˜ ë³´ì¥
        device = original_vad.device
        
        # ê°ì • ë³€í™” íŒ¨í„´
        emotional_consequences = {}
        
        if decision_type == 'altruistic':
            # ì´íƒ€ì  ê²°ì • â†’ ë§Œì¡±ê°, ìë¶€ì‹¬ ì¦ê°€
            emotional_consequences = {
                'valence_change': torch.tensor(0.3, device=device),     # ê¸ì •ì  ê°ì • ì¦ê°€
                'arousal_change': torch.tensor(-0.1, device=device),    # í‰ì˜¨í•¨ ì¦ê°€
                'dominance_change': torch.tensor(0.2, device=device),   # ìê¸°íš¨ëŠ¥ê° ì¦ê°€
                'guilt_level': torch.tensor(0.0, device=device),        # ì£„ì±…ê° ì—†ìŒ
                'pride_level': torch.tensor(0.4, device=device),        # ìë¶€ì‹¬ ë†’ìŒ
                'regret_probability': torch.tensor(0.1, device=device)  # í›„íšŒ ê°€ëŠ¥ì„± ë‚®ìŒ
            }
        elif decision_type == 'selfish':
            # ì´ê¸°ì  ê²°ì • â†’ ì¼ì‹œì  ë§Œì¡±, ì¥ê¸°ì  ì£„ì±…ê°
            emotional_consequences = {
                'valence_change': torch.tensor(0.1, device=device),     # ì¼ì‹œì  ê¸ì •ê°
                'arousal_change': torch.tensor(0.2, device=device),     # ë¶ˆì•ˆ ì¦ê°€
                'dominance_change': torch.tensor(-0.1, device=device),  # ìê¸° ì˜ì‹¬
                'guilt_level': torch.tensor(0.3, device=device),        # ì£„ì±…ê° ë°œìƒ
                'pride_level': torch.tensor(0.0, device=device),        # ìë¶€ì‹¬ ì—†ìŒ
                'regret_probability': torch.tensor(0.6, device=device)  # í›„íšŒ ê°€ëŠ¥ì„± ë†’ìŒ
            }
        elif decision_type == 'harmful':
            # íƒ€ì¸ì—ê²Œ í•´ë¥¼ ë¼ì¹˜ëŠ” ê²°ì • â†’ ì£„ì±…ê°, í›„íšŒ
            emotional_consequences = {
                'valence_change': torch.tensor(-0.4, device=device),    # ë¶€ì •ì  ê°ì • ê°•í•¨
                'arousal_change': torch.tensor(0.3, device=device),     # ìŠ¤íŠ¸ë ˆìŠ¤ ì¦ê°€
                'dominance_change': torch.tensor(-0.3, device=device),  # ìê¸° í˜ì˜¤
                'guilt_level': torch.tensor(0.6, device=device),        # ë†’ì€ ì£„ì±…ê°
                'pride_level': torch.tensor(0.0, device=device),        # ìë¶€ì‹¬ ì—†ìŒ
                'regret_probability': torch.tensor(0.8, device=device)  # ë§¤ìš° ë†’ì€ í›„íšŒ ê°€ëŠ¥ì„±
            }
        else:
            # ì¤‘ì„±ì  ê²°ì • â†’ ìµœì†Œí•œì˜ ê°ì • ë³€í™”
            emotional_consequences = {
                'valence_change': torch.tensor(0.0, device=device),
                'arousal_change': torch.tensor(0.0, device=device),
                'dominance_change': torch.tensor(0.0, device=device),
                'guilt_level': torch.tensor(0.1, device=device),
                'pride_level': torch.tensor(0.1, device=device),
                'regret_probability': torch.tensor(0.2, device=device)
            }
        
        # ìƒˆë¡œìš´ VAD ìƒíƒœ ê³„ì‚° (ì°¨ì› ì•ˆì „ì„± ë³´ì¥)
        new_vad = original_vad.clone()
        
        # original_vadê°€ ìµœì†Œ 3ì°¨ì›ì¸ì§€ í™•ì¸
        if original_vad.shape[1] >= 3:
            # ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ìœ„í•´ ì°¨ì› í™•ì¥
            batch_size = original_vad.shape[0]
            valence_change = emotional_consequences['valence_change'].expand(batch_size)
            arousal_change = emotional_consequences['arousal_change'].expand(batch_size)
            dominance_change = emotional_consequences['dominance_change'].expand(batch_size)
            
            new_vad[:, 0] += valence_change     # Valence
            new_vad[:, 1] += arousal_change     # Arousal  
            new_vad[:, 2] += dominance_change   # Dominance
        else:
            print(f"original_vad ì°¨ì› ë¶€ì¡±: {original_vad.shape}, 3ì°¨ì›ìœ¼ë¡œ í™•ì¥")
            # 3ì°¨ì›ìœ¼ë¡œ í™•ì¥
            batch_size = original_vad.shape[0]
            new_vad = torch.zeros(batch_size, 3, device=original_vad.device)
            # ê¸°ì¡´ ì°¨ì›ë§Œí¼ ë³µì‚¬
            min_dim = min(original_vad.shape[1], 3)
            new_vad[:, :min_dim] = original_vad[:, :min_dim]
            
            # ê°ì • ë³€í™” ì ìš©
            valence_change = emotional_consequences['valence_change'].expand(batch_size)
            arousal_change = emotional_consequences['arousal_change'].expand(batch_size)
            dominance_change = emotional_consequences['dominance_change'].expand(batch_size)
            
            new_vad[:, 0] += valence_change
            new_vad[:, 1] += arousal_change
            new_vad[:, 2] += dominance_change
        
        # VAD ë²”ìœ„ ì œí•œ (-1 ~ 1)
        new_vad = torch.clamp(new_vad, -1.0, 1.0)
        
        return {
            'new_vad': new_vad,
            'emotional_consequences': emotional_consequences,
            'decision_type': decision_type
        }
    
    def _classify_ethics_decision(self, ethics_decision: Dict[str, torch.Tensor]) -> str:
        """ìœ¤ë¦¬ì  ê²°ì • ë¶„ë¥˜"""
        # ë‹¨ìˆœí™”ëœ ë¶„ë¥˜ ë¡œì§
        # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ íŒ¨í„´ ì¸ì‹ì´ í•„ìš”
        
        rule_priority = ethics_decision.get('rule_based_priority', torch.tensor(1.0))
        care_priority = ethics_decision.get('care_priority', torch.tensor(1.0))
        
        if care_priority > 1.2:
            return 'altruistic'
        elif rule_priority < 0.8:
            return 'selfish'
        elif care_priority < 0.5:
            return 'harmful'
        else:
            return 'neutral'

    def train_step(self, batch: Dict[str, torch.Tensor], step_idx: int) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ìµœì í™”ëœ í•™ìŠµ ìŠ¤í… (VAD í”¼ë“œë°± ë£¨í”„ í†µí•©)"""
        self.model.train()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if step_idx % 5 == 0:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        text_embeddings = batch['text_embedding'].to(self.device)
        labels = batch['labels'].to(self.device)
        
        # ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
        current_batch_size = text_embeddings.size(0)
        if current_batch_size > self.config.max_safe_batch_size:
            return self._process_large_batch(batch, step_idx)
        
        # ë¹„ë™ê¸° í›„íšŒ ê³„ì‚° ì‹œì‘ (ìµœì í™”)
        for i, embedding in enumerate(text_embeddings[:min(len(text_embeddings), 6)]):
            self.regret_calculator.calculate_async(embedding, step_idx * self.config.batch_size + i)
        
        # ëª¨ë¸ ìˆœì „íŒŒ (Mixed Precision) - ì•ˆì „ì„± ê°•í™”
        try:
            if self.scaler:
                with torch.cuda.amp.autocast():
                    outputs = self.model(text_embeddings.unsqueeze(1))
            else:
                outputs = self.model(text_embeddings.unsqueeze(1))
            
            # ëª¨ë¸ ì¶œë ¥ ì „ì²´ì— ëŒ€í•œ NaN/Inf ê²€ì‚¬
            has_nan_output = False
            for key, value in outputs.items():
                if torch.isnan(value).any() or torch.isinf(value).any():
                    print(f"ëª¨ë¸ ì¶œë ¥ {key}ì—ì„œ NaN/Inf ë°œê²¬: {value.shape}")
                    has_nan_output = True
            
            if has_nan_output:
                # ëª¨ë¸ ì¶œë ¥ ìì²´ì— ë¬¸ì œê°€ ìˆìœ¼ë©´ ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
                print("âš ï¸ ëª¨ë¸ ì¶œë ¥ì— NaN/Inf ë°œê²¬ - í•™ìŠµ ìŠ¤í‚µ")
                return {
                    'loss': 0.0,
                    'classification_loss': 0.0,
                    'regret_count': 0,
                    'bentham_count': 0
                }
                
        except Exception as e:
            print(f"ëª¨ë¸ ìˆœì „íŒŒ ì˜¤ë¥˜: {e}")
            return {
                'loss': 0.0,
                'classification_loss': 0.0,
                'regret_count': 0,
                'bentham_count': 0
            }
        
        # ê¸°ë³¸ ë¶„ë¥˜ ì†ì‹¤ (NaN ë°©ì§€ ì•ˆì •ì„± ê°œì„ )
        emotion_predictions = outputs['emotion_predictions']
        
        # Mixed Precision í˜¸í™˜ì„ ìœ„í•´ float32ë¡œ ë³€í™˜
        if emotion_predictions.dtype == torch.float16:
            emotion_predictions = emotion_predictions.float()
        
        # ìµœì¢… NaN ê²€ì‚¬
        if torch.isnan(emotion_predictions).any() or torch.isinf(emotion_predictions).any():
            print("ê°ì • ì˜ˆì¸¡ì— NaN/Inf ë°œê²¬, ì œë¡œë¡œ ì´ˆê¸°í™”")
            emotion_predictions = torch.zeros_like(emotion_predictions, requires_grad=True)
        
        # Phase 3: VAD ë²¡í„° ì‹¤ì‹œê°„ í”¼ë“œë°± ë£¨í”„ (ì‹¬ì¸µ ì—°ë™ ê°•í™”)
        vad_feedback_results = self.vad_feedback_loop(emotion_predictions, step_idx)
        
        # Phase 4: í›„íšŒ ê¸°ë°˜ ìœ¤ë¦¬ ê¸°ì¤€ ë™ì  ì¡°ì • (ëª¨ì˜ í›„íšŒ íŒ¨í„´ ì‚¬ìš©)
        recent_regret_patterns = [
            {'type': 'rule_violation', 'step': step_idx-1},
            {'type': 'lack_of_care', 'step': step_idx-2}
        ] if step_idx > 5 else []
        
        ethics_adjustment = self.regret_based_ethics_adjustment(step_idx, recent_regret_patterns)
        
        # Phase 5: ì‚¬í›„ ìœ¤ë¦¬ íŒë‹¨ì— ë”°ë¥¸ ê°ì • ë³€í™”
        post_decision_response = self.post_decision_emotional_response(
            ethics_adjustment, vad_feedback_results['original_vad']
        )
        
        # í†µí•©ëœ VADë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ì˜ˆì¸¡ ì—…ë°ì´íŠ¸ (Phase 5 ê²°ê³¼ ë°˜ì˜)
        integrated_vad = vad_feedback_results['integrated_vad']
        final_vad = post_decision_response['new_vad']  # ì‚¬í›„ ê°ì • ë³€í™” ë°˜ì˜
        
        # NaN ê²€ì‚¬ ë° ì•ˆì „í•œ ì²˜ë¦¬
        if torch.isnan(final_vad).any():
            print("final_vadì— NaN ë°œê²¬, ì œë¡œë¡œ ì´ˆê¸°í™”")
            final_vad = torch.zeros_like(final_vad)
        
        # ì°¨ì› ì•ˆì „ì„± í™•ë³´
        enhanced_emotion_predictions = emotion_predictions.clone()
        if enhanced_emotion_predictions.shape[1] >= 3 and final_vad.shape[1] >= 3:
            # final_vadê°€ 3ì°¨ì› ì´ìƒì¸ ê²½ìš°ë§Œ ì—…ë°ì´íŠ¸
            enhanced_emotion_predictions[:, :3] = final_vad[:, :3]
        elif final_vad.shape[1] == 3:
            # final_vadê°€ ì •í™•íˆ 3ì°¨ì›ì¸ ê²½ìš°
            enhanced_emotion_predictions[:, :3] = final_vad
        else:
            # ì°¨ì›ì´ ì•ˆ ë§ëŠ” ê²½ìš° ì•ˆì „í•œ ì²˜ë¦¬
            print(f"ì°¨ì› ë¶ˆì¼ì¹˜: enhanced_emotion_predictions {enhanced_emotion_predictions.shape}, final_vad {final_vad.shape}")
            # ìµœì†Œ ì°¨ì›ë§Œí¼ ë³µì‚¬
            min_dim = min(enhanced_emotion_predictions.shape[1], final_vad.shape[1], 3)
            enhanced_emotion_predictions[:, :min_dim] = final_vad[:, :min_dim]
        
        # ì•ˆì „í•œ í‰ê·  ê³„ì‚° (ì°¨ì› í˜¸í™˜ì„± ë³´ì¥)
        if enhanced_emotion_predictions.shape[1] >= 3:
            emotion_avg = enhanced_emotion_predictions[:, :3]  # ì²˜ìŒ 3ì°¨ì›ë§Œ ì‚¬ìš©
        else:
            # 3ì°¨ì›ë³´ë‹¤ ì‘ì€ ê²½ìš° ì œë¡œ íŒ¨ë”©
            batch_size = enhanced_emotion_predictions.shape[0]
            emotion_avg = torch.zeros(batch_size, 3, device=enhanced_emotion_predictions.device)
            min_dim = enhanced_emotion_predictions.shape[1]
            emotion_avg[:, :min_dim] = enhanced_emotion_predictions
        
        # labels ì°¨ì›ë„ í™•ì¸
        if labels.shape[1] != 3:
            batch_size = labels.shape[0]
            labels_resized = torch.zeros(batch_size, 3, device=labels.device)
            min_dim = min(labels.shape[1], 3)
            labels_resized[:, :min_dim] = labels[:, :min_dim]
            labels = labels_resized
        
        # NaN ê²€ì‚¬ í›„ ì†ì‹¤ ê³„ì‚°
        if torch.isnan(emotion_avg).any() or torch.isnan(labels).any():
            print("emotion_avg ë˜ëŠ” labelsì— NaN ë°œê²¬")
            classification_loss = torch.tensor(0.1, device=emotion_predictions.device, requires_grad=True)
        else:
            classification_loss = F.mse_loss(emotion_avg, labels)
            # ì†ì‹¤ì´ ë„ˆë¬´ í° ê²½ìš° í´ë¦¬í•‘
            classification_loss = torch.clamp(classification_loss, max=10.0)
        
        # ì†ì‹¤ NaN ê°ì§€ ë° ì²˜ë¦¬
        if torch.isnan(classification_loss):
            self.logger.error("ë¶„ë¥˜ ì†ì‹¤ì´ NaNì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •")
            classification_loss = torch.tensor(1.0, device=emotion_predictions.device, requires_grad=True)
        
        total_loss = classification_loss
        regret_count = 0
        bentham_count = 0
        
        # í›„íšŒ ê²°ê³¼ ìˆ˜ì§‘ ë° ì•ˆì „í•œ ì†ì‹¤ ê³„ì‚°
        for _ in range(7):  # ì›ë˜ 7ê°œ ìœ ì§€
            result = self.regret_calculator.get_result()
            if result is None:
                break
            
            task_id, regret_scenarios = result
            regret_count += len(regret_scenarios)
            bentham_count += len(regret_scenarios) * self.config.bentham_calculations_per_regret
            
            # í›„íšŒ ì†ì‹¤ ì¶”ê°€ (ì•ˆì „í•œ ê³„ì‚° ë°©ì‹)
            for scenario in regret_scenarios[:2]:  # ìµœëŒ€ 2ê°œë¡œ ì œí•œí•˜ì—¬ ì•ˆì •ì„± í™•ë³´
                try:
                    # ê¸°ë³¸ê°’ì„ ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
                    bentham_raw = scenario.get('bentham_scores', 0.0)
                    weight_raw = scenario.get('regret_weight', 0.5)  # ê¸°ë³¸ ê°€ì¤‘ì¹˜ë¥¼ 0.5ë¡œ ë‚®ì¶¤
                    
                    # ê°’ ê²€ì¦ ë° ì •ì œ
                    if not isinstance(bentham_raw, (int, float)) or not (-100 <= bentham_raw <= 100):
                        bentham_raw = 0.0
                    if not isinstance(weight_raw, (int, float)) or not (0 <= weight_raw <= 2):
                        weight_raw = 0.5
                    
                    # ë§¤ìš° ì‘ì€ í›„íšŒ ì†ì‹¤ ê³„ì‚° (NaN ìœ„í—˜ ìµœì†Œí™”)
                    safe_regret_loss = torch.tensor(
                        float(bentham_raw) * 0.01 * float(weight_raw),  # ê³„ìˆ˜ë¥¼ 0.01ë¡œ ëŒ€í­ ì¶•ì†Œ
                        dtype=torch.float32, 
                        device=total_loss.device, 
                        requires_grad=True
                    )
                    
                    # ì—„ê²©í•œ ë²”ìœ„ ì œí•œ
                    safe_regret_loss = torch.clamp(safe_regret_loss, min=-0.1, max=0.1)
                    
                    # ìµœì¢… ì•ˆì „ì„± ê²€ì‚¬
                    if not (torch.isfinite(safe_regret_loss) and not torch.isnan(safe_regret_loss)):
                        continue
                    
                    total_loss = total_loss + safe_regret_loss
                    
                except Exception as e:
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¡°ìš©íˆ ê±´ë„ˆë›°ê¸°
                    continue
        
        # ìµœì¢… NaN ê²€ì‚¬ ë° ì—­ì „íŒŒ ì „ ì•ˆì „ì„± í™•ì¸
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            self.logger.error(f"ìµœì¢… ì†ì‹¤ì´ ë¹„ì •ìƒì…ë‹ˆë‹¤: {total_loss}. ì—­ì „íŒŒ ìŠ¤í‚¨")
            return {
                'loss': float('nan'),
                'classification_loss': classification_loss.item() if not torch.isnan(classification_loss) else float('nan'),
                'regret_count': regret_count,
                'bentham_count': bentham_count
            }
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ìœ„í•œ ì†ì‹¤ ì •ê·œí™”
        total_loss = total_loss / self.config.gradient_accumulation_steps
        
        # ì—­ì „íŒŒ (ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì§€ì›)
        if self.scaler:
            # Mixed Precision ì—­ì „íŒŒ
            self.scaler.scale(total_loss).backward()
        else:
            # FP32 ëª¨ë“œ ì—­ì „íŒŒ
            total_loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì™„ë£Œ ì‹œì—ë§Œ ì˜µí‹°ë§ˆì´ì € ìŠ¤í… ì‹¤í–‰
        if (step_idx + 1) % self.config.gradient_accumulation_steps == 0:
            if self.scaler:
                # Mixed Precision ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                self.scaler.unscale_(self.optimizer)
                total_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ì•ˆì •ì„± ê²€ì‚¬
                if torch.isnan(total_norm) or torch.isinf(total_norm):
                    self.logger.error(f"ê·¸ë˜ë””ì–¸íŠ¸ normì´ ë¹„ì •ìƒì…ë‹ˆë‹¤: {total_norm}. ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”")
                    self.optimizer.zero_grad()
                    self.scaler.update()
                    return {
                        'loss': total_loss.item() * self.config.gradient_accumulation_steps,
                        'classification_loss': classification_loss.item(),
                        'regret_count': regret_count,
                        'bentham_count': bentham_count
                    }
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
                self.optimizer.zero_grad()
            else:
                # FP32 ëª¨ë“œ ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                total_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
                
                if torch.isnan(total_norm) or torch.isinf(total_norm):
                    self.logger.error(f"ê·¸ë˜ë””ì–¸íŠ¸ normì´ ë¹„ì •ìƒì…ë‹ˆë‹¤: {total_norm}. ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”")
                    self.optimizer.zero_grad()
                    return {
                        'loss': total_loss.item() * self.config.gradient_accumulation_steps,
                        'classification_loss': classification_loss.item(),
                        'regret_count': regret_count,
                        'bentham_count': bentham_count
                    }
                
                self.optimizer.step()
                self.optimizer.zero_grad()
        
        # ì§€ì†ì  ê²€ì¦ ìˆ˜í–‰ (Phase 2 ê°œì„ )
        validation_results = self.continuous_validation(step_idx, batch)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        result = {
            'loss': total_loss.item(),
            'classification_loss': classification_loss.item(),
            'regret_count': regret_count,
            'bentham_count': bentham_count,
            # Phase 3: VAD í”¼ë“œë°± ë£¨í”„ ê²°ê³¼ ì¶”ê°€ 
            'vad_feedback': {
                'original_vad_mean': vad_feedback_results['original_vad'].mean(dim=0).tolist(),
                'integrated_vad_mean': vad_feedback_results['integrated_vad'].mean(dim=0).tolist(),
                'final_vad_mean': final_vad.mean(dim=0).tolist(),
                'ethics_weights_mean': vad_feedback_results['ethics_weights'].mean(dim=0).tolist(),
                'emotion_adjustment_norm': torch.norm(vad_feedback_results['emotion_adjustment']).item()
            },
            # Phase 4: í›„íšŒ ê¸°ë°˜ ìœ¤ë¦¬ ì¡°ì • ê²°ê³¼
            'ethics_adjustment': {
                'rule_based_priority': ethics_adjustment['rule_based_priority'].item(),
                'consequence_priority': ethics_adjustment['consequence_priority'].item(),
                'virtue_priority': ethics_adjustment['virtue_priority'].item(),
                'care_priority': ethics_adjustment['care_priority'].item()
            },
            # Phase 5: ì‚¬í›„ ê°ì • ë³€í™” ê²°ê³¼
            'post_decision': {
                'decision_type': post_decision_response['decision_type'],
                'guilt_level': post_decision_response['emotional_consequences']['guilt_level'].item(),
                'pride_level': post_decision_response['emotional_consequences']['pride_level'].item(),
                'regret_probability': post_decision_response['emotional_consequences']['regret_probability'].item()
            }
        }
        
        # ê²€ì¦ ê²°ê³¼ ì¶”ê°€
        if validation_results:
            result['validation'] = validation_results
            
        return result
    
    def _process_large_batch(self, batch: Dict[str, torch.Tensor], step_idx: int) -> Dict[str, float]:
        """í° ë°°ì¹˜ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬"""
        text_embeddings = batch['text_embedding']
        labels = batch['labels']
        
        chunk_size = self.config.max_safe_batch_size
        total_loss = 0.0
        total_classification_loss = 0.0
        total_regret_count = 0
        total_bentham_count = 0
        
        num_chunks = (len(text_embeddings) + chunk_size - 1) // chunk_size
        
        for i in range(num_chunks):
            start_idx = i * chunk_size
            end_idx = min((i + 1) * chunk_size, len(text_embeddings))
            
            chunk_embeddings = text_embeddings[start_idx:end_idx]
            chunk_labels = labels[start_idx:end_idx]
            
            chunk_batch = {
                'text_embedding': chunk_embeddings,
                'labels': chunk_labels
            }
            
            chunk_stats = self.train_step(chunk_batch, step_idx * num_chunks + i)
            
            total_loss += chunk_stats['loss']
            total_classification_loss += chunk_stats['classification_loss']
            total_regret_count += chunk_stats['regret_count']
            total_bentham_count += chunk_stats['bentham_count']
        
        return {
            'loss': total_loss / num_chunks,
            'classification_loss': total_classification_loss / num_chunks,
            'regret_count': total_regret_count,
            'bentham_count': total_bentham_count
        }
    
    def train_epoch(self, dataloader: DataLoader, epoch: int):
        """ì—í¬í¬ í•™ìŠµ"""
        self.logger.info(f"ğŸ¯ ì—í¬í¬ {epoch+1}/{self.config.epochs} ì‹œì‘")
        
        epoch_stats = defaultdict(list)
        
        for batch_idx, batch in enumerate(dataloader):
            step_stats = self.train_step(batch, batch_idx)
            
            # í†µê³„ ìˆ˜ì§‘
            for key, value in step_stats.items():
                epoch_stats[key].append(value)
                self.training_stats[key].append(value)
            
            self.step_count += 1
            
            # ì£¼ê¸°ì  ë¡œê¹… (ì•ˆì „í•œ í‰ê·  ê³„ì‚°)
            if self.step_count % self.config.log_every_n_steps == 0:
                recent_losses = epoch_stats['loss'][-self.config.log_every_n_steps:]
                recent_regrets = epoch_stats['regret_count'][-self.config.log_every_n_steps:]
                
                avg_loss = np.mean(recent_losses) if recent_losses else 0.0
                avg_regret = np.mean(recent_regrets) if recent_regrets else 0.0
                
                self.logger.info(
                    f"ìŠ¤í… {self.step_count}: ì†ì‹¤={avg_loss:.4f}, "
                    f"í›„íšŒ={avg_regret:.1f}, GPUë©”ëª¨ë¦¬={self._get_gpu_memory():.1f}MB"
                )
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if self.step_count % self.config.save_checkpoint_every == 0:
                self.save_checkpoint(epoch, batch_idx)
        
        # ì—í¬í¬ ì¢…ë£Œ í›„ ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í… (ë‹¨ê³„ë³„ ê°œì„ )
        if self.scheduler is not None:
            old_lr = self.optimizer.param_groups[0]['lr']
            self.scheduler.step()
            new_lr = self.optimizer.param_groups[0]['lr']
            self.logger.info(f"í•™ìŠµë¥  ì—…ë°ì´íŠ¸: {old_lr:.6f} â†’ {new_lr:.6f}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if batch_idx % 5 == 0:
                self._cleanup_memory()
        
        # ì—í¬í¬ ìš”ì•½
        avg_epoch_loss = np.mean(epoch_stats['total_loss'])
        total_regrets = sum(epoch_stats['regret_count'])
        
        self.logger.info(
            f"âœ… ì—í¬í¬ {epoch+1} ì™„ë£Œ: í‰ê·  ì†ì‹¤={avg_epoch_loss:.4f}, "
            f"ì´ í›„íšŒ={total_regrets}"
        )
    
    def _get_gpu_memory(self) -> float:
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ MB"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024 / 1024
        return 0.0
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def save_checkpoint(self, epoch: int, batch_idx: int):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = self.checkpoints_dir / f'hybrid_model_epoch_{epoch}_step_{self.step_count}.pth'
        
        checkpoint = {
            'epoch': epoch,
            'step': self.step_count,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'config': asdict(self.config),
            'training_stats': dict(self.training_stats),
            'timestamp': datetime.now().isoformat()
        }
        
        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        torch.save(checkpoint, checkpoint_path)
        self.logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    def train(self):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
        start_time = time.time()
        
        self.logger.info("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° í•™ìŠµ ì‹œì‘")
        
        # ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ì¤€ë¹„ (ë‹¨ê³„ë³„ ê°œì„  ì ìš©)
        self.setup_model_and_optimizer()
        
        # ë°ì´í„° ì¤€ë¹„
        dataloader = self.prepare_data()
        
        actual_params = self.model.get_parameter_count()
        self.logger.info(f"ì‹¤ì œ íŒŒë¼ë¯¸í„° ìˆ˜: {actual_params:,}ê°œ")
        
        # í•™ìŠµ ì‹¤í–‰
        for epoch in range(self.config.epochs):
            self.train_epoch(dataloader, epoch)
            self.save_checkpoint(epoch, -1)
        
        # ìµœì¢… ì €ì¥
        final_checkpoint = self.checkpoints_dir / 'final_hybrid_model.pth'
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'config': asdict(self.config),
            'training_stats': dict(self.training_stats),
            'total_parameters': actual_params,
            'timestamp': datetime.now().isoformat()
        }, final_checkpoint)
        
        training_time = time.time() - start_time
        
        # í•™ìŠµ ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'training_summary': {
                'total_steps': len(self.training_stats['total_loss']),
                'total_regrets': sum(self.training_stats['regret_count']),
                'total_bentham_calculations': sum(self.training_stats['bentham_count']),
                'final_loss': self.training_stats['total_loss'][-1] if self.training_stats['total_loss'] else 0,
                'training_duration': training_time,
                'average_regrets_per_step': sum(self.training_stats['regret_count']) / len(self.training_stats['regret_count']) if self.training_stats['regret_count'] else 0,
                'average_benthams_per_step': sum(self.training_stats['bentham_count']) / len(self.training_stats['bentham_count']) if self.training_stats['bentham_count'] else 0
            },
            'model_info': {
                'main_model_parameters': actual_params,
                'target_parameters': self.config.target_params,
                'device': str(self.device),
                'hybrid_mode': True,
                'gpu_available': torch.cuda.is_available()
            },
            'configuration': asdict(self.config),
            'training_stats': dict(self.training_stats),
            'storage_usage': {
                'final_size_gb': 0,  # ê³„ì‚° ìƒëµ
                'max_allowed_gb': self.config.max_storage_gb
            },
            'xai_integration': {
                'xai_logs_generated': len(xai_logger.logs) if hasattr(xai_logger, 'logs') else 0,
                'session_id': getattr(xai_logger, 'session_id', 'unknown')
            }
        }
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = self.reports_dir / f'hybrid_training_report_{int(time.time())}.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ ì™„ë£Œ! ì´ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
        self.logger.info(f"ğŸ“Š ì´ í›„íšŒ: {sum(self.training_stats['regret_count'])}")
        self.logger.info(f"ğŸ“Š ì´ ë²¤ë‹´ ê³„ì‚°: {sum(self.training_stats['bentham_count'])}")
        self.logger.info(f"ğŸ“‹ ë¦¬í¬íŠ¸: {report_path}")
        
        return report, final_checkpoint
    
    def compare_initialization_methods(self):
        """ì´ˆê¸°í™” ë°©ë²• ë¹„êµ ì‹¤í—˜ (He vs Xavier)"""
        self.logger.info("ğŸ—‹ ì´ˆê¸°í™” ë°©ë²• ë¹„êµ ì‹¤í—˜ ì‹œì‘")
        
        results = {}
        
        for init_method in ['he', 'xavier']:
            self.logger.info(f"\nğŸ¨ {init_method.upper()} ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸")
            
            # ì†Œí˜• ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
            test_config = HybridConfig(
                cpu_memory_gb=2.0,
                gpu_memory_gb=1.0,
                target_params=500_000,
                test_mode=True,
                test_samples=5,
                initialization_method=init_method
            )
            
            test_model = MemoryOptimizedModel(test_config)
            test_model.eval()
            
            # ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
            stability_scores = []
            gradient_norms = []
            
            for i in range(10):  # 10ë²ˆ ë°˜ë³µ í…ŒìŠ¤íŠ¸
                dummy_input = torch.randn(1, 8, 1024)
                dummy_labels = torch.randn(1, 3)
                
                test_model.train()
                with torch.enable_grad():
                    outputs = test_model(dummy_input)
                    loss = F.mse_loss(outputs['emotion_predictions'].mean(dim=1, keepdim=True).expand(-1, 3), dummy_labels)
                    
                    # NaN/Inf ê²€ì‚¬
                    is_stable = not (torch.isnan(loss) or torch.isinf(loss))
                    stability_scores.append(is_stable)
                    
                    if is_stable:
                        loss.backward()
                        total_norm = torch.nn.utils.clip_grad_norm_(test_model.parameters(), 1.0)
                        gradient_norms.append(total_norm.item())
                    
                test_model.zero_grad()
            
            # ê²°ê³¼ ì •ë¦¬
            stability_rate = sum(stability_scores) / len(stability_scores)
            avg_grad_norm = np.mean(gradient_norms) if gradient_norms else float('inf')
            
            results[init_method] = {
                'stability_rate': stability_rate,
                'avg_gradient_norm': avg_grad_norm,
                'gradient_norms': gradient_norms
            }
            
            self.logger.info(f"  ì•ˆì •ì„±: {stability_rate*100:.1f}%")
            self.logger.info(f"  í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ norm: {avg_grad_norm:.4f}")
        
        # ìµœì¢… ë¹„êµ ë° ì¶”ì²œ
        self.logger.info("\nğŸ“ˆ ì´ˆê¸°í™” ë°©ë²• ë¹„êµ ê²°ê³¼:")
        for method, result in results.items():
            self.logger.info(f"{method.upper()}: ì•ˆì •ì„± {result['stability_rate']*100:.1f}%, ê·¸ë˜ë””ì–¸íŠ¸ {result['avg_gradient_norm']:.4f}")
        
        # ìë™ ì¶”ì²œ
        best_method = max(results.keys(), key=lambda k: results[k]['stability_rate'])
        self.logger.info(f"\nâœ… ì¶”ì²œ ì´ˆê¸°í™” ë°©ë²•: {best_method.upper()}")
        
        return results
    
    def continuous_validation(self, step_number: int, batch: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """ì§€ì†ì  ê²€ì¦ ì‹œìŠ¤í…œ (Phase 2 ê°œì„ )"""
        if not self.config.enable_continuous_monitoring:
            return {}
        
        if step_number % self.config.validation_frequency != 0:
            return {}
        
        self.logger.info(f"ğŸ” ì§€ì†ì  ê²€ì¦ ì‹¤í–‰ (Step {step_number})")
        
        validation_results = {
            'step': step_number,
            'timestamp': time.time()
        }
        
        # 1. ë°ì´í„° ë¶„í¬ ê²€ì¦
        try:
            if 'source' in batch:
                sources = batch['source'] if isinstance(batch['source'], list) else [batch['source']]
                from collections import Counter
                distribution = Counter(sources)
                total = len(sources)
                if total > 0:
                    distribution_pct = {k: v/total for k, v in distribution.items()}
                    validation_results['data_distribution'] = distribution_pct
                    
                    # ê· ë“±ì„± ê²€ì‚¬
                    target_weights = self.config.data_folder_weights
                    balance_score = 0
                    for folder, actual_pct in distribution_pct.items():
                        if folder in target_weights:
                            target_pct = target_weights[folder]
                            balance_score += abs(actual_pct - target_pct)
                    
                    validation_results['balance_score'] = 1.0 - (balance_score / 2.0)  # 0~1 ì ìˆ˜
                else:
                    validation_results['balance_score'] = 1.0  # ê¸°ë³¸ê°’
            else:
                validation_results['balance_score'] = 1.0  # source í‚¤ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
        except Exception as e:
            self.logger.warning(f"ë°ì´í„° ë¶„í¬ ê²€ì¦ ì‹¤íŒ¨: {e}")
            validation_results['balance_score'] = 1.0  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’
        
        # 2. ëª¨ë¸ ì•ˆì •ì„± ê²€ì¦
        self.model.eval()
        with torch.no_grad():
            dummy_input = torch.randn(2, 16, 1024, device=self.model.cpu_device)
            try:
                outputs = self.model(dummy_input)
                stability_check = {
                    'model_stable': True,
                    'output_ranges': {}
                }
                
                for key, value in outputs.items():
                    has_nan = torch.isnan(value).any()
                    has_inf = torch.isinf(value).any()
                    if has_nan or has_inf:
                        stability_check['model_stable'] = False
                    
                    stability_check['output_ranges'][key] = {
                        'min': value.min().item(),
                        'max': value.max().item(),
                        'has_nan': has_nan.item(),
                        'has_inf': has_inf.item()
                    }
                
                validation_results['stability'] = stability_check
                
            except Exception as e:
                validation_results['stability'] = {
                    'model_stable': False,
                    'error': str(e)
                }
        
        self.model.train()  # í•™ìŠµ ëª¨ë“œë¡œ ë³µê·€
        
        # 3. í•™ìŠµ ì§„í–‰ ìƒí™© ê²€ì¦
        if hasattr(self, 'training_stats') and self.training_stats:
            recent_losses = self.training_stats['total_loss'][-10:]  # ìµœê·¼ 10ê°œ
            if recent_losses:
                validation_results['recent_loss_trend'] = {
                    'mean': np.mean(recent_losses),
                    'std': np.std(recent_losses),
                    'trend': 'improving' if len(recent_losses) > 5 and recent_losses[-1] < recent_losses[0] else 'stable'
                }
        
        # 4. ê²°ê³¼ ë¡œê¹…
        if validation_results.get('balance_score', 0) < 0.7:
            self.logger.warning(f"âš ï¸  ë°ì´í„° ë¶ˆê· í˜• ê°ì§€: {validation_results['balance_score']:.3f}")
        
        if not validation_results.get('stability', {}).get('model_stable', True):
            self.logger.error("âŒ ëª¨ë¸ ì•ˆì •ì„± ë¬¸ì œ ë°œê²¬")
        
        if validation_results.get('stability', {}).get('model_stable', True) and validation_results.get('balance_score', 0) > 0.7:
            self.logger.info("âœ… ì§€ì†ì  ê²€ì¦ í†µê³¼")
        
        return validation_results

if __name__ == "__main__":
    # í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    config = HybridConfig()
    trainer = HybridDistributedTrainer(config)
    
    print("ğŸ§ª í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° í•™ìŠµ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
    print(f"ğŸ“Š ì„¤ì •: {config.regrets_per_step}íšŒ í›„íšŒ/ìŠ¤í…, {config.epochs}ë²ˆ ì„ íšŒ")
    print(f"ğŸ¤– ëª¨ë¸: {config.target_params:,}ê°œ íŒŒë¼ë¯¸í„°")
    print(f"âš¡ ì›Œì»¤: {config.num_workers}ê°œ")
    print("ì¤€ë¹„ëœ í•™ìŠµì„ ì‹œì‘í•˜ë ¤ë©´ trainer.train()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")