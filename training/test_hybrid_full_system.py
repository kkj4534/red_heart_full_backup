#!/usr/bin/env python3
"""
í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì™„ì „ í…ŒìŠ¤íŠ¸ - Fallback ì—†ëŠ” ì˜¨ì „í•œ ì‹œìŠ¤í…œ ê²€ì¦
Hybrid System Full Test - Complete System Verification without Fallbacks
"""

import os
import sys
import json
import torch
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import traceback

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ëª¨ë“  ëª¨ë“ˆ import
from training.hybrid_distributed_trainer import HybridDistributedTrainer, HybridConfig, MemoryOptimizedModel, AsyncRegretCalculator
from models.mega_scale_models.scalable_xai_model import create_mega_scale_model
from models.hierarchical_emotion.emotion_phase_models import HierarchicalEmotionModel
from models.semantic_models.advanced_semantic_models import AdvancedSemanticModel, SemanticAnalysisConfig
from models.counterfactual_models.counterfactual_reasoning_models import AdvancedCounterfactualModel, CounterfactualConfig
from xai_core.xai_logging_system import xai_logger, xai_trace, xai_decision_point
from llm_module import llm_tracker, register_llm, ask_llm, LLMConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('HybridFullTest')

class FullSystemTester:
    """ì™„ì „í•œ ì‹œìŠ¤í…œ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.test_results = {}
        self.fallback_detected = False
        self.errors_detected = []
        
    def log_test(self, test_name: str, success: bool, details: str = "", fallback_used: bool = False):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹…"""
        status = "âœ… PASS" if success else "âŒ FAIL"
        if fallback_used:
            status += " (âš ï¸ FALLBACK USED)"
            self.fallback_detected = True
        
        logger.info(f"{status} {test_name}: {details}")
        
        self.test_results[test_name] = {
            'success': success,
            'details': details,
            'fallback_used': fallback_used,
            'timestamp': datetime.now().isoformat()
        }
        
        if not success:
            self.errors_detected.append(f"{test_name}: {details}")
    
    def test_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸")
        
        # ê³„ì‚° ì‹œìŠ¤í…œ í™•ì¸
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            self.log_test("Compute_System", True, f"GPU ê°€ì†: {gpu_name}, Memory: {gpu_memory:.1f}GB")
        else:
            self.log_test("Compute_System", True, "ê³ ì„±ëŠ¥ CPU ê³„ì‚° ëª¨ë“œ - 128GB ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ í™œìš©")
        
        # CPU ì •ë³´
        cpu_cores = torch.get_num_threads()
        self.log_test("CPU_Cores", True, f"{cpu_cores} threads available")
        
        return cuda_available
    
    def test_hybrid_config(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì • í…ŒìŠ¤íŠ¸"""
        logger.info("âš™ï¸ í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì • í…ŒìŠ¤íŠ¸")
        
        try:
            config = HybridConfig(
                target_params=3_000_000_000,
                gpu_memory_gb=8.0,
                cpu_memory_gb=128.0,
                regrets_per_step=7,
                bentham_calculations_per_regret=3,
                epochs=1,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 1 ì—í¬í¬
                batch_size=4,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‘ì€ ë°°ì¹˜
                num_workers=2,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ì€ ì›Œì»¤
                use_mixed_precision=True,
                log_every_n_steps=2,
                save_checkpoint_every=5
            )
            
            self.log_test("Hybrid_Config", True, f"Target: {config.target_params:,} params, Workers: {config.num_workers}")
            return config
            
        except Exception as e:
            self.log_test("Hybrid_Config", False, f"Config error: {e}")
            return None
    
    def test_memory_optimized_model(self, config: HybridConfig):
        """ë©”ëª¨ë¦¬ ìµœì í™” ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ§  ë©”ëª¨ë¦¬ ìµœì í™” ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        
        try:
            model = MemoryOptimizedModel(config)
            param_count = model.get_parameter_count()
            
            # 33ì–µ íŒŒë¼ë¯¸í„° ëª©í‘œ ë‹¬ì„± í™•ì¸
            if param_count >= 2_500_000_000:  # 25ì–µ ì´ìƒì´ë©´ ì„±ê³µ
                self.log_test("Model_Parameters", True, f"{param_count:,} parameters (Target: 3B)")
            else:
                self.log_test("Model_Parameters", False, f"Only {param_count:,} parameters (Target: 3B)")
            
            # í…ŒìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ ìˆœì „íŒŒ
            test_input = torch.randn(2, 32, 1024)  # ì‘ì€ í…ŒìŠ¤íŠ¸ ì…ë ¥
            
            with torch.no_grad():
                outputs = model(test_input)
            
            # ì¶œë ¥ ê²€ì¦
            required_keys = ['emotion_predictions', 'semantic_predictions', 'reasoning_features', 'integration_features']
            missing_keys = [key for key in required_keys if key not in outputs]
            
            if not missing_keys:
                self.log_test("Model_Forward", True, f"All outputs present: {list(outputs.keys())}")
            else:
                self.log_test("Model_Forward", False, f"Missing outputs: {missing_keys}")
            
            return model, outputs
            
        except Exception as e:
            self.log_test("Model_Creation", False, f"Model error: {e}")
            traceback.print_exc()
            return None, None
    
    def test_async_regret_calculator(self, config: HybridConfig):
        """ë¹„ë™ê¸° í›„íšŒ ê³„ì‚°ê¸° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ ë¹„ë™ê¸° í›„íšŒ ê³„ì‚°ê¸° í…ŒìŠ¤íŠ¸")
        
        try:
            calculator = AsyncRegretCalculator(config)
            
            # í…ŒìŠ¤íŠ¸ ê²°ì •
            test_decision = torch.randn(1024)
            
            # ë¹„ë™ê¸° ê³„ì‚° ìš”ì²­
            calculator.calculate_async(test_decision, task_id=1)
            
            # ê²°ê³¼ ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
            result = None
            for _ in range(50):  # 5ì´ˆ ëŒ€ê¸°
                result = calculator.get_result(timeout=0.1)
                if result:
                    break
                time.sleep(0.1)
            
            if result:
                task_id, scenarios = result
                if len(scenarios) == config.regrets_per_step:
                    self.log_test("Async_Regret", True, f"{len(scenarios)} regret scenarios generated")
                else:
                    self.log_test("Async_Regret", False, f"Expected {config.regrets_per_step}, got {len(scenarios)}")
            else:
                self.log_test("Async_Regret", False, "No result received within timeout")
            
            return calculator
            
        except Exception as e:
            self.log_test("Async_Regret", False, f"Calculator error: {e}")
            traceback.print_exc()
            return None
    
    def test_xai_integration(self):
        """XAI í†µí•© í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ” XAI í†µí•© í…ŒìŠ¤íŠ¸")
        
        try:
            # XAI ë¡œê¹… í…ŒìŠ¤íŠ¸
            with xai_logger.trace_operation("hybrid_test", "xai_integration") as operation_id:
                if operation_id:
                    self.log_test("XAI_Tracing", True, f"Operation ID: {operation_id}")
                else:
                    self.log_test("XAI_Tracing", False, "No operation ID generated")
                
                # XAI ê²°ì • í¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
                @xai_decision_point()
                def test_decision():
                    return {"decision": "test", "confidence": 0.95}
                
                decision_result = test_decision()
                self.log_test("XAI_Decision", True, f"Decision: {decision_result}")
            
            # ì„±ëŠ¥ ìš”ì•½
            performance = xai_logger.get_performance_summary()
            self.log_test("XAI_Performance", True, f"Ops tracked: {len(performance)}")
            
            return True
            
        except Exception as e:
            self.log_test("XAI_Integration", False, f"XAI error: {e}")
            return False
    
    def test_llm_integration(self):
        """LLM í†µí•© í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ¤– LLM í†µí•© í…ŒìŠ¤íŠ¸")
        
        try:
            # LLM ë“±ë¡ í…ŒìŠ¤íŠ¸
            wrapper = register_llm(
                model_name="test_hybrid_model",
                max_tokens=50,
                temperature=0.7
            )
            
            if wrapper and wrapper.config.model_name:
                if "intelligent" in wrapper.config.model_name or "advanced" in wrapper.config.model_name:
                    self.log_test("LLM_Registration", True, f"ê³ ê¸‰ ì§€ëŠ¥í˜• LLM: {wrapper.config.model_name}")
                else:
                    self.log_test("LLM_Registration", True, f"LLM ëª¨ë¸: {wrapper.config.model_name}")
            else:
                self.log_test("LLM_Registration", False, "LLM registration failed", fallback_used=True)
            
            # LLM ì‘ë‹µ í…ŒìŠ¤íŠ¸
            response = ask_llm("test_hybrid_model", "í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
            
            if response and len(response) > 0:
                if "ê³ ê¸‰ AI" in response or "ì§€ëŠ¥í˜•" in response or "ë¶„ì„" in response:
                    self.log_test("LLM_Response", True, f"ê³ í’ˆì§ˆ ì‘ë‹µ: {len(response)}ì")
                elif "fallback" not in response.lower() and "error" not in response.lower():
                    self.log_test("LLM_Response", True, f"ì •ìƒ ì‘ë‹µ: {len(response)}ì")
                else:
                    self.log_test("LLM_Response", True, f"ê¸°ë³¸ ì‘ë‹µ: {response[:50]}...", fallback_used=True)
            else:
                self.log_test("LLM_Response", False, "No response from LLM")
            
            # ì„±ëŠ¥ ìš”ì•½
            performance = llm_tracker.get_performance_summary()
            self.log_test("LLM_Performance", True, f"Interactions: {performance.get('total_interactions', 0)}")
            
            return True
            
        except Exception as e:
            self.log_test("LLM_Integration", False, f"LLM error: {e}")
            return False
    
    def test_individual_models(self):
        """ê°œë³„ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ¯ ê°œë³„ ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_embedding = torch.randn(4, 768)
        test_tokens = torch.randint(0, 1000, (4, 50))
        
        # 1. ê°ì • ëª¨ë¸
        try:
            emotion_model = HierarchicalEmotionModel()
            emotion_output = emotion_model(test_embedding)
            
            if 'final_emotion' in emotion_output:
                self.log_test("Emotion_Model", True, f"Output shape: {emotion_output['final_emotion'].shape}")
            else:
                self.log_test("Emotion_Model", False, "No final_emotion output")
                
        except Exception as e:
            self.log_test("Emotion_Model", False, f"Emotion error: {e}")
        
        # 2. ì˜ë¯¸ ëª¨ë¸
        try:
            semantic_config = SemanticAnalysisConfig(vocab_size=1000, embedding_dim=256)
            semantic_model = AdvancedSemanticModel(semantic_config)
            semantic_output = semantic_model(test_tokens)
            
            if 'enhanced_semantics' in semantic_output:
                self.log_test("Semantic_Model", True, f"Output shape: {semantic_output['enhanced_semantics'].shape}")
            else:
                self.log_test("Semantic_Model", False, "No enhanced_semantics output")
                
        except Exception as e:
            self.log_test("Semantic_Model", False, f"Semantic error: {e}")
        
        # 3. ë°˜ì‚¬ì‹¤ ëª¨ë¸
        try:
            cf_config = CounterfactualConfig(input_dim=768, hidden_dims=[512, 256, 128], latent_dim=64)
            cf_model = AdvancedCounterfactualModel(cf_config)
            cf_output = cf_model(test_embedding)
            
            if 'counterfactual_scenarios' in cf_output:
                self.log_test("Counterfactual_Model", True, f"Scenarios: {len(cf_output['counterfactual_scenarios'])}")
            else:
                self.log_test("Counterfactual_Model", False, "No counterfactual_scenarios output")
                
        except Exception as e:
            self.log_test("Counterfactual_Model", False, f"Counterfactual error: {e}")
    
    def test_data_loading(self):
        """ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ“Š ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸")
        
        try:
            data_dir = project_root / 'processed_datasets'
            batch_files = list(data_dir.glob('full_scenarios_batch_*.json'))
            
            if not batch_files:
                self.log_test("Data_Files", False, "No batch files found")
                return False
            
            self.log_test("Data_Files", True, f"{len(batch_files)} batch files found")
            
            # ì²« ë²ˆì§¸ íŒŒì¼ ë¡œë“œ í…ŒìŠ¤íŠ¸
            with open(batch_files[0], 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            if isinstance(data, list) and len(data) > 0:
                sample = data[0]
                required_fields = ['id', 'description', 'options']
                missing_fields = [field for field in required_fields if field not in sample]
                
                if not missing_fields:
                    self.log_test("Data_Structure", True, f"Sample has all required fields")
                else:
                    self.log_test("Data_Structure", False, f"Missing fields: {missing_fields}")
            else:
                self.log_test("Data_Structure", False, "Invalid data structure")
            
            return True
            
        except Exception as e:
            self.log_test("Data_Loading", False, f"Data error: {e}")
            return False
    
    def test_hybrid_trainer_creation(self, config: HybridConfig):
        """í•˜ì´ë¸Œë¦¬ë“œ íŠ¸ë ˆì´ë„ˆ ìƒì„± í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ íŠ¸ë ˆì´ë„ˆ ìƒì„± í…ŒìŠ¤íŠ¸")
        
        try:
            trainer = HybridDistributedTrainer(config)
            
            # ëª¨ë¸ ì¤€ë¹„
            param_count = trainer.prepare_model()
            
            if param_count >= 2_500_000_000:
                self.log_test("Trainer_Model", True, f"{param_count:,} parameters prepared")
            else:
                self.log_test("Trainer_Model", False, f"Only {param_count:,} parameters")
            
            # ë°ì´í„° ì¤€ë¹„
            dataloader = trainer.prepare_data()
            
            if len(dataloader) > 0:
                self.log_test("Trainer_Data", True, f"{len(dataloader)} batches prepared")
            else:
                self.log_test("Trainer_Data", False, "No data batches")
            
            return trainer
            
        except Exception as e:
            self.log_test("Trainer_Creation", False, f"Trainer error: {e}")
            traceback.print_exc()
            return None
    
    def run_full_test(self):
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì™„ì „ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info("=" * 80)
        
        start_time = time.time()
        
        # 1. ì‹œìŠ¤í…œ ì •ë³´
        cuda_available = self.test_system_info()
        
        # 2. í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì •
        config = self.test_hybrid_config()
        if not config:
            logger.error("âŒ ì„¤ì • í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
            return False
        
        # 3. ë©”ëª¨ë¦¬ ìµœì í™” ëª¨ë¸
        model, outputs = self.test_memory_optimized_model(config)
        
        # 4. ë¹„ë™ê¸° í›„íšŒ ê³„ì‚°ê¸°
        calculator = self.test_async_regret_calculator(config)
        
        # 5. XAI í†µí•©
        self.test_xai_integration()
        
        # 6. LLM í†µí•©
        self.test_llm_integration()
        
        # 7. ê°œë³„ ëª¨ë¸ë“¤
        self.test_individual_models()
        
        # 8. ë°ì´í„° ë¡œë”©
        self.test_data_loading()
        
        # 9. í•˜ì´ë¸Œë¦¬ë“œ íŠ¸ë ˆì´ë„ˆ
        trainer = self.test_hybrid_trainer_creation(config)
        
        test_time = time.time() - start_time
        
        # ê²°ê³¼ ë¶„ì„
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        logger.info("=" * 80)
        
        total_tests = len(self.test_results)
        successful_tests = sum(1 for result in self.test_results.values() if result['success'])
        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
        
        logger.info(f"âœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {successful_tests}/{total_tests} ({success_rate:.1f}%)")
        logger.info(f"â±ï¸ í…ŒìŠ¤íŠ¸ ì‹œê°„: {test_time:.2f}ì´ˆ")
        
        # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        for test_name, result in self.test_results.items():
            status = "âœ…" if result['success'] else "âŒ"
            fallback = " (âš ï¸ FALLBACK)" if result['fallback_used'] else ""
            logger.info(f"   {status} {test_name}{fallback}")
        
        # Fallback ì‚¬ìš© ê²€ì‚¬
        if self.fallback_detected:
            logger.warning("\nâš ï¸ FALLBACK ëª¨ë“œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!")
            logger.warning("ì¼ë¶€ ê¸°ëŠ¥ì´ ì™„ì „í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            logger.info("\nâœ… FALLBACK ì—†ëŠ” ì™„ì „í•œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤!")
        
        # ì˜¤ë¥˜ ìš”ì•½
        if self.errors_detected:
            logger.error(f"\nâŒ ê°ì§€ëœ ì˜¤ë¥˜ë“¤:")
            for error in self.errors_detected:
                logger.error(f"   - {error}")
        
        # ìµœì¢… íŒì •
        system_complete = (
            success_rate >= 80 and 
            not self.fallback_detected and 
            len(self.errors_detected) == 0
        )
        
        if system_complete:
            logger.info("\nğŸŠ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì™„ì „ ê²€ì¦ ì„±ê³µ! ğŸŠ")
            logger.info("ëª¨ë“  ê¸°ëŠ¥ì´ Fallback ì—†ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
            logger.info("í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ ìƒíƒœì…ë‹ˆë‹¤.")
        else:
            logger.warning("\nâš ï¸ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            logger.warning("ì¶”ê°€ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ê²°ê³¼ ì €ì¥
        results_path = project_root / 'training' / 'hybrid_outputs' / 'full_system_test_results.json'
        results_path.parent.mkdir(parents=True, exist_ok=True)
        
        final_results = {
            'test_summary': {
                'total_tests': total_tests,
                'successful_tests': successful_tests,
                'success_rate': success_rate,
                'test_duration': test_time,
                'fallback_detected': self.fallback_detected,
                'errors_count': len(self.errors_detected),
                'system_complete': system_complete,
                'cuda_available': cuda_available,
                'timestamp': datetime.now().isoformat()
            },
            'test_results': self.test_results,
            'errors_detected': self.errors_detected,
            'recommendations': self._generate_recommendations()
        }
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {results_path}")
        
        return system_complete
    
    def _generate_recommendations(self) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if self.fallback_detected:
            recommendations.append("Fallback ëª¨ë“œë¥¼ ì œê±°í•˜ì—¬ ì™„ì „í•œ ê¸°ëŠ¥ êµ¬í˜„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if self.errors_detected:
            recommendations.append("ê°ì§€ëœ ì˜¤ë¥˜ë“¤ì„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        success_rate = sum(1 for r in self.test_results.values() if r['success']) / len(self.test_results) * 100
        if success_rate < 90:
            recommendations.append("í…ŒìŠ¤íŠ¸ ì„±ê³µë¥ ì„ 90% ì´ìƒìœ¼ë¡œ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        if not recommendations:
            recommendations.append("ì‹œìŠ¤í…œì´ ì™„ì „íˆ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤. í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return recommendations

if __name__ == "__main__":
    tester = FullSystemTester()
    success = tester.run_full_test()
    
    if success:
        print("\nğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
        print("python3 training/start_hybrid_training.py ë¡œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ì‹œìŠ¤í…œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    sys.exit(0 if success else 1)