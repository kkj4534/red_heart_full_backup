#!/usr/bin/env python3
"""
í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì‹œìŠ¤í…œ ê²€ì¦
"""

import os
import sys
import torch
import logging
from pathlib import Path
import argparse
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('RedHeart.TestTraining')


def test_components():
    """ê°œë³„ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸"""
    logger.info("=" * 70)
    logger.info("ğŸ§ª ì»´í¬ë„ŒíŠ¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 70)
    
    test_results = {}
    
    # 1. CheckpointManager í…ŒìŠ¤íŠ¸
    try:
        from training.enhanced_checkpoint_manager import EnhancedCheckpointManager
        manager = EnhancedCheckpointManager(
            checkpoint_dir="training/test_checkpoints",
            max_checkpoints=3,
            save_interval=1
        )
        logger.info("âœ… CheckpointManager: ì •ìƒ")
        test_results['checkpoint_manager'] = True
    except Exception as e:
        logger.error(f"âŒ CheckpointManager: {e}")
        test_results['checkpoint_manager'] = False
    
    # 2. LRSweepOptimizer í…ŒìŠ¤íŠ¸
    try:
        from training.lr_sweep_optimizer import LRSweepOptimizer
        lr_sweep = LRSweepOptimizer(
            base_lr=1e-4,
            sweep_range=(1e-5, 1e-3),
            num_sweep_points=3,
            sweep_epochs=1,
            sweep_steps_per_epoch=10
        )
        logger.info("âœ… LRSweepOptimizer: ì •ìƒ")
        test_results['lr_sweep'] = True
    except Exception as e:
        logger.error(f"âŒ LRSweepOptimizer: {e}")
        test_results['lr_sweep'] = False
    
    # 3. SweetSpotDetector í…ŒìŠ¤íŠ¸
    try:
        from training.sweet_spot_detector import SweetSpotDetector
        detector = SweetSpotDetector(
            window_size=3,
            stability_threshold=0.01,
            patience=5,
            min_epochs=3
        )
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—…ë°ì´íŠ¸
        for epoch in range(5):
            detector.update(
                epoch=epoch,
                module_metrics={'test_module': {'loss': 1.0 / (epoch + 1)}},
                learning_rate=1e-4
            )
        logger.info("âœ… SweetSpotDetector: ì •ìƒ")
        test_results['sweet_spot'] = True
    except Exception as e:
        logger.error(f"âŒ SweetSpotDetector: {e}")
        test_results['sweet_spot'] = False
    
    # 4. ParameterCrossoverSystem í…ŒìŠ¤íŠ¸
    try:
        from training.parameter_crossover_system import ParameterCrossoverSystem
        crossover = ParameterCrossoverSystem(
            crossover_strategy='selective',
            blend_ratio=0.7,
            mutation_rate=0.01
        )
        logger.info("âœ… ParameterCrossoverSystem: ì •ìƒ")
        test_results['crossover'] = True
    except Exception as e:
        logger.error(f"âŒ ParameterCrossoverSystem: {e}")
        test_results['crossover'] = False
    
    # 5. OOMHandler í…ŒìŠ¤íŠ¸
    try:
        from training.oom_handler import OOMHandler
        oom_handler = OOMHandler(
            initial_batch_size=4,
            min_batch_size=1,
            gradient_accumulation=16,
            memory_threshold=0.85
        )
        # ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
        status = oom_handler.check_memory_status()
        logger.info(f"  - CPU ë©”ëª¨ë¦¬: {status['cpu']['percent']:.1f}%")
        if 'percent' in status.get('gpu', {}):
            logger.info(f"  - GPU ë©”ëª¨ë¦¬: {status['gpu']['percent']:.1f}%")
        logger.info("âœ… OOMHandler: ì •ìƒ")
        test_results['oom_handler'] = True
    except Exception as e:
        logger.error(f"âŒ OOMHandler: {e}")
        test_results['oom_handler'] = False
    
    # 6. AdvancedTrainingManager í…ŒìŠ¤íŠ¸
    try:
        from training.advanced_training_techniques import AdvancedTrainingManager
        training_manager = AdvancedTrainingManager(
            enable_label_smoothing=True,
            enable_rdrop=True,
            enable_ema=True,
            enable_llrd=True,
            label_smoothing=0.1,
            rdrop_alpha=1.0,
            ema_decay=0.999,
            llrd_decay=0.8
        )
        logger.info("âœ… AdvancedTrainingManager: ì •ìƒ")
        test_results['advanced_training'] = True
    except Exception as e:
        logger.error(f"âŒ AdvancedTrainingManager: {e}")
        test_results['advanced_training'] = False
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    for component, result in test_results.items():
        status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
        logger.info(f"  - {component}: {status}")
    
    all_passed = all(test_results.values())
    if all_passed:
        logger.info("\nğŸ‰ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ í†µê³¼!")
    else:
        logger.error("\nâš ï¸ ì¼ë¶€ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    return all_passed


def test_mini_training():
    """ë¯¸ë‹ˆ í•™ìŠµ í…ŒìŠ¤íŠ¸ (2 ì—í­)"""
    logger.info("\n" + "=" * 70)
    logger.info("ğŸš€ ë¯¸ë‹ˆ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹œì‘ (2 ì—í­)")
    logger.info("=" * 70)
    
    try:
        from training.unified_training_final import UnifiedTrainingConfig, UnifiedTrainer
        
        # í…ŒìŠ¤íŠ¸ìš© ì„¤ì •
        config = UnifiedTrainingConfig()
        config.total_epochs = 2
        config.micro_batch_size = 2
        config.gradient_accumulation = 4  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        config.checkpoint_interval = 1  # ë§¤ ì—í­ ì €ì¥
        config.checkpoint_dir = "training/test_checkpoints"
        config.lr_sweep_enabled = False  # ìŠ¤ìœ• ê±´ë„ˆë›°ê¸°
        config.log_interval = 5
        config.val_interval = 10
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = UnifiedTrainer(config)
        
        # í•™ìŠµ ì‹¤í–‰
        start_time = time.time()
        trainer.train()
        elapsed_time = time.time() - start_time
        
        logger.info(f"\nâœ… ë¯¸ë‹ˆ í•™ìŠµ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")
        
        # ê²°ê³¼ í™•ì¸
        if trainer.checkpoint_manager.checkpoint_metadata:
            logger.info(f"  - ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸: {len(trainer.checkpoint_manager.checkpoint_metadata)}ê°œ")
            logger.info(f"  - ìµœì¢… ì†ì‹¤: {trainer.best_loss:.4f}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë¯¸ë‹ˆ í•™ìŠµ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_checkpoint_loading():
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 70)
    
    try:
        from training.enhanced_checkpoint_manager import EnhancedCheckpointManager
        
        manager = EnhancedCheckpointManager(
            checkpoint_dir="training/test_checkpoints"
        )
        
        # ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        if manager.checkpoint_metadata:
            # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint_data = manager.load_checkpoint()
            logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ")
            logger.info(f"  - ì—í­: {checkpoint_data['epoch']}")
            logger.info(f"  - LR: {checkpoint_data['lr']:.1e}")
            
            # ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
            best_checkpoint = manager.get_best_checkpoint('loss')
            if best_checkpoint:
                logger.info(f"  - ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸: {Path(best_checkpoint).name}")
            
            return True
        else:
            logger.info("  - ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ (ì •ìƒ)")
            return True
            
    except Exception as e:
        logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


def test_memory_monitoring():
    """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ” ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 70)
    
    try:
        from training.oom_handler import OOMHandler
        
        handler = OOMHandler(
            initial_batch_size=4,
            min_batch_size=1,
            gradient_accumulation=16
        )
        
        # í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ
        status = handler.check_memory_status()
        
        logger.info("ğŸ“Š í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ:")
        logger.info(f"  CPU:")
        logger.info(f"    - ì „ì²´: {status['cpu']['total_gb']:.1f} GB")
        logger.info(f"    - ì‚¬ìš© ì¤‘: {status['cpu']['used_gb']:.1f} GB")
        logger.info(f"    - ì‚¬ìš©ë¥ : {status['cpu']['percent']:.1f}%")
        
        if 'error' not in status['gpu']:
            logger.info(f"  GPU:")
            logger.info(f"    - ì „ì²´: {status['gpu'].get('total_gb', 0):.1f} GB")
            logger.info(f"    - í• ë‹¹: {status['gpu'].get('allocated_gb', 0):.1f} GB")
            logger.info(f"    - ì‚¬ìš©ë¥ : {status['gpu'].get('percent', 0):.1f}%")
        else:
            logger.info(f"  GPU: ì‚¬ìš© ë¶ˆê°€")
        
        # ë©”ëª¨ë¦¬ ì„ê³„ ìƒíƒœ ì²´í¬
        is_critical = handler.is_memory_critical()
        if is_critical:
            logger.warning("  âš ï¸ ë©”ëª¨ë¦¬ê°€ ì„ê³„ ìƒíƒœì…ë‹ˆë‹¤!")
        else:
            logger.info("  âœ… ë©”ëª¨ë¦¬ ìƒíƒœ ì–‘í˜¸")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
        return False


def run_all_tests():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ§ª Red Heart AI í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì „ì²´ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 70)
    
    test_results = {
        'ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸': test_components(),
        'ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§': test_memory_monitoring(),
        'ì²´í¬í¬ì¸íŠ¸ ë¡œë”©': test_checkpoint_loading(),
        'ë¯¸ë‹ˆ í•™ìŠµ': test_mini_training()
    }
    
    # ìµœì¢… ê²°ê³¼
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    logger.info("=" * 70)
    
    for test_name, result in test_results.items():
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        logger.info(f"  {test_name}: {status}")
    
    all_passed = all(test_results.values())
    
    if all_passed:
        logger.info("\n" + "ğŸ‰ " * 10)
        logger.info("ğŸŠ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™ í™•ì¸!")
        logger.info("ğŸ‰ " * 10)
    else:
        logger.error("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    return all_passed


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ì»´í¬ë„ŒíŠ¸ë§Œ)')
    parser.add_argument('--training', action='store_true', help='ë¯¸ë‹ˆ í•™ìŠµ í…ŒìŠ¤íŠ¸ë§Œ')
    parser.add_argument('--memory', action='store_true', help='ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ë§Œ')
    
    args = parser.parse_args()
    
    if args.quick:
        success = test_components()
    elif args.training:
        success = test_mini_training()
    elif args.memory:
        success = test_memory_monitoring()
    else:
        success = run_all_tests()
    
    return 0 if success else 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)