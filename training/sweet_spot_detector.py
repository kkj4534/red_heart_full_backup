"""
Sweet Spot Detector - Î™®ÎìàÎ≥Ñ ÏµúÏ†Å ÏóêÌè≠ ÌÉêÏßÄ ÏãúÏä§ÌÖú
Í∞Å Î™®ÎìàÏù¥ ÏµúÍ≥† ÏÑ±Îä•ÏùÑ Î≥¥Ïù¥Îäî ÏóêÌè≠ÏùÑ ÏûêÎèôÏúºÎ°ú ÌÉêÏßÄ
"""

import numpy as np
import torch
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
import json
import logging
from datetime import datetime
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.signal import find_peaks
import pandas as pd

logger = logging.getLogger(__name__)


class SweetSpotDetector:
    """
    Sweet Spot ÌÉêÏßÄ ÏãúÏä§ÌÖú
    - Î™®ÎìàÎ≥Ñ ÏµúÏ†Å ÏÑ±Îä• ÏóêÌè≠ ÌÉêÏßÄ
    - ÏàòÎ†¥ Ìå®ÌÑ¥ Î∂ÑÏÑù
    - Í≥ºÏ†ÅÌï© ÏãúÏ†ê Í∞êÏßÄ
    - ÏïàÏ†ïÏÑ± ÌèâÍ∞Ä
    """
    
    def __init__(self,
                 window_size: int = 5,
                 stability_threshold: float = 0.01,
                 patience: int = 10,
                 min_epochs: int = 10):
        """
        Args:
            window_size: Ïù¥Îèô ÌèâÍ∑† ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞
            stability_threshold: ÏïàÏ†ïÏÑ± ÌåêÎã® ÏûÑÍ≥ÑÍ∞í
            patience: ÏÑ±Îä• Í∞úÏÑ† ÏóÜÏù¥ Í∏∞Îã§Î¶¨Îäî ÏóêÌè≠ Ïàò
            min_epochs: ÏµúÏÜå ÌïôÏäµ ÏóêÌè≠ (Ïù¥Ï†ÑÏóêÎäî Sweet Spot ÌåêÎã® ÏïàÌï®)
        """
        self.window_size = window_size
        self.stability_threshold = stability_threshold
        self.patience = patience
        self.min_epochs = min_epochs
        
        # Î™®ÎìàÎ≥Ñ Î©îÌä∏Î¶≠ ÌûàÏä§ÌÜ†Î¶¨ (train/val Î∂ÑÎ¶¨)
        self.module_histories = defaultdict(lambda: {
            'train_losses': [],
            'val_losses': [],
            'train_accuracies': [],
            'val_accuracies': [],
            'epochs': [],
            'gradients': [],
            'learning_rates': [],
            'overfitting_scores': [],  # val_loss - train_loss
            'generalization_gaps': []  # val_acc - train_acc
        })
        
        # Î™®ÎìàÍ∞Ñ ÏÉÅÌò∏ÏûëÏö© Î©îÌä∏Î¶≠
        self.interaction_metrics = defaultdict(lambda: {
            'synergy_scores': [],  # Î™®Îìà Ï°∞Ìï© ÏãúÎÑàÏßÄ
            'correlation_matrix': [],  # Î™®ÎìàÍ∞Ñ ÏÑ±Îä• ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ
            'coupling_strength': [],  # Î™®ÎìàÍ∞Ñ Í≤∞Ìï©ÎèÑ
            'information_flow': []  # Î™®ÎìàÍ∞Ñ Ï†ïÎ≥¥ ÌùêÎ¶Ñ
        })
        
        # Sweet Spot Ï†ïÎ≥¥
        self.sweet_spots = {}
        self.convergence_points = {}
        self.overfitting_points = {}
        self.interaction_sweet_spots = {}  # Î™®Îìà Ï°∞Ìï© ÏµúÏ†ÅÏ†ê
        
        logger.info("‚úÖ Sweet Spot Detector Ï¥àÍ∏∞Ìôî")
        logger.info(f"  - ÏúàÎèÑÏö∞ ÌÅ¨Í∏∞: {window_size}")
        logger.info(f"  - ÏïàÏ†ïÏÑ± ÏûÑÍ≥ÑÍ∞í: {stability_threshold}")
        logger.info(f"  - Patience: {patience}")
    
    def update(self, 
               epoch: int,
               train_module_metrics: Dict[str, Dict[str, float]],
               val_module_metrics: Dict[str, Dict[str, float]],
               learning_rate: float = None):
        """
        Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Sweet Spot ÌÉêÏßÄ (train/val Î∂ÑÎ¶¨)
        
        Args:
            epoch: ÌòÑÏû¨ ÏóêÌè≠
            train_module_metrics: ÌïôÏäµ Î™®ÎìàÎ≥Ñ Î©îÌä∏Î¶≠ ÎîïÏÖîÎÑàÎ¶¨
            val_module_metrics: Í≤ÄÏ¶ù Î™®ÎìàÎ≥Ñ Î©îÌä∏Î¶≠ ÎîïÏÖîÎÑàÎ¶¨
            learning_rate: ÌòÑÏû¨ ÌïôÏäµÎ•†
        """
        # Î™®Îì† Î™®Îìà Ïù¥Î¶Ñ ÏàòÏßë
        all_modules = set(train_module_metrics.keys()) | set(val_module_metrics.keys())
        
        for module_name in all_modules:
            history = self.module_histories[module_name]
            train_metrics = train_module_metrics.get(module_name, {})
            val_metrics = val_module_metrics.get(module_name, {})
            
            # ÌûàÏä§ÌÜ†Î¶¨ ÏóÖÎç∞Ïù¥Ìä∏
            history['epochs'].append(epoch)
            history['train_losses'].append(train_metrics.get('loss', 0))
            history['val_losses'].append(val_metrics.get('loss', 0))
            history['train_accuracies'].append(train_metrics.get('accuracy', 0))
            history['val_accuracies'].append(val_metrics.get('accuracy', 0))
            
            # Í≥ºÏ†ÅÌï© Ï†êÏàò Í≥ÑÏÇ∞ (val_loss - train_loss)
            overfitting_score = val_metrics.get('loss', 0) - train_metrics.get('loss', 0)
            history['overfitting_scores'].append(overfitting_score)
            
            # ÏùºÎ∞òÌôî Í∞≠ Í≥ÑÏÇ∞ (train_acc - val_acc)
            generalization_gap = train_metrics.get('accuracy', 0) - val_metrics.get('accuracy', 0)
            history['generalization_gaps'].append(generalization_gap)
            
            if 'gradient_norm' in train_metrics:
                history['gradients'].append(train_metrics['gradient_norm'])
            
            if learning_rate:
                history['learning_rates'].append(learning_rate)
            
            # Sweet Spot ÌÉêÏßÄ (Ï∂©Î∂ÑÌïú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏåìÏù∏ ÌõÑ)
            if epoch >= self.min_epochs:
                self._detect_sweet_spot(module_name, epoch)
                self._detect_convergence(module_name, epoch)
                self._detect_overfitting_improved(module_name, epoch)
        
        # Î™®ÎìàÍ∞Ñ ÏÉÅÌò∏ÏûëÏö© Î∂ÑÏÑù
        if epoch >= self.min_epochs:
            self._analyze_module_interactions(epoch, all_modules)
    
    def _detect_sweet_spot(self, module_name: str, epoch: int):
        """Î™®ÎìàÎ≥Ñ Sweet Spot ÌÉêÏßÄ (train/val Í∑†Ìòï Í≥†Î†§)"""
        history = self.module_histories[module_name]
        val_losses = history['val_losses']
        train_losses = history['train_losses']
        
        if len(val_losses) < self.window_size:
            return
        
        # ÏµúÍ∑º ÏúàÎèÑÏö∞Ïùò ÏÜêÏã§
        recent_val_losses = val_losses[-self.window_size:]
        recent_train_losses = train_losses[-self.window_size:]
        recent_overfitting = history['overfitting_scores'][-self.window_size:]
        
        # Ï°∞Í±¥ 1: ÎÇÆÏùÄ Í≤ÄÏ¶ù ÏÜêÏã§
        avg_val_loss = np.mean(recent_val_losses)
        avg_train_loss = np.mean(recent_train_losses)
        
        # Ï°∞Í±¥ 2: ÏïàÏ†ïÏÑ± (ÎÇÆÏùÄ Î∂ÑÏÇ∞)
        val_std = np.std(recent_val_losses)
        is_stable = val_std < self.stability_threshold
        
        # Ï°∞Í±¥ 3: ÏàòÎ†¥ (ÏÜêÏã§ Í∞êÏÜåÏú®Ïù¥ ÎÇÆÏùå)
        if len(val_losses) >= self.window_size * 2:
            prev_window = val_losses[-self.window_size*2:-self.window_size]
            improvement = (np.mean(prev_window) - avg_val_loss) / (np.mean(prev_window) + 1e-10)
            is_converged = abs(improvement) < 0.01  # 1% ÎØ∏Îßå Í∞úÏÑ†
        else:
            is_converged = False
        
        # Ï°∞Í±¥ 4: Í≥ºÏ†ÅÌï© Ï†úÏñ¥
        avg_overfitting = np.mean(recent_overfitting)
        is_not_overfitting = avg_overfitting < 0.1  # 10% ÎØ∏Îßå Ï∞®Ïù¥
        
        # Sweet Spot ÌåêÎã®
        if is_stable and is_not_overfitting and (is_converged or avg_val_loss < 0.1):
            # Ïù¥Ï†Ñ Sweet SpotÎ≥¥Îã§ ÎÇòÏùÄÏßÄ ÌôïÏù∏
            if module_name not in self.sweet_spots or \
               avg_val_loss < self.sweet_spots[module_name]['val_loss']:
                
                self.sweet_spots[module_name] = {
                    'epoch': epoch,
                    'val_loss': avg_val_loss,
                    'train_loss': avg_train_loss,
                    'loss': avg_val_loss,  # Ìò∏ÌôòÏÑ± Ïú†ÏßÄ
                    'std': val_std,
                    'stable': is_stable,
                    'converged': is_converged,
                    'overfitting_score': avg_overfitting
                }
                
                logger.info(f"  üéØ Sweet Spot Î∞úÍ≤¨: {module_name}")
                logger.info(f"     - ÏóêÌè≠: {epoch}")
                logger.info(f"     - Val Loss: {avg_val_loss:.4f} (¬±{val_std:.4f})")
                logger.info(f"     - Overfitting: {avg_overfitting:.4f}")
    
    def _detect_convergence(self, module_name: str, epoch: int):
        """ÏàòÎ†¥ ÏãúÏ†ê ÌÉêÏßÄ (val_loss Í∏∞Ï§Ä)"""
        history = self.module_histories[module_name]
        val_losses = history['val_losses']
        
        if len(val_losses) < self.patience:
            return
        
        # ÏµúÍ∑º patience ÏóêÌè≠ ÎèôÏïàÏùò Í∞úÏÑ† ÌôïÏù∏
        recent_val_losses = val_losses[-self.patience:]
        best_recent = min(recent_val_losses)
        
        # Í∞úÏÑ†Ïù¥ Í±∞Ïùò ÏóÜÏúºÎ©¥ ÏàòÎ†¥ÏúºÎ°ú ÌåêÎã®
        improvements = []
        for i in range(1, len(recent_val_losses)):
            if recent_val_losses[i-1] > 0:
                improvement = (recent_val_losses[i-1] - recent_val_losses[i]) / recent_val_losses[i-1]
                improvements.append(improvement)
        
        avg_improvement = np.mean(improvements) if improvements else 0
        
        if abs(avg_improvement) < 0.001:  # 0.1% ÎØ∏Îßå Í∞úÏÑ†
            if module_name not in self.convergence_points:
                self.convergence_points[module_name] = {
                    'epoch': epoch,
                    'val_loss': best_recent,
                    'train_loss': history['train_losses'][-1] if history['train_losses'] else 0,
                    'improvement_rate': avg_improvement
                }
                logger.info(f"  üìä ÏàòÎ†¥ Í∞êÏßÄ: {module_name} @ epoch {epoch}")
    
    def _detect_overfitting_improved(self, module_name: str, epoch: int):
        """Í∞úÏÑ†Îêú Í≥ºÏ†ÅÌï© ÏãúÏ†ê ÌÉêÏßÄ (train/val Í∞≠ Í∏∞Î∞ò)"""
        history = self.module_histories[module_name]
        val_losses = history['val_losses']
        train_losses = history['train_losses']
        overfitting_scores = history['overfitting_scores']
        
        if len(val_losses) < self.window_size * 2:
            return
        
        # ÏµúÍ∑º ÏúàÎèÑÏö∞Ïùò Í≥ºÏ†ÅÌï© Ï†êÏàò
        recent_overfitting = np.mean(overfitting_scores[-self.window_size:])
        prev_overfitting = np.mean(overfitting_scores[-self.window_size*2:-self.window_size])
        
        # Í≤ÄÏ¶ù ÏÜêÏã§ Ï¶ùÍ∞Ä ÌôïÏù∏
        recent_val = np.mean(val_losses[-self.window_size:])
        prev_val = np.mean(val_losses[-self.window_size*2:-self.window_size])
        val_increase = (recent_val - prev_val) / (prev_val + 1e-10)
        
        # ÌïôÏäµ ÏÜêÏã§ÏùÄ Í≥ÑÏÜç Í∞êÏÜåÌïòÎäîÏßÄ ÌôïÏù∏
        recent_train = np.mean(train_losses[-self.window_size:])
        prev_train = np.mean(train_losses[-self.window_size*2:-self.window_size])
        train_decrease = (prev_train - recent_train) / (prev_train + 1e-10)
        
        # Í≥ºÏ†ÅÌï© Ï°∞Í±¥: val loss Ï¶ùÍ∞Ä & train loss Í∞êÏÜå & Í≥ºÏ†ÅÌï© Ï†êÏàò Ï¶ùÍ∞Ä
        if (val_increase > 0.02 and  # val loss 2% Ïù¥ÏÉÅ Ï¶ùÍ∞Ä
            train_decrease > 0.01 and  # train lossÎäî Í≥ÑÏÜç Í∞êÏÜå
            recent_overfitting > prev_overfitting * 1.2):  # Í≥ºÏ†ÅÌï© Ï†êÏàò 20% Ï¶ùÍ∞Ä
            
            if module_name not in self.overfitting_points:
                self.overfitting_points[module_name] = {
                    'epoch': epoch - self.window_size,  # Í≥ºÏ†ÅÌï© ÏãúÏûë ÏãúÏ†ê
                    'val_increase': val_increase,
                    'train_decrease': train_decrease,
                    'overfitting_score': recent_overfitting
                }
                logger.warning(f"  ‚ö†Ô∏è Í≥ºÏ†ÅÌï© Í∞êÏßÄ: {module_name} @ epoch {epoch - self.window_size}")
                logger.warning(f"     - Val Ï¶ùÍ∞Ä: {val_increase:.2%}, Train Í∞êÏÜå: {train_decrease:.2%}")
    
    def _analyze_module_interactions(self, epoch: int, module_names: set):
        """Î™®ÎìàÍ∞Ñ ÏÉÅÌò∏ÏûëÏö© Î∂ÑÏÑù"""
        import itertools
        
        # Î™®Îìà ÏåçÎ≥Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Í≥ÑÏÇ∞
        correlation_matrix = {}
        synergy_scores = {}
        
        module_list = list(module_names)
        for mod1, mod2 in itertools.combinations(module_list, 2):
            if mod1 not in self.module_histories or mod2 not in self.module_histories:
                continue
                
            # ÏµúÍ∑º ÏÜêÏã§Í∞íÎì§Ïùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ
            losses1 = self.module_histories[mod1]['val_losses'][-self.window_size:]
            losses2 = self.module_histories[mod2]['val_losses'][-self.window_size:]
            
            if len(losses1) == len(losses2) and len(losses1) > 1:
                correlation = np.corrcoef(losses1, losses2)[0, 1]
                correlation_matrix[f"{mod1}-{mod2}"] = correlation
                
                # ÏãúÎÑàÏßÄ Ï†êÏàò: ÏùåÏùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÎäî Î≥¥ÏôÑÏ†Å, ÏñëÏùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÎäî ÏùòÏ°¥Ï†Å
                if correlation < -0.3:  # Î≥¥ÏôÑÏ†Å Í¥ÄÍ≥Ñ
                    synergy_scores[f"{mod1}-{mod2}"] = 1.0 - abs(correlation)
                elif correlation > 0.7:  # Í∞ïÌïú ÏùòÏ°¥ Í¥ÄÍ≥Ñ
                    synergy_scores[f"{mod1}-{mod2}"] = correlation * 0.5
                else:  # ÎèÖÎ¶ΩÏ†Å Í¥ÄÍ≥Ñ
                    synergy_scores[f"{mod1}-{mod2}"] = 0.7
        
        # Ï†ÑÏ≤¥ Î™®Îìà Ï°∞Ìï©Ïùò ÏãúÎÑàÏßÄ Í≥ÑÏÇ∞
        if synergy_scores:
            avg_synergy = np.mean(list(synergy_scores.values()))
            
            # ÏÉÅÌò∏ÏûëÏö© Î©îÌä∏Î¶≠ Ï†ÄÏû•
            self.interaction_metrics[epoch] = {
                'synergy_scores': synergy_scores,
                'correlation_matrix': correlation_matrix,
                'avg_synergy': avg_synergy,
                'module_count': len(module_list)
            }
            
            # Sweet Spot Ï°∞Ìï© Ï∞æÍ∏∞
            if avg_synergy > 0.7 and epoch not in self.interaction_sweet_spots:
                self.interaction_sweet_spots[epoch] = {
                    'synergy': avg_synergy,
                    'best_pairs': sorted(synergy_scores.items(), 
                                        key=lambda x: x[1], reverse=True)[:3]
                }
                logger.info(f"  üîó Î™®Îìà ÏÉÅÌò∏ÏûëÏö© Sweet Spot @ epoch {epoch}")
                logger.info(f"     - ÌèâÍ∑† ÏãúÎÑàÏßÄ: {avg_synergy:.3f}")
    
    def get_optimal_epochs(self) -> Dict[str, int]:
        """
        Í∞Å Î™®ÎìàÏùò ÏµúÏ†Å ÏóêÌè≠ Î∞òÌôò
        
        Returns:
            Î™®ÎìàÎ≥Ñ ÏµúÏ†Å ÏóêÌè≠ ÎîïÏÖîÎÑàÎ¶¨
        """
        optimal_epochs = {}
        
        for module_name in self.module_histories.keys():
            # Sweet SpotÏù¥ ÏûàÏúºÎ©¥ Í∑∏Í≤ÉÏùÑ ÏÇ¨Ïö©
            if module_name in self.sweet_spots:
                optimal_epochs[module_name] = self.sweet_spots[module_name]['epoch']
            # ÏàòÎ†¥Ï†êÏù¥ ÏûàÏúºÎ©¥ Í∑∏Í≤ÉÏùÑ ÏÇ¨Ïö©
            elif module_name in self.convergence_points:
                optimal_epochs[module_name] = self.convergence_points[module_name]['epoch']
            # Í≥ºÏ†ÅÌï© ÏßÅÏ†Ñ ÏÇ¨Ïö©
            elif module_name in self.overfitting_points:
                optimal_epochs[module_name] = max(1, self.overfitting_points[module_name]['epoch'] - 1)
            # Í∏∞Î≥∏Í∞í: ÏµúÏ†Ä Í≤ÄÏ¶ù ÏÜêÏã§ ÏóêÌè≠
            else:
                val_losses = self.module_histories[module_name]['val_losses']
                if val_losses:
                    optimal_epochs[module_name] = val_losses.index(min(val_losses)) + 1
        
        return optimal_epochs
    
    def get_module_status(self, module_name: str) -> Dict[str, Any]:
        """
        ÌäπÏ†ï Î™®ÎìàÏùò ÌòÑÏû¨ ÏÉÅÌÉú Î∞òÌôò
        
        Args:
            module_name: Î™®Îìà Ïù¥Î¶Ñ
            
        Returns:
            Î™®Îìà ÏÉÅÌÉú Ï†ïÎ≥¥
        """
        if module_name not in self.module_histories:
            return {'status': 'not_found'}
        
        history = self.module_histories[module_name]
        # train/val Î∂ÑÎ¶¨Îêú ÏÜêÏã§ Ï≤òÎ¶¨
        train_losses = history.get('train_losses', [])
        val_losses = history.get('val_losses', [])
        
        # validation lossÎ•º Ï£ºÏöî ÏßÄÌëúÎ°ú ÏÇ¨Ïö© (Í≥ºÏ†ÅÌï© Î∞©ÏßÄ)
        primary_losses = val_losses if val_losses else train_losses
        
        status = {
            'total_epochs': len(history['epochs']),
            'current_train_loss': train_losses[-1] if train_losses else None,
            'current_val_loss': val_losses[-1] if val_losses else None,
            'best_train_loss': min(train_losses) if train_losses else None,
            'best_val_loss': min(val_losses) if val_losses else None,
            'best_epoch': val_losses.index(min(val_losses)) + 1 if val_losses else 
                         (train_losses.index(min(train_losses)) + 1 if train_losses else None),
            'overfitting_score': (val_losses[-1] - train_losses[-1]) if (val_losses and train_losses) else None
        }
        
        # Sweet Spot Ï†ïÎ≥¥
        if module_name in self.sweet_spots:
            status['sweet_spot'] = self.sweet_spots[module_name]
        
        # ÏàòÎ†¥ Ï†ïÎ≥¥
        if module_name in self.convergence_points:
            status['convergence'] = self.convergence_points[module_name]
        
        # Í≥ºÏ†ÅÌï© Ï†ïÎ≥¥
        if module_name in self.overfitting_points:
            status['overfitting'] = self.overfitting_points[module_name]
        
        return status
    
    def plot_module_analysis(self, module_name: str, save_path: Optional[str] = None):
        """
        Î™®Îìà Î∂ÑÏÑù Í≤∞Í≥º ÏãúÍ∞ÅÌôî
        
        Args:
            module_name: Î™®Îìà Ïù¥Î¶Ñ
            save_path: Ï†ÄÏû• Í≤ΩÎ°ú (NoneÏù¥Î©¥ ÌëúÏãúÎßå)
        """
        if module_name not in self.module_histories:
            logger.warning(f"Î™®Îìà {module_name}Ïùò ÌûàÏä§ÌÜ†Î¶¨Í∞Ä ÏóÜÏäµÎãàÎã§")
            return
        
        history = self.module_histories[module_name]
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. ÏÜêÏã§ Í≥°ÏÑ† (train/val Î∂ÑÎ¶¨)
        ax = axes[0, 0]
        if history.get('train_losses'):
            ax.plot(history['epochs'], history['train_losses'], 'b-', label='Train Loss', alpha=0.7)
        if history.get('val_losses'):
            ax.plot(history['epochs'], history['val_losses'], 'r-', label='Val Loss', alpha=0.7)
        
        # Sweet Spot ÌëúÏãú
        if module_name in self.sweet_spots:
            spot = self.sweet_spots[module_name]
            ax.axvline(x=spot['epoch'], color='g', linestyle='--', label=f"Sweet Spot (epoch {spot['epoch']})")
        
        # ÏàòÎ†¥Ï†ê ÌëúÏãú
        if module_name in self.convergence_points:
            conv = self.convergence_points[module_name]
            ax.axvline(x=conv['epoch'], color='orange', linestyle='--', label=f"Convergence (epoch {conv['epoch']})")
        
        # Í≥ºÏ†ÅÌï© ÏãúÏ†ê ÌëúÏãú
        if module_name in self.overfitting_points:
            overfit = self.overfitting_points[module_name]
            ax.axvline(x=overfit['epoch'], color='r', linestyle='--', label=f"Overfitting (epoch {overfit['epoch']})")
        
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.set_title(f'{module_name} - Loss Curve')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. Ï†ïÌôïÎèÑ Í≥°ÏÑ† (ÏûàÎäî Í≤ΩÏö∞)
        ax = axes[0, 1]
        if history['accuracies']:
            ax.plot(history['epochs'], history['accuracies'], 'g-', alpha=0.7)
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Accuracy')
            ax.set_title(f'{module_name} - Accuracy Curve')
            ax.grid(True, alpha=0.3)
        
        # 3. Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÎÖ∏Î¶Ñ (ÏûàÎäî Í≤ΩÏö∞)
        ax = axes[1, 0]
        if history['gradients']:
            ax.plot(history['epochs'][:len(history['gradients'])], 
                   history['gradients'], 'r-', alpha=0.7)
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Gradient Norm')
            ax.set_title(f'{module_name} - Gradient Norm')
            ax.set_yscale('log')
            ax.grid(True, alpha=0.3)
        
        # 4. ÏÜêÏã§ Î≥ÄÌôîÏú® (validation Í∏∞Ï§Ä)
        ax = axes[1, 1]
        val_losses = history.get('val_losses', [])
        train_losses = history.get('train_losses', [])
        primary_losses = val_losses if val_losses else train_losses
        
        if len(primary_losses) > 1:
            loss_changes = np.diff(primary_losses)
            ax.plot(history['epochs'][1:], loss_changes, 'b-', alpha=0.7)
            ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Loss Change')
            ax.set_title(f'{module_name} - Loss Change Rate')
            ax.grid(True, alpha=0.3)
        
        plt.suptitle(f'Sweet Spot Analysis: {module_name}', fontsize=14)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=100, bbox_inches='tight')
            logger.info(f"üìä Î∂ÑÏÑù ÌîåÎ°Ø Ï†ÄÏû•: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    def _mann_kendall_test(self, data: List[float]) -> Dict:
        """Mann-Kendall Ìä∏Î†åÎìú ÌÖåÏä§Ìä∏"""
        n = len(data)
        s = 0
        
        for i in range(n-1):
            for j in range(i+1, n):
                s += np.sign(data[j] - data[i])
        
        # Calculate variance
        var_s = n * (n - 1) * (2 * n + 5) / 18
        
        if s > 0:
            z = (s - 1) / np.sqrt(var_s)
        elif s < 0:
            z = (s + 1) / np.sqrt(var_s)
        else:
            z = 0
        
        return {'statistic': z, 's': s}
    
    def _cusum_detection(self, data: List[float], threshold: float = None) -> List[int]:
        """CUSUM Î≥ÄÌôîÏ†ê ÌÉêÏßÄ"""
        if threshold is None:
            threshold = np.std(data) * 2
        
        mean = np.mean(data)
        cusum_pos = np.zeros(len(data))
        cusum_neg = np.zeros(len(data))
        changes = []
        
        for i in range(1, len(data)):
            cusum_pos[i] = max(0, cusum_pos[i-1] + data[i] - mean - threshold/2)
            cusum_neg[i] = max(0, cusum_neg[i-1] + mean - data[i] - threshold/2)
            
            if cusum_pos[i] > threshold or cusum_neg[i] > threshold:
                changes.append(i)
                cusum_pos[i] = 0
                cusum_neg[i] = 0
        
        return changes
    
    def statistical_plateau_detection(self, losses: List[float]) -> Dict:
        """Statistical Plateau Detection using Mann-Kendall and CUSUM"""
        if len(losses) < 5:
            return {'detected': False}
        
        # Mann-Kendall Trend Test
        mk_result = self._mann_kendall_test(losses)
        
        # CUSUM Change Detection
        cusum_changes = self._cusum_detection(losses)
        
        # Find plateau region
        plateau_start = None
        plateau_end = None
        
        # Plateau: Ìä∏Î†åÎìúÍ∞Ä ÏóÜÍ≥† Î≥ÄÌôîÏ†êÏù¥ ÏóÜÎäî Íµ¨Í∞Ñ
        for i in range(len(losses) - 5):
            window = losses[i:i+5]
            window_trend = self._mann_kendall_test(window)
            
            if abs(window_trend['statistic']) < 0.5:  # No significant trend
                if plateau_start is None:
                    plateau_start = i
                plateau_end = i + 5
        
        if plateau_start is not None:
            plateau_center = (plateau_start + plateau_end) // 2
            plateau_mean = np.mean(losses[plateau_start:plateau_end])
            plateau_std = np.std(losses[plateau_start:plateau_end])
            
            return {
                'detected': True,
                'start': plateau_start,
                'end': plateau_end,
                'center': plateau_center,
                'mean_loss': plateau_mean,
                'std': plateau_std,
                'mk_statistic': mk_result['statistic'],
                'cusum_changes': cusum_changes
            }
        
        return {'detected': False}
    
    def calculate_task_metrics(self, module: str, metrics: Dict) -> Dict:
        """Î™®ÎìàÎ≥Ñ Task-Specific Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞"""
        task_scores = {}
        
        if 'head' in module or module == 'heads':
            # Ìó§Îìú ÌÜµÌï© Ï†êÏàò
            task_scores['emotion_score'] = np.mean(metrics.get('emotion_f1', [0]))
            task_scores['bentham_score'] = 1.0 - np.mean(metrics.get('bentham_rmse', [1.0]))
            task_scores['regret_score'] = np.mean(metrics.get('regret_accuracy', [0]))
            task_scores['surd_score'] = np.mean(metrics.get('surd_pid_acc', [0]))
            task_scores['combined'] = np.mean(list(task_scores.values()))
            
        elif 'analyzer' in module:
            # Analyzer ÌäπÌôî Î©îÌä∏Î¶≠ (validation Ïö∞ÏÑ†)
            val_losses = metrics.get('val_losses', [])
            train_losses = metrics.get('train_losses', [])
            losses = val_losses if val_losses else train_losses
            task_scores['stability'] = 1.0 / (1.0 + np.std(losses if losses else [1.0]))
            task_scores['convergence'] = self._calculate_convergence_rate(losses)
            
        elif 'kalman' in module or 'dsp' in module:
            # DSP/Kalman ÌäπÌôî Î©îÌä∏Î¶≠
            task_scores['tracking_accuracy'] = 1.0 - np.mean(metrics.get('tracking_error', [1.0]))
            task_scores['filter_stability'] = 1.0 / (1.0 + np.std(metrics.get('filter_output', [1.0])))
        
        else:
            # Í∏∞Î≥∏ Î©îÌä∏Î¶≠
            val_losses = metrics.get('val_losses', [])
            train_losses = metrics.get('train_losses', [])
            losses = val_losses if val_losses else train_losses
            task_scores['accuracy'] = np.mean(metrics.get('val_accuracies', metrics.get('accuracies', [0])))
            task_scores['loss_improvement'] = self._calculate_improvement(losses)
        
        return task_scores
    
    def _calculate_convergence_rate(self, losses: List[float]) -> float:
        """ÏàòÎ†¥ ÏÜçÎèÑ Í≥ÑÏÇ∞"""
        if len(losses) < 2:
            return 0.0
        
        improvements = []
        for i in range(1, len(losses)):
            if losses[i-1] > 0:
                improvement = (losses[i-1] - losses[i]) / losses[i-1]
                improvements.append(improvement)
        
        return np.mean(improvements) if improvements else 0.0
    
    def _calculate_improvement(self, values: List[float]) -> float:
        """Í∞úÏÑ†ÎèÑ Í≥ÑÏÇ∞"""
        if len(values) < 2:
            return 0.0
        
        initial = np.mean(values[:3]) if len(values) >= 3 else values[0]
        final = np.mean(values[-3:]) if len(values) >= 3 else values[-1]
        
        if initial > 0:
            return (initial - final) / initial
        return 0.0
    
    def mcda_analysis(self, module: str, metrics: Dict) -> Dict:
        """Multi-Criteria Decision Analysis"""
        
        # Í∏∞Ï§ÄÎ≥Ñ Ï†êÏàò Í≥ÑÏÇ∞ (validation Ïö∞ÏÑ†)
        val_losses = metrics.get('val_losses', [])
        train_losses = metrics.get('train_losses', [])
        val_accs = metrics.get('val_accuracies', [])
        train_accs = metrics.get('train_accuracies', [])
        
        losses = val_losses if val_losses else train_losses
        accuracies = val_accs if val_accs else train_accs
        
        criteria = {
            'loss': 1.0 - np.array(losses if losses else [1.0]),  # Lower is better
            'accuracy': np.array(accuracies if accuracies else [0]),
            'stability': self._calculate_stability_scores(metrics),
            'gradient_health': self._calculate_gradient_health(metrics)
        }
        
        # Ï†ïÍ∑úÌôî (0-1 Î≤îÏúÑ)
        normalized = {}
        for key, values in criteria.items():
            if len(values) > 0 and np.std(values) > 0:
                normalized[key] = (values - np.min(values)) / (np.max(values) - np.min(values))
            else:
                normalized[key] = values
        
        # Í∞ÄÏ§ëÏπò
        weights = {
            'loss': 0.30,
            'accuracy': 0.40,
            'stability': 0.15,
            'gradient_health': 0.15
        }
        
        # MCDA Ï†êÏàò Í≥ÑÏÇ∞
        mcda_scores = np.zeros(len(metrics.get('epochs', [])))
        for key, weight in weights.items():
            if key in normalized and len(normalized[key]) == len(mcda_scores):
                mcda_scores += weight * normalized[key]
        
        # ÏµúÏ†Å epoch Ï∞æÍ∏∞
        best_epoch_idx = np.argmax(mcda_scores) if len(mcda_scores) > 0 else 0
        
        return {
            'scores': mcda_scores.tolist(),
            'best_epoch_idx': int(best_epoch_idx),
            'best_epoch': metrics.get('epochs', [])[best_epoch_idx] if best_epoch_idx < len(metrics.get('epochs', [])) else -1,
            'best_score': float(mcda_scores[best_epoch_idx]) if len(mcda_scores) > 0 else 0.0,
            'weights': weights
        }
    
    def _calculate_stability_scores(self, metrics: Dict) -> np.ndarray:
        """ÏïàÏ†ïÏÑ± Ï†êÏàò Í≥ÑÏÇ∞"""
        val_losses = metrics.get('val_losses', [])
        train_losses = metrics.get('train_losses', [])
        losses = val_losses if val_losses else train_losses
        if len(losses) < 3:
            return np.zeros(len(losses))
        
        stability_scores = []
        for i in range(len(losses)):
            start = max(0, i-2)
            end = min(len(losses), i+3)
            window = losses[start:end]
            
            # ÎÇÆÏùÄ Î∂ÑÏÇ∞ = ÎÜíÏùÄ ÏïàÏ†ïÏÑ±
            stability = 1.0 / (1.0 + np.std(window))
            stability_scores.append(stability)
        
        return np.array(stability_scores)
    
    def _calculate_gradient_health(self, metrics: Dict) -> np.ndarray:
        """Gradient Health Ï†êÏàò Í≥ÑÏÇ∞"""
        grad_norms = metrics.get('gradients', [])
        if not grad_norms:
            return np.zeros(len(metrics.get('epochs', [])))
        
        health_scores = []
        for norm in grad_norms:
            # GradientÍ∞Ä ÎÑàÎ¨¥ ÌÅ¨Í±∞ÎÇò ÏûëÏúºÎ©¥ Î∂àÍ±¥Ï†Ñ
            if norm > 0:
                if 0.001 < norm < 10.0:  # Í±¥Ï†ÑÌïú Î≤îÏúÑ
                    health = 1.0
                elif norm < 0.001:  # Vanishing
                    health = norm / 0.001
                else:  # Exploding
                    health = 10.0 / norm
            else:
                health = 0.0
            health_scores.append(health)
        
        return np.array(health_scores)
    
    def ensemble_voting(self, module: str, analyses: Dict) -> Dict:
        """Ïó¨Îü¨ Î∂ÑÏÑù Í∏∞Î≤ïÏùò ÏïôÏÉÅÎ∏î Ìà¨Ìëú"""
        candidates = {}
        
        # Í∞Å Í∏∞Î≤ïÏùò Ï∂îÏ≤ú ÏàòÏßë
        if analyses.get('plateau', {}).get('detected'):
            candidates['plateau'] = analyses['plateau']['center']
        
        if 'best_epoch_idx' in analyses.get('mcda', {}):
            candidates['mcda'] = analyses['mcda']['best_epoch_idx']
        
        # Task metric ÏµúÍ≥†Ï†ê
        task_scores = analyses.get('task_scores', {})
        if task_scores and 'combined' in task_scores:
            candidates['task'] = task_scores.get('best_idx', 0)
        
        # Minimum loss (validation Ïö∞ÏÑ†)
        module_history = self.module_histories.get(module, {})
        val_losses = module_history.get('val_losses', [])
        train_losses = module_history.get('train_losses', [])
        losses = val_losses if val_losses else train_losses
        if losses:
            candidates['min_loss'] = np.argmin(losses)
        
        # Ìà¨Ìëú ÏßëÍ≥Ñ
        if not candidates:
            return {'selected_epoch': -1, 'confidence': 0.0}
        
        # Í∞ÄÏû• ÎßéÏùÄ ÌëúÎ•º Î∞õÏùÄ epoch
        vote_counts = Counter(candidates.values())
        winner, votes = vote_counts.most_common(1)[0]
        
        # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
        confidence = votes / len(candidates) if candidates else 0.0
        
        epochs = self.module_histories.get(module, {}).get('epochs', [])
        
        return {
            'candidates': candidates,
            'selected_epoch_idx': int(winner),
            'selected_epoch': epochs[winner] if winner < len(epochs) else -1,
            'votes': votes,
            'total_voters': len(candidates),
            'confidence': float(confidence)
        }
    
    def analyze_all(self, output_dir: str = 'analysis_results') -> Dict:
        """
        ÌïôÏäµ ÏôÑÎ£å ÌõÑ Ï†ÑÏ≤¥ Î∂ÑÏÑù Ïã§Ìñâ
        
        Args:
            output_dir: Í≤∞Í≥º Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨
            
        Returns:
            Î∂ÑÏÑù Í≤∞Í≥º ÎîïÏÖîÎÑàÎ¶¨
        """
        logger.info("\n" + "=" * 70)
        logger.info("üéØ Sweet Spot Ï¢ÖÌï© Î∂ÑÏÑù ÏãúÏûë")
        logger.info("=" * 70)
        
        # ÎîîÎ≤ÑÍ∑∏: ÏàòÏßëÎêú Î©îÌä∏Î¶≠ ÌôïÏù∏
        logger.debug("üìä ÏàòÏßëÎêú Î©îÌä∏Î¶≠ ÌôïÏù∏:")
        for module_name, history in self.module_histories.items():
            train_losses = history.get('train_losses', [])
            val_losses = history.get('val_losses', [])
            
            if train_losses or val_losses:
                if val_losses:
                    logger.debug(f"  - {module_name}: {len(val_losses)}Í∞ú ÏóêÌè≠, "
                               f"Val: Ï≤´={val_losses[0]:.4f}, ÎßàÏßÄÎßâ={val_losses[-1]:.4f}")
                if train_losses:
                    logger.debug(f"    Train: Ï≤´={train_losses[0]:.4f}, ÎßàÏßÄÎßâ={train_losses[-1]:.4f}")
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)
        
        analysis_results = {}
        
        # Î™®ÎìàÎ≥Ñ Î∂ÑÏÑù
        for module_name in self.module_histories.keys():
            logger.info(f"\nüîç Î∂ÑÏÑù Ï§ë: {module_name}")
            
            metrics = self.module_histories[module_name]
            analyses = {}
            
            # 1. Statistical Plateau Detection (validation Ïö∞ÏÑ†)
            val_losses = metrics.get('val_losses', [])
            train_losses = metrics.get('train_losses', [])
            losses = val_losses if val_losses else train_losses
            analyses['plateau'] = self.statistical_plateau_detection(losses)
            
            # 2. Task-Specific Metrics
            task_scores = self.calculate_task_metrics(module_name, metrics)
            analyses['task_scores'] = task_scores
            
            # 3. MCDA
            analyses['mcda'] = self.mcda_analysis(module_name, metrics)
            
            # 4. Ensemble Voting
            analyses['voting'] = self.ensemble_voting(module_name, analyses)
            
            # Ï¢ÖÌï©
            result = {
                'module': module_name,
                'metrics': metrics,
                'analyses': analyses,
                'recommendation': {
                    'epoch_idx': analyses['voting']['selected_epoch_idx'],
                    'epoch': analyses['voting']['selected_epoch'],
                    'confidence': analyses['voting']['confidence'],
                    'reasoning': self._generate_reasoning(module_name, analyses)
                }
            }
            
            analysis_results[module_name] = result
        
        # ÏãúÍ∞ÅÌôî ÏÉùÏÑ±
        self._generate_visualizations(analysis_results, output_path)
        
        # Í≤∞Í≥º Ï†ÄÏû•
        self._save_analysis_results(analysis_results, output_path)
        
        logger.info("\n" + "=" * 70)
        logger.info("‚úÖ Sweet Spot Î∂ÑÏÑù ÏôÑÎ£å!")
        logger.info(f"üìÅ Í≤∞Í≥º Ï†ÄÏû• ÏúÑÏπò: {output_path}")
        logger.info("=" * 70)
        
        # ÏöîÏïΩ Ï∂úÎ†•
        print("\nüìä Sweet Spot Recommendations:")
        print("-" * 50)
        for module, result in analysis_results.items():
            rec = result['recommendation']
            print(f"{module:20s}: Epoch {rec['epoch']:3d} (Confidence: {rec['confidence']:.1%})")
        print("-" * 50)
        
        return analysis_results
    
    def _generate_reasoning(self, module: str, analyses: Dict) -> List[str]:
        """Ï∂îÏ≤ú Í∑ºÍ±∞ ÏÉùÏÑ±"""
        reasons = []
        
        if analyses.get('plateau', {}).get('detected'):
            plateau = analyses['plateau']
            reasons.append(f"Plateau Íµ¨Í∞Ñ ÌÉêÏßÄ (Epoch {plateau['start']}-{plateau['end']})")
        
        if analyses.get('mcda', {}).get('best_score', 0) > 0.8:
            reasons.append(f"MCDA Ï†êÏàò Ïö∞Ïàò ({analyses['mcda']['best_score']:.3f})")
        
        if analyses.get('voting', {}).get('confidence', 0) > 0.6:
            reasons.append(f"ÎÜíÏùÄ Ìà¨Ìëú Ïã†Î¢∞ÎèÑ ({analyses['voting']['confidence']:.1%})")
        
        task_scores = analyses.get('task_scores', {})
        if task_scores.get('combined', 0) > 0.7:
            reasons.append(f"Task Î©îÌä∏Î¶≠ Ïö∞Ïàò ({task_scores['combined']:.3f})")
        
        return reasons
    
    def _generate_visualizations(self, analysis_results: Dict, output_path: Path):
        """ÏãúÍ∞ÅÌôî ÏÉùÏÑ±"""
        logger.info("\nüìà ÏãúÍ∞ÅÌôî ÏÉùÏÑ± Ï§ë...")
        
        viz_dir = output_path / 'visualizations'
        viz_dir.mkdir(exist_ok=True)
        
        for module, result in analysis_results.items():
            self._plot_module_analysis(module, result, viz_dir)
        
        # Ï¢ÖÌï© ÌûàÌä∏Îßµ
        self._plot_summary_heatmap(analysis_results, viz_dir)
    
    def _plot_module_analysis(self, module: str, result: Dict, viz_dir: Path):
        """Î™®ÎìàÎ≥Ñ Î∂ÑÏÑù ÏãúÍ∞ÅÌôî"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'{module} Sweet Spot Analysis', fontsize=16)
            
            metrics = result['metrics']
            analyses = result['analyses']
            
            # 1. Loss curve with plateau
            ax = axes[0, 0]
            epochs = metrics.get('epochs', [])
            val_losses = metrics.get('val_losses', [])
            train_losses = metrics.get('train_losses', [])
            losses = val_losses if val_losses else train_losses
            
            if epochs and losses:
                ax.plot(epochs, losses, 'b-', label='Training Loss')
                
                if analyses.get('plateau', {}).get('detected'):
                    plateau = analyses['plateau']
                    ax.axvspan(epochs[plateau['start']], epochs[plateau['end']], 
                              alpha=0.3, color='green', label='Plateau')
                    ax.axvline(epochs[plateau['center']], color='red', 
                              linestyle='--', label='Plateau Center')
            
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Loss')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # 2. MCDA Scores
            ax = axes[0, 1]
            mcda_scores = analyses.get('mcda', {}).get('scores', [])
            
            if epochs and mcda_scores:
                ax.plot(epochs[:len(mcda_scores)], mcda_scores, 'g-', label='MCDA Score')
                best_idx = analyses.get('mcda', {}).get('best_epoch_idx', 0)
                if best_idx < len(epochs) and best_idx < len(mcda_scores):
                    ax.scatter(epochs[best_idx], mcda_scores[best_idx], 
                              color='red', s=100, label='Best MCDA')
            
            ax.set_xlabel('Epoch')
            ax.set_ylabel('MCDA Score')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # 3. Accuracies
            ax = axes[1, 0]
            accuracies = metrics.get('accuracies', [])
            
            if epochs and accuracies:
                ax.plot(epochs[:len(accuracies)], accuracies, label='Accuracy')
            
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Accuracy')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # 4. Voting Results
            ax = axes[1, 1]
            voting = analyses.get('voting', {})
            
            if voting.get('candidates'):
                candidates = list(voting['candidates'].keys())
                values = list(voting['candidates'].values())
                colors = ['green' if v == voting['selected_epoch_idx'] else 'blue' for v in values]
                ax.bar(candidates, values, color=colors)
                ax.set_xlabel('Analysis Method')
                ax.set_ylabel('Recommended Epoch Index')
                ax.set_title(f"Final: Epoch {voting.get('selected_epoch', -1)} (Confidence: {voting.get('confidence', 0):.1%})")
            
            plt.tight_layout()
            plt.savefig(viz_dir / f'{module}_analysis.png', dpi=150)
            plt.close()
            
        except Exception as e:
            logger.warning(f"ÏãúÍ∞ÅÌôî ÏÉùÏÑ± Ïã§Ìå® ({module}): {e}")
    
    def _plot_summary_heatmap(self, analysis_results: Dict, viz_dir: Path):
        """Ï¢ÖÌï© ÌûàÌä∏Îßµ"""
        try:
            modules = []
            recommended_epochs = []
            confidences = []
            
            for module, result in analysis_results.items():
                modules.append(module)
                recommended_epochs.append(result['recommendation']['epoch'])
                confidences.append(result['recommendation']['confidence'])
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # Epoch recommendations
            ax1.barh(modules, recommended_epochs, color='steelblue')
            ax1.set_xlabel('Recommended Epoch')
            ax1.set_title('Sweet Spot Epochs by Module')
            ax1.grid(True, alpha=0.3)
            
            # Confidence scores
            ax2.barh(modules, confidences, color='coral')
            ax2.set_xlabel('Confidence Score')
            ax2.set_title('Recommendation Confidence')
            ax2.set_xlim([0, 1])
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(viz_dir / 'summary_sweetspots.png', dpi=150)
            plt.close()
            
        except Exception as e:
            logger.warning(f"Ï¢ÖÌï© ÌûàÌä∏Îßµ ÏÉùÏÑ± Ïã§Ìå®: {e}")
    
    def _save_analysis_results(self, analysis_results: Dict, output_path: Path):
        """Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•"""
        # JSON Ï†ÄÏû•
        json_path = output_path / 'sweet_spot_analysis.json'
        with open(json_path, 'w') as f:
            json.dump(analysis_results, f, indent=2, default=str)
        logger.info(f"üìÅ JSON Í≤∞Í≥º Ï†ÄÏû•: {json_path}")
        
        # Markdown Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
        self._generate_markdown_report(analysis_results, output_path)
    
    def _generate_markdown_report(self, analysis_results: Dict, output_path: Path):
        """Markdown ÌòïÏãù Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±"""
        report_path = output_path / 'sweet_spot_report.md'
        
        with open(report_path, 'w') as f:
            f.write("# üéØ Sweet Spot Analysis Report\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Summary table
            f.write("## üìä Summary\n\n")
            f.write("| Module | Recommended Epoch | Confidence | Key Reasoning |\n")
            f.write("|--------|------------------|------------|---------------|\n")
            
            for module, result in analysis_results.items():
                rec = result['recommendation']
                reasons = ', '.join(rec['reasoning'][:2]) if rec['reasoning'] else 'N/A'
                f.write(f"| {module} | {rec['epoch']} | {rec['confidence']:.1%} | {reasons} |\n")
            
            # Detailed analysis
            f.write("\n## üîç Detailed Analysis\n\n")
            
            for module, result in analysis_results.items():
                f.write(f"### Module: {module}\n\n")
                
                analyses = result['analyses']
                
                # Plateau
                if analyses.get('plateau', {}).get('detected'):
                    plateau = analyses['plateau']
                    f.write(f"**Plateau Detection:**\n")
                    f.write(f"- Range: Epoch {plateau['start']}-{plateau['end']}\n")
                    f.write(f"- Center: Epoch {plateau['center']}\n")
                    f.write(f"- Mean Loss: {plateau['mean_loss']:.4f} (¬±{plateau['std']:.4f})\n\n")
                else:
                    f.write("**Plateau Detection:** Not detected\n\n")
                
                # MCDA
                mcda = analyses.get('mcda', {})
                f.write(f"**MCDA Analysis:**\n")
                f.write(f"- Best Epoch: {mcda.get('best_epoch', -1)}\n")
                f.write(f"- Best Score: {mcda.get('best_score', 0):.3f}\n\n")
                
                # Voting
                voting = analyses.get('voting', {})
                f.write(f"**Ensemble Voting:**\n")
                f.write(f"- Selected: Epoch {voting.get('selected_epoch', -1)}\n")
                f.write(f"- Confidence: {voting.get('confidence', 0):.1%}\n")
                f.write(f"- Votes: {voting.get('votes', 0)}/{voting.get('total_voters', 0)}\n\n")
                
                f.write("---\n\n")
            
            # Threshold recommendations
            f.write("## üéØ Recommended Thresholds for Automation\n\n")
            f.write("```python\n")
            f.write("# Based on empirical analysis\n")
            f.write("thresholds = {\n")
            
            # Calculate empirical thresholds
            all_plateau_stds = []
            for result in analysis_results.values():
                if result['analyses'].get('plateau', {}).get('detected'):
                    all_plateau_stds.append(result['analyses']['plateau']['std'])
            
            if all_plateau_stds:
                f.write(f"    'plateau_variance': {np.mean(all_plateau_stds):.4f},\n")
            else:
                f.write(f"    'plateau_variance': 0.01,  # Default\n")
            
            f.write("    'stability_window': 5,\n")
            f.write("    'mcda_weights': {\n")
            f.write("        'loss': 0.30,\n")
            f.write("        'accuracy': 0.40,\n")
            f.write("        'stability': 0.15,\n")
            f.write("        'gradient_health': 0.15\n")
            f.write("    },\n")
            
            # Average confidence
            avg_confidence = np.mean([r['recommendation']['confidence'] 
                                     for r in analysis_results.values()])
            f.write(f"    'min_confidence': {avg_confidence * 0.8:.2f}\n")
            f.write("}\n```\n\n")
            
            # Next steps
            f.write("## üìù Next Steps\n\n")
            f.write("1. Review the recommendations above\n")
            f.write("2. Manually combine modules using recommended epochs\n")
            f.write("3. Evaluate combined model performance\n")
            f.write("4. Adjust thresholds based on results\n")
            f.write("5. Enable automated sweet spot detection\n")
        
        logger.info(f"üìÑ Markdown Î¶¨Ìè¨Ìä∏ Ï†ÄÏû•: {report_path}")
    
    def export_analysis(self, output_dir: str = "training/sweet_spot_analysis"):
        """
        Ï†ÑÏ≤¥ Î∂ÑÏÑù Í≤∞Í≥º ÎÇ¥Î≥¥ÎÇ¥Í∏∞
        
        Args:
            output_dir: Ï∂úÎ†• ÎîîÎ†âÌÜ†Î¶¨
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. JSON ÌòïÏãùÏúºÎ°ú Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
        analysis_data = {
            'timestamp': timestamp,
            'sweet_spots': self.sweet_spots,
            'convergence_points': self.convergence_points,
            'overfitting_points': self.overfitting_points,
            'optimal_epochs': self.get_optimal_epochs(),
            'module_summaries': {}
        }
        
        for module_name in self.module_histories.keys():
            analysis_data['module_summaries'][module_name] = self.get_module_status(module_name)
        
        json_file = output_dir / f"sweet_spot_analysis_{timestamp}.json"
        with open(json_file, 'w') as f:
            json.dump(analysis_data, f, indent=2)
        
        logger.info(f"üìä Sweet Spot Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•: {json_file}")
        
        # 2. Í∞Å Î™®ÎìàÎ≥Ñ ÌîåÎ°Ø ÏÉùÏÑ±
        plots_dir = output_dir / "plots"
        plots_dir.mkdir(exist_ok=True)
        
        for module_name in self.module_histories.keys():
            plot_file = plots_dir / f"{module_name}_{timestamp}.png"
            self.plot_module_analysis(module_name, str(plot_file))
        
        # 3. ÏöîÏïΩ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
        report_file = output_dir / f"sweet_spot_report_{timestamp}.txt"
        with open(report_file, 'w') as f:
            f.write("=" * 60 + "\n")
            f.write("Sweet Spot Analysis Report\n")
            f.write(f"Generated: {timestamp}\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("Optimal Epochs by Module:\n")
            f.write("-" * 30 + "\n")
            for module, epoch in self.get_optimal_epochs().items():
                f.write(f"  {module}: Epoch {epoch}\n")
            
            f.write("\n")
            f.write("Sweet Spots Detected:\n")
            f.write("-" * 30 + "\n")
            for module, info in self.sweet_spots.items():
                f.write(f"  {module}:\n")
                f.write(f"    - Epoch: {info['epoch']}\n")
                f.write(f"    - Loss: {info['loss']:.4f} (¬±{info['std']:.4f})\n")
                f.write(f"    - Stable: {info['stable']}\n")
                f.write(f"    - Converged: {info['converged']}\n")
            
            f.write("\n")
            f.write("Convergence Points:\n")
            f.write("-" * 30 + "\n")
            for module, info in self.convergence_points.items():
                f.write(f"  {module}: Epoch {info['epoch']} (Loss: {info['loss']:.4f})\n")
            
            if self.overfitting_points:
                f.write("\n")
                f.write("‚ö†Ô∏è Overfitting Warnings:\n")
                f.write("-" * 30 + "\n")
                for module, info in self.overfitting_points.items():
                    f.write(f"  {module}: Started at epoch {info['epoch']}\n")
        
        logger.info(f"üìÑ Sweet Spot Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±: {report_file}")
        
        return {
            'json_file': str(json_file),
            'report_file': str(report_file),
            'plots_dir': str(plots_dir)
        }