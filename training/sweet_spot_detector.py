"""
Sweet Spot Detector - ëª¨ë“ˆë³„ ìµœì  ì—í­ íƒì§€ ì‹œìŠ¤í…œ
ê° ëª¨ë“ˆì´ ìµœê³  ì„±ëŠ¥ì„ ë³´ì´ëŠ” ì—í­ì„ ìë™ìœ¼ë¡œ íƒì§€
"""

import numpy as np
import torch
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
import json
import logging
from datetime import datetime
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.signal import find_peaks
import pandas as pd

logger = logging.getLogger(__name__)


class SweetSpotDetector:
    """
    Sweet Spot íƒì§€ ì‹œìŠ¤í…œ
    - ëª¨ë“ˆë³„ ìµœì  ì„±ëŠ¥ ì—í­ íƒì§€
    - ìˆ˜ë ´ íŒ¨í„´ ë¶„ì„
    - ê³¼ì í•© ì‹œì  ê°ì§€
    - ì•ˆì •ì„± í‰ê°€
    """
    
    def __init__(self,
                 window_size: int = 5,
                 stability_threshold: float = 0.01,
                 patience: int = 10,
                 min_epochs: int = 10):
        """
        Args:
            window_size: ì´ë™ í‰ê·  ìœˆë„ìš° í¬ê¸°
            stability_threshold: ì•ˆì •ì„± íŒë‹¨ ì„ê³„ê°’
            patience: ì„±ëŠ¥ ê°œì„  ì—†ì´ ê¸°ë‹¤ë¦¬ëŠ” ì—í­ ìˆ˜
            min_epochs: ìµœì†Œ í•™ìŠµ ì—í­ (ì´ì „ì—ëŠ” Sweet Spot íŒë‹¨ ì•ˆí•¨)
        """
        self.window_size = window_size
        self.stability_threshold = stability_threshold
        self.patience = patience
        self.min_epochs = min_epochs
        
        # ëª¨ë“ˆë³„ ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬
        self.module_histories = defaultdict(lambda: {
            'losses': [],
            'accuracies': [],
            'epochs': [],
            'gradients': [],
            'learning_rates': []
        })
        
        # Sweet Spot ì •ë³´
        self.sweet_spots = {}
        self.convergence_points = {}
        self.overfitting_points = {}
        
        logger.info("âœ… Sweet Spot Detector ì´ˆê¸°í™”")
        logger.info(f"  - ìœˆë„ìš° í¬ê¸°: {window_size}")
        logger.info(f"  - ì•ˆì •ì„± ì„ê³„ê°’: {stability_threshold}")
        logger.info(f"  - Patience: {patience}")
    
    def update(self, 
               epoch: int,
               module_metrics: Dict[str, Dict[str, float]],
               learning_rate: float = None):
        """
        ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ë° Sweet Spot íƒì§€
        
        Args:
            epoch: í˜„ì¬ ì—í­
            module_metrics: ëª¨ë“ˆë³„ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
            learning_rate: í˜„ì¬ í•™ìŠµë¥ 
        """
        for module_name, metrics in module_metrics.items():
            history = self.module_histories[module_name]
            
            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            history['epochs'].append(epoch)
            history['losses'].append(metrics.get('loss', 0))
            history['accuracies'].append(metrics.get('accuracy', 0))
            
            if 'gradient_norm' in metrics:
                history['gradients'].append(metrics['gradient_norm'])
            
            if learning_rate:
                history['learning_rates'].append(learning_rate)
            
            # Sweet Spot íƒì§€ (ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìŒ“ì¸ í›„)
            if epoch >= self.min_epochs:
                self._detect_sweet_spot(module_name, epoch)
                self._detect_convergence(module_name, epoch)
                self._detect_overfitting(module_name, epoch)
    
    def _detect_sweet_spot(self, module_name: str, epoch: int):
        """ëª¨ë“ˆë³„ Sweet Spot íƒì§€"""
        history = self.module_histories[module_name]
        losses = history['losses']
        
        if len(losses) < self.window_size:
            return
        
        # ìµœê·¼ ìœˆë„ìš°ì˜ ì†ì‹¤
        recent_losses = losses[-self.window_size:]
        
        # ì¡°ê±´ 1: ë‚®ì€ ì†ì‹¤
        avg_loss = np.mean(recent_losses)
        
        # ì¡°ê±´ 2: ì•ˆì •ì„± (ë‚®ì€ ë¶„ì‚°)
        loss_std = np.std(recent_losses)
        is_stable = loss_std < self.stability_threshold
        
        # ì¡°ê±´ 3: ìˆ˜ë ´ (ì†ì‹¤ ê°ì†Œìœ¨ì´ ë‚®ìŒ)
        if len(losses) >= self.window_size * 2:
            prev_window = losses[-self.window_size*2:-self.window_size]
            improvement = (np.mean(prev_window) - avg_loss) / np.mean(prev_window)
            is_converged = abs(improvement) < 0.01  # 1% ë¯¸ë§Œ ê°œì„ 
        else:
            is_converged = False
        
        # ì¡°ê±´ 4: ê³¼ì í•© ì—†ìŒ (ê²€ì¦ ì†ì‹¤ì´ ì¦ê°€í•˜ì§€ ì•ŠìŒ)
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” val_lossë„ ì¶”ì  í•„ìš”
        is_not_overfitting = True  # í˜„ì¬ëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬
        
        # Sweet Spot íŒë‹¨
        if is_stable and (is_converged or avg_loss < 0.1):
            # ì´ì „ Sweet Spotë³´ë‹¤ ë‚˜ì€ì§€ í™•ì¸
            if module_name not in self.sweet_spots or \
               avg_loss < self.sweet_spots[module_name]['loss']:
                
                self.sweet_spots[module_name] = {
                    'epoch': epoch,
                    'loss': avg_loss,
                    'std': loss_std,
                    'stable': is_stable,
                    'converged': is_converged
                }
                
                logger.info(f"  ğŸ¯ Sweet Spot ë°œê²¬: {module_name}")
                logger.info(f"     - ì—í­: {epoch}")
                logger.info(f"     - ì†ì‹¤: {avg_loss:.4f} (Â±{loss_std:.4f})")
    
    def _detect_convergence(self, module_name: str, epoch: int):
        """ìˆ˜ë ´ ì‹œì  íƒì§€"""
        history = self.module_histories[module_name]
        losses = history['losses']
        
        if len(losses) < self.patience:
            return
        
        # ìµœê·¼ patience ì—í­ ë™ì•ˆì˜ ê°œì„  í™•ì¸
        recent_losses = losses[-self.patience:]
        best_recent = min(recent_losses)
        
        # ê°œì„ ì´ ê±°ì˜ ì—†ìœ¼ë©´ ìˆ˜ë ´ìœ¼ë¡œ íŒë‹¨
        improvements = []
        for i in range(1, len(recent_losses)):
            if recent_losses[i-1] > 0:
                improvement = (recent_losses[i-1] - recent_losses[i]) / recent_losses[i-1]
                improvements.append(improvement)
        
        avg_improvement = np.mean(improvements) if improvements else 0
        
        if abs(avg_improvement) < 0.001:  # 0.1% ë¯¸ë§Œ ê°œì„ 
            if module_name not in self.convergence_points:
                self.convergence_points[module_name] = {
                    'epoch': epoch,
                    'loss': best_recent,
                    'improvement_rate': avg_improvement
                }
                logger.info(f"  ğŸ“Š ìˆ˜ë ´ ê°ì§€: {module_name} @ epoch {epoch}")
    
    def _detect_overfitting(self, module_name: str, epoch: int):
        """ê³¼ì í•© ì‹œì  íƒì§€"""
        history = self.module_histories[module_name]
        losses = history['losses']
        
        # ì‹¤ì œë¡œëŠ” train/val loss ë¹„êµ í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
        if len(losses) >= self.window_size * 3:
            # ì†ì‹¤ì´ ë‹¤ì‹œ ì¦ê°€í•˜ê¸° ì‹œì‘í•˜ë©´ ê³¼ì í•© ì˜ì‹¬
            recent = np.mean(losses[-self.window_size:])
            previous = np.mean(losses[-self.window_size*2:-self.window_size])
            
            if recent > previous * 1.05:  # 5% ì´ìƒ ì¦ê°€
                if module_name not in self.overfitting_points:
                    self.overfitting_points[module_name] = {
                        'epoch': epoch - self.window_size,  # ì¦ê°€ ì‹œì‘ ì‹œì 
                        'loss_increase': (recent - previous) / previous
                    }
                    logger.warning(f"  âš ï¸ ê³¼ì í•© ê°ì§€: {module_name} @ epoch {epoch - self.window_size}")
    
    def get_optimal_epochs(self) -> Dict[str, int]:
        """
        ê° ëª¨ë“ˆì˜ ìµœì  ì—í­ ë°˜í™˜
        
        Returns:
            ëª¨ë“ˆë³„ ìµœì  ì—í­ ë”•ì…”ë„ˆë¦¬
        """
        optimal_epochs = {}
        
        for module_name in self.module_histories.keys():
            # Sweet Spotì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
            if module_name in self.sweet_spots:
                optimal_epochs[module_name] = self.sweet_spots[module_name]['epoch']
            # ìˆ˜ë ´ì ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
            elif module_name in self.convergence_points:
                optimal_epochs[module_name] = self.convergence_points[module_name]['epoch']
            # ê³¼ì í•© ì§ì „ ì‚¬ìš©
            elif module_name in self.overfitting_points:
                optimal_epochs[module_name] = max(1, self.overfitting_points[module_name]['epoch'] - 1)
            # ê¸°ë³¸ê°’: ìµœì € ì†ì‹¤ ì—í­
            else:
                losses = self.module_histories[module_name]['losses']
                if losses:
                    optimal_epochs[module_name] = losses.index(min(losses)) + 1
        
        return optimal_epochs
    
    def get_module_status(self, module_name: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ëª¨ë“ˆì˜ í˜„ì¬ ìƒíƒœ ë°˜í™˜
        
        Args:
            module_name: ëª¨ë“ˆ ì´ë¦„
            
        Returns:
            ëª¨ë“ˆ ìƒíƒœ ì •ë³´
        """
        if module_name not in self.module_histories:
            return {'status': 'not_found'}
        
        history = self.module_histories[module_name]
        status = {
            'total_epochs': len(history['epochs']),
            'current_loss': history['losses'][-1] if history['losses'] else None,
            'best_loss': min(history['losses']) if history['losses'] else None,
            'best_epoch': history['losses'].index(min(history['losses'])) + 1 if history['losses'] else None
        }
        
        # Sweet Spot ì •ë³´
        if module_name in self.sweet_spots:
            status['sweet_spot'] = self.sweet_spots[module_name]
        
        # ìˆ˜ë ´ ì •ë³´
        if module_name in self.convergence_points:
            status['convergence'] = self.convergence_points[module_name]
        
        # ê³¼ì í•© ì •ë³´
        if module_name in self.overfitting_points:
            status['overfitting'] = self.overfitting_points[module_name]
        
        return status
    
    def plot_module_analysis(self, module_name: str, save_path: Optional[str] = None):
        """
        ëª¨ë“ˆ ë¶„ì„ ê²°ê³¼ ì‹œê°í™”
        
        Args:
            module_name: ëª¨ë“ˆ ì´ë¦„
            save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ í‘œì‹œë§Œ)
        """
        if module_name not in self.module_histories:
            logger.warning(f"ëª¨ë“ˆ {module_name}ì˜ íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        history = self.module_histories[module_name]
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. ì†ì‹¤ ê³¡ì„ 
        ax = axes[0, 0]
        ax.plot(history['epochs'], history['losses'], 'b-', label='Loss', alpha=0.7)
        
        # Sweet Spot í‘œì‹œ
        if module_name in self.sweet_spots:
            spot = self.sweet_spots[module_name]
            ax.axvline(x=spot['epoch'], color='g', linestyle='--', label=f"Sweet Spot (epoch {spot['epoch']})")
        
        # ìˆ˜ë ´ì  í‘œì‹œ
        if module_name in self.convergence_points:
            conv = self.convergence_points[module_name]
            ax.axvline(x=conv['epoch'], color='orange', linestyle='--', label=f"Convergence (epoch {conv['epoch']})")
        
        # ê³¼ì í•© ì‹œì  í‘œì‹œ
        if module_name in self.overfitting_points:
            overfit = self.overfitting_points[module_name]
            ax.axvline(x=overfit['epoch'], color='r', linestyle='--', label=f"Overfitting (epoch {overfit['epoch']})")
        
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.set_title(f'{module_name} - Loss Curve')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. ì •í™•ë„ ê³¡ì„  (ìˆëŠ” ê²½ìš°)
        ax = axes[0, 1]
        if history['accuracies']:
            ax.plot(history['epochs'], history['accuracies'], 'g-', alpha=0.7)
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Accuracy')
            ax.set_title(f'{module_name} - Accuracy Curve')
            ax.grid(True, alpha=0.3)
        
        # 3. ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ (ìˆëŠ” ê²½ìš°)
        ax = axes[1, 0]
        if history['gradients']:
            ax.plot(history['epochs'][:len(history['gradients'])], 
                   history['gradients'], 'r-', alpha=0.7)
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Gradient Norm')
            ax.set_title(f'{module_name} - Gradient Norm')
            ax.set_yscale('log')
            ax.grid(True, alpha=0.3)
        
        # 4. ì†ì‹¤ ë³€í™”ìœ¨
        ax = axes[1, 1]
        if len(history['losses']) > 1:
            loss_changes = np.diff(history['losses'])
            ax.plot(history['epochs'][1:], loss_changes, 'b-', alpha=0.7)
            ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Loss Change')
            ax.set_title(f'{module_name} - Loss Change Rate')
            ax.grid(True, alpha=0.3)
        
        plt.suptitle(f'Sweet Spot Analysis: {module_name}', fontsize=14)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=100, bbox_inches='tight')
            logger.info(f"ğŸ“Š ë¶„ì„ í”Œë¡¯ ì €ì¥: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    def _mann_kendall_test(self, data: List[float]) -> Dict:
        """Mann-Kendall íŠ¸ë Œë“œ í…ŒìŠ¤íŠ¸"""
        n = len(data)
        s = 0
        
        for i in range(n-1):
            for j in range(i+1, n):
                s += np.sign(data[j] - data[i])
        
        # Calculate variance
        var_s = n * (n - 1) * (2 * n + 5) / 18
        
        if s > 0:
            z = (s - 1) / np.sqrt(var_s)
        elif s < 0:
            z = (s + 1) / np.sqrt(var_s)
        else:
            z = 0
        
        return {'statistic': z, 's': s}
    
    def _cusum_detection(self, data: List[float], threshold: float = None) -> List[int]:
        """CUSUM ë³€í™”ì  íƒì§€"""
        if threshold is None:
            threshold = np.std(data) * 2
        
        mean = np.mean(data)
        cusum_pos = np.zeros(len(data))
        cusum_neg = np.zeros(len(data))
        changes = []
        
        for i in range(1, len(data)):
            cusum_pos[i] = max(0, cusum_pos[i-1] + data[i] - mean - threshold/2)
            cusum_neg[i] = max(0, cusum_neg[i-1] + mean - data[i] - threshold/2)
            
            if cusum_pos[i] > threshold or cusum_neg[i] > threshold:
                changes.append(i)
                cusum_pos[i] = 0
                cusum_neg[i] = 0
        
        return changes
    
    def statistical_plateau_detection(self, losses: List[float]) -> Dict:
        """Statistical Plateau Detection using Mann-Kendall and CUSUM"""
        if len(losses) < 5:
            return {'detected': False}
        
        # Mann-Kendall Trend Test
        mk_result = self._mann_kendall_test(losses)
        
        # CUSUM Change Detection
        cusum_changes = self._cusum_detection(losses)
        
        # Find plateau region
        plateau_start = None
        plateau_end = None
        
        # Plateau: íŠ¸ë Œë“œê°€ ì—†ê³  ë³€í™”ì ì´ ì—†ëŠ” êµ¬ê°„
        for i in range(len(losses) - 5):
            window = losses[i:i+5]
            window_trend = self._mann_kendall_test(window)
            
            if abs(window_trend['statistic']) < 0.5:  # No significant trend
                if plateau_start is None:
                    plateau_start = i
                plateau_end = i + 5
        
        if plateau_start is not None:
            plateau_center = (plateau_start + plateau_end) // 2
            plateau_mean = np.mean(losses[plateau_start:plateau_end])
            plateau_std = np.std(losses[plateau_start:plateau_end])
            
            return {
                'detected': True,
                'start': plateau_start,
                'end': plateau_end,
                'center': plateau_center,
                'mean_loss': plateau_mean,
                'std': plateau_std,
                'mk_statistic': mk_result['statistic'],
                'cusum_changes': cusum_changes
            }
        
        return {'detected': False}
    
    def calculate_task_metrics(self, module: str, metrics: Dict) -> Dict:
        """ëª¨ë“ˆë³„ Task-Specific ë©”íŠ¸ë¦­ ê³„ì‚°"""
        task_scores = {}
        
        if 'head' in module or module == 'heads':
            # í—¤ë“œ í†µí•© ì ìˆ˜
            task_scores['emotion_score'] = np.mean(metrics.get('emotion_f1', [0]))
            task_scores['bentham_score'] = 1.0 - np.mean(metrics.get('bentham_rmse', [1.0]))
            task_scores['regret_score'] = np.mean(metrics.get('regret_accuracy', [0]))
            task_scores['surd_score'] = np.mean(metrics.get('surd_pid_acc', [0]))
            task_scores['combined'] = np.mean(list(task_scores.values()))
            
        elif 'analyzer' in module:
            # Analyzer íŠ¹í™” ë©”íŠ¸ë¦­
            task_scores['stability'] = 1.0 / (1.0 + np.std(metrics.get('losses', [1.0])))
            task_scores['convergence'] = self._calculate_convergence_rate(metrics.get('losses', []))
            
        elif 'kalman' in module or 'dsp' in module:
            # DSP/Kalman íŠ¹í™” ë©”íŠ¸ë¦­
            task_scores['tracking_accuracy'] = 1.0 - np.mean(metrics.get('tracking_error', [1.0]))
            task_scores['filter_stability'] = 1.0 / (1.0 + np.std(metrics.get('filter_output', [1.0])))
        
        else:
            # ê¸°ë³¸ ë©”íŠ¸ë¦­
            task_scores['accuracy'] = np.mean(metrics.get('accuracies', [0]))
            task_scores['loss_improvement'] = self._calculate_improvement(metrics.get('losses', []))
        
        return task_scores
    
    def _calculate_convergence_rate(self, losses: List[float]) -> float:
        """ìˆ˜ë ´ ì†ë„ ê³„ì‚°"""
        if len(losses) < 2:
            return 0.0
        
        improvements = []
        for i in range(1, len(losses)):
            if losses[i-1] > 0:
                improvement = (losses[i-1] - losses[i]) / losses[i-1]
                improvements.append(improvement)
        
        return np.mean(improvements) if improvements else 0.0
    
    def _calculate_improvement(self, values: List[float]) -> float:
        """ê°œì„ ë„ ê³„ì‚°"""
        if len(values) < 2:
            return 0.0
        
        initial = np.mean(values[:3]) if len(values) >= 3 else values[0]
        final = np.mean(values[-3:]) if len(values) >= 3 else values[-1]
        
        if initial > 0:
            return (initial - final) / initial
        return 0.0
    
    def mcda_analysis(self, module: str, metrics: Dict) -> Dict:
        """Multi-Criteria Decision Analysis"""
        
        # ê¸°ì¤€ë³„ ì ìˆ˜ ê³„ì‚°
        criteria = {
            'loss': 1.0 - np.array(metrics.get('losses', [1.0])),  # Lower is better
            'accuracy': np.array(metrics.get('accuracies', [0])),
            'stability': self._calculate_stability_scores(metrics),
            'gradient_health': self._calculate_gradient_health(metrics)
        }
        
        # ì •ê·œí™” (0-1 ë²”ìœ„)
        normalized = {}
        for key, values in criteria.items():
            if len(values) > 0 and np.std(values) > 0:
                normalized[key] = (values - np.min(values)) / (np.max(values) - np.min(values))
            else:
                normalized[key] = values
        
        # ê°€ì¤‘ì¹˜
        weights = {
            'loss': 0.30,
            'accuracy': 0.40,
            'stability': 0.15,
            'gradient_health': 0.15
        }
        
        # MCDA ì ìˆ˜ ê³„ì‚°
        mcda_scores = np.zeros(len(metrics.get('epochs', [])))
        for key, weight in weights.items():
            if key in normalized and len(normalized[key]) == len(mcda_scores):
                mcda_scores += weight * normalized[key]
        
        # ìµœì  epoch ì°¾ê¸°
        best_epoch_idx = np.argmax(mcda_scores) if len(mcda_scores) > 0 else 0
        
        return {
            'scores': mcda_scores.tolist(),
            'best_epoch_idx': int(best_epoch_idx),
            'best_epoch': metrics.get('epochs', [])[best_epoch_idx] if best_epoch_idx < len(metrics.get('epochs', [])) else -1,
            'best_score': float(mcda_scores[best_epoch_idx]) if len(mcda_scores) > 0 else 0.0,
            'weights': weights
        }
    
    def _calculate_stability_scores(self, metrics: Dict) -> np.ndarray:
        """ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚°"""
        losses = metrics.get('losses', [])
        if len(losses) < 3:
            return np.zeros(len(losses))
        
        stability_scores = []
        for i in range(len(losses)):
            start = max(0, i-2)
            end = min(len(losses), i+3)
            window = losses[start:end]
            
            # ë‚®ì€ ë¶„ì‚° = ë†’ì€ ì•ˆì •ì„±
            stability = 1.0 / (1.0 + np.std(window))
            stability_scores.append(stability)
        
        return np.array(stability_scores)
    
    def _calculate_gradient_health(self, metrics: Dict) -> np.ndarray:
        """Gradient Health ì ìˆ˜ ê³„ì‚°"""
        grad_norms = metrics.get('gradients', [])
        if not grad_norms:
            return np.zeros(len(metrics.get('epochs', [])))
        
        health_scores = []
        for norm in grad_norms:
            # Gradientê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ ë¶ˆê±´ì „
            if norm > 0:
                if 0.001 < norm < 10.0:  # ê±´ì „í•œ ë²”ìœ„
                    health = 1.0
                elif norm < 0.001:  # Vanishing
                    health = norm / 0.001
                else:  # Exploding
                    health = 10.0 / norm
            else:
                health = 0.0
            health_scores.append(health)
        
        return np.array(health_scores)
    
    def ensemble_voting(self, module: str, analyses: Dict) -> Dict:
        """ì—¬ëŸ¬ ë¶„ì„ ê¸°ë²•ì˜ ì•™ìƒë¸” íˆ¬í‘œ"""
        candidates = {}
        
        # ê° ê¸°ë²•ì˜ ì¶”ì²œ ìˆ˜ì§‘
        if analyses.get('plateau', {}).get('detected'):
            candidates['plateau'] = analyses['plateau']['center']
        
        if 'best_epoch_idx' in analyses.get('mcda', {}):
            candidates['mcda'] = analyses['mcda']['best_epoch_idx']
        
        # Task metric ìµœê³ ì 
        task_scores = analyses.get('task_scores', {})
        if task_scores and 'combined' in task_scores:
            candidates['task'] = task_scores.get('best_idx', 0)
        
        # Minimum loss
        losses = self.module_histories.get(module, {}).get('losses', [])
        if losses:
            candidates['min_loss'] = np.argmin(losses)
        
        # íˆ¬í‘œ ì§‘ê³„
        if not candidates:
            return {'selected_epoch': -1, 'confidence': 0.0}
        
        # ê°€ì¥ ë§ì€ í‘œë¥¼ ë°›ì€ epoch
        vote_counts = Counter(candidates.values())
        winner, votes = vote_counts.most_common(1)[0]
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = votes / len(candidates) if candidates else 0.0
        
        epochs = self.module_histories.get(module, {}).get('epochs', [])
        
        return {
            'candidates': candidates,
            'selected_epoch_idx': int(winner),
            'selected_epoch': epochs[winner] if winner < len(epochs) else -1,
            'votes': votes,
            'total_voters': len(candidates),
            'confidence': float(confidence)
        }
    
    def analyze_all(self, output_dir: str = 'analysis_results') -> Dict:
        """
        í•™ìŠµ ì™„ë£Œ í›„ ì „ì²´ ë¶„ì„ ì‹¤í–‰
        
        Args:
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ¯ Sweet Spot ì¢…í•© ë¶„ì„ ì‹œì‘")
        logger.info("=" * 70)
        
        # ë””ë²„ê·¸: ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­ í™•ì¸
        logger.debug("ğŸ“Š ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­ í™•ì¸:")
        for module_name, history in self.module_histories.items():
            if history['losses']:
                logger.debug(f"  - {module_name}: {len(history['losses'])}ê°œ ì—í­, "
                           f"ì²« ì†ì‹¤={history['losses'][0]:.4f}, "
                           f"ë§ˆì§€ë§‰ ì†ì‹¤={history['losses'][-1]:.4f}")
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)
        
        analysis_results = {}
        
        # ëª¨ë“ˆë³„ ë¶„ì„
        for module_name in self.module_histories.keys():
            logger.info(f"\nğŸ” ë¶„ì„ ì¤‘: {module_name}")
            
            metrics = self.module_histories[module_name]
            analyses = {}
            
            # 1. Statistical Plateau Detection
            losses = metrics.get('losses', [])
            analyses['plateau'] = self.statistical_plateau_detection(losses)
            
            # 2. Task-Specific Metrics
            task_scores = self.calculate_task_metrics(module_name, metrics)
            analyses['task_scores'] = task_scores
            
            # 3. MCDA
            analyses['mcda'] = self.mcda_analysis(module_name, metrics)
            
            # 4. Ensemble Voting
            analyses['voting'] = self.ensemble_voting(module_name, analyses)
            
            # ì¢…í•©
            result = {
                'module': module_name,
                'metrics': metrics,
                'analyses': analyses,
                'recommendation': {
                    'epoch_idx': analyses['voting']['selected_epoch_idx'],
                    'epoch': analyses['voting']['selected_epoch'],
                    'confidence': analyses['voting']['confidence'],
                    'reasoning': self._generate_reasoning(module_name, analyses)
                }
            }
            
            analysis_results[module_name] = result
        
        # ì‹œê°í™” ìƒì„±
        self._generate_visualizations(analysis_results, output_path)
        
        # ê²°ê³¼ ì €ì¥
        self._save_analysis_results(analysis_results, output_path)
        
        logger.info("\n" + "=" * 70)
        logger.info("âœ… Sweet Spot ë¶„ì„ ì™„ë£Œ!")
        logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_path}")
        logger.info("=" * 70)
        
        # ìš”ì•½ ì¶œë ¥
        print("\nğŸ“Š Sweet Spot Recommendations:")
        print("-" * 50)
        for module, result in analysis_results.items():
            rec = result['recommendation']
            print(f"{module:20s}: Epoch {rec['epoch']:3d} (Confidence: {rec['confidence']:.1%})")
        print("-" * 50)
        
        return analysis_results
    
    def _generate_reasoning(self, module: str, analyses: Dict) -> List[str]:
        """ì¶”ì²œ ê·¼ê±° ìƒì„±"""
        reasons = []
        
        if analyses.get('plateau', {}).get('detected'):
            plateau = analyses['plateau']
            reasons.append(f"Plateau êµ¬ê°„ íƒì§€ (Epoch {plateau['start']}-{plateau['end']})")
        
        if analyses.get('mcda', {}).get('best_score', 0) > 0.8:
            reasons.append(f"MCDA ì ìˆ˜ ìš°ìˆ˜ ({analyses['mcda']['best_score']:.3f})")
        
        if analyses.get('voting', {}).get('confidence', 0) > 0.6:
            reasons.append(f"ë†’ì€ íˆ¬í‘œ ì‹ ë¢°ë„ ({analyses['voting']['confidence']:.1%})")
        
        task_scores = analyses.get('task_scores', {})
        if task_scores.get('combined', 0) > 0.7:
            reasons.append(f"Task ë©”íŠ¸ë¦­ ìš°ìˆ˜ ({task_scores['combined']:.3f})")
        
        return reasons
    
    def _generate_visualizations(self, analysis_results: Dict, output_path: Path):
        """ì‹œê°í™” ìƒì„±"""
        logger.info("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        viz_dir = output_path / 'visualizations'
        viz_dir.mkdir(exist_ok=True)
        
        for module, result in analysis_results.items():
            self._plot_module_analysis(module, result, viz_dir)
        
        # ì¢…í•© íˆíŠ¸ë§µ
        self._plot_summary_heatmap(analysis_results, viz_dir)
    
    def _plot_module_analysis(self, module: str, result: Dict, viz_dir: Path):
        """ëª¨ë“ˆë³„ ë¶„ì„ ì‹œê°í™”"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'{module} Sweet Spot Analysis', fontsize=16)
            
            metrics = result['metrics']
            analyses = result['analyses']
            
            # 1. Loss curve with plateau
            ax = axes[0, 0]
            epochs = metrics.get('epochs', [])
            losses = metrics.get('losses', [])
            
            if epochs and losses:
                ax.plot(epochs, losses, 'b-', label='Training Loss')
                
                if analyses.get('plateau', {}).get('detected'):
                    plateau = analyses['plateau']
                    ax.axvspan(epochs[plateau['start']], epochs[plateau['end']], 
                              alpha=0.3, color='green', label='Plateau')
                    ax.axvline(epochs[plateau['center']], color='red', 
                              linestyle='--', label='Plateau Center')
            
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Loss')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # 2. MCDA Scores
            ax = axes[0, 1]
            mcda_scores = analyses.get('mcda', {}).get('scores', [])
            
            if epochs and mcda_scores:
                ax.plot(epochs[:len(mcda_scores)], mcda_scores, 'g-', label='MCDA Score')
                best_idx = analyses.get('mcda', {}).get('best_epoch_idx', 0)
                if best_idx < len(epochs) and best_idx < len(mcda_scores):
                    ax.scatter(epochs[best_idx], mcda_scores[best_idx], 
                              color='red', s=100, label='Best MCDA')
            
            ax.set_xlabel('Epoch')
            ax.set_ylabel('MCDA Score')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # 3. Accuracies
            ax = axes[1, 0]
            accuracies = metrics.get('accuracies', [])
            
            if epochs and accuracies:
                ax.plot(epochs[:len(accuracies)], accuracies, label='Accuracy')
            
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Accuracy')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # 4. Voting Results
            ax = axes[1, 1]
            voting = analyses.get('voting', {})
            
            if voting.get('candidates'):
                candidates = list(voting['candidates'].keys())
                values = list(voting['candidates'].values())
                colors = ['green' if v == voting['selected_epoch_idx'] else 'blue' for v in values]
                ax.bar(candidates, values, color=colors)
                ax.set_xlabel('Analysis Method')
                ax.set_ylabel('Recommended Epoch Index')
                ax.set_title(f"Final: Epoch {voting.get('selected_epoch', -1)} (Confidence: {voting.get('confidence', 0):.1%})")
            
            plt.tight_layout()
            plt.savefig(viz_dir / f'{module}_analysis.png', dpi=150)
            plt.close()
            
        except Exception as e:
            logger.warning(f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨ ({module}): {e}")
    
    def _plot_summary_heatmap(self, analysis_results: Dict, viz_dir: Path):
        """ì¢…í•© íˆíŠ¸ë§µ"""
        try:
            modules = []
            recommended_epochs = []
            confidences = []
            
            for module, result in analysis_results.items():
                modules.append(module)
                recommended_epochs.append(result['recommendation']['epoch'])
                confidences.append(result['recommendation']['confidence'])
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # Epoch recommendations
            ax1.barh(modules, recommended_epochs, color='steelblue')
            ax1.set_xlabel('Recommended Epoch')
            ax1.set_title('Sweet Spot Epochs by Module')
            ax1.grid(True, alpha=0.3)
            
            # Confidence scores
            ax2.barh(modules, confidences, color='coral')
            ax2.set_xlabel('Confidence Score')
            ax2.set_title('Recommendation Confidence')
            ax2.set_xlim([0, 1])
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(viz_dir / 'summary_sweetspots.png', dpi=150)
            plt.close()
            
        except Exception as e:
            logger.warning(f"ì¢…í•© íˆíŠ¸ë§µ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _save_analysis_results(self, analysis_results: Dict, output_path: Path):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        # JSON ì €ì¥
        json_path = output_path / 'sweet_spot_analysis.json'
        with open(json_path, 'w') as f:
            json.dump(analysis_results, f, indent=2, default=str)
        logger.info(f"ğŸ“ JSON ê²°ê³¼ ì €ì¥: {json_path}")
        
        # Markdown ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_markdown_report(analysis_results, output_path)
    
    def _generate_markdown_report(self, analysis_results: Dict, output_path: Path):
        """Markdown í˜•ì‹ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_path = output_path / 'sweet_spot_report.md'
        
        with open(report_path, 'w') as f:
            f.write("# ğŸ¯ Sweet Spot Analysis Report\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Summary table
            f.write("## ğŸ“Š Summary\n\n")
            f.write("| Module | Recommended Epoch | Confidence | Key Reasoning |\n")
            f.write("|--------|------------------|------------|---------------|\n")
            
            for module, result in analysis_results.items():
                rec = result['recommendation']
                reasons = ', '.join(rec['reasoning'][:2]) if rec['reasoning'] else 'N/A'
                f.write(f"| {module} | {rec['epoch']} | {rec['confidence']:.1%} | {reasons} |\n")
            
            # Detailed analysis
            f.write("\n## ğŸ” Detailed Analysis\n\n")
            
            for module, result in analysis_results.items():
                f.write(f"### Module: {module}\n\n")
                
                analyses = result['analyses']
                
                # Plateau
                if analyses.get('plateau', {}).get('detected'):
                    plateau = analyses['plateau']
                    f.write(f"**Plateau Detection:**\n")
                    f.write(f"- Range: Epoch {plateau['start']}-{plateau['end']}\n")
                    f.write(f"- Center: Epoch {plateau['center']}\n")
                    f.write(f"- Mean Loss: {plateau['mean_loss']:.4f} (Â±{plateau['std']:.4f})\n\n")
                else:
                    f.write("**Plateau Detection:** Not detected\n\n")
                
                # MCDA
                mcda = analyses.get('mcda', {})
                f.write(f"**MCDA Analysis:**\n")
                f.write(f"- Best Epoch: {mcda.get('best_epoch', -1)}\n")
                f.write(f"- Best Score: {mcda.get('best_score', 0):.3f}\n\n")
                
                # Voting
                voting = analyses.get('voting', {})
                f.write(f"**Ensemble Voting:**\n")
                f.write(f"- Selected: Epoch {voting.get('selected_epoch', -1)}\n")
                f.write(f"- Confidence: {voting.get('confidence', 0):.1%}\n")
                f.write(f"- Votes: {voting.get('votes', 0)}/{voting.get('total_voters', 0)}\n\n")
                
                f.write("---\n\n")
            
            # Threshold recommendations
            f.write("## ğŸ¯ Recommended Thresholds for Automation\n\n")
            f.write("```python\n")
            f.write("# Based on empirical analysis\n")
            f.write("thresholds = {\n")
            
            # Calculate empirical thresholds
            all_plateau_stds = []
            for result in analysis_results.values():
                if result['analyses'].get('plateau', {}).get('detected'):
                    all_plateau_stds.append(result['analyses']['plateau']['std'])
            
            if all_plateau_stds:
                f.write(f"    'plateau_variance': {np.mean(all_plateau_stds):.4f},\n")
            else:
                f.write(f"    'plateau_variance': 0.01,  # Default\n")
            
            f.write("    'stability_window': 5,\n")
            f.write("    'mcda_weights': {\n")
            f.write("        'loss': 0.30,\n")
            f.write("        'accuracy': 0.40,\n")
            f.write("        'stability': 0.15,\n")
            f.write("        'gradient_health': 0.15\n")
            f.write("    },\n")
            
            # Average confidence
            avg_confidence = np.mean([r['recommendation']['confidence'] 
                                     for r in analysis_results.values()])
            f.write(f"    'min_confidence': {avg_confidence * 0.8:.2f}\n")
            f.write("}\n```\n\n")
            
            # Next steps
            f.write("## ğŸ“ Next Steps\n\n")
            f.write("1. Review the recommendations above\n")
            f.write("2. Manually combine modules using recommended epochs\n")
            f.write("3. Evaluate combined model performance\n")
            f.write("4. Adjust thresholds based on results\n")
            f.write("5. Enable automated sweet spot detection\n")
        
        logger.info(f"ğŸ“„ Markdown ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    
    def export_analysis(self, output_dir: str = "training/sweet_spot_analysis"):
        """
        ì „ì²´ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
        
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. JSON í˜•ì‹ìœ¼ë¡œ ì „ì²´ ë°ì´í„° ì €ì¥
        analysis_data = {
            'timestamp': timestamp,
            'sweet_spots': self.sweet_spots,
            'convergence_points': self.convergence_points,
            'overfitting_points': self.overfitting_points,
            'optimal_epochs': self.get_optimal_epochs(),
            'module_summaries': {}
        }
        
        for module_name in self.module_histories.keys():
            analysis_data['module_summaries'][module_name] = self.get_module_status(module_name)
        
        json_file = output_dir / f"sweet_spot_analysis_{timestamp}.json"
        with open(json_file, 'w') as f:
            json.dump(analysis_data, f, indent=2)
        
        logger.info(f"ğŸ“Š Sweet Spot ë¶„ì„ ê²°ê³¼ ì €ì¥: {json_file}")
        
        # 2. ê° ëª¨ë“ˆë³„ í”Œë¡¯ ìƒì„±
        plots_dir = output_dir / "plots"
        plots_dir.mkdir(exist_ok=True)
        
        for module_name in self.module_histories.keys():
            plot_file = plots_dir / f"{module_name}_{timestamp}.png"
            self.plot_module_analysis(module_name, str(plot_file))
        
        # 3. ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        report_file = output_dir / f"sweet_spot_report_{timestamp}.txt"
        with open(report_file, 'w') as f:
            f.write("=" * 60 + "\n")
            f.write("Sweet Spot Analysis Report\n")
            f.write(f"Generated: {timestamp}\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("Optimal Epochs by Module:\n")
            f.write("-" * 30 + "\n")
            for module, epoch in self.get_optimal_epochs().items():
                f.write(f"  {module}: Epoch {epoch}\n")
            
            f.write("\n")
            f.write("Sweet Spots Detected:\n")
            f.write("-" * 30 + "\n")
            for module, info in self.sweet_spots.items():
                f.write(f"  {module}:\n")
                f.write(f"    - Epoch: {info['epoch']}\n")
                f.write(f"    - Loss: {info['loss']:.4f} (Â±{info['std']:.4f})\n")
                f.write(f"    - Stable: {info['stable']}\n")
                f.write(f"    - Converged: {info['converged']}\n")
            
            f.write("\n")
            f.write("Convergence Points:\n")
            f.write("-" * 30 + "\n")
            for module, info in self.convergence_points.items():
                f.write(f"  {module}: Epoch {info['epoch']} (Loss: {info['loss']:.4f})\n")
            
            if self.overfitting_points:
                f.write("\n")
                f.write("âš ï¸ Overfitting Warnings:\n")
                f.write("-" * 30 + "\n")
                for module, info in self.overfitting_points.items():
                    f.write(f"  {module}: Started at epoch {info['epoch']}\n")
        
        logger.info(f"ğŸ“„ Sweet Spot ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")
        
        return {
            'json_file': str(json_file),
            'report_file': str(report_file),
            'plots_dir': str(plots_dir)
        }