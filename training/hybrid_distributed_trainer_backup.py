#!/usr/bin/env python3
"""
í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° í•™ìŠµ ì‹œìŠ¤í…œ (CPU 128GB + GPU RTX 2070S)
Hybrid Distributed Training System (CPU 128GB + GPU RTX 2070S)
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import time
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict
import math
import gc
from collections import defaultdict
import threading
import queue
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, as_completed

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ëª¨ë“ˆ imports
from xai_core.xai_logging_system import xai_logger, xai_trace
from llm_module import llm_tracker, register_llm, ask_llm

@dataclass
class HybridConfig:
    """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì„¤ì •"""
    # ëª¨ë¸ ë¶„í•  ì„¤ì • (ê³µê²©ì  ìµœì í™” 70GB)
    gpu_memory_gb: float = 8.0          # RTX 2070S ë©”ëª¨ë¦¬
    cpu_memory_gb: float = 70.0         # WSL ê³µê²©ì  ë©”ëª¨ë¦¬ í™œìš©
    target_params: int = 4_300_000_000   # 43ì–µ íŒŒë¼ë¯¸í„° (í˜„ì‹¤ì  70GB)
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì •
    test_mode: bool = False             # í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”
    test_samples: int = 10              # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ìˆ˜
    
    # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
    max_safe_batch_size: int = 8        # GPU í™œìš©ìœ¼ë¡œ ë°°ì¹˜ ì¦ê°€ ê°€ëŠ¥
    gradient_accumulation_steps: int = 8 # íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° ì¦ëŒ€
    use_gradient_checkpointing: bool = True  # í™œì„±í™” ë©”ëª¨ë¦¬ ì ˆì•½
    use_parameter_sharing: bool = True   # ë ˆì´ì–´ê°„ íŒŒë¼ë¯¸í„° ê³µìœ 
    
    # í•™ìŠµ ì„¤ì • (ê³µê²©ì  ê³ ì„±ëŠ¥)
    regrets_per_step: int = 7           # ì›ë˜ ì„¤ì • ë³µì› (ìµœê³  í’ˆì§ˆ)
    bentham_calculations_per_regret: int = 3  # ì›ë˜ ì„¤ì • ë³µì› (ì •í™•ë„ ê·¹ëŒ€í™”)
    epochs: int = 3
    batch_size: int = 12                # ëŒ€í˜• ë°°ì¹˜ (ì²˜ë¦¬ëŸ‰ ì¦ê°€)
    micro_batch_size: int = 3           # ë©”ëª¨ë¦¬ ì—¬ìœ ë¡œ ì¦ê°€
    
    # ë¶„ì‚° ì„¤ì • (ê³µê²©ì  ë³‘ë ¬í™”)
    num_workers: int = 8                # ìµœëŒ€ CPU í™œìš©
    gpu_layers_ratio: float = 0.6       # ê· í˜•ì¡íŒ GPU/CPU ë¶„í• 
    overlap_computation: bool = True     # ê³„ì‚° ì˜¤ë²„ë©
    use_cpu_offload: bool = True        # CPU ì˜¤í”„ë¡œë“œ í™œì„±í™”
    enable_memory_monitoring: bool = True  # ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
    
    # ìµœì í™” ì„¤ì •
    use_mixed_precision: bool = True    # FP16 ì‚¬ìš©
    gradient_accumulation_steps: int = 4
    max_grad_norm: float = 1.0
    
    # ë¡œê¹…/ì²´í¬í¬ì¸íŠ¸
    log_every_n_steps: int = 5          # ë” ìì£¼ ë¡œê¹…
    save_checkpoint_every: int = 20     # ë” ìì£¼ ì €ì¥
    max_storage_gb: float = 50.0        # ìŠ¤í† ë¦¬ì§€ ì ˆì•½

class MemoryOptimizedModel(nn.Module):
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ëª¨ë¸"""
    
    def __init__(self, config: HybridConfig):
        super().__init__()
        self.config = config
        
        # í˜„ì‹¤ì  ê³ ì„±ëŠ¥ ì„¤ê³„ (43ì–µ íŒŒë¼ë¯¸í„°, 70GB)
        self.hidden_dim = 2560      # í˜„ì‹¤ì  í¬ê¸° (ì„±ëŠ¥ vs ë©”ëª¨ë¦¬ ê· í˜•)
        self.num_layers = 32        # ì ë‹¹í•œ ê¹Šì´ (ì•ˆì •ì„± ìœ ì§€)
        self.num_heads = 40         # ì–´í…ì…˜ í’ˆì§ˆ ìœ ì§€
        self.intermediate_size = 10240  # FFN í¬ê¸° í˜„ì‹¤ì  ì¡°ì •
        
        # ì…ë ¥ ë ˆì´ì–´ (CPU)
        self.input_projection = nn.Linear(1024, self.hidden_dim)
        self.input_norm = nn.LayerNorm(self.hidden_dim, eps=1e-6)  # Loss NaN ë°©ì§€ ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ 
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤ (ë¶„í•  ë°°ì¹˜)
        gpu_layers = int(self.num_layers * config.gpu_layers_ratio)
        cpu_layers = self.num_layers - gpu_layers
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŠ¸ í™œì„±í™”
        use_checkpointing = getattr(config, 'use_gradient_checkpointing', True)
        
        self.gpu_layers = nn.ModuleList([
            OptimizedTransformerLayer(self.hidden_dim, self.num_heads, self.intermediate_size, use_checkpointing)
            for _ in range(gpu_layers)
        ])
        
        self.cpu_layers = nn.ModuleList([
            OptimizedTransformerLayer(self.hidden_dim, self.num_heads, self.intermediate_size, use_checkpointing)
            for _ in range(cpu_layers)
        ])
        
        # ì¶œë ¥ í—¤ë“œë“¤ (GPU) - ë©”ëª¨ë¦¬ ìµœì í™”
        # SwiGLU ê¸°ë°˜ ê°ì • í—¤ë“œ (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        self.emotion_swiglu = SwiGLU(self.hidden_dim, self.hidden_dim // 2)
        self.emotion_out = nn.Linear(self.hidden_dim // 2, 6)  # 6ì°¨ì› ê°ì •
        self.emotion_dropout = nn.Dropout(0.02)
        self.emotion_activation = nn.Tanh()
        
        # SwiGLU ê¸°ë°˜ ì˜ë¯¸ í—¤ë“œ (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        self.semantic_swiglu = SwiGLU(self.hidden_dim, self.hidden_dim // 2)
        self.semantic_out = nn.Linear(self.hidden_dim // 2, 512)  # 1000â†’512 ì¶•ì†Œ
        self.semantic_dropout = nn.Dropout(0.02)
        self.semantic_activation = nn.Softmax(dim=-1)
        
        # SwiGLU ê¸°ë°˜ ì¶”ë¡  í—¤ë“œ (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        self.reasoning_swiglu = SwiGLU(self.hidden_dim, self.hidden_dim // 4)
        self.reasoning_out = nn.Linear(self.hidden_dim // 4, 128)  # ì¶”ë¡  íŠ¹ì§•
        self.reasoning_dropout = nn.Dropout(0.05)
        
        # SwiGLU ê¸°ë°˜ í†µí•© í—¤ë“œ (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        self.integration_swiglu = SwiGLU(self.hidden_dim, self.hidden_dim)
        self.integration_out = nn.Linear(self.hidden_dim, 512)  # í†µí•© íŠ¹ì§•
        self.integration_dropout = nn.Dropout(0.05)
        self.integration_activation = nn.Tanh()
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.gpu_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.cpu_device = torch.device('cpu')
        
        self._setup_devices()
        
    def _setup_devices(self):
        """ë””ë°”ì´ìŠ¤ë³„ ëª¨ë¸ ë°°ì¹˜"""
        # CPU ë ˆì´ì–´ë“¤
        self.input_projection = self.input_projection.to(self.cpu_device)
        self.input_norm = self.input_norm.to(self.cpu_device)
        self.cpu_layers = self.cpu_layers.to(self.cpu_device)
        
        # GPU ë ˆì´ì–´ë“¤ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if torch.cuda.is_available():
            self.gpu_layers = self.gpu_layers.to(self.gpu_device)
            # SwiGLU ê¸°ë°˜ í—¤ë“œë“¤ GPUë¡œ ì´ë™
            self.emotion_swiglu = self.emotion_swiglu.to(self.gpu_device)
            self.emotion_out = self.emotion_out.to(self.gpu_device)
            self.emotion_dropout = self.emotion_dropout.to(self.gpu_device)
            self.emotion_activation = self.emotion_activation.to(self.gpu_device)
            
            self.semantic_swiglu = self.semantic_swiglu.to(self.gpu_device)
            self.semantic_out = self.semantic_out.to(self.gpu_device)
            self.semantic_dropout = self.semantic_dropout.to(self.gpu_device)
            self.semantic_activation = self.semantic_activation.to(self.gpu_device)
            
            self.reasoning_swiglu = self.reasoning_swiglu.to(self.gpu_device)
            self.reasoning_out = self.reasoning_out.to(self.gpu_device)
            self.reasoning_dropout = self.reasoning_dropout.to(self.gpu_device)
            
            self.integration_swiglu = self.integration_swiglu.to(self.gpu_device)
            self.integration_out = self.integration_out.to(self.gpu_device)
            self.integration_dropout = self.integration_dropout.to(self.gpu_device)
            self.integration_activation = self.integration_activation.to(self.gpu_device)
        else:
            # GPU ì—†ìœ¼ë©´ CPUì— ë°°ì¹˜
            self.gpu_layers = self.gpu_layers.to(self.cpu_device)
            # SwiGLU ê¸°ë°˜ í—¤ë“œë“¤ CPUë¡œ ì´ë™
            self.emotion_swiglu = self.emotion_swiglu.to(self.cpu_device)
            self.emotion_out = self.emotion_out.to(self.cpu_device)
            self.emotion_dropout = self.emotion_dropout.to(self.cpu_device)
            self.emotion_activation = self.emotion_activation.to(self.cpu_device)
            
            self.semantic_swiglu = self.semantic_swiglu.to(self.cpu_device)
            self.semantic_out = self.semantic_out.to(self.cpu_device)
            self.semantic_dropout = self.semantic_dropout.to(self.cpu_device)
            self.semantic_activation = self.semantic_activation.to(self.cpu_device)
            
            self.reasoning_swiglu = self.reasoning_swiglu.to(self.cpu_device)
            self.reasoning_out = self.reasoning_out.to(self.cpu_device)
            self.reasoning_dropout = self.reasoning_dropout.to(self.cpu_device)
            
            self.integration_swiglu = self.integration_swiglu.to(self.cpu_device)
            self.integration_out = self.integration_out.to(self.cpu_device)
            self.integration_dropout = self.integration_dropout.to(self.cpu_device)
            self.integration_activation = self.integration_activation.to(self.cpu_device)
            self.gpu_device = self.cpu_device
    
    def get_parameter_count(self) -> int:
        """íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        return sum(p.numel() for p in self.parameters())
    
    def forward(self, input_embeddings: torch.Tensor) -> Dict[str, torch.Tensor]:
        batch_size, seq_len, input_dim = input_embeddings.shape
        
        # CPUì—ì„œ ì…ë ¥ ì²˜ë¦¬
        hidden_states = input_embeddings.to(self.cpu_device)
        hidden_states = self.input_projection(hidden_states)
        hidden_states = self.input_norm(hidden_states)
        
        # CPU ë ˆì´ì–´ë“¤ í†µê³¼
        for layer in self.cpu_layers:
            hidden_states = layer(hidden_states)
        
        # GPUë¡œ ì´ë™ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if torch.cuda.is_available():
            hidden_states = hidden_states.to(self.gpu_device)
        
        # GPU ë ˆì´ì–´ë“¤ í†µê³¼
        for layer in self.gpu_layers:
            hidden_states = layer(hidden_states)
        
        # í‰ê·  í’€ë§
        pooled_output = hidden_states.mean(dim=1)
        
        # SwiGLU ê¸°ë°˜ ì¶œë ¥ í—¤ë“œë“¤ (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        emotion_swiglu = self.emotion_swiglu(pooled_output)
        emotion_output = self.emotion_activation(self.emotion_out(self.emotion_dropout(emotion_swiglu)))
        
        semantic_swiglu = self.semantic_swiglu(pooled_output)
        semantic_output = self.semantic_activation(self.semantic_out(self.semantic_dropout(semantic_swiglu)))
        
        reasoning_swiglu = self.reasoning_swiglu(pooled_output)
        reasoning_output = self.reasoning_out(self.reasoning_dropout(reasoning_swiglu))
        
        integration_swiglu = self.integration_swiglu(pooled_output)
        integration_output = self.integration_activation(self.integration_out(self.integration_dropout(integration_swiglu)))
        
        return {
            'emotion_predictions': emotion_output,
            'semantic_predictions': semantic_output,
            'reasoning_features': reasoning_output,
            'integration_features': integration_output,
            'pooled_output': pooled_output
        }

class SwiGLU(nn.Module):
    """
    SwiGLU í™œì„±í™” í•¨ìˆ˜ - ìˆ˜ì¹˜ì  ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì´ GELUë³´ë‹¤ ìš°ìˆ˜
    SwiGLU(x) = Swish(xW + b) âŠ— (xV + c)
    """
    def __init__(self, dim: int, hidden_dim: int):
        super().__init__()
        self.dim = dim
        self.hidden_dim = hidden_dim
        # SwiGLUëŠ” ë‘ ê°œì˜ ì„ í˜• ë³€í™˜ì„ í•„ìš”ë¡œ í•¨
        self.w = nn.Linear(dim, hidden_dim, bias=True)
        self.v = nn.Linear(dim, hidden_dim, bias=True)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Swish(x) = x * sigmoid(x), ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •ì 
        w_out = self.w(x)
        swish_w = w_out * torch.sigmoid(w_out)
        v_out = self.v(x)
        return swish_w * v_out  # Hadamard product

class OptimizedTransformerLayer(nn.Module):
    """ì„±ëŠ¥ ìœ ì§€ ë©”ëª¨ë¦¬ ìµœì í™” íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ (SwiGLU ì ìš©)"""
    
    def __init__(self, hidden_dim: int, num_heads: int, intermediate_size: int, use_gradient_checkpointing: bool = True):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        self.use_gradient_checkpointing = use_gradient_checkpointing
        
        # ê³ íš¨ìœ¨ ì–´í…ì…˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=0.02,  # Dropout ì¶”ê°€ ê°ì†Œ (Loss NaN ë°©ì§€)
            batch_first=True
        )
        self.attention_norm = nn.LayerNorm(hidden_dim, eps=1e-6)  # Loss NaN ë°©ì§€ ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ 
        
        # SwiGLU ê¸°ë°˜ FFN (ìˆ˜ì¹˜ ì•ˆì •ì„± ë° ì„±ëŠ¥ ê°œì„ )
        self.swiglu = SwiGLU(hidden_dim, intermediate_size)
        self.ffn_out = nn.Linear(intermediate_size, hidden_dim)
        self.ffn_dropout = nn.Dropout(0.02)  # Dropout ì¶”ê°€ ê°ì†Œ (Loss NaN ë°©ì§€)
        self.ffn_norm = nn.LayerNorm(hidden_dim, eps=1e-6)  # Loss NaN ë°©ì§€ ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ 
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
        if self.use_gradient_checkpointing and self.training:
            from torch.utils.checkpoint import checkpoint
            return checkpoint(self._forward_impl, hidden_states, use_reentrant=False)
        else:
            return self._forward_impl(hidden_states)
    
    def _forward_impl(self, hidden_states: torch.Tensor) -> torch.Tensor:
        # ì–´í…ì…˜ ë¸”ë¡ (Post-LayerNorm ì•ˆì •ì„± ê°œì„ )
        attention_output, _ = self.attention(hidden_states, hidden_states, hidden_states)
        hidden_states = hidden_states + attention_output
        hidden_states = self.attention_norm(hidden_states)
        
        # SwiGLU ê¸°ë°˜ FFN ë¸”ë¡ (Post-LayerNorm ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
        swiglu_output = self.swiglu(hidden_states)
        ffn_output = self.ffn_dropout(self.ffn_out(swiglu_output))
        hidden_states = hidden_states + ffn_output
        hidden_states = self.ffn_norm(hidden_states)
        
        return hidden_states

class AsyncRegretCalculator:
    """ë¹„ë™ê¸° í›„íšŒ ê³„ì‚°ê¸°"""
    
    def __init__(self, config: HybridConfig):
        self.config = config
        self.regret_queue = queue.Queue(maxsize=100)
        self.result_queue = queue.Queue(maxsize=100)
        self.workers = []
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œë“¤ ì‹œì‘
        for i in range(config.num_workers):
            worker = threading.Thread(target=self._worker_process, daemon=True)
            worker.start()
            self.workers.append(worker)
    
    def _worker_process(self):
        """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤"""
        while True:
            try:
                task = self.regret_queue.get(timeout=1)
                if task is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                
                original_decision, task_id = task
                regret_scenarios = self._calculate_regret_scenarios(original_decision)
                self.result_queue.put((task_id, regret_scenarios))
                self.regret_queue.task_done()
            except queue.Empty:
                continue
            except Exception as e:
                print(f"Worker error: {e}")
    
    def _calculate_regret_scenarios(self, original_decision: torch.Tensor) -> List[Dict[str, torch.Tensor]]:
        """7ê°€ì§€ í›„íšŒ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        scenarios = []
        regret_types = ['counterfactual', 'temporal', 'moral', 'opportunity', 'social']
        
        for i, regret_type in enumerate(regret_types):
            if i >= self.config.regrets_per_step:
                break
            
            # ê° í›„íšŒ ìœ í˜•ë³„ ë³€í˜•ëœ ê²°ì • ìƒì„±
            regret_decision = self._generate_regret_decision(original_decision, regret_type)
            
            # ë²¤ë‹´ ì¾Œë½ ê³„ì‚° (3íšŒ)
            bentham_scores = []
            for _ in range(self.config.bentham_calculations_per_regret):
                score = self._calculate_bentham_score(original_decision, regret_decision)
                bentham_scores.append(score)
            
            scenarios.append({
                'regret_type': regret_type,
                'regret_decision': regret_decision,
                'bentham_scores': torch.tensor(bentham_scores).mean(),
                'regret_weight': 0.2
            })
        
        # ë¶€ì¡±í•œ ê²½ìš° ì¶”ê°€
        while len(scenarios) < self.config.regrets_per_step:
            scenarios.append(scenarios[0])  # ì²« ë²ˆì§¸ ì‹œë‚˜ë¦¬ì˜¤ ë³µì‚¬
        
        return scenarios[:self.config.regrets_per_step]
    
    def _generate_regret_decision(self, original: torch.Tensor, regret_type: str) -> torch.Tensor:
        """í›„íšŒ ìœ í˜•ë³„ ë³€í˜•ëœ ê²°ì • ìƒì„±"""
        if regret_type == 'counterfactual':
            return -original + torch.randn_like(original) * 0.1
        elif regret_type == 'temporal':
            return original * 0.7 + torch.randn_like(original) * 0.2
        elif regret_type == 'moral':
            return original + torch.ones_like(original) * 0.3
        elif regret_type == 'opportunity':
            return original * 1.3 + torch.randn_like(original) * 0.15
        elif regret_type == 'social':
            return original + torch.ones_like(original) * 0.2
        else:
            return original + torch.randn_like(original) * 0.1
    
    def _calculate_bentham_score(self, original: torch.Tensor, regret: torch.Tensor) -> float:
        """ë²¤ë‹´ ì¾Œë½ ì ìˆ˜ ê³„ì‚°"""
        diff = torch.abs(regret - original).mean()
        intensity = diff.item()
        return intensity * 0.8  # ê°„ì†Œí™”ëœ ë²¤ë‹´ ê³„ì‚°
    
    def calculate_async(self, original_decision: torch.Tensor, task_id: int):
        """ë¹„ë™ê¸° í›„íšŒ ê³„ì‚° ìš”ì²­"""
        self.regret_queue.put((original_decision.cpu().clone(), task_id))
    
    def get_result(self, timeout: float = 0.1) -> Optional[Tuple[int, List[Dict]]]:
        """ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None

class OptimizedDataset(Dataset):
    """ìµœì í™”ëœ ë°ì´í„°ì…‹"""
    
    def __init__(self, data_files: List[Path], config: HybridConfig):
        self.config = config
        self.scenarios = []
        
        # ë°ì´í„° ë¡œë“œ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì§€ì›)
        print("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
        total_loaded = 0
        for file_path in data_files:
            if hasattr(config, 'test_mode') and config.test_mode and total_loaded >= config.test_samples:
                break
                
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    if hasattr(config, 'test_mode') and config.test_mode:
                        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì •í™•íˆ test_samplesê°œë§Œ ë¡œë“œ
                        remaining = config.test_samples - total_loaded
                        sample_size = min(len(data), remaining)
                        self.scenarios.extend(data[:sample_size])
                        total_loaded += sample_size
                    else:
                        # ì¼ë°˜ ëª¨ë“œ: íŒŒì¼ë‹¹ ìµœëŒ€ 5000ê°œ
                        sample_size = min(len(data), 5000)
                        self.scenarios.extend(data[:sample_size])
        
        print(f"âœ… ì´ {len(self.scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œë¨")
    
    def __len__(self):
        return len(self.scenarios)
    
    def __getitem__(self, idx):
        scenario = self.scenarios[idx]
        
        # í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ìµœì í™”ëœ ì°¨ì›)
        embedding = torch.randn(1024, dtype=torch.float32)
        
        # ë¼ë²¨ ì¤€ë¹„
        options = scenario.get('options', [])
        if len(options) >= 3:
            labels = torch.tensor([0.5, 0.3, 0.2], dtype=torch.float32)
        else:
            labels = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float32)
        
        return {
            'text_embedding': embedding,
            'labels': labels,
            'scenario_id': scenario.get('id', f'scenario_{idx}'),
            'category': scenario.get('category', 'general')
        }

class HybridDistributedTrainer:
    """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° í•™ìŠµê¸°"""
    
    def __init__(self, config: HybridConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = project_root / 'training' / 'hybrid_outputs'
        self.logs_dir = self.output_dir / 'logs'
        self.checkpoints_dir = self.output_dir / 'checkpoints'
        self.reports_dir = self.output_dir / 'reports'
        
        for dir_path in [self.output_dir, self.logs_dir, self.checkpoints_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # ë¹„ë™ê¸° í›„íšŒ ê³„ì‚°ê¸°
        self.regret_calculator = AsyncRegretCalculator(config)
        
        # í•™ìŠµ í†µê³„
        self.training_stats = defaultdict(list)
        self.step_count = 0
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
        print(f"ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        if torch.cuda.is_available():
            print(f"   - GPU ê°€ì†: âœ… CUDA í™œì„±í™”")
            print(f"   - GPU ë©”ëª¨ë¦¬: {config.gpu_memory_gb}GB")
        else:
            print(f"   - CPU ìµœì í™”: âœ… ê³ ì„±ëŠ¥ CPU ëª¨ë“œ")
            print(f"   - CPU ë©”ëª¨ë¦¬: {config.cpu_memory_gb}GB (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬)")
        print(f"   - ë³‘ë ¬ ì›Œì»¤: {config.num_workers}ê°œ")
        print(f"   - ë¶„ì‚° ì²˜ë¦¬: CPU+ë©”ëª¨ë¦¬ ìµœì í™”")
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = self.logs_dir / f'hybrid_training_{int(time.time())}.log'
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('HybridTrainer')
    
    def prepare_model(self):
        """ëª¨ë¸ ì¤€ë¹„"""
        self.logger.info("ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”ëœ ëª¨ë¸
        self.model = MemoryOptimizedModel(self.config)
        
        # Mixed Precision ì„¤ì •
        if self.config.use_mixed_precision and torch.cuda.is_available():
            self.scaler = torch.cuda.amp.GradScaler()
        else:
            self.scaler = None
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=1e-4,
            weight_decay=0.01
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=1000, eta_min=1e-6
        )
        
        actual_params = self.model.get_parameter_count()
        self.logger.info(f"âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ: {actual_params:,}ê°œ íŒŒë¼ë¯¸í„°")
        
        return actual_params
    
    def prepare_data(self) -> DataLoader:
        """ë°ì´í„° ì¤€ë¹„"""
        self.logger.info("ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # ë°ì´í„° íŒŒì¼ ì°¾ê¸°
        data_dir = project_root / 'processed_datasets'
        batch_files = list(data_dir.glob('full_scenarios_batch_*.json'))
        
        if not batch_files:
            raise FileNotFoundError("ë°°ì¹˜ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìµœì í™”ëœ ë°ì´í„°ì…‹
        dataset = OptimizedDataset(batch_files, self.config)
        
        # ë°ì´í„°ë¡œë” (ìµœì í™”ëœ ì„¤ì •)
        dataloader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=torch.cuda.is_available(),
            persistent_workers=True
        )
        
        self.logger.info(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(dataset)}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
        return dataloader
    
    def train_step(self, batch: Dict[str, torch.Tensor], step_idx: int) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ìµœì í™”ëœ í•™ìŠµ ìŠ¤í…"""
        self.model.train()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if step_idx % 5 == 0:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        text_embeddings = batch['text_embedding']
        labels = batch['labels']
        
        # ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
        current_batch_size = text_embeddings.size(0)
        if current_batch_size > self.config.max_safe_batch_size:
            return self._process_large_batch(batch, step_idx)
        
        # ë¹„ë™ê¸° í›„íšŒ ê³„ì‚° ì‹œì‘ (ìµœì í™”)
        for i, embedding in enumerate(text_embeddings[:min(len(text_embeddings), 6)]):
            self.regret_calculator.calculate_async(embedding, step_idx * self.config.batch_size + i)
        
        # ëª¨ë¸ ìˆœì „íŒŒ (Mixed Precision)
        if self.scaler:
            with torch.cuda.amp.autocast():
                outputs = self.model(text_embeddings.unsqueeze(1))
        else:
            outputs = self.model(text_embeddings.unsqueeze(1))
        
        # ê¸°ë³¸ ë¶„ë¥˜ ì†ì‹¤ (NaN ë°©ì§€ ì•ˆì •ì„± ê°œì„ )
        emotion_predictions = outputs['emotion_predictions']
        
        # NaN ê°ì§€ ë° ì²˜ë¦¬
        if torch.isnan(emotion_predictions).any():
            logger.warning("ê°ì • ì˜ˆì¸¡ì— NaN ë°œê²¬, ì œë¡œë¡œ ì´ˆê¸°í™”")
            emotion_predictions = torch.zeros_like(emotion_predictions)
        
        emotion_avg = emotion_predictions.mean(dim=1, keepdim=True).expand(-1, 3)
        classification_loss = F.mse_loss(emotion_avg, labels)
        
        # ì†ì‹¤ NaN ê°ì§€ ë° ì²˜ë¦¬
        if torch.isnan(classification_loss):
            logger.error("ë¶„ë¥˜ ì†ì‹¤ì´ NaNì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •")
            classification_loss = torch.tensor(1.0, device=emotion_predictions.device, requires_grad=True)
        
        total_loss = classification_loss
        regret_count = 0
        bentham_count = 0
        
        # í›„íšŒ ê²°ê³¼ ìˆ˜ì§‘ (ê· í˜• ìœ ì§€)
        for _ in range(7):  # ì›ë˜ 7ê°œ ìœ ì§€
            result = self.regret_calculator.get_result()
            if result is None:
                break
            
            task_id, regret_scenarios = result
            regret_count += len(regret_scenarios)
            bentham_count += len(regret_scenarios) * self.config.bentham_calculations_per_regret
            
            # í›„íšŒ ì†ì‹¤ ì¶”ê°€ (ê· í˜• ìœ ì§€ + NaN ë°©ì§€)
            for scenario in regret_scenarios[:3]:  # ìµœëŒ€ 3ê°œ ì²˜ë¦¬
                # ì•ˆì „í•œ í›„íšŒ ì†ì‹¤ ê³„ì‚°
                bentham_scores = scenario.get('bentham_scores', 0.0)
                regret_weight = scenario.get('regret_weight', 1.0)
                
                # NaN/Inf ê°ì§€ ë° ì œí•œ
                if isinstance(bentham_scores, (int, float)):
                    bentham_scores = torch.tensor(bentham_scores, dtype=torch.float32)
                if isinstance(regret_weight, (int, float)):
                    regret_weight = torch.tensor(regret_weight, dtype=torch.float32)
                
                # NaN/Inf ì²˜ë¦¬
                if torch.isnan(bentham_scores) or torch.isinf(bentham_scores):
                    bentham_scores = torch.tensor(0.0, dtype=torch.float32)
                if torch.isnan(regret_weight) or torch.isinf(regret_weight):
                    regret_weight = torch.tensor(1.0, dtype=torch.float32)
                
                # í›„íšŒ ì†ì‹¤ ê³„ì‚° ë° ì œí•œ
                regret_loss = torch.clamp(bentham_scores * 0.1, min=-10.0, max=10.0)
                weighted_regret_loss = torch.clamp(regret_loss * regret_weight, min=-10.0, max=10.0)
                
                # NaN ìµœì¢… ê²€ì‚¬
                if torch.isnan(weighted_regret_loss):
                    logger.warning(f"í›„íšŒ ì†ì‹¤ì— NaN ë°œê²¬, ìŠ¤í‚¨")
                    continue
                    
                total_loss += weighted_regret_loss
        
        # ìµœì¢… NaN ê²€ì‚¬ ë° ì—­ì „íŒŒ ì „ ì•ˆì „ì„± í™•ì¸
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            logger.error(f"ìµœì¢… ì†ì‹¤ì´ ë¹„ì •ìƒì…ë‹ˆë‹¤: {total_loss}. ì—­ì „íŒŒ ìŠ¤í‚¨")
            return {
                'loss': float('nan'),
                'classification_loss': classification_loss.item() if not torch.isnan(classification_loss) else float('nan'),
                'regret_count': regret_count,
                'bentham_count': bentham_count
            }
        
        # ì—­ì „íŒŒ (Mixed Precision)
        self.optimizer.zero_grad()
        
        if self.scaler:
            self.scaler.scale(total_loss).backward()
            self.scaler.unscale_(self.optimizer)
            # ê·¸ë˜ë””ì–¸íŠ¸ NaN ê²€ì‚¬ ë° í´ë¦¬í•‘ (Mixed Precision)
            total_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
            if torch.isnan(total_norm) or torch.isinf(total_norm):
                logger.error(f"ê·¸ë˜ë””ì–¸íŠ¸ normì´ ë¹„ì •ìƒì…ë‹ˆë‹¤: {total_norm}. ì˜µí‹°ë§ˆì´ì € ìŠ¤í‚¨")
                return {
                    'loss': total_loss.item(),
                    'classification_loss': classification_loss.item(),
                    'regret_count': regret_count,
                    'bentham_count': bentham_count
                }
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            total_loss.backward()
            # ê·¸ë˜ë””ì–¸íŠ¸ NaN ê²€ì‚¬ ë° í´ë¦¬í•‘ (FP32 ëª¨ë“œ)
            total_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
            if torch.isnan(total_norm) or torch.isinf(total_norm):
                logger.error(f"ê·¸ë˜ë””ì–¸íŠ¸ normì´ ë¹„ì •ìƒì…ë‹ˆë‹¤: {total_norm}. ì˜µí‹°ë§ˆì´ì € ìŠ¤í‚¨")
                return {
                    'loss': total_loss.item(),
                    'classification_loss': classification_loss.item(),
                    'regret_count': regret_count,
                    'bentham_count': bentham_count
                }
            self.optimizer.step()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        return {
            'loss': total_loss.item(),
            'classification_loss': classification_loss.item(),
            'regret_count': regret_count,
            'bentham_count': bentham_count
        }
    
    def _process_large_batch(self, batch: Dict[str, torch.Tensor], step_idx: int) -> Dict[str, float]:
        """í° ë°°ì¹˜ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬"""
        text_embeddings = batch['text_embedding']
        labels = batch['labels']
        
        chunk_size = self.config.max_safe_batch_size
        total_loss = 0.0
        total_classification_loss = 0.0
        total_regret_count = 0
        total_bentham_count = 0
        
        num_chunks = (len(text_embeddings) + chunk_size - 1) // chunk_size
        
        for i in range(num_chunks):
            start_idx = i * chunk_size
            end_idx = min((i + 1) * chunk_size, len(text_embeddings))
            
            chunk_embeddings = text_embeddings[start_idx:end_idx]
            chunk_labels = labels[start_idx:end_idx]
            
            chunk_batch = {
                'text_embedding': chunk_embeddings,
                'labels': chunk_labels
            }
            
            chunk_stats = self.train_step(chunk_batch, step_idx * num_chunks + i)
            
            total_loss += chunk_stats['loss']
            total_classification_loss += chunk_stats['classification_loss']
            total_regret_count += chunk_stats['regret_count']
            total_bentham_count += chunk_stats['bentham_count']
        
        return {
            'loss': total_loss / num_chunks,
            'classification_loss': total_classification_loss / num_chunks,
            'regret_count': total_regret_count,
            'bentham_count': total_bentham_count
        }
    
    def train_epoch(self, dataloader: DataLoader, epoch: int):
        """ì—í¬í¬ í•™ìŠµ"""
        self.logger.info(f"ğŸ¯ ì—í¬í¬ {epoch+1}/{self.config.epochs} ì‹œì‘")
        
        epoch_stats = defaultdict(list)
        
        for batch_idx, batch in enumerate(dataloader):
            step_stats = self.train_step(batch, batch_idx)
            
            # í†µê³„ ìˆ˜ì§‘
            for key, value in step_stats.items():
                epoch_stats[key].append(value)
                self.training_stats[key].append(value)
            
            self.step_count += 1
            
            # ì£¼ê¸°ì  ë¡œê¹…
            if self.step_count % self.config.log_every_n_steps == 0:
                avg_loss = np.mean(epoch_stats['total_loss'][-self.config.log_every_n_steps:])
                avg_regret = np.mean(epoch_stats['regret_count'][-self.config.log_every_n_steps:])
                
                self.logger.info(
                    f"ìŠ¤í… {self.step_count}: ì†ì‹¤={avg_loss:.4f}, "
                    f"í›„íšŒ={avg_regret:.1f}, GPUë©”ëª¨ë¦¬={self._get_gpu_memory():.1f}MB"
                )
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if self.step_count % self.config.save_checkpoint_every == 0:
                self.save_checkpoint(epoch, batch_idx)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if batch_idx % 5 == 0:
                self._cleanup_memory()
        
        # ì—í¬í¬ ìš”ì•½
        avg_epoch_loss = np.mean(epoch_stats['total_loss'])
        total_regrets = sum(epoch_stats['regret_count'])
        
        self.logger.info(
            f"âœ… ì—í¬í¬ {epoch+1} ì™„ë£Œ: í‰ê·  ì†ì‹¤={avg_epoch_loss:.4f}, "
            f"ì´ í›„íšŒ={total_regrets}"
        )
    
    def _get_gpu_memory(self) -> float:
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ MB"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024 / 1024
        return 0.0
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def save_checkpoint(self, epoch: int, batch_idx: int):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = self.checkpoints_dir / f'hybrid_model_epoch_{epoch}_step_{self.step_count}.pth'
        
        checkpoint = {
            'epoch': epoch,
            'step': self.step_count,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'config': asdict(self.config),
            'training_stats': dict(self.training_stats),
            'timestamp': datetime.now().isoformat()
        }
        
        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        torch.save(checkpoint, checkpoint_path)
        self.logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    def train(self):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤"""
        start_time = time.time()
        
        self.logger.info("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° í•™ìŠµ ì‹œì‘")
        
        # ëª¨ë¸ ë° ë°ì´í„° ì¤€ë¹„
        actual_params = self.prepare_model()
        dataloader = self.prepare_data()
        
        # í•™ìŠµ ì‹¤í–‰
        for epoch in range(self.config.epochs):
            self.train_epoch(dataloader, epoch)
            self.save_checkpoint(epoch, -1)
        
        # ìµœì¢… ì €ì¥
        final_checkpoint = self.checkpoints_dir / 'final_hybrid_model.pth'
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'config': asdict(self.config),
            'training_stats': dict(self.training_stats),
            'total_parameters': actual_params,
            'timestamp': datetime.now().isoformat()
        }, final_checkpoint)
        
        training_time = time.time() - start_time
        
        # í•™ìŠµ ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'training_summary': {
                'total_steps': len(self.training_stats['total_loss']),
                'total_regrets': sum(self.training_stats['regret_count']),
                'total_bentham_calculations': sum(self.training_stats['bentham_count']),
                'final_loss': self.training_stats['total_loss'][-1] if self.training_stats['total_loss'] else 0,
                'training_duration': training_time,
                'average_regrets_per_step': sum(self.training_stats['regret_count']) / len(self.training_stats['regret_count']) if self.training_stats['regret_count'] else 0,
                'average_benthams_per_step': sum(self.training_stats['bentham_count']) / len(self.training_stats['bentham_count']) if self.training_stats['bentham_count'] else 0
            },
            'model_info': {
                'main_model_parameters': actual_params,
                'target_parameters': self.config.target_params,
                'device': str(self.device),
                'hybrid_mode': True,
                'gpu_available': torch.cuda.is_available()
            },
            'configuration': asdict(self.config),
            'training_stats': dict(self.training_stats),
            'storage_usage': {
                'final_size_gb': 0,  # ê³„ì‚° ìƒëµ
                'max_allowed_gb': self.config.max_storage_gb
            },
            'xai_integration': {
                'xai_logs_generated': len(xai_logger.logs) if hasattr(xai_logger, 'logs') else 0,
                'session_id': getattr(xai_logger, 'session_id', 'unknown')
            }
        }
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = self.reports_dir / f'hybrid_training_report_{int(time.time())}.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ ì™„ë£Œ! ì´ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
        self.logger.info(f"ğŸ“Š ì´ í›„íšŒ: {sum(self.training_stats['regret_count'])}")
        self.logger.info(f"ğŸ“Š ì´ ë²¤ë‹´ ê³„ì‚°: {sum(self.training_stats['bentham_count'])}")
        self.logger.info(f"ğŸ“‹ ë¦¬í¬íŠ¸: {report_path}")
        
        return report, final_checkpoint

if __name__ == "__main__":
    # í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    config = HybridConfig()
    trainer = HybridDistributedTrainer(config)
    
    print("ğŸ§ª í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° í•™ìŠµ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
    print(f"ğŸ“Š ì„¤ì •: {config.regrets_per_step}íšŒ í›„íšŒ/ìŠ¤í…, {config.epochs}ë²ˆ ì„ íšŒ")
    print(f"ğŸ¤– ëª¨ë¸: {config.target_params:,}ê°œ íŒŒë¼ë¯¸í„°")
    print(f"âš¡ ì›Œì»¤: {config.num_workers}ê°œ")
    print("ì¤€ë¹„ëœ í•™ìŠµì„ ì‹œì‘í•˜ë ¤ë©´ trainer.train()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")