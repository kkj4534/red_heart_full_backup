"""
OOM (Out of Memory) Handler
ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ìë™ìœ¼ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì • ë° DSM í™œì„±í™”
"""

import torch
import gc
import os
import psutil
import logging
from typing import Dict, Any, Optional, Tuple, Callable
from functools import wraps
import traceback
from datetime import datetime
import json
from pathlib import Path

logger = logging.getLogger(__name__)


class OOMHandler:
    """
    OOM í•¸ë“¤ë§ ì‹œìŠ¤í…œ
    - GPU/CPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
    - ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìë™ ì¡°ì •
    - Dynamic Swap Manager í™œì„±í™”
    - Gradient Accumulation ì¡°ì •
    """
    
    def __init__(self,
                 initial_batch_size: int = 4,
                 min_batch_size: int = 1,
                 gradient_accumulation: int = 16,
                 memory_threshold: float = 0.85,
                 enable_dsm: bool = True):
        """
        Args:
            initial_batch_size: ì´ˆê¸° ë°°ì¹˜ ì‚¬ì´ì¦ˆ
            min_batch_size: ìµœì†Œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
            gradient_accumulation: Gradient Accumulation ìŠ¤í…
            memory_threshold: ë©”ëª¨ë¦¬ ì„ê³„ê°’ (0.85 = 85%)
            enable_dsm: Dynamic Swap Manager ì‚¬ìš© ì—¬ë¶€
        """
        self.initial_batch_size = initial_batch_size
        self.current_batch_size = initial_batch_size
        self.min_batch_size = min_batch_size
        self.gradient_accumulation = gradient_accumulation
        self.memory_threshold = memory_threshold
        self.enable_dsm = enable_dsm
        
        # ë©”ëª¨ë¦¬ í†µê³„
        self.oom_count = 0
        self.batch_size_history = [initial_batch_size]
        self.memory_usage_history = []
        
        # DSM ìƒíƒœ
        self.dsm_active = False
        self.dsm_manager = None
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.has_gpu = torch.cuda.is_available()
        
        logger.info("âœ… OOM Handler ì´ˆê¸°í™”")
        logger.info(f"  - ì´ˆê¸° ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {initial_batch_size}")
        logger.info(f"  - ìµœì†Œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {min_batch_size}")
        logger.info(f"  - Gradient Accumulation: {gradient_accumulation}")
        logger.info(f"  - ë©”ëª¨ë¦¬ ì„ê³„ê°’: {memory_threshold * 100}%")
        logger.info(f"  - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def check_memory_status(self) -> Dict[str, Any]:
        """
        í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        
        Returns:
            ë©”ëª¨ë¦¬ ìƒíƒœ ì •ë³´
        """
        status = {
            'timestamp': datetime.now().isoformat(),
            'cpu': {},
            'gpu': {}
        }
        
        # CPU ë©”ëª¨ë¦¬
        cpu_mem = psutil.virtual_memory()
        status['cpu'] = {
            'total_gb': cpu_mem.total / (1024**3),
            'available_gb': cpu_mem.available / (1024**3),
            'used_gb': cpu_mem.used / (1024**3),
            'percent': cpu_mem.percent
        }
        
        # GPU ë©”ëª¨ë¦¬
        if self.has_gpu:
            try:
                gpu_mem_allocated = torch.cuda.memory_allocated() / (1024**3)
                gpu_mem_reserved = torch.cuda.memory_reserved() / (1024**3)
                gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                
                status['gpu'] = {
                    'total_gb': gpu_mem_total,
                    'allocated_gb': gpu_mem_allocated,
                    'reserved_gb': gpu_mem_reserved,
                    'free_gb': gpu_mem_total - gpu_mem_reserved,
                    'percent': (gpu_mem_reserved / gpu_mem_total) * 100
                }
            except Exception as e:
                logger.warning(f"GPU ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
                status['gpu'] = {'error': str(e)}
        
        return status
    
    def is_memory_critical(self) -> bool:
        """
        ë©”ëª¨ë¦¬ê°€ ì„ê³„ ìƒíƒœì¸ì§€ í™•ì¸
        
        Returns:
            True if memory is critical
        """
        status = self.check_memory_status()
        
        # CPU ë©”ëª¨ë¦¬ ì²´í¬
        if status['cpu']['percent'] > self.memory_threshold * 100:
            return True
        
        # GPU ë©”ëª¨ë¦¬ ì²´í¬
        if self.has_gpu and 'percent' in status['gpu']:
            if status['gpu']['percent'] > self.memory_threshold * 100:
                return True
        
        return False
    
    def handle_oom(self, exception: Exception) -> bool:
        """
        OOM ì˜ˆì™¸ ì²˜ë¦¬
        
        Args:
            exception: ë°œìƒí•œ ì˜ˆì™¸
            
        Returns:
            ë³µêµ¬ ì„±ê³µ ì—¬ë¶€
        """
        self.oom_count += 1
        logger.warning(f"âš ï¸ OOM ë°œìƒ! (#{self.oom_count})")
        logger.warning(f"   ì˜ˆì™¸: {str(exception)}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        self._clear_memory()
        
        # # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°ì†Œ (ì£¼ì„ ì²˜ë¦¬: ë°°ì¹˜ ì‚¬ì´ì¦ˆ 2ì—ì„œ ë” ì´ìƒ ê°ì†Œí•˜ì§€ ì•ŠìŒ)
        # if self.current_batch_size > self.min_batch_size:
        #     old_batch_size = self.current_batch_size
        #     self.current_batch_size = max(self.min_batch_size, self.current_batch_size // 2)
        #     self.batch_size_history.append(self.current_batch_size)
        #     
        #     # Gradient Accumulation ì¡°ì • (ìœ íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìœ ì§€)
        #     effective_batch = old_batch_size * self.gradient_accumulation
        #     self.gradient_accumulation = effective_batch // self.current_batch_size
        #     
        #     logger.info(f"  ğŸ“‰ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •: {old_batch_size} â†’ {self.current_batch_size}")
        #     logger.info(f"  ğŸ“Š Gradient Accumulation ì¡°ì •: {self.gradient_accumulation}")
        #     
        #     return True
        
        # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°ì†Œ ëŒ€ì‹  ë°”ë¡œ False ë°˜í™˜
        logger.warning("  âš ï¸ OOM ë°œìƒ: ë°°ì¹˜ ì‚¬ì´ì¦ˆ 2 ìœ ì§€ (í´ë°± ë¹„í™œì„±í™”)")
        return False
        
        # DSM í™œì„±í™” ì‹œë„
        if self.enable_dsm and not self.dsm_active:
            if self._activate_dsm():
                return True
        
        # ë³µêµ¬ ì‹¤íŒ¨
        logger.error("  âŒ OOM ë³µêµ¬ ì‹¤íŒ¨: ìµœì†Œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë„ë‹¬")
        return False
    
    def _clear_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.has_gpu:
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        gc.collect()
        
        logger.info("  ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    def _activate_dsm(self) -> bool:
        """Dynamic Swap Manager í™œì„±í™”"""
        try:
            # DSM ì„¤ì •
            os.environ['DSM_SYNC_MODE'] = 'true'  # ì´ë²¤íŠ¸ ë£¨í”„ ì—†ëŠ” í™˜ê²½ ëŒ€ë¹„
            
            # DSM import ì‹œë„
            try:
                from dynamic_swap_manager import get_swap_manager
                self.dsm_manager = get_swap_manager()
                self.dsm_active = True
                logger.info("  âœ… Dynamic Swap Manager í™œì„±í™”")
                return True
            except ImportError:
                logger.warning("  âš ï¸ DSM ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
                
        except Exception as e:
            logger.error(f"  âŒ DSM í™œì„±í™” ì‹¤íŒ¨: {e}")
            return False
    
    def safe_forward_pass(self, func: Callable) -> Callable:
        """
        Forward passë¥¼ OOM-safeí•˜ê²Œ ë˜í•‘í•˜ëŠ” ë°ì½”ë ˆì´í„°
        
        Args:
            func: Forward pass í•¨ìˆ˜
            
        Returns:
            ë˜í•‘ëœ í•¨ìˆ˜
        """
        @wraps(func)
        def wrapper(*args, **kwargs):
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                try:
                    # ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
                    if self.is_memory_critical():
                        logger.warning("  âš ï¸ ë©”ëª¨ë¦¬ ì„ê³„ ìƒíƒœ ê°ì§€")
                        self._clear_memory()
                    
                    # Forward pass ì‹¤í–‰
                    result = func(*args, **kwargs)
                    return result
                    
                except RuntimeError as e:
                    if 'out of memory' in str(e).lower() or 'cuda' in str(e).lower():
                        retry_count += 1
                        
                        if self.handle_oom(e):
                            logger.info(f"  ğŸ”„ ì¬ì‹œë„ {retry_count}/{max_retries}")
                            # ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ ì¡°ì •ë˜ì—ˆìœ¼ë¯€ë¡œ í•¨ìˆ˜ ì¸ì ìˆ˜ì • í•„ìš”
                            # ì´ ë¶€ë¶„ì€ ì‹¤ì œ êµ¬í˜„ì—ì„œ ì¡°ì • í•„ìš”
                            continue
                        else:
                            raise
                    else:
                        raise
                        
            raise RuntimeError(f"OOM ë³µêµ¬ ì‹¤íŒ¨: {max_retries}íšŒ ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨")
            
        return wrapper
    
    def adjust_dataloader(self, dataloader: Any) -> Any:
        """
        í˜„ì¬ ë°°ì¹˜ ì‚¬ì´ì¦ˆì— ë§ê²Œ DataLoader ì¡°ì •
        
        Args:
            dataloader: ì›ë³¸ DataLoader
            
        Returns:
            ì¡°ì •ëœ DataLoader
        """
        if hasattr(dataloader, 'batch_size'):
            if dataloader.batch_size != self.current_batch_size:
                logger.info(f"  ğŸ“Š DataLoader ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •: {dataloader.batch_size} â†’ {self.current_batch_size}")
                
                # ìƒˆ DataLoader ìƒì„±
                from torch.utils.data import DataLoader
                new_dataloader = DataLoader(
                    dataset=dataloader.dataset,
                    batch_size=self.current_batch_size,
                    shuffle=dataloader.shuffle if hasattr(dataloader, 'shuffle') else False,
                    num_workers=dataloader.num_workers if hasattr(dataloader, 'num_workers') else 0,
                    pin_memory=dataloader.pin_memory if hasattr(dataloader, 'pin_memory') else False,
                    drop_last=dataloader.drop_last if hasattr(dataloader, 'drop_last') else False
                )
                return new_dataloader
        
        return dataloader
    
    def get_effective_batch_size(self) -> int:
        """
        ìœ íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê³„ì‚°
        
        Returns:
            ìœ íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ (ë°°ì¹˜ ì‚¬ì´ì¦ˆ * gradient accumulation)
        """
        return self.current_batch_size * self.gradient_accumulation
    
    def log_memory_stats(self, step: int, phase: str = 'train'):
        """
        ë©”ëª¨ë¦¬ í†µê³„ ë¡œê¹…
        
        Args:
            step: í˜„ì¬ ìŠ¤í…
            phase: í•™ìŠµ ë‹¨ê³„ ('train', 'val', 'test')
        """
        status = self.check_memory_status()
        
        self.memory_usage_history.append({
            'step': step,
            'phase': phase,
            'batch_size': self.current_batch_size,
            'cpu_percent': status['cpu']['percent'],
            'gpu_percent': status['gpu'].get('percent', 0) if self.has_gpu else 0,
            'timestamp': status['timestamp']
        })
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ìƒì„¸ ë¡œê·¸
        if step % 100 == 0:
            logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ ìƒíƒœ (Step {step}):")
            logger.info(f"   - CPU: {status['cpu']['percent']:.1f}% ({status['cpu']['used_gb']:.1f}/{status['cpu']['total_gb']:.1f} GB)")
            if self.has_gpu and 'percent' in status['gpu']:
                logger.info(f"   - GPU: {status['gpu']['percent']:.1f}% ({status['gpu']['allocated_gb']:.1f}/{status['gpu']['total_gb']:.1f} GB)")
            logger.info(f"   - ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {self.current_batch_size}")
            logger.info(f"   - OOM ë°œìƒ íšŸìˆ˜: {self.oom_count}")
    
    def save_stats(self, output_dir: str = "training/oom_stats"):
        """
        OOM í†µê³„ ì €ì¥
        
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        stats = {
            'initial_batch_size': self.initial_batch_size,
            'final_batch_size': self.current_batch_size,
            'min_batch_size': self.min_batch_size,
            'gradient_accumulation': self.gradient_accumulation,
            'oom_count': self.oom_count,
            'batch_size_history': self.batch_size_history,
            'memory_usage_history': self.memory_usage_history[-1000:],  # ìµœê·¼ 1000ê°œë§Œ
            'dsm_active': self.dsm_active,
            'timestamp': timestamp
        }
        
        output_file = output_dir / f"oom_stats_{timestamp}.json"
        with open(output_file, 'w') as f:
            json.dump(stats, f, indent=2)
        
        logger.info(f"ğŸ“Š OOM í†µê³„ ì €ì¥: {output_file}")
        
        # ìš”ì•½ ë¦¬í¬íŠ¸
        report_file = output_dir / f"oom_report_{timestamp}.txt"
        with open(report_file, 'w') as f:
            f.write("=" * 60 + "\n")
            f.write("OOM Handler Report\n")
            f.write(f"Generated: {timestamp}\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"ì´ˆê¸° ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {self.initial_batch_size}\n")
            f.write(f"ìµœì¢… ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {self.current_batch_size}\n")
            f.write(f"OOM ë°œìƒ íšŸìˆ˜: {self.oom_count}\n")
            f.write(f"DSM í™œì„±í™”: {self.dsm_active}\n")
            f.write(f"ìœ íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {self.get_effective_batch_size()}\n")
            
            if self.batch_size_history:
                f.write("\në°°ì¹˜ ì‚¬ì´ì¦ˆ ë³€ê²½ ì´ë ¥:\n")
                for i, bs in enumerate(self.batch_size_history):
                    f.write(f"  [{i}] {bs}\n")
        
        return str(output_file)
    
    def reset(self):
        """í•¸ë“¤ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”"""
        self.current_batch_size = self.initial_batch_size
        self.batch_size_history = [self.initial_batch_size]
        self.oom_count = 0
        self.memory_usage_history = []
        self.dsm_active = False
        self._clear_memory()
        
        logger.info("ğŸ”„ OOM Handler ì´ˆê¸°í™” ì™„ë£Œ")