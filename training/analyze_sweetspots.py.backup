#!/usr/bin/env python3
"""
Sweet Spot Analysis System
메트릭 수집 및 분석만 수행 (자동 교체 없음)
"""

import json
import numpy as np
import torch
from pathlib import Path
import argparse
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.signal import find_peaks
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SweetSpotAnalyzer:
    """
    Sweet Spot 분석 전용 시스템
    - 5가지 분석 기법 적용
    - 메트릭만 출력 (자동 교체 없음)
    - JSON 및 시각화 리포트 생성
    """
    
    def __init__(self, checkpoint_dir: str, output_dir: str):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # 모듈 정의
        self.modules = [
            'backbone',
            'heads',  # emotion, bentham, regret, surd 통합
            'neural_analyzers',
            'dsp_kalman',
            'advanced_emotion',
            'advanced_bentham', 
            'advanced_regret',
            'advanced_surd'
        ]
        
        # 분석 결과 저장소
        self.analysis_results = {}
        self.raw_metrics = {}
        
    def collect_metrics(self) -> Dict[str, Dict]:
        """모든 checkpoint에서 메트릭 수집"""
        logger.info("📊 메트릭 수집 시작...")
        
        checkpoints = sorted(self.checkpoint_dir.glob("checkpoint_epoch_*.pt"))
        metrics_by_module = defaultdict(lambda: defaultdict(list))
        
        for ckpt_path in checkpoints:
            logger.info(f"  처리 중: {ckpt_path.name}")
            checkpoint = torch.load(ckpt_path, map_location='cpu')
            
            epoch = checkpoint['epoch']
            metrics = checkpoint.get('metrics', {})
            
            # 모듈별 메트릭 추출
            for module in self.modules:
                module_metrics = {
                    'epoch': epoch,
                    'loss': metrics.get(f'{module}_loss', metrics.get('train_loss', 0)),
                    'val_loss': metrics.get(f'{module}_val_loss', metrics.get('val_loss', 0)),
                    'accuracy': metrics.get(f'{module}_acc', 0),
                    'lr': checkpoint.get('lr', 1e-4),
                    'gradient_norm': metrics.get(f'{module}_grad_norm', 0)
                }
                
                # Task-specific metrics
                if module == 'heads':
                    module_metrics['emotion_f1'] = metrics.get('emotion_f1', 0)
                    module_metrics['bentham_rmse'] = metrics.get('bentham_rmse', 0)
                    module_metrics['regret_accuracy'] = metrics.get('regret_acc', 0)
                    module_metrics['surd_pid_acc'] = metrics.get('surd_pid_acc', 0)
                
                for key, value in module_metrics.items():
                    metrics_by_module[module][key].append(value)
        
        self.raw_metrics = dict(metrics_by_module)
        logger.info(f"✅ {len(checkpoints)}개 checkpoint에서 메트릭 수집 완료")
        return self.raw_metrics
    
    def statistical_plateau_detection(self, losses: List[float]) -> Dict:
        """Statistical Plateau Detection using Mann-Kendall and CUSUM"""
        if len(losses) < 5:
            return {'detected': False}
        
        # Mann-Kendall Trend Test
        mk_result = self._mann_kendall_test(losses)
        
        # CUSUM Change Detection
        cusum_changes = self._cusum_detection(losses)
        
        # Find plateau region
        plateau_start = None
        plateau_end = None
        
        # Plateau: 트렌드가 없고 변화점이 없는 구간
        for i in range(len(losses) - 5):
            window = losses[i:i+5]
            window_trend = self._mann_kendall_test(window)
            
            if abs(window_trend['statistic']) < 0.5:  # No significant trend
                if plateau_start is None:
                    plateau_start = i
                plateau_end = i + 5
        
        if plateau_start is not None:
            plateau_center = (plateau_start + plateau_end) // 2
            plateau_mean = np.mean(losses[plateau_start:plateau_end])
            plateau_std = np.std(losses[plateau_start:plateau_end])
            
            return {
                'detected': True,
                'start': plateau_start,
                'end': plateau_end,
                'center': plateau_center,
                'mean_loss': plateau_mean,
                'std': plateau_std,
                'mk_statistic': mk_result['statistic'],
                'cusum_changes': cusum_changes
            }
        
        return {'detected': False}
    
    def _mann_kendall_test(self, data: List[float]) -> Dict:
        """Mann-Kendall 트렌드 테스트"""
        n = len(data)
        s = 0
        
        for i in range(n-1):
            for j in range(i+1, n):
                s += np.sign(data[j] - data[i])
        
        # Calculate variance
        var_s = n * (n - 1) * (2 * n + 5) / 18
        
        if s > 0:
            z = (s - 1) / np.sqrt(var_s)
        elif s < 0:
            z = (s + 1) / np.sqrt(var_s)
        else:
            z = 0
        
        return {'statistic': z, 's': s}
    
    def _cusum_detection(self, data: List[float], threshold: float = None) -> List[int]:
        """CUSUM 변화점 탐지"""
        if threshold is None:
            threshold = np.std(data) * 2
        
        mean = np.mean(data)
        cusum_pos = np.zeros(len(data))
        cusum_neg = np.zeros(len(data))
        changes = []
        
        for i in range(1, len(data)):
            cusum_pos[i] = max(0, cusum_pos[i-1] + data[i] - mean - threshold/2)
            cusum_neg[i] = max(0, cusum_neg[i-1] + mean - data[i] - threshold/2)
            
            if cusum_pos[i] > threshold or cusum_neg[i] > threshold:
                changes.append(i)
                cusum_pos[i] = 0
                cusum_neg[i] = 0
        
        return changes
    
    def calculate_task_metrics(self, module: str, metrics: Dict) -> Dict:
        """모듈별 Task-Specific 메트릭 계산"""
        task_scores = {}
        
        if module == 'heads':
            # 4개 헤드의 통합 점수
            task_scores['emotion_score'] = np.mean(metrics.get('emotion_f1', [0]))
            task_scores['bentham_score'] = 1.0 - np.mean(metrics.get('bentham_rmse', [1.0]))
            task_scores['regret_score'] = np.mean(metrics.get('regret_accuracy', [0]))
            task_scores['surd_score'] = np.mean(metrics.get('surd_pid_acc', [0]))
            task_scores['combined'] = np.mean(list(task_scores.values()))
            
        elif module == 'neural_analyzers':
            # Neural Analyzer 특화 메트릭
            task_scores['stability'] = 1.0 / (1.0 + np.std(metrics.get('loss', [1.0])))
            task_scores['convergence'] = self._calculate_convergence_rate(metrics.get('loss', []))
            
        elif module == 'dsp_kalman':
            # DSP/Kalman 특화 메트릭
            task_scores['tracking_accuracy'] = 1.0 - np.mean(metrics.get('tracking_error', [1.0]))
            task_scores['filter_stability'] = 1.0 / (1.0 + np.std(metrics.get('filter_output', [1.0])))
        
        else:
            # 기본 메트릭
            task_scores['accuracy'] = np.mean(metrics.get('accuracy', [0]))
            task_scores['loss_improvement'] = self._calculate_improvement(metrics.get('loss', []))
        
        return task_scores
    
    def _calculate_convergence_rate(self, losses: List[float]) -> float:
        """수렴 속도 계산"""
        if len(losses) < 2:
            return 0.0
        
        improvements = []
        for i in range(1, len(losses)):
            if losses[i-1] > 0:
                improvement = (losses[i-1] - losses[i]) / losses[i-1]
                improvements.append(improvement)
        
        return np.mean(improvements) if improvements else 0.0
    
    def _calculate_improvement(self, values: List[float]) -> float:
        """개선도 계산"""
        if len(values) < 2:
            return 0.0
        
        initial = np.mean(values[:3]) if len(values) >= 3 else values[0]
        final = np.mean(values[-3:]) if len(values) >= 3 else values[-1]
        
        if initial > 0:
            return (initial - final) / initial
        return 0.0
    
    def mcda_analysis(self, module: str, metrics: Dict) -> Dict:
        """Multi-Criteria Decision Analysis"""
        
        # 기준별 점수 계산
        criteria = {
            'loss': 1.0 - np.array(metrics.get('loss', [1.0])),  # Lower is better
            'val_loss': 1.0 - np.array(metrics.get('val_loss', [1.0])),
            'accuracy': np.array(metrics.get('accuracy', [0])),
            'stability': self._calculate_stability_scores(metrics),
            'gradient_health': self._calculate_gradient_health(metrics)
        }
        
        # 정규화 (0-1 범위)
        normalized = {}
        for key, values in criteria.items():
            if len(values) > 0 and np.std(values) > 0:
                normalized[key] = (values - np.min(values)) / (np.max(values) - np.min(values))
            else:
                normalized[key] = values
        
        # 가중치 (나중에 조정 가능)
        weights = {
            'loss': 0.25,
            'val_loss': 0.25,
            'accuracy': 0.30,
            'stability': 0.10,
            'gradient_health': 0.10
        }
        
        # MCDA 점수 계산
        mcda_scores = np.zeros(len(metrics.get('epoch', [])))
        for key, weight in weights.items():
            if key in normalized and len(normalized[key]) == len(mcda_scores):
                mcda_scores += weight * normalized[key]
        
        # 최적 epoch 찾기
        best_epoch_idx = np.argmax(mcda_scores)
        
        return {
            'scores': mcda_scores.tolist(),
            'best_epoch_idx': int(best_epoch_idx),
            'best_epoch': metrics.get('epoch', [])[best_epoch_idx] if best_epoch_idx < len(metrics.get('epoch', [])) else -1,
            'best_score': float(mcda_scores[best_epoch_idx]),
            'weights': weights
        }
    
    def _calculate_stability_scores(self, metrics: Dict) -> np.ndarray:
        """안정성 점수 계산"""
        losses = metrics.get('loss', [])
        if len(losses) < 3:
            return np.zeros(len(losses))
        
        stability_scores = []
        for i in range(len(losses)):
            start = max(0, i-2)
            end = min(len(losses), i+3)
            window = losses[start:end]
            
            # 낮은 분산 = 높은 안정성
            stability = 1.0 / (1.0 + np.std(window))
            stability_scores.append(stability)
        
        return np.array(stability_scores)
    
    def _calculate_gradient_health(self, metrics: Dict) -> np.ndarray:
        """Gradient Health 점수 계산"""
        grad_norms = metrics.get('gradient_norm', [])
        if not grad_norms:
            return np.zeros(len(metrics.get('epoch', [])))
        
        health_scores = []
        for norm in grad_norms:
            # Gradient가 너무 크거나 작으면 불건전
            if norm > 0:
                if 0.001 < norm < 10.0:  # 건전한 범위
                    health = 1.0
                elif norm < 0.001:  # Vanishing
                    health = norm / 0.001
                else:  # Exploding
                    health = 10.0 / norm
            else:
                health = 0.0
            health_scores.append(health)
        
        return np.array(health_scores)
    
    def ensemble_voting(self, module: str, analyses: Dict) -> Dict:
        """여러 분석 기법의 앙상블 투표"""
        candidates = {}
        
        # 각 기법의 추천 수집
        if analyses['plateau']['detected']:
            candidates['plateau'] = analyses['plateau']['center']
        
        if 'best_epoch_idx' in analyses['mcda']:
            candidates['mcda'] = analyses['mcda']['best_epoch_idx']
        
        # Task metric 최고점
        task_scores = analyses.get('task_scores', {})
        if task_scores:
            combined_scores = task_scores.get('combined', [])
            if combined_scores:
                candidates['task'] = np.argmax(combined_scores)
        
        # Minimum loss
        losses = self.raw_metrics[module].get('loss', [])
        if losses:
            candidates['min_loss'] = np.argmin(losses)
        
        # Gradient health peak
        grad_health = analyses['mcda'].get('gradient_health', [])
        if len(grad_health) > 0:
            candidates['gradient'] = np.argmax(grad_health)
        
        # 투표 집계
        if not candidates:
            return {'selected_epoch': -1, 'confidence': 0.0}
        
        # 가장 많은 표를 받은 epoch
        from collections import Counter
        vote_counts = Counter(candidates.values())
        winner, votes = vote_counts.most_common(1)[0]
        
        # 신뢰도 계산
        confidence = votes / len(candidates) if candidates else 0.0
        
        return {
            'candidates': candidates,
            'selected_epoch_idx': int(winner),
            'selected_epoch': self.raw_metrics[module]['epoch'][winner] if winner < len(self.raw_metrics[module]['epoch']) else -1,
            'votes': votes,
            'total_voters': len(candidates),
            'confidence': float(confidence)
        }
    
    def analyze_module(self, module: str) -> Dict:
        """단일 모듈 종합 분석"""
        logger.info(f"\n🔍 분석 중: {module}")
        
        metrics = self.raw_metrics[module]
        analyses = {}
        
        # 1. Statistical Plateau Detection
        losses = metrics.get('loss', [])
        analyses['plateau'] = self.statistical_plateau_detection(losses)
        
        # 2. Task-Specific Metrics
        task_scores = self.calculate_task_metrics(module, metrics)
        analyses['task_scores'] = task_scores
        
        # 3. MCDA
        analyses['mcda'] = self.mcda_analysis(module, metrics)
        
        # 4. Ensemble Voting
        analyses['voting'] = self.ensemble_voting(module, analyses)
        
        # 종합
        result = {
            'module': module,
            'metrics': metrics,
            'analyses': analyses,
            'recommendation': {
                'epoch_idx': analyses['voting']['selected_epoch_idx'],
                'epoch': analyses['voting']['selected_epoch'],
                'confidence': analyses['voting']['confidence'],
                'reasoning': self._generate_reasoning(module, analyses)
            }
        }
        
        return result
    
    def _generate_reasoning(self, module: str, analyses: Dict) -> List[str]:
        """추천 근거 생성"""
        reasons = []
        
        if analyses['plateau']['detected']:
            reasons.append(f"Plateau 구간 탐지 (Epoch {analyses['plateau']['start']}-{analyses['plateau']['end']})")
        
        if analyses['mcda']['best_score'] > 0.8:
            reasons.append(f"MCDA 점수 우수 ({analyses['mcda']['best_score']:.3f})")
        
        if analyses['voting']['confidence'] > 0.6:
            reasons.append(f"높은 투표 신뢰도 ({analyses['voting']['confidence']:.1%})")
        
        task_scores = analyses.get('task_scores', {})
        if task_scores.get('combined', 0) > 0.7:
            reasons.append(f"Task 메트릭 우수 ({task_scores['combined']:.3f})")
        
        return reasons
    
    def generate_visualizations(self):
        """시각화 생성"""
        logger.info("\n📈 시각화 생성 중...")
        
        viz_dir = self.output_dir / 'visualizations'
        viz_dir.mkdir(exist_ok=True)
        
        for module, result in self.analysis_results.items():
            self._plot_module_analysis(module, result, viz_dir)
        
        # 종합 히트맵
        self._plot_summary_heatmap(viz_dir)
        
    def _plot_module_analysis(self, module: str, result: Dict, viz_dir: Path):
        """모듈별 분석 시각화"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'{module} Sweet Spot Analysis', fontsize=16)
        
        metrics = result['metrics']
        analyses = result['analyses']
        
        # 1. Loss curve with plateau
        ax = axes[0, 0]
        epochs = metrics['epoch']
        losses = metrics['loss']
        ax.plot(epochs, losses, 'b-', label='Training Loss')
        
        if analyses['plateau']['detected']:
            plateau = analyses['plateau']
            ax.axvspan(epochs[plateau['start']], epochs[plateau['end']], 
                      alpha=0.3, color='green', label='Plateau')
            ax.axvline(epochs[plateau['center']], color='red', 
                      linestyle='--', label='Plateau Center')
        
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. MCDA Scores
        ax = axes[0, 1]
        mcda_scores = analyses['mcda']['scores']
        ax.plot(epochs, mcda_scores, 'g-', label='MCDA Score')
        best_idx = analyses['mcda']['best_epoch_idx']
        ax.scatter(epochs[best_idx], mcda_scores[best_idx], 
                  color='red', s=100, label='Best MCDA')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('MCDA Score')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. Task Metrics
        ax = axes[1, 0]
        if module == 'heads' and 'emotion_f1' in metrics:
            ax.plot(epochs, metrics['emotion_f1'], label='Emotion F1')
            ax.plot(epochs, 1-np.array(metrics.get('bentham_rmse', [0]*len(epochs))), label='Bentham Score')
            ax.set_ylabel('Task Scores')
        else:
            ax.plot(epochs, metrics.get('accuracy', [0]*len(epochs)), label='Accuracy')
            ax.set_ylabel('Accuracy')
        
        ax.set_xlabel('Epoch')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 4. Voting Results
        ax = axes[1, 1]
        voting = analyses['voting']
        if voting['candidates']:
            candidates = list(voting['candidates'].keys())
            values = list(voting['candidates'].values())
            colors = ['green' if v == voting['selected_epoch_idx'] else 'blue' for v in values]
            ax.bar(candidates, values, color=colors)
            ax.set_xlabel('Analysis Method')
            ax.set_ylabel('Recommended Epoch Index')
            ax.set_title(f"Final: Epoch {voting['selected_epoch']} (Confidence: {voting['confidence']:.1%})")
        
        plt.tight_layout()
        plt.savefig(viz_dir / f'{module}_analysis.png', dpi=150)
        plt.close()
    
    def _plot_summary_heatmap(self, viz_dir: Path):
        """종합 히트맵"""
        modules = []
        recommended_epochs = []
        confidences = []
        
        for module, result in self.analysis_results.items():
            modules.append(module)
            recommended_epochs.append(result['recommendation']['epoch'])
            confidences.append(result['recommendation']['confidence'])
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Epoch recommendations
        ax1.barh(modules, recommended_epochs, color='steelblue')
        ax1.set_xlabel('Recommended Epoch')
        ax1.set_title('Sweet Spot Epochs by Module')
        ax1.grid(True, alpha=0.3)
        
        # Confidence scores
        ax2.barh(modules, confidences, color='coral')
        ax2.set_xlabel('Confidence Score')
        ax2.set_title('Recommendation Confidence')
        ax2.set_xlim([0, 1])
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'summary_sweetspots.png', dpi=150)
        plt.close()
    
    def save_results(self):
        """결과 저장"""
        # JSON 저장
        json_path = self.output_dir / 'module_metrics.json'
        with open(json_path, 'w') as f:
            json.dump(self.analysis_results, f, indent=2, default=str)
        logger.info(f"📁 JSON 결과 저장: {json_path}")
        
        # Markdown 리포트 생성
        self.generate_markdown_report()
    
    def generate_markdown_report(self):
        """Markdown 형식 리포트 생성"""
        report_path = self.output_dir / 'analysis_report.md'
        
        with open(report_path, 'w') as f:
            f.write("# 🎯 Sweet Spot Analysis Report\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Summary table
            f.write("## 📊 Summary\n\n")
            f.write("| Module | Recommended Epoch | Confidence | Key Reasoning |\n")
            f.write("|--------|------------------|------------|---------------|\n")
            
            for module, result in self.analysis_results.items():
                rec = result['recommendation']
                reasons = ', '.join(rec['reasoning'][:2]) if rec['reasoning'] else 'N/A'
                f.write(f"| {module} | {rec['epoch']} | {rec['confidence']:.1%} | {reasons} |\n")
            
            # Detailed analysis
            f.write("\n## 🔍 Detailed Analysis\n\n")
            
            for module, result in self.analysis_results.items():
                f.write(f"### Module: {module}\n\n")
                
                analyses = result['analyses']
                
                # Plateau
                if analyses['plateau']['detected']:
                    plateau = analyses['plateau']
                    f.write(f"**Plateau Detection:**\n")
                    f.write(f"- Range: Epoch {plateau['start']}-{plateau['end']}\n")
                    f.write(f"- Center: Epoch {plateau['center']}\n")
                    f.write(f"- Mean Loss: {plateau['mean_loss']:.4f} (±{plateau['std']:.4f})\n\n")
                else:
                    f.write("**Plateau Detection:** Not detected\n\n")
                
                # MCDA
                mcda = analyses['mcda']
                f.write(f"**MCDA Analysis:**\n")
                f.write(f"- Best Epoch: {mcda['best_epoch']}\n")
                f.write(f"- Best Score: {mcda['best_score']:.3f}\n\n")
                
                # Voting
                voting = analyses['voting']
                f.write(f"**Ensemble Voting:**\n")
                f.write(f"- Selected: Epoch {voting['selected_epoch']}\n")
                f.write(f"- Confidence: {voting['confidence']:.1%}\n")
                f.write(f"- Votes: {voting['votes']}/{voting['total_voters']}\n\n")
                
                f.write("---\n\n")
            
            # Threshold recommendations
            f.write("## 🎯 Recommended Thresholds for Automation\n\n")
            f.write("```python\n")
            f.write("# Based on empirical analysis\n")
            f.write("thresholds = {\n")
            
            # Calculate empirical thresholds
            all_plateau_stds = []
            for result in self.analysis_results.values():
                if result['analyses']['plateau']['detected']:
                    all_plateau_stds.append(result['analyses']['plateau']['std'])
            
            if all_plateau_stds:
                f.write(f"    'plateau_variance': {np.mean(all_plateau_stds):.4f},\n")
            else:
                f.write(f"    'plateau_variance': 0.01,  # Default\n")
            
            f.write("    'stability_window': 5,\n")
            f.write("    'mcda_weights': {\n")
            f.write("        'loss': 0.25,\n")
            f.write("        'val_loss': 0.25,\n")
            f.write("        'accuracy': 0.30,\n")
            f.write("        'stability': 0.10,\n")
            f.write("        'gradient_health': 0.10\n")
            f.write("    },\n")
            
            # Average confidence
            avg_confidence = np.mean([r['recommendation']['confidence'] 
                                     for r in self.analysis_results.values()])
            f.write(f"    'min_confidence': {avg_confidence * 0.8:.2f}\n")
            f.write("}\n```\n\n")
            
            # Next steps
            f.write("## 📝 Next Steps\n\n")
            f.write("1. Review the recommendations above\n")
            f.write("2. Manually combine modules using recommended epochs\n")
            f.write("3. Evaluate combined model performance\n")
            f.write("4. Adjust thresholds based on results\n")
            f.write("5. Enable automated sweet spot detection\n")
        
        logger.info(f"📄 Markdown 리포트 저장: {report_path}")
    
    def run(self):
        """전체 분석 실행"""
        logger.info("=" * 70)
        logger.info("🎯 Sweet Spot Analysis System 시작")
        logger.info("=" * 70)
        
        # 1. 메트릭 수집
        self.collect_metrics()
        
        # 2. 모듈별 분석
        for module in self.modules:
            if module in self.raw_metrics:
                result = self.analyze_module(module)
                self.analysis_results[module] = result
            else:
                logger.warning(f"⚠️ {module} 메트릭 없음")
        
        # 3. 시각화
        self.generate_visualizations()
        
        # 4. 결과 저장
        self.save_results()
        
        logger.info("\n" + "=" * 70)
        logger.info("✅ 분석 완료!")
        logger.info(f"📁 결과 저장 위치: {self.output_dir}")
        logger.info("=" * 70)
        
        # 요약 출력
        print("\n📊 Sweet Spot Recommendations:")
        print("-" * 50)
        for module, result in self.analysis_results.items():
            rec = result['recommendation']
            print(f"{module:20s}: Epoch {rec['epoch']:3d} (Confidence: {rec['confidence']:.1%})")
        print("-" * 50)


# ============================================
# 자동화 시스템 (나중에 활성화)
# ============================================

"""
# 아래 코드는 threshold 값 확정 후 주석 해제

class AutomatedSweetSpotDetector:
    '''
    Threshold 기반 자동 Sweet Spot 탐색 및 교체
    '''
    
    def __init__(self, thresholds: Dict):
        self.thresholds = thresholds
        
    def auto_detect_and_combine(self, checkpoint_dir: str):
        '''
        자동으로 sweet spot을 찾고 모델 조합
        '''
        # analyzer = SweetSpotAnalyzer(checkpoint_dir, 'temp_analysis')
        # analyzer.run()
        # 
        # # 자동 선택
        # selected_checkpoints = {}
        # for module, result in analyzer.analysis_results.items():
        #     if result['recommendation']['confidence'] > self.thresholds['min_confidence']:
        #         epoch = result['recommendation']['epoch']
        #         ckpt_path = f"{checkpoint_dir}/checkpoint_epoch_{epoch:04d}.pt"
        #         selected_checkpoints[module] = ckpt_path
        # 
        # # 모델 조합
        # combined_model = self.combine_modules(selected_checkpoints)
        # return combined_model
        pass
"""


def main():
    parser = argparse.ArgumentParser(description='Sweet Spot Analysis System')
    parser.add_argument('--checkpoint-dir', type=str, 
                       default='training/checkpoints_final',
                       help='Checkpoint 디렉토리')
    parser.add_argument('--output', type=str, 
                       default='analysis_results',
                       help='출력 디렉토리')
    parser.add_argument('--auto', action='store_true',
                       help='자동 교체 활성화 (threshold 필요)')
    parser.add_argument('--thresholds', type=str,
                       help='Threshold JSON 파일 경로')
    
    args = parser.parse_args()
    
    if args.auto:
        print("⚠️ 자동 교체는 아직 비활성화되어 있습니다.")
        print("먼저 분석 결과를 검토하고 threshold를 설정하세요.")
        return
    
    # 분석만 실행
    analyzer = SweetSpotAnalyzer(args.checkpoint_dir, args.output)
    analyzer.run()


if __name__ == "__main__":
    main()