#!/usr/bin/env python3
"""
Sweet Spot Analysis System
ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„ë§Œ ìˆ˜í–‰ (ìë™ êµì²´ ì—†ìŒ)
"""

import json
import numpy as np
import torch
from pathlib import Path
import argparse
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.signal import find_peaks
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SweetSpotAnalyzer:
    """
    Sweet Spot ë¶„ì„ ì „ìš© ì‹œìŠ¤í…œ
    - 5ê°€ì§€ ë¶„ì„ ê¸°ë²• ì ìš©
    - ë©”íŠ¸ë¦­ë§Œ ì¶œë ¥ (ìë™ êµì²´ ì—†ìŒ)
    - JSON ë° ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„±
    """
    
    def __init__(self, checkpoint_dir: str, output_dir: str):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # ëª¨ë“ˆ ì •ì˜
        self.modules = [
            'backbone',
            'heads',  # emotion, bentham, regret, surd í†µí•©
            'neural_analyzers',
            'dsp_kalman',
            'advanced_emotion',
            'advanced_bentham', 
            'advanced_regret',
            'advanced_surd'
        ]
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ì†Œ
        self.analysis_results = {}
        self.raw_metrics = {}
        
    def collect_metrics(self) -> Dict[str, Dict]:
        """ëª¨ë“  checkpointì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        logger.info("ğŸ“Š ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘...")
        
        checkpoints = sorted(self.checkpoint_dir.glob("checkpoint_epoch_*.pt"))
        metrics_by_module = defaultdict(lambda: defaultdict(list))
        
        for ckpt_path in checkpoints:
            logger.info(f"  ì²˜ë¦¬ ì¤‘: {ckpt_path.name}")
            checkpoint = torch.load(ckpt_path, map_location='cpu')
            
            epoch = checkpoint['epoch']
            metrics = checkpoint.get('metrics', {})
            
            # ëª¨ë“ˆë³„ ë©”íŠ¸ë¦­ ì¶”ì¶œ
            for module in self.modules:
                module_metrics = {
                    'epoch': epoch,
                    'loss': metrics.get(f'{module}_loss', metrics.get('train_loss', 0)),
                    'val_loss': metrics.get(f'{module}_val_loss', metrics.get('val_loss', 0)),
                    'accuracy': metrics.get(f'{module}_acc', 0),
                    'lr': checkpoint.get('lr', 1e-4),
                    'gradient_norm': metrics.get(f'{module}_grad_norm', 0)
                }
                
                # Task-specific metrics
                if module == 'heads':
                    module_metrics['emotion_f1'] = metrics.get('emotion_f1', 0)
                    module_metrics['bentham_rmse'] = metrics.get('bentham_rmse', 0)
                    module_metrics['regret_accuracy'] = metrics.get('regret_acc', 0)
                    module_metrics['surd_pid_acc'] = metrics.get('surd_pid_acc', 0)
                
                for key, value in module_metrics.items():
                    metrics_by_module[module][key].append(value)
        
        self.raw_metrics = dict(metrics_by_module)
        logger.info(f"âœ… {len(checkpoints)}ê°œ checkpointì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ")
        return self.raw_metrics
    
    def statistical_plateau_detection(self, losses: List[float]) -> Dict:
        """Statistical Plateau Detection using Mann-Kendall and CUSUM"""
        if len(losses) < 5:
            return {'detected': False}
        
        # Mann-Kendall Trend Test
        mk_result = self._mann_kendall_test(losses)
        
        # CUSUM Change Detection
        cusum_changes = self._cusum_detection(losses)
        
        # Find plateau region
        plateau_start = None
        plateau_end = None
        
        # Plateau: íŠ¸ë Œë“œê°€ ì—†ê³  ë³€í™”ì ì´ ì—†ëŠ” êµ¬ê°„
        for i in range(len(losses) - 5):
            window = losses[i:i+5]
            window_trend = self._mann_kendall_test(window)
            
            if abs(window_trend['statistic']) < 0.5:  # No significant trend
                if plateau_start is None:
                    plateau_start = i
                plateau_end = i + 5
        
        if plateau_start is not None:
            plateau_center = (plateau_start + plateau_end) // 2
            plateau_mean = np.mean(losses[plateau_start:plateau_end])
            plateau_std = np.std(losses[plateau_start:plateau_end])
            
            return {
                'detected': True,
                'start': plateau_start,
                'end': plateau_end,
                'center': plateau_center,
                'mean_loss': plateau_mean,
                'std': plateau_std,
                'mk_statistic': mk_result['statistic'],
                'cusum_changes': cusum_changes
            }
        
        return {'detected': False}
    
    def _mann_kendall_test(self, data: List[float]) -> Dict:
        """Mann-Kendall íŠ¸ë Œë“œ í…ŒìŠ¤íŠ¸"""
        n = len(data)
        s = 0
        
        for i in range(n-1):
            for j in range(i+1, n):
                s += np.sign(data[j] - data[i])
        
        # Calculate variance
        var_s = n * (n - 1) * (2 * n + 5) / 18
        
        if s > 0:
            z = (s - 1) / np.sqrt(var_s)
        elif s < 0:
            z = (s + 1) / np.sqrt(var_s)
        else:
            z = 0
        
        return {'statistic': z, 's': s}
    
    def _cusum_detection(self, data: List[float], threshold: float = None) -> List[int]:
        """CUSUM ë³€í™”ì  íƒì§€"""
        if threshold is None:
            threshold = np.std(data) * 2
        
        mean = np.mean(data)
        cusum_pos = np.zeros(len(data))
        cusum_neg = np.zeros(len(data))
        changes = []
        
        for i in range(1, len(data)):
            cusum_pos[i] = max(0, cusum_pos[i-1] + data[i] - mean - threshold/2)
            cusum_neg[i] = max(0, cusum_neg[i-1] + mean - data[i] - threshold/2)
            
            if cusum_pos[i] > threshold or cusum_neg[i] > threshold:
                changes.append(i)
                cusum_pos[i] = 0
                cusum_neg[i] = 0
        
        return changes
    
    def calculate_task_metrics(self, module: str, metrics: Dict) -> Dict:
        """ëª¨ë“ˆë³„ Task-Specific ë©”íŠ¸ë¦­ ê³„ì‚°"""
        task_scores = {}
        
        if module == 'heads':
            # 4ê°œ í—¤ë“œì˜ í†µí•© ì ìˆ˜
            task_scores['emotion_score'] = np.mean(metrics.get('emotion_f1', [0]))
            task_scores['bentham_score'] = 1.0 - np.mean(metrics.get('bentham_rmse', [1.0]))
            task_scores['regret_score'] = np.mean(metrics.get('regret_accuracy', [0]))
            task_scores['surd_score'] = np.mean(metrics.get('surd_pid_acc', [0]))
            task_scores['combined'] = np.mean(list(task_scores.values()))
            
        elif module == 'neural_analyzers':
            # Neural Analyzer íŠ¹í™” ë©”íŠ¸ë¦­
            task_scores['stability'] = 1.0 / (1.0 + np.std(metrics.get('loss', [1.0])))
            task_scores['convergence'] = self._calculate_convergence_rate(metrics.get('loss', []))
            
        elif module == 'dsp_kalman':
            # DSP/Kalman íŠ¹í™” ë©”íŠ¸ë¦­
            task_scores['tracking_accuracy'] = 1.0 - np.mean(metrics.get('tracking_error', [1.0]))
            task_scores['filter_stability'] = 1.0 / (1.0 + np.std(metrics.get('filter_output', [1.0])))
        
        else:
            # ê¸°ë³¸ ë©”íŠ¸ë¦­
            task_scores['accuracy'] = np.mean(metrics.get('accuracy', [0]))
            task_scores['loss_improvement'] = self._calculate_improvement(metrics.get('loss', []))
        
        return task_scores
    
    def _calculate_convergence_rate(self, losses: List[float]) -> float:
        """ìˆ˜ë ´ ì†ë„ ê³„ì‚°"""
        if len(losses) < 2:
            return 0.0
        
        improvements = []
        for i in range(1, len(losses)):
            if losses[i-1] > 0:
                improvement = (losses[i-1] - losses[i]) / losses[i-1]
                improvements.append(improvement)
        
        return np.mean(improvements) if improvements else 0.0
    
    def _calculate_improvement(self, values: List[float]) -> float:
        """ê°œì„ ë„ ê³„ì‚°"""
        if len(values) < 2:
            return 0.0
        
        initial = np.mean(values[:3]) if len(values) >= 3 else values[0]
        final = np.mean(values[-3:]) if len(values) >= 3 else values[-1]
        
        if initial > 0:
            return (initial - final) / initial
        return 0.0
    
    def mcda_analysis(self, module: str, metrics: Dict) -> Dict:
        """Multi-Criteria Decision Analysis"""
        
        # ê¸°ì¤€ë³„ ì ìˆ˜ ê³„ì‚°
        criteria = {
            'loss': 1.0 - np.array(metrics.get('loss', [1.0])),  # Lower is better
            'val_loss': 1.0 - np.array(metrics.get('val_loss', [1.0])),
            'accuracy': np.array(metrics.get('accuracy', [0])),
            'stability': self._calculate_stability_scores(metrics),
            'gradient_health': self._calculate_gradient_health(metrics)
        }
        
        # ì •ê·œí™” (0-1 ë²”ìœ„)
        normalized = {}
        for key, values in criteria.items():
            if len(values) > 0 and np.std(values) > 0:
                normalized[key] = (values - np.min(values)) / (np.max(values) - np.min(values))
            else:
                normalized[key] = values
        
        # ê°€ì¤‘ì¹˜ (ë‚˜ì¤‘ì— ì¡°ì • ê°€ëŠ¥)
        weights = {
            'loss': 0.25,
            'val_loss': 0.25,
            'accuracy': 0.30,
            'stability': 0.10,
            'gradient_health': 0.10
        }
        
        # MCDA ì ìˆ˜ ê³„ì‚°
        mcda_scores = np.zeros(len(metrics.get('epoch', [])))
        for key, weight in weights.items():
            if key in normalized and len(normalized[key]) == len(mcda_scores):
                mcda_scores += weight * normalized[key]
        
        # ìµœì  epoch ì°¾ê¸°
        best_epoch_idx = np.argmax(mcda_scores)
        
        return {
            'scores': mcda_scores.tolist(),
            'best_epoch_idx': int(best_epoch_idx),
            'best_epoch': metrics.get('epoch', [])[best_epoch_idx] if best_epoch_idx < len(metrics.get('epoch', [])) else -1,
            'best_score': float(mcda_scores[best_epoch_idx]),
            'weights': weights
        }
    
    def _calculate_stability_scores(self, metrics: Dict) -> np.ndarray:
        """ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚°"""
        losses = metrics.get('loss', [])
        if len(losses) < 3:
            return np.zeros(len(losses))
        
        stability_scores = []
        for i in range(len(losses)):
            start = max(0, i-2)
            end = min(len(losses), i+3)
            window = losses[start:end]
            
            # ë‚®ì€ ë¶„ì‚° = ë†’ì€ ì•ˆì •ì„±
            stability = 1.0 / (1.0 + np.std(window))
            stability_scores.append(stability)
        
        return np.array(stability_scores)
    
    def _calculate_gradient_health(self, metrics: Dict) -> np.ndarray:
        """Gradient Health ì ìˆ˜ ê³„ì‚°"""
        grad_norms = metrics.get('gradient_norm', [])
        if not grad_norms:
            return np.zeros(len(metrics.get('epoch', [])))
        
        health_scores = []
        for norm in grad_norms:
            # Gradientê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ ë¶ˆê±´ì „
            if norm > 0:
                if 0.001 < norm < 10.0:  # ê±´ì „í•œ ë²”ìœ„
                    health = 1.0
                elif norm < 0.001:  # Vanishing
                    health = norm / 0.001
                else:  # Exploding
                    health = 10.0 / norm
            else:
                health = 0.0
            health_scores.append(health)
        
        return np.array(health_scores)
    
    def ensemble_voting(self, module: str, analyses: Dict) -> Dict:
        """ì—¬ëŸ¬ ë¶„ì„ ê¸°ë²•ì˜ ì•™ìƒë¸” íˆ¬í‘œ"""
        candidates = {}
        
        # ê° ê¸°ë²•ì˜ ì¶”ì²œ ìˆ˜ì§‘
        if analyses['plateau']['detected']:
            candidates['plateau'] = analyses['plateau']['center']
        
        if 'best_epoch_idx' in analyses['mcda']:
            candidates['mcda'] = analyses['mcda']['best_epoch_idx']
        
        # Task metric ìµœê³ ì 
        task_scores = analyses.get('task_scores', {})
        if task_scores:
            combined_scores = task_scores.get('combined', [])
            if combined_scores:
                candidates['task'] = np.argmax(combined_scores)
        
        # Minimum loss
        losses = self.raw_metrics[module].get('loss', [])
        if losses:
            candidates['min_loss'] = np.argmin(losses)
        
        # Gradient health peak
        grad_health = analyses['mcda'].get('gradient_health', [])
        if len(grad_health) > 0:
            candidates['gradient'] = np.argmax(grad_health)
        
        # íˆ¬í‘œ ì§‘ê³„
        if not candidates:
            return {'selected_epoch': -1, 'confidence': 0.0}
        
        # ê°€ì¥ ë§ì€ í‘œë¥¼ ë°›ì€ epoch
        from collections import Counter
        vote_counts = Counter(candidates.values())
        winner, votes = vote_counts.most_common(1)[0]
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = votes / len(candidates) if candidates else 0.0
        
        return {
            'candidates': candidates,
            'selected_epoch_idx': int(winner),
            'selected_epoch': self.raw_metrics[module]['epoch'][winner] if winner < len(self.raw_metrics[module]['epoch']) else -1,
            'votes': votes,
            'total_voters': len(candidates),
            'confidence': float(confidence)
        }
    
    def analyze_module(self, module: str) -> Dict:
        """ë‹¨ì¼ ëª¨ë“ˆ ì¢…í•© ë¶„ì„"""
        logger.info(f"\nğŸ” ë¶„ì„ ì¤‘: {module}")
        
        metrics = self.raw_metrics[module]
        analyses = {}
        
        # 1. Statistical Plateau Detection
        losses = metrics.get('loss', [])
        analyses['plateau'] = self.statistical_plateau_detection(losses)
        
        # 2. Task-Specific Metrics
        task_scores = self.calculate_task_metrics(module, metrics)
        analyses['task_scores'] = task_scores
        
        # 3. MCDA
        analyses['mcda'] = self.mcda_analysis(module, metrics)
        
        # 4. Ensemble Voting
        analyses['voting'] = self.ensemble_voting(module, analyses)
        
        # ì¢…í•©
        result = {
            'module': module,
            'metrics': metrics,
            'analyses': analyses,
            'recommendation': {
                'epoch_idx': analyses['voting']['selected_epoch_idx'],
                'epoch': analyses['voting']['selected_epoch'],
                'confidence': analyses['voting']['confidence'],
                'reasoning': self._generate_reasoning(module, analyses)
            }
        }
        
        return result
    
    def _generate_reasoning(self, module: str, analyses: Dict) -> List[str]:
        """ì¶”ì²œ ê·¼ê±° ìƒì„±"""
        reasons = []
        
        if analyses['plateau']['detected']:
            reasons.append(f"Plateau êµ¬ê°„ íƒì§€ (Epoch {analyses['plateau']['start']}-{analyses['plateau']['end']})")
        
        if analyses['mcda']['best_score'] > 0.8:
            reasons.append(f"MCDA ì ìˆ˜ ìš°ìˆ˜ ({analyses['mcda']['best_score']:.3f})")
        
        if analyses['voting']['confidence'] > 0.6:
            reasons.append(f"ë†’ì€ íˆ¬í‘œ ì‹ ë¢°ë„ ({analyses['voting']['confidence']:.1%})")
        
        task_scores = analyses.get('task_scores', {})
        if task_scores.get('combined', 0) > 0.7:
            reasons.append(f"Task ë©”íŠ¸ë¦­ ìš°ìˆ˜ ({task_scores['combined']:.3f})")
        
        return reasons
    
    def generate_visualizations(self):
        """ì‹œê°í™” ìƒì„±"""
        logger.info("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        viz_dir = self.output_dir / 'visualizations'
        viz_dir.mkdir(exist_ok=True)
        
        for module, result in self.analysis_results.items():
            self._plot_module_analysis(module, result, viz_dir)
        
        # ì¢…í•© íˆíŠ¸ë§µ
        self._plot_summary_heatmap(viz_dir)
        
    def _plot_module_analysis(self, module: str, result: Dict, viz_dir: Path):
        """ëª¨ë“ˆë³„ ë¶„ì„ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'{module} Sweet Spot Analysis', fontsize=16)
        
        metrics = result['metrics']
        analyses = result['analyses']
        
        # 1. Loss curve with plateau
        ax = axes[0, 0]
        epochs = metrics['epoch']
        losses = metrics['loss']
        ax.plot(epochs, losses, 'b-', label='Training Loss')
        
        if analyses['plateau']['detected']:
            plateau = analyses['plateau']
            ax.axvspan(epochs[plateau['start']], epochs[plateau['end']], 
                      alpha=0.3, color='green', label='Plateau')
            ax.axvline(epochs[plateau['center']], color='red', 
                      linestyle='--', label='Plateau Center')
        
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. MCDA Scores
        ax = axes[0, 1]
        mcda_scores = analyses['mcda']['scores']
        ax.plot(epochs, mcda_scores, 'g-', label='MCDA Score')
        best_idx = analyses['mcda']['best_epoch_idx']
        ax.scatter(epochs[best_idx], mcda_scores[best_idx], 
                  color='red', s=100, label='Best MCDA')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('MCDA Score')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 3. Task Metrics
        ax = axes[1, 0]
        if module == 'heads' and 'emotion_f1' in metrics:
            ax.plot(epochs, metrics['emotion_f1'], label='Emotion F1')
            ax.plot(epochs, 1-np.array(metrics.get('bentham_rmse', [0]*len(epochs))), label='Bentham Score')
            ax.set_ylabel('Task Scores')
        else:
            ax.plot(epochs, metrics.get('accuracy', [0]*len(epochs)), label='Accuracy')
            ax.set_ylabel('Accuracy')
        
        ax.set_xlabel('Epoch')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 4. Voting Results
        ax = axes[1, 1]
        voting = analyses['voting']
        if voting['candidates']:
            candidates = list(voting['candidates'].keys())
            values = list(voting['candidates'].values())
            colors = ['green' if v == voting['selected_epoch_idx'] else 'blue' for v in values]
            ax.bar(candidates, values, color=colors)
            ax.set_xlabel('Analysis Method')
            ax.set_ylabel('Recommended Epoch Index')
            ax.set_title(f"Final: Epoch {voting['selected_epoch']} (Confidence: {voting['confidence']:.1%})")
        
        plt.tight_layout()
        plt.savefig(viz_dir / f'{module}_analysis.png', dpi=150)
        plt.close()
    
    def _plot_summary_heatmap(self, viz_dir: Path):
        """ì¢…í•© íˆíŠ¸ë§µ"""
        modules = []
        recommended_epochs = []
        confidences = []
        
        for module, result in self.analysis_results.items():
            modules.append(module)
            recommended_epochs.append(result['recommendation']['epoch'])
            confidences.append(result['recommendation']['confidence'])
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Epoch recommendations
        ax1.barh(modules, recommended_epochs, color='steelblue')
        ax1.set_xlabel('Recommended Epoch')
        ax1.set_title('Sweet Spot Epochs by Module')
        ax1.grid(True, alpha=0.3)
        
        # Confidence scores
        ax2.barh(modules, confidences, color='coral')
        ax2.set_xlabel('Confidence Score')
        ax2.set_title('Recommendation Confidence')
        ax2.set_xlim([0, 1])
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'summary_sweetspots.png', dpi=150)
        plt.close()
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        # JSON ì €ì¥
        json_path = self.output_dir / 'module_metrics.json'
        with open(json_path, 'w') as f:
            json.dump(self.analysis_results, f, indent=2, default=str)
        logger.info(f"ğŸ“ JSON ê²°ê³¼ ì €ì¥: {json_path}")
        
        # Markdown ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_markdown_report()
    
    def generate_markdown_report(self):
        """Markdown í˜•ì‹ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_path = self.output_dir / 'analysis_report.md'
        
        with open(report_path, 'w') as f:
            f.write("# ğŸ¯ Sweet Spot Analysis Report\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Summary table
            f.write("## ğŸ“Š Summary\n\n")
            f.write("| Module | Recommended Epoch | Confidence | Key Reasoning |\n")
            f.write("|--------|------------------|------------|---------------|\n")
            
            for module, result in self.analysis_results.items():
                rec = result['recommendation']
                reasons = ', '.join(rec['reasoning'][:2]) if rec['reasoning'] else 'N/A'
                f.write(f"| {module} | {rec['epoch']} | {rec['confidence']:.1%} | {reasons} |\n")
            
            # Detailed analysis
            f.write("\n## ğŸ” Detailed Analysis\n\n")
            
            for module, result in self.analysis_results.items():
                f.write(f"### Module: {module}\n\n")
                
                analyses = result['analyses']
                
                # Plateau
                if analyses['plateau']['detected']:
                    plateau = analyses['plateau']
                    f.write(f"**Plateau Detection:**\n")
                    f.write(f"- Range: Epoch {plateau['start']}-{plateau['end']}\n")
                    f.write(f"- Center: Epoch {plateau['center']}\n")
                    f.write(f"- Mean Loss: {plateau['mean_loss']:.4f} (Â±{plateau['std']:.4f})\n\n")
                else:
                    f.write("**Plateau Detection:** Not detected\n\n")
                
                # MCDA
                mcda = analyses['mcda']
                f.write(f"**MCDA Analysis:**\n")
                f.write(f"- Best Epoch: {mcda['best_epoch']}\n")
                f.write(f"- Best Score: {mcda['best_score']:.3f}\n\n")
                
                # Voting
                voting = analyses['voting']
                f.write(f"**Ensemble Voting:**\n")
                f.write(f"- Selected: Epoch {voting['selected_epoch']}\n")
                f.write(f"- Confidence: {voting['confidence']:.1%}\n")
                f.write(f"- Votes: {voting['votes']}/{voting['total_voters']}\n\n")
                
                f.write("---\n\n")
            
            # Threshold recommendations
            f.write("## ğŸ¯ Recommended Thresholds for Automation\n\n")
            f.write("```python\n")
            f.write("# Based on empirical analysis\n")
            f.write("thresholds = {\n")
            
            # Calculate empirical thresholds
            all_plateau_stds = []
            for result in self.analysis_results.values():
                if result['analyses']['plateau']['detected']:
                    all_plateau_stds.append(result['analyses']['plateau']['std'])
            
            if all_plateau_stds:
                f.write(f"    'plateau_variance': {np.mean(all_plateau_stds):.4f},\n")
            else:
                f.write(f"    'plateau_variance': 0.01,  # Default\n")
            
            f.write("    'stability_window': 5,\n")
            f.write("    'mcda_weights': {\n")
            f.write("        'loss': 0.25,\n")
            f.write("        'val_loss': 0.25,\n")
            f.write("        'accuracy': 0.30,\n")
            f.write("        'stability': 0.10,\n")
            f.write("        'gradient_health': 0.10\n")
            f.write("    },\n")
            
            # Average confidence
            avg_confidence = np.mean([r['recommendation']['confidence'] 
                                     for r in self.analysis_results.values()])
            f.write(f"    'min_confidence': {avg_confidence * 0.8:.2f}\n")
            f.write("}\n```\n\n")
            
            # Next steps
            f.write("## ğŸ“ Next Steps\n\n")
            f.write("1. Review the recommendations above\n")
            f.write("2. Manually combine modules using recommended epochs\n")
            f.write("3. Evaluate combined model performance\n")
            f.write("4. Adjust thresholds based on results\n")
            f.write("5. Enable automated sweet spot detection\n")
        
        logger.info(f"ğŸ“„ Markdown ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    
    def run(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        logger.info("=" * 70)
        logger.info("ğŸ¯ Sweet Spot Analysis System ì‹œì‘")
        logger.info("=" * 70)
        
        # 1. ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        self.collect_metrics()
        
        # 2. ëª¨ë“ˆë³„ ë¶„ì„
        for module in self.modules:
            if module in self.raw_metrics:
                result = self.analyze_module(module)
                self.analysis_results[module] = result
            else:
                logger.warning(f"âš ï¸ {module} ë©”íŠ¸ë¦­ ì—†ìŒ")
        
        # 3. ì‹œê°í™”
        self.generate_visualizations()
        
        # 4. ê²°ê³¼ ì €ì¥
        self.save_results()
        
        logger.info("\n" + "=" * 70)
        logger.info("âœ… ë¶„ì„ ì™„ë£Œ!")
        logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}")
        logger.info("=" * 70)
        
        # ìš”ì•½ ì¶œë ¥
        print("\nğŸ“Š Sweet Spot Recommendations:")
        print("-" * 50)
        for module, result in self.analysis_results.items():
            rec = result['recommendation']
            print(f"{module:20s}: Epoch {rec['epoch']:3d} (Confidence: {rec['confidence']:.1%})")
        print("-" * 50)


# ============================================
# ìë™í™” ì‹œìŠ¤í…œ (ë‚˜ì¤‘ì— í™œì„±í™”)
# ============================================

"""
# ì•„ë˜ ì½”ë“œëŠ” threshold ê°’ í™•ì • í›„ ì£¼ì„ í•´ì œ

class AutomatedSweetSpotDetector:
    '''
    Threshold ê¸°ë°˜ ìë™ Sweet Spot íƒìƒ‰ ë° êµì²´
    '''
    
    def __init__(self, thresholds: Dict):
        self.thresholds = thresholds
        
    def auto_detect_and_combine(self, checkpoint_dir: str):
        '''
        ìë™ìœ¼ë¡œ sweet spotì„ ì°¾ê³  ëª¨ë¸ ì¡°í•©
        '''
        # analyzer = SweetSpotAnalyzer(checkpoint_dir, 'temp_analysis')
        # analyzer.run()
        # 
        # # ìë™ ì„ íƒ
        # selected_checkpoints = {}
        # for module, result in analyzer.analysis_results.items():
        #     if result['recommendation']['confidence'] > self.thresholds['min_confidence']:
        #         epoch = result['recommendation']['epoch']
        #         ckpt_path = f"{checkpoint_dir}/checkpoint_epoch_{epoch:04d}.pt"
        #         selected_checkpoints[module] = ckpt_path
        # 
        # # ëª¨ë¸ ì¡°í•©
        # combined_model = self.combine_modules(selected_checkpoints)
        # return combined_model
        pass
"""


def main():
    parser = argparse.ArgumentParser(description='Sweet Spot Analysis System')
    parser.add_argument('--checkpoint-dir', type=str, 
                       default='training/checkpoints_final',
                       help='Checkpoint ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', type=str, 
                       default='analysis_results',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--auto', action='store_true',
                       help='ìë™ êµì²´ í™œì„±í™” (threshold í•„ìš”)')
    parser.add_argument('--thresholds', type=str,
                       help='Threshold JSON íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    if args.auto:
        print("âš ï¸ ìë™ êµì²´ëŠ” ì•„ì§ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € ë¶„ì„ ê²°ê³¼ë¥¼ ê²€í† í•˜ê³  thresholdë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return
    
    # ë¶„ì„ë§Œ ì‹¤í–‰
    analyzer = SweetSpotAnalyzer(args.checkpoint_dir, args.output)
    analyzer.run()


if __name__ == "__main__":
    main()