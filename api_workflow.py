#!/usr/bin/env python3
"""
Claude API ì „ìš© ì›Œí¬í”Œë¡œìš°
- DSM ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
- ë¶„í•  ì´ˆê¸°í™”ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”
- Claude APIë¡œ ì§ì ‘ ì²˜ë¦¬ (ë²ˆì—­ê¸° ë¶ˆí•„ìš”)
"""

import os
import sys
import asyncio
import argparse
import logging
import torch
import torch.nn as nn
import gc
import time
import json
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

# í™˜ê²½ ì„¤ì •
os.environ['TORCH_HOME'] = str(PROJECT_ROOT / '.cache' / 'torch')
os.environ['HF_HOME'] = str(PROJECT_ROOT / '.cache' / 'huggingface')
os.environ['TRANSFORMERS_CACHE'] = str(PROJECT_ROOT / '.cache' / 'transformers')
os.environ['FORCE_CPU_INIT'] = '1'  # SentenceTransformerëŠ” í•­ìƒ CPU

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


class GPUMemoryManager:
    """í•˜ë“œì½”ë”©ëœ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.cpu_device = torch.device('cpu')
        self.models_on_gpu = {}
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def get_gpu_memory_info(self) -> Tuple[float, float, float]:
        """GPU ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜ (used_mb, free_mb, total_mb)"""
        if not torch.cuda.is_available():
            return 0.0, 0.0, 0.0
            
        torch.cuda.synchronize()
        allocated = torch.cuda.memory_allocated() / 1024**2
        reserved = torch.cuda.memory_reserved() / 1024**2
        total = torch.cuda.get_device_properties(0).total_memory / 1024**2
        free = total - reserved
        
        return allocated, free, total
        
    def clear_cache(self):
        """GPU ìºì‹œ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        
    def move_to_gpu(self, model: nn.Module, name: str, max_memory_mb: float = 4000) -> nn.Module:
        """ëª¨ë¸ì„ GPUë¡œ ì´ë™ (ë©”ëª¨ë¦¬ ì œí•œ í™•ì¸)"""
        used, free, total = self.get_gpu_memory_info()
        
        if free < max_memory_mb:
            self.logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {free:.1f}MB < {max_memory_mb:.1f}MB")
            # ê°€ì¥ ì˜¤ë˜ëœ ëª¨ë¸ ì–¸ë¡œë“œ
            if self.models_on_gpu:
                oldest_name = list(self.models_on_gpu.keys())[0]
                self.move_to_cpu(self.models_on_gpu[oldest_name], oldest_name)
                self.clear_cache()
                
        model = model.to(self.device)
        self.models_on_gpu[name] = model
        
        used, free, total = self.get_gpu_memory_info()
        self.logger.info(f"âœ… {name} GPU ë¡œë“œ ì™„ë£Œ: {used:.1f}/{total:.1f}MB ì‚¬ìš©ì¤‘")
        
        return model
        
    def move_to_cpu(self, model: nn.Module, name: str) -> nn.Module:
        """ëª¨ë¸ì„ CPUë¡œ ì´ë™"""
        model = model.to(self.cpu_device)
        
        if name in self.models_on_gpu:
            del self.models_on_gpu[name]
            
        self.clear_cache()
        
        used, free, total = self.get_gpu_memory_info()
        self.logger.info(f"âœ… {name} CPU ì–¸ë¡œë“œ ì™„ë£Œ: {used:.1f}/{total:.1f}MB ì‚¬ìš©ì¤‘")
        
        return model


class ClaudeAPIWorkflow:
    """Claude API ì „ìš© ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self, args):
        self.args = args
        self.logger = logging.getLogger(self.__class__.__name__)
        self.gpu_manager = GPUMemoryManager()
        
        # ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ (ì§€ì—° ë¡œë”©)
        self.sentence_transformer = None
        self.unified_model = None
        self.emotion_analyzer = None
        self.bentham_calculator = None
        self.regret_system = None
        self.counterfactual = None
        self.circuit = None
        self.claude_client = None
        
        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        self.checkpoint_path = PROJECT_ROOT / 'training' / 'checkpoints_final'
        
    async def initialize(self):
        """ë¶„í•  ì´ˆê¸°í™” - ë‹¨ê³„ë³„ë¡œ ë©”ëª¨ë¦¬ ê´€ë¦¬"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸš€ Claude API ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì‹œì‘")
        self.logger.info("=" * 60)
        
        try:
            # 1ë‹¨ê³„: SentenceTransformer (CPU ì „ìš©)
            await self._init_sentence_transformer()
            
            # 2ë‹¨ê³„: Claude API í´ë¼ì´ì–¸íŠ¸
            await self._init_claude_api()
            
            # 3ë‹¨ê³„: UnifiedModel (í•„ìš”ì‹œì—ë§Œ GPU ë¡œë“œ)
            await self._init_unified_model()
            
            # 4ë‹¨ê³„: ë¶„ì„ ëª¨ë“ˆë“¤ (í•„ìš”ì‹œì—ë§Œ ë¡œë“œ)
            # ë‚˜ì¤‘ì— í•„ìš”í•  ë•Œ ê°œë³„ì ìœ¼ë¡œ ë¡œë“œ
            
            self.logger.info("âœ… ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
            
    async def _init_sentence_transformer(self):
        """SentenceTransformer ì´ˆê¸°í™” (CPU ì „ìš©)"""
        self.logger.info("\nğŸ“ SentenceTransformer ì´ˆê¸°í™” (CPU)...")
        
        from sentence_transformer_singleton import SentenceTransformerManager
        
        # CPU ê°•ì œ ì„¤ì •
        self.sentence_transformer = SentenceTransformerManager(device='cpu')
        
        self.logger.info("âœ… SentenceTransformer CPU ì´ˆê¸°í™” ì™„ë£Œ")
        
    async def _init_claude_api(self):
        """Claude API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.logger.info("\nğŸ¤– Claude API ì´ˆê¸°í™”...")
        
        from api_key_manager.llm_clients.claude_client import ClaudeAPIClient
        
        self.claude_client = ClaudeAPIClient()
        await self.claude_client.initialize()
        
        self.logger.info("âœ… Claude API ì´ˆê¸°í™” ì™„ë£Œ")
        
    async def _init_unified_model(self):
        """UnifiedModel ì´ˆê¸°í™” (CPUë¡œ ë¨¼ì € ë¡œë“œ)"""
        self.logger.info("\nğŸ§  UnifiedModel ì´ˆê¸°í™”...")
        
        from unified_model import UnifiedModel
        from config import Config
        
        # ì„¤ì • ë¡œë“œ
        config = Config()
        
        # CPUì—ì„œ ë¨¼ì € ë¡œë“œ
        self.unified_model = UnifiedModel(config)
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint_file = self.checkpoint_path / 'accumulated_checkpoint.pt'
        if checkpoint_file.exists():
            self.logger.info(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_file}")
            checkpoint = torch.load(checkpoint_file, map_location='cpu')
            
            if 'unified_model_state' in checkpoint:
                self.unified_model.load_state_dict(checkpoint['unified_model_state'])
                self.logger.info("âœ… UnifiedModel ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
            else:
                self.logger.warning("âš ï¸ ì²´í¬í¬ì¸íŠ¸ì— UnifiedModel ìƒíƒœ ì—†ìŒ")
                
        self.unified_model.eval()
        self.logger.info("âœ… UnifiedModel ì´ˆê¸°í™” ì™„ë£Œ (CPU)")
        
    async def _load_analyzer_on_demand(self, analyzer_name: str):
        """í•„ìš”ì‹œ ë¶„ì„ê¸° ë¡œë“œ"""
        self.logger.info(f"\nğŸ”„ {analyzer_name} ë¶„ì„ê¸° ë¡œë“œ...")
        
        if analyzer_name == 'emotion' and not self.emotion_analyzer:
            from advanced_emotion_analyzer import AdvancedEmotionAnalyzer
            self.emotion_analyzer = AdvancedEmotionAnalyzer()
            
        elif analyzer_name == 'bentham' and not self.bentham_calculator:
            from advanced_bentham_calculator import AdvancedBenthamCalculator
            self.bentham_calculator = AdvancedBenthamCalculator()
            
        elif analyzer_name == 'regret' and not self.regret_system:
            from advanced_regret_learning_system import AdvancedRegretLearningSystem
            self.regret_system = AdvancedRegretLearningSystem()
            
        elif analyzer_name == 'counterfactual' and not self.counterfactual:
            from advanced_counterfactual_reasoning import AdvancedCounterfactualReasoning
            self.counterfactual = AdvancedCounterfactualReasoning()
            
        self.logger.info(f"âœ… {analyzer_name} ë¡œë“œ ì™„ë£Œ")
        
    async def inference(self, text: str) -> Dict[str, Any]:
        """ì¶”ë¡  ì‹¤í–‰ - ë‹¨ê³„ë³„ GPU ê´€ë¦¬"""
        self.logger.info("\n" + "=" * 60)
        self.logger.info(f"ğŸ¯ ì¶”ë¡  ì‹œì‘: {text[:50]}...")
        self.logger.info("=" * 60)
        
        results = {}
        
        try:
            # Phase 1: í…ìŠ¤íŠ¸ ì„ë² ë”© (CPU)
            self.logger.info("\nğŸ“ Phase 1: í…ìŠ¤íŠ¸ ì„ë² ë”©...")
            embeddings = await self._get_embeddings(text)
            results['embeddings'] = embeddings
            
            # Phase 2: Claude API ì´ˆê¸° ë¶„ì„
            self.logger.info("\nğŸ¤– Phase 2: Claude API ë¶„ì„...")
            claude_analysis = await self._claude_analysis(text)
            results['claude_initial'] = claude_analysis
            
            # Phase 3: UnifiedModel ì²˜ë¦¬ (GPU ë¡œë“œ/ì–¸ë¡œë“œ)
            self.logger.info("\nğŸ§  Phase 3: UnifiedModel ì²˜ë¦¬...")
            unified_results = await self._unified_model_inference(embeddings)
            results['unified'] = unified_results
            
            # Phase 4: ì‹¬ì¸µ ë¶„ì„ê¸° (ì„ íƒì , GPU ê´€ë¦¬)
            if self.args.deep_analysis:
                self.logger.info("\nğŸ” Phase 4: ì‹¬ì¸µ ë¶„ì„...")
                deep_results = await self._deep_analysis(embeddings, unified_results)
                results['deep_analysis'] = deep_results
                
            # Phase 5: Circuit í†µí•©
            self.logger.info("\nâš¡ Phase 5: Circuit í†µí•©...")
            circuit_results = await self._circuit_integration(results)
            results['circuit'] = circuit_results
            
            # Phase 6: Claude API ìµœì¢… ë³´ì •
            self.logger.info("\nğŸ¯ Phase 6: ìµœì¢… ë³´ì •...")
            final_results = await self._claude_final_correction(results)
            results['final'] = final_results
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise
            
    async def _get_embeddings(self, text: str) -> torch.Tensor:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (CPU)"""
        if not self.sentence_transformer:
            await self._init_sentence_transformer()
            
        embeddings = self.sentence_transformer.encode(
            [text],
            convert_to_tensor=True
        )
        
        self.logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: shape={embeddings.shape}")
        return embeddings
        
    async def _claude_analysis(self, text: str) -> Dict[str, Any]:
        """Claude APIë¡œ ì´ˆê¸° ë¶„ì„"""
        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìœ¤ë¦¬ì , ê°ì •ì  ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
        
        í…ìŠ¤íŠ¸: {text}
        
        ë¶„ì„ í•­ëª©:
        1. ê°ì • ìƒíƒœ ë° ê°•ë„
        2. ìœ¤ë¦¬ì  ë”œë ˆë§ˆ í¬ì¸íŠ¸
        3. ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤ë“¤
        4. ì ì¬ì  ê²°ê³¼
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
        """
        
        response = await self.claude_client.generate_async(prompt)
        
        try:
            analysis = json.loads(response.generated_text)
        except json.JSONDecodeError:
            analysis = {"raw_response": response.generated_text}
            
        self.logger.info("âœ… Claude ì´ˆê¸° ë¶„ì„ ì™„ë£Œ")
        return analysis
        
    async def _unified_model_inference(self, embeddings: torch.Tensor) -> Dict[str, Any]:
        """UnifiedModel ì¶”ë¡  (GPU ë¡œë“œ/ì–¸ë¡œë“œ)"""
        results = {}
        
        # GPUë¡œ ëª¨ë¸ ì´ë™
        self.unified_model = self.gpu_manager.move_to_gpu(
            self.unified_model, 
            "UnifiedModel",
            max_memory_mb=3000  # 3GB ì œí•œ
        )
        
        # ì„ë² ë”©ë„ GPUë¡œ
        embeddings = embeddings.to(self.gpu_manager.device)
        
        try:
            with torch.no_grad():
                # Emotion íƒœìŠ¤í¬
                emotion_out = self.unified_model(
                    x=embeddings,
                    task_type='emotion',
                    epoch=self.args.epoch
                )
                results['emotion'] = emotion_out
                
                # Bentham íƒœìŠ¤í¬
                bentham_out = self.unified_model(
                    x=embeddings,
                    task_type='bentham',
                    epoch=self.args.epoch
                )
                results['bentham'] = bentham_out
                
                # Regret íƒœìŠ¤í¬
                regret_out = self.unified_model(
                    x=embeddings,
                    task_type='regret',
                    epoch=self.args.epoch
                )
                results['regret'] = regret_out
                
        finally:
            # GPUì—ì„œ ì–¸ë¡œë“œ
            self.unified_model = self.gpu_manager.move_to_cpu(
                self.unified_model,
                "UnifiedModel"
            )
            
        self.logger.info("âœ… UnifiedModel ì²˜ë¦¬ ì™„ë£Œ")
        return results
        
    async def _deep_analysis(self, embeddings: torch.Tensor, unified_results: Dict) -> Dict[str, Any]:
        """ì‹¬ì¸µ ë¶„ì„ (ì„ íƒì )"""
        results = {}
        
        # ê°ì • ë¶„ì„
        if self.args.use_emotion:
            await self._load_analyzer_on_demand('emotion')
            emotion_result = await self.emotion_analyzer.analyze(
                embeddings, 
                unified_results.get('emotion')
            )
            results['emotion'] = emotion_result
            
        # Bentham ê³„ì‚°
        if self.args.use_bentham:
            await self._load_analyzer_on_demand('bentham')
            bentham_result = await self.bentham_calculator.calculate(
                unified_results.get('bentham')
            )
            results['bentham'] = bentham_result
            
        return results
        
    async def _circuit_integration(self, all_results: Dict) -> Dict[str, Any]:
        """Circuit í†µí•©"""
        if not self.circuit:
            from emotion_ethics_regret_circuit import EmotionEthicsRegretCircuit
            self.circuit = EmotionEthicsRegretCircuit()
            
        # Circuitì€ CPUì—ì„œ ì‹¤í–‰
        integrated = await self.circuit.integrate(all_results)
        
        self.logger.info("âœ… Circuit í†µí•© ì™„ë£Œ")
        return integrated
        
    async def _claude_final_correction(self, results: Dict) -> Dict[str, Any]:
        """Claude APIë¡œ ìµœì¢… ë³´ì •"""
        prompt = f"""
        ë‹¤ìŒ AI ë¶„ì„ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ìœ¤ë¦¬ì  íŒë‹¨ì„ ë‚´ë ¤ì£¼ì„¸ìš”:
        
        ë¶„ì„ ê²°ê³¼:
        {json.dumps(results, indent=2, ensure_ascii=False)}
        
        ìµœì¢… íŒë‹¨:
        1. ì¢…í•©ì  ìœ¤ë¦¬ í‰ê°€
        2. ê¶Œì¥ í–‰ë™
        3. ì£¼ì˜ì‚¬í•­
        4. ì‹ ë¢°ë„ ì ìˆ˜
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
        """
        
        response = await self.claude_client.generate_async(prompt)
        
        try:
            final = json.loads(response.generated_text)
        except json.JSONDecodeError:
            final = {"raw_response": response.generated_text}
            
        self.logger.info("âœ… ìµœì¢… ë³´ì • ì™„ë£Œ")
        return final
        
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("\nğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬...")
        
        # ëª¨ë“  ëª¨ë¸ CPUë¡œ ì´ë™
        if self.unified_model:
            self.unified_model = self.unified_model.cpu()
            
        # GPU ìºì‹œ ì •ë¦¬
        self.gpu_manager.clear_cache()
        
        # SentenceTransformer ì •ë¦¬
        if self.sentence_transformer:
            pass  # SentenceTransformerManagerëŠ” ìë™ìœ¼ë¡œ ì •ë¦¬ë¨
            
        self.logger.info("âœ… ì •ë¦¬ ì™„ë£Œ")


async def main():
    parser = argparse.ArgumentParser(description='Claude API ì›Œí¬í”Œë¡œìš°')
    
    # ê¸°ë³¸ ì¸ì
    parser.add_argument('--text', type=str, required=True, help='ë¶„ì„í•  í…ìŠ¤íŠ¸')
    parser.add_argument('--epoch', type=int, default=50, help='ì—í­ ë²ˆí˜¸')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
    
    # ë¶„ì„ ì˜µì…˜
    parser.add_argument('--deep-analysis', action='store_true', help='ì‹¬ì¸µ ë¶„ì„ ìˆ˜í–‰')
    parser.add_argument('--use-emotion', action='store_true', help='ê°ì • ë¶„ì„ ì‚¬ìš©')
    parser.add_argument('--use-bentham', action='store_true', help='Bentham ê³„ì‚° ì‚¬ìš©')
    parser.add_argument('--use-regret', action='store_true', help='í›„íšŒ í•™ìŠµ ì‚¬ìš©')
    
    # ì¶œë ¥ ì˜µì…˜
    parser.add_argument('--output', type=str, help='ê²°ê³¼ ì €ì¥ íŒŒì¼')
    
    args = parser.parse_args()
    
    # ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        
    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    workflow = ClaudeAPIWorkflow(args)
    
    try:
        # ì´ˆê¸°í™”
        await workflow.initialize()
        
        # ì¶”ë¡  ì‹¤í–‰
        results = await workflow.inference(args.text)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“Š ìµœì¢… ê²°ê³¼:")
        print("=" * 60)
        
        if 'final' in results:
            print(json.dumps(results['final'], indent=2, ensure_ascii=False))
        else:
            print(json.dumps(results, indent=2, ensure_ascii=False))
            
        # íŒŒì¼ ì €ì¥
        if args.output:
            output_path = Path(args.output)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
            
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise
        
    finally:
        # ì •ë¦¬
        await workflow.cleanup()
        

if __name__ == '__main__':
    asyncio.run(main())