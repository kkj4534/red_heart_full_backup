#!/usr/bin/env python3
"""
íŒŒì¼ ë¡œê¹… í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
HelpingAI ì¬í™œì„±í™” í›„ í…ŒìŠ¤íŠ¸, ë¡œê·¸ë¥¼ íŒŒì¼ì— ì €ì¥
"""

import sys
import os
import time
import logging
import json
from datetime import datetime
from pathlib import Path
import traceback

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, '/mnt/c/large_project/linux_red_heart')

def setup_file_logging():
    """íŒŒì¼ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    log_dir = Path("/mnt/c/large_project/linux_red_heart/logs")
    log_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ë¡œê·¸ íŒŒì¼ë“¤
    main_log = log_dir / f"helpingai_test_{timestamp}.log"
    summary_file = log_dir / f"helpingai_summary_{timestamp}.json"
    
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger('HelpingAITest')
    logger.setLevel(logging.INFO)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(main_log, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # í¬ë§·í„°
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    return logger, main_log, summary_file

def test_helpingai_system():
    """HelpingAI ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger, main_log, summary_file = setup_file_logging()
    
    test_results = {
        'timestamp': datetime.now().isoformat(),
        'test_type': 'HelpingAI ì¬í™œì„±í™” í…ŒìŠ¤íŠ¸',
        'results': {},
        'summary': {
            'total_tests': 0,
            'passed': 0,
            'failed': 0,
            'gpu_memory_before': None,
            'gpu_memory_after': None
        }
    }
    
    logger.info("=" * 60)
    logger.info("HelpingAI ì¬í™œì„±í™” í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 60)
    
    # GPU ë©”ëª¨ë¦¬ ì²´í¬ (í…ŒìŠ¤íŠ¸ ì „)
    try:
        import subprocess
        nvidia_result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'], 
                                     capture_output=True, text=True)
        test_results['summary']['gpu_memory_before'] = f"{nvidia_result.stdout.strip()}MB"
        logger.info(f"í…ŒìŠ¤íŠ¸ ì „ GPU ë©”ëª¨ë¦¬: {test_results['summary']['gpu_memory_before']}")
    except:
        logger.warning("nvidia-smië¡œ GPU ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨")
    
    # 1. LLM ì—”ì§„ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
    logger.info("1. LLM ì—”ì§„ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸")
    test_results['summary']['total_tests'] += 1
    
    try:
        from llm_module.advanced_llm_engine import AdvancedLLMEngine, LLMRequest, TaskComplexity
        
        engine = AdvancedLLMEngine()
        available_models = list(engine.model_configs.keys())
        
        test_results['results']['engine_init'] = {
            'status': 'PASS',
            'available_models': available_models,
            'helpingai_available': 'helpingai' in available_models
        }
        
        logger.info(f"âœ… LLM ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
        logger.info(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}")
        logger.info(f"   HelpingAI ì‚¬ìš© ê°€ëŠ¥: {'helpingai' in available_models}")
        
        test_results['summary']['passed'] += 1
        
    except Exception as e:
        test_results['results']['engine_init'] = {
            'status': 'FAIL',
            'error': str(e),
            'traceback': traceback.format_exc()
        }
        logger.error(f"âŒ LLM ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        test_results['summary']['failed'] += 1
    
    # 2. HelpingAI ë¡œë”© í…ŒìŠ¤íŠ¸
    logger.info("2. HelpingAI ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    test_results['summary']['total_tests'] += 1
    
    try:
        # ê°ì • ë¶„ì„ ìš”ì²­ (HelpingAI ìš°ì„  ì‚¬ìš©ë¨)
        request = LLMRequest(
            prompt="Analyze the emotion in this text: I feel very happy today because I achieved my goal.",
            task_type="emotion_interpretation",
            complexity=TaskComplexity.MODERATE,
            max_tokens=100
        )
        
        start_time = time.time()
        response = engine.generate_sync(request)
        processing_time = time.time() - start_time
        
        test_results['results']['helpingai_loading'] = {
            'status': 'PASS' if response.success else 'FAIL',
            'model_used': response.model_used,
            'processing_time': processing_time,
            'response_length': len(response.generated_text),
            'confidence': response.confidence,
            'response_preview': response.generated_text[:100] if response.generated_text else "",
            'error_message': response.error_message
        }
        
        if response.success:
            logger.info(f"âœ… HelpingAI ë¡œë”© ë° ìƒì„± ì„±ê³µ")
            logger.info(f"   ì‚¬ìš©ëœ ëª¨ë¸: {response.model_used}")
            logger.info(f"   ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            logger.info(f"   ì‹ ë¢°ë„: {response.confidence:.2f}")
            logger.info(f"   ì‘ë‹µ ê¸¸ì´: {len(response.generated_text)}ì")
            logger.info(f"   ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response.generated_text[:100]}...")
            test_results['summary']['passed'] += 1
        else:
            logger.error(f"âŒ HelpingAI ìƒì„± ì‹¤íŒ¨: {response.error_message}")
            test_results['summary']['failed'] += 1
            
    except Exception as e:
        test_results['results']['helpingai_loading'] = {
            'status': 'FAIL',
            'error': str(e),
            'traceback': traceback.format_exc()
        }
        logger.error(f"âŒ HelpingAI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        test_results['summary']['failed'] += 1
    
    # 3. ë‹¤ì¤‘ ìš”ì²­ í…ŒìŠ¤íŠ¸
    logger.info("3. ë‹¤ì¤‘ ìš”ì²­ í…ŒìŠ¤íŠ¸ (3ê°œ ì—°ì†)")
    test_results['summary']['total_tests'] += 1
    
    multi_results = []
    test_prompts = [
        {"prompt": "What emotion is expressed here: I am feeling anxious about the upcoming exam.", "task": "emotion_interpretation"},
        {"prompt": "Explain the causal relationship: Regular exercise leads to better health.", "task": "causal_explanation"},
        {"prompt": "Generate an alternative scenario: What if I had studied harder for the test?", "task": "counterfactual_scenario"}
    ]
    
    try:
        for i, test_case in enumerate(test_prompts, 1):
            logger.info(f"   í…ŒìŠ¤íŠ¸ {i}: {test_case['task']}")
            
            request = LLMRequest(
                prompt=test_case['prompt'],
                task_type=test_case['task'],
                complexity=TaskComplexity.SIMPLE,
                max_tokens=80
            )
            
            start_time = time.time()
            response = engine.generate_sync(request)
            processing_time = time.time() - start_time
            
            result = {
                'test_number': i,
                'success': response.success,
                'model_used': response.model_used,
                'processing_time': processing_time,
                'response_length': len(response.generated_text)
            }
            multi_results.append(result)
            
            logger.info(f"     ê²°ê³¼: {'ì„±ê³µ' if response.success else 'ì‹¤íŒ¨'} | ëª¨ë¸: {response.model_used} | ì‹œê°„: {processing_time:.2f}ì´ˆ")
        
        success_count = sum(1 for r in multi_results if r['success'])
        test_results['results']['multi_request'] = {
            'status': 'PASS' if success_count >= 2 else 'FAIL',
            'success_count': success_count,
            'total_requests': len(test_prompts),
            'results': multi_results
        }
        
        logger.info(f"âœ… ë‹¤ì¤‘ ìš”ì²­ í…ŒìŠ¤íŠ¸: {success_count}/{len(test_prompts)} ì„±ê³µ")
        test_results['summary']['passed'] += 1
        
    except Exception as e:
        test_results['results']['multi_request'] = {
            'status': 'FAIL',
            'error': str(e),
            'traceback': traceback.format_exc()
        }
        logger.error(f"âŒ ë‹¤ì¤‘ ìš”ì²­ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        test_results['summary']['failed'] += 1
    
    # GPU ë©”ëª¨ë¦¬ ì²´í¬ (í…ŒìŠ¤íŠ¸ í›„)
    try:
        nvidia_result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'], 
                                     capture_output=True, text=True)
        test_results['summary']['gpu_memory_after'] = f"{nvidia_result.stdout.strip()}MB"
        logger.info(f"í…ŒìŠ¤íŠ¸ í›„ GPU ë©”ëª¨ë¦¬: {test_results['summary']['gpu_memory_after']}")
    except:
        logger.warning("nvidia-smië¡œ GPU ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨")
    
    # ìµœì¢… ìš”ì•½
    success_rate = (test_results['summary']['passed'] / test_results['summary']['total_tests']) * 100
    test_results['summary']['success_rate'] = success_rate
    
    logger.info("=" * 60)
    logger.info("í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ìµœì¢… ìš”ì•½")
    logger.info("=" * 60)
    logger.info(f"ì´ í…ŒìŠ¤íŠ¸: {test_results['summary']['total_tests']}")
    logger.info(f"ì„±ê³µ: {test_results['summary']['passed']}")
    logger.info(f"ì‹¤íŒ¨: {test_results['summary']['failed']}")
    logger.info(f"ì„±ê³µë¥ : {success_rate:.1f}%")
    logger.info("=" * 60)
    
    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ¯ HelpingAI ì¬í™œì„±í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ğŸ“ ìƒì„¸ ë¡œê·¸: {main_log}")
    print(f"ğŸ“Š ìš”ì•½ íŒŒì¼: {summary_file}")
    print(f"âœ… ì„±ê³µë¥ : {success_rate:.1f}% ({test_results['summary']['passed']}/{test_results['summary']['total_tests']})")
    
    if 'helpingai_loading' in test_results['results'] and test_results['results']['helpingai_loading']['status'] == 'PASS':
        print(f"ğŸš€ HelpingAI ëª¨ë¸: {test_results['results']['helpingai_loading']['model_used']}")
        print(f"â±ï¸  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {test_results['results']['helpingai_loading']['processing_time']:.2f}ì´ˆ")
    
    return test_results

if __name__ == "__main__":
    test_helpingai_system()