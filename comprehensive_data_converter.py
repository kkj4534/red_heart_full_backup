#!/usr/bin/env python3
"""
ì¢…í•© ë°ì´í„° ë³€í™˜ê¸° - Red Heart í•™ìŠµìš© ë°ì´í„° ë³€í™˜
Comprehensive Data Converter for Red Heart Learning System

ëª¨ë“  ìœ í˜•ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ Red Heart ì‹œìŠ¤í…œì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
ì†ì‹¤ ìµœì†Œí™”í•˜ë©´ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ì—¬ëŸ¬ íŒŒì¼ë¡œ ë¶„í• 
"""

import os
import json
import re
import uuid
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime
import asyncio
import logging
from dataclasses import dataclass, asdict
from collections import defaultdict
import math

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from config import ADVANCED_CONFIG, PROCESSED_DATASETS_DIR
from data_models import (
    EthicalSituation, DecisionScenario, EmotionData, EmotionState, 
    HedonicValues, BenthamVariable, IntentionCategory
)

logger = logging.getLogger('RedHeart.DataConverter')

@dataclass
class ConversionStats:
    """ë³€í™˜ í†µê³„"""
    total_files: int = 0
    processed_files: int = 0
    failed_files: int = 0
    total_scenarios: int = 0
    split_files: int = 0
    data_loss_percentage: float = 0.0
    
class DataType:
    """ë°ì´í„° ìœ í˜• ìƒìˆ˜"""
    EBS_KOREAN_LITERATURE = "ebs_korean_literature"
    SCRUPLES_ANECDOTES = "scruples_anecdotes"
    SCRUPLES_DILEMMAS = "scruples_dilemmas"
    CLASSIC_LITERATURE = "classic_literature"
    AI_GENERATED = "ai_generated"
    UNKNOWN = "unknown"

class ComprehensiveDataConverter:
    """ì¢…í•© ë°ì´í„° ë³€í™˜ê¸°"""
    
    def __init__(self):
        self.stats = ConversionStats()
        self.max_scenarios_per_file = 1000  # íŒŒì¼ë‹¹ ìµœëŒ€ ì‹œë‚˜ë¦¬ì˜¤ ìˆ˜
        self.max_file_size_mb = 50  # ìµœëŒ€ íŒŒì¼ í¬ê¸° (MB)
        
        # ê°ì • í‚¤ì›Œë“œ ë§¤í•‘ (ì‚¬ìš© ê°€ëŠ¥í•œ EmotionStateë§Œ)
        self.emotion_keywords = {
            'ìŠ¬í””': EmotionState.SADNESS,
            'ë¶„ë…¸': EmotionState.ANGER,
            'ì• ì²˜ë¡œì›€': EmotionState.SADNESS,
            'ì”ì“¸í•¨': EmotionState.DISGUST,
            'ê¸°ì¨': EmotionState.JOY,
            'í–‰ë³µ': EmotionState.JOY,
            'ë‘ë ¤ì›€': EmotionState.FEAR,
            'ë†€ë¼ì›€': EmotionState.SURPRISE,
            'í˜ì˜¤': EmotionState.DISGUST,
            'ì‚¬ë‘': EmotionState.JOY,  # LOVEê°€ ì—†ìœ¼ë¯€ë¡œ JOYë¡œ ë§¤í•‘
            'ì£„ì±…ê°': EmotionState.SADNESS,  # GUILTê°€ ì—†ìœ¼ë¯€ë¡œ SADNESSë¡œ ë§¤í•‘
            'ìˆ˜ì¹˜ì‹¬': EmotionState.SADNESS,  # SHAMEì´ ì—†ìœ¼ë¯€ë¡œ SADNESSë¡œ ë§¤í•‘
            'ìë¶€ì‹¬': EmotionState.JOY,  # PRIDEê°€ ì—†ìœ¼ë¯€ë¡œ JOYë¡œ ë§¤í•‘
            'ì§ˆíˆ¬': EmotionState.ANGER  # ENVYê°€ ì—†ìœ¼ë¯€ë¡œ ANGERë¡œ ë§¤í•‘
        }
        
    async def convert_all_datasets(self, source_dir: str) -> Dict[str, Any]:
        """ëª¨ë“  ë°ì´í„°ì…‹ ë³€í™˜"""
        logger.info(f"ğŸš€ ì¢…í•© ë°ì´í„° ë³€í™˜ ì‹œì‘: {source_dir}")
        
        source_path = Path(source_dir)
        if not source_path.exists():
            raise FileNotFoundError(f"ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_dir}")
        
        conversion_results = {}
        
        # 1. EBS í•œêµ­ ë¬¸í•™ ë°ì´í„° ë³€í™˜
        ebs_dir = source_path / "ai_ebs"
        if ebs_dir.exists():
            logger.info("ğŸ“š EBS í•œêµ­ ë¬¸í•™ ë°ì´í„° ë³€í™˜ ì¤‘...")
            ebs_result = await self._convert_ebs_data(ebs_dir)
            conversion_results['ebs_korean_literature'] = ebs_result
            
        # 2. Scruples ë°ì´í„° ë³€í™˜
        scruples_dir = source_path / "scruples_real_data"
        if scruples_dir.exists():
            logger.info("âš–ï¸ Scruples ìœ¤ë¦¬ ë°ì´í„° ë³€í™˜ ì¤‘...")
            scruples_result = await self._convert_scruples_data(scruples_dir)
            conversion_results['scruples'] = scruples_result
            
        # 3. ê³ ì „ ë¬¸í•™ ë°ì´í„° ë³€í™˜
        books_dir = source_path / "book"
        if books_dir.exists():
            logger.info("ğŸ“– ê³ ì „ ë¬¸í•™ ë°ì´í„° ë³€í™˜ ì¤‘...")
            books_result = await self._convert_classic_literature(books_dir)
            conversion_results['classic_literature'] = books_result
            
        # 4. AI ìƒì„± ë°ì´í„° ë³€í™˜
        ai_dir = source_path / "ai_generated_dataset"
        if ai_dir.exists():
            logger.info("ğŸ¤– AI ìƒì„± ë°ì´í„° ë³€í™˜ ì¤‘...")
            ai_result = await self._convert_ai_generated_data(ai_dir)
            conversion_results['ai_generated'] = ai_result
        
        # 5. ë³€í™˜ í†µê³„ ì €ì¥
        await self._save_conversion_report(conversion_results)
        
        logger.info(f"âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {self.stats.total_scenarios}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±")
        return conversion_results
    
    async def _convert_ebs_data(self, ebs_dir: Path) -> Dict[str, Any]:
        """EBS í•œêµ­ ë¬¸í•™ ë°ì´í„° ë³€í™˜"""
        scenarios = []
        
        for file_path in ebs_dir.glob("*.txt"):
            try:
                logger.info(f"  ì²˜ë¦¬ ì¤‘: {file_path.name}")
                content = file_path.read_text(encoding='utf-8')
                
                # ì‘í’ˆë³„ë¡œ ë¶„ë¦¬
                works = content.split('ã…¡' * 50)
                
                for work_content in works:
                    if not work_content.strip():
                        continue
                        
                    scenario = await self._parse_korean_literature_work(work_content.strip())
                    if scenario:
                        scenarios.append(scenario)
                        
                self.stats.processed_files += 1
                
            except Exception as e:
                logger.error(f"EBS íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                self.stats.failed_files += 1
        
        # íŒŒì¼ ë¶„í•  ë° ì €ì¥
        return await self._save_scenarios_with_splitting(scenarios, "ebs_korean_literature")
    
    async def _parse_korean_literature_work(self, content: str) -> Optional[DecisionScenario]:
        """í•œêµ­ ë¬¸í•™ ì‘í’ˆ íŒŒì‹±"""
        lines = [line.strip() for line in content.split('\n') if line.strip()]
        if not lines:
            return None
            
        try:
            # ì œëª© ì¶”ì¶œ
            title = lines[0] if lines else "í•œêµ­ ë¬¸í•™ ì‘í’ˆ"
            
            # ì£¼ìš” ì •ë³´ ì¶”ì¶œ
            work_data = {}
            for line in lines:
                if line.startswith('ìƒí™©ì„¤ëª…:'):
                    work_data['situation'] = line[5:].strip()
                elif line.startswith('ì´í•´ê´€ê³„ì:'):
                    work_data['stakeholders'] = [s.strip() for s in line[6:].split(',')]
                elif line.startswith('ìœ¤ë¦¬ì  ë”œë ˆë§ˆ:'):
                    work_data['ethical_dilemma'] = line[7:].strip()
                elif line.startswith('ì„ íƒì§€'):
                    if 'ì„ íƒì§€1:' in line:
                        work_data['option1'] = line.split('ì„ íƒì§€1:')[1].strip()
                    elif 'ì„ íƒì§€2:' in line:
                        work_data['option2'] = line.split('ì„ íƒì§€2:')[1].strip()
                    elif 'ì„ íƒì§€3:' in line:
                        work_data['option3'] = line.split('ì„ íƒì§€3:')[1].strip()
                elif 'í›„íšŒ' in line:
                    work_data['regret'] = line.strip()
            
            # ê°ì • ë°ì´í„° ì¶”ì¶œ
            emotions = self._extract_emotions_from_korean_text(content)
            
            # DecisionScenario ìƒì„±
            scenario = DecisionScenario(
                title=title,
                description=work_data.get('situation', ''),
                context={
                    'source': 'korean_literature',
                    'stakeholders': work_data.get('stakeholders', []),
                    'ethical_dilemma': work_data.get('ethical_dilemma', ''),
                    'emotions': emotions,
                    'regret_info': work_data.get('regret', ''),
                    'cultural_context': 'korean'
                },
                options=[
                    work_data.get('option1', ''),
                    work_data.get('option2', ''),
                    work_data.get('option3', '')
                ],
                metadata={
                    'data_type': DataType.EBS_KOREAN_LITERATURE,
                    'processing_quality': 'high',
                    'emotion_count': len(emotions),
                    'language': 'korean'
                }
            )
            
            self.stats.total_scenarios += 1
            return scenario
            
        except Exception as e:
            logger.error(f"í•œêµ­ ë¬¸í•™ ì‘í’ˆ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    async def _convert_scruples_data(self, scruples_dir: Path) -> Dict[str, Any]:
        """Scruples ë°ì´í„° ë³€í™˜"""
        scenarios = []
        
        # anecdotesì™€ dilemmas ì²˜ë¦¬
        for subdir in ['anecdotes', 'dilemmas']:
            subdir_path = scruples_dir / subdir
            if not subdir_path.exists():
                continue
                
            for file_path in subdir_path.glob("*.jsonl"):
                try:
                    logger.info(f"  ì²˜ë¦¬ ì¤‘: {file_path.name}")
                    
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line_num, line in enumerate(f, 1):
                            if not line.strip():
                                continue
                                
                            try:
                                data = json.loads(line)
                                scenario = await self._parse_scruples_entry(data, subdir)
                                if scenario:
                                    scenarios.append(scenario)
                                    
                            except json.JSONDecodeError as e:
                                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨ {file_path}:{line_num}: {e}")
                    
                    self.stats.processed_files += 1
                    
                except Exception as e:
                    logger.error(f"Scruples íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                    self.stats.failed_files += 1
        
        return await self._save_scenarios_with_splitting(scenarios, "scruples")
    
    async def _parse_scruples_entry(self, data: Dict, subdir: str) -> Optional[DecisionScenario]:
        """Scruples í•­ëª© íŒŒì‹±"""
        try:
            # ê¸°ë³¸ ì •ë³´
            title = data.get('title', 'Ethical Scenario')
            text = data.get('text', '')
            action_desc = data.get('action', {}).get('description', '')
            
            # ë¼ë²¨ ì •ë³´
            label = data.get('label', 'UNKNOWN')
            label_scores = data.get('label_scores', {})
            binarized_label = data.get('binarized_label', 'UNKNOWN')
            
            # ê°ì • ì¶”ì • (í…ìŠ¤íŠ¸ ê¸°ë°˜)
            emotions = self._estimate_emotions_from_english_text(text)
            
            scenario = DecisionScenario(
                title=title,
                description=text,
                context={
                    'source': f'scruples_{subdir}',
                    'action_description': action_desc,
                    'moral_judgment': label,
                    'label_scores': label_scores,
                    'binarized_judgment': binarized_label,
                    'emotions': emotions,
                    'language': 'english',
                    'post_type': data.get('post_type', 'UNKNOWN')
                },
                options=[
                    "Follow through with the action",
                    "Avoid the action", 
                    "Seek alternative approach"
                ],
                metadata={
                    'data_type': DataType.SCRUPLES_ANECDOTES if subdir == 'anecdotes' else DataType.SCRUPLES_DILEMMAS,
                    'processing_quality': 'medium',
                    'original_id': data.get('id', ''),
                    'post_id': data.get('post_id', ''),
                    'moral_complexity': len(label_scores)
                }
            )
            
            self.stats.total_scenarios += 1
            return scenario
            
        except Exception as e:
            logger.error(f"Scruples í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    async def _convert_classic_literature(self, books_dir: Path) -> Dict[str, Any]:
        """ê³ ì „ ë¬¸í•™ ë³€í™˜"""
        scenarios = []
        
        for file_path in books_dir.glob("*.txt"):
            try:
                logger.info(f"  ì²˜ë¦¬ ì¤‘: {file_path.name}")
                content = file_path.read_text(encoding='utf-8')
                
                # ëŒ€ìš©ëŸ‰ ì±…ì„ ì¥ë³„ë¡œ ë¶„í• 
                book_scenarios = await self._extract_scenarios_from_book(
                    content, file_path.stem
                )
                scenarios.extend(book_scenarios)
                
                self.stats.processed_files += 1
                
            except Exception as e:
                logger.error(f"ê³ ì „ ë¬¸í•™ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                self.stats.failed_files += 1
        
        return await self._save_scenarios_with_splitting(scenarios, "classic_literature")
    
    async def _extract_scenarios_from_book(self, content: str, book_title: str) -> List[DecisionScenario]:
        """ì±…ì—ì„œ ìœ¤ë¦¬ì  ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ"""
        scenarios = []
        
        # ì±…ì„ ë‹¨ë½ë³„ë¡œ ë¶„í•  (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        paragraphs = [p.strip() for p in content.split('\n\n') if len(p.strip()) > 100]
        
        # ìœ¤ë¦¬ì  ê°ˆë“±ì´ ìˆì„ ê²ƒ ê°™ì€ ë‹¨ë½ë“¤ ì‹ë³„
        ethical_paragraphs = []
        ethical_keywords = [
            'should', 'ought', 'right', 'wrong', 'moral', 'decision', 'choice',
            'dilemma', 'conflict', 'conscience', 'duty', 'responsibility'
        ]
        
        for para in paragraphs[:50]:  # ì²˜ìŒ 50ê°œ ë‹¨ë½ë§Œ ì²˜ë¦¬ (ì„±ëŠ¥ìƒ)
            if any(keyword in para.lower() for keyword in ethical_keywords):
                ethical_paragraphs.append(para)
        
        # ì„ ë³„ëœ ë‹¨ë½ë“¤ì„ ì‹œë‚˜ë¦¬ì˜¤ë¡œ ë³€í™˜
        for i, para in enumerate(ethical_paragraphs[:10]):  # ìµœëŒ€ 10ê°œ
            try:
                scenario = DecisionScenario(
                    title=f"{book_title} - Ethical Scenario {i+1}",
                    description=para[:1000] + "..." if len(para) > 1000 else para,
                    context={
                        'source': 'classic_literature',
                        'book_title': book_title,
                        'chapter_estimate': i + 1,
                        'language': 'english',
                        'literary_period': self._estimate_literary_period(book_title)
                    },
                    options=[
                        "Follow moral duty",
                        "Choose personal benefit", 
                        "Seek compromise solution"
                    ],
                    metadata={
                        'data_type': DataType.CLASSIC_LITERATURE,
                        'processing_quality': 'low',  # ìë™ ì¶”ì¶œì´ë¯€ë¡œ í’ˆì§ˆ ë‚®ìŒ
                        'extraction_method': 'keyword_based',
                        'paragraph_length': len(para)
                    }
                )
                scenarios.append(scenario)
                self.stats.total_scenarios += 1
                
            except Exception as e:
                logger.warning(f"ì±… ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return scenarios
    
    async def _convert_ai_generated_data(self, ai_dir: Path) -> Dict[str, Any]:
        """AI ìƒì„± ë°ì´í„° ë³€í™˜"""
        scenarios = []
        
        for file_path in ai_dir.glob("*.txt"):
            try:
                logger.info(f"  ì²˜ë¦¬ ì¤‘: {file_path.name}")
                content = file_path.read_text(encoding='utf-8')
                
                # AI ìƒì„± ë°ì´í„°ëŠ” êµ¬ì¡°í™”ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
                ai_scenarios = await self._parse_ai_generated_scenarios(content)
                scenarios.extend(ai_scenarios)
                
                self.stats.processed_files += 1
                
            except Exception as e:
                logger.error(f"AI ìƒì„± ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                self.stats.failed_files += 1
        
        return await self._save_scenarios_with_splitting(scenarios, "ai_generated")
    
    async def _parse_ai_generated_scenarios(self, content: str) -> List[DecisionScenario]:
        """AI ìƒì„± ì‹œë‚˜ë¦¬ì˜¤ íŒŒì‹±"""
        scenarios = []
        
        # "ì‚¬ë¡€"ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ë“¤ì„ ì°¾ì•„ ë¶„í• 
        cases = re.split(r'ì‚¬ë¡€ \d+:', content)
        
        for i, case_content in enumerate(cases[1:], 1):  # ì²« ë²ˆì§¸ëŠ” ë³´í†µ ë¹ˆ ë‚´ìš©
            try:
                lines = [line.strip() for line in case_content.split('\n') if line.strip()]
                if not lines:
                    continue
                
                # ì œëª©ê³¼ ìƒí™© ì„¤ëª… ì¶”ì¶œ
                title = f"AI ìƒì„± ì‹œë‚˜ë¦¬ì˜¤ {i}"
                situation = ""
                options = []
                
                for line in lines:
                    if line.startswith('ìƒí™© ì„¤ëª…'):
                        situation = line.split(':', 1)[1].strip() if ':' in line else line
                    elif line.startswith('ì„ íƒ ê°€ëŠ¥í•œ ì˜µì…˜'):
                        # ë‹¤ìŒ ëª‡ ì¤„ì´ ì˜µì…˜ë“¤
                        continue
                    elif re.match(r'^\d+\.', line):
                        options.append(line)
                
                if situation:
                    scenario = DecisionScenario(
                        title=title,
                        description=situation,
                        context={
                            'source': 'ai_generated',
                            'generation_method': 'claude_ai',
                            'language': 'korean',
                            'complexity_level': 'medium'
                        },
                        options=options[:5],  # ìµœëŒ€ 5ê°œ ì˜µì…˜
                        metadata={
                            'data_type': DataType.AI_GENERATED,
                            'processing_quality': 'high',
                            'case_number': i,
                            'options_count': len(options)
                        }
                    )
                    scenarios.append(scenario)
                    self.stats.total_scenarios += 1
                    
            except Exception as e:
                logger.warning(f"AI ì‹œë‚˜ë¦¬ì˜¤ íŒŒì‹± ì‹¤íŒ¨ (ì‚¬ë¡€ {i}): {e}")
        
        return scenarios
    
    async def _save_scenarios_with_splitting(self, scenarios: List[DecisionScenario], dataset_name: str) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ë“¤ì„ ë¶„í• í•˜ì—¬ ì €ì¥"""
        if not scenarios:
            return {'files_created': 0, 'total_scenarios': 0}
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = PROCESSED_DATASETS_DIR / dataset_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ ë¶„í•  ê³„ì‚°
        total_scenarios = len(scenarios)
        files_needed = math.ceil(total_scenarios / self.max_scenarios_per_file)
        
        logger.info(f"  ğŸ“¦ {total_scenarios}ê°œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ {files_needed}ê°œ íŒŒì¼ë¡œ ë¶„í• ")
        
        created_files = []
        
        for file_idx in range(files_needed):
            start_idx = file_idx * self.max_scenarios_per_file
            end_idx = min(start_idx + self.max_scenarios_per_file, total_scenarios)
            
            file_scenarios = scenarios[start_idx:end_idx]
            
            # íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{dataset_name}_batch_{file_idx+1:03d}_of_{files_needed:03d}_{timestamp}.json"
            file_path = output_dir / filename
            
            # ì‹œë‚˜ë¦¬ì˜¤ë“¤ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (datetime ì²˜ë¦¬)
            scenarios_dicts = []
            for scenario in file_scenarios:
                scenario_dict = asdict(scenario)
                # datetime ê°ì²´ë¥¼ ISO í˜•ì‹ ë¬¸ìì—´ë¡œ ë³€í™˜
                if 'created_at' in scenario_dict:
                    scenario_dict['created_at'] = scenario_dict['created_at'].isoformat()
                scenarios_dicts.append(scenario_dict)
            
            scenarios_data = {
                'metadata': {
                    'dataset_name': dataset_name,
                    'batch_info': f"{file_idx+1}/{files_needed}",
                    'scenario_count': len(file_scenarios),
                    'creation_time': datetime.now().isoformat(),
                    'converter_version': '1.0'
                },
                'scenarios': scenarios_dicts
            }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(scenarios_data, f, ensure_ascii=False, indent=2)
            
            created_files.append(str(file_path))
            logger.info(f"    âœ… ì €ì¥: {filename} ({len(file_scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤)")
        
        self.stats.split_files += len(created_files)
        
        return {
            'files_created': len(created_files),
            'total_scenarios': total_scenarios,
            'files_list': created_files,
            'average_scenarios_per_file': total_scenarios / len(created_files)
        }
    
    def _extract_emotions_from_korean_text(self, text: str) -> Dict[str, float]:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ê°ì • ì¶”ì¶œ"""
        emotions = {}
        
        # ê°ì • ì ìˆ˜ê°€ ëª…ì‹œëœ ê²½ìš°
        emotion_pattern = r'(\w+):\s*(\d+)'
        matches = re.findall(emotion_pattern, text)
        
        for emotion_name, score in matches:
            if emotion_name in self.emotion_keywords:
                emotion_state = self.emotion_keywords[emotion_name]
                emotions[emotion_state.name] = float(score) / 10.0  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        
        return emotions
    
    def _estimate_emotions_from_english_text(self, text: str) -> Dict[str, float]:
        """ì˜ì–´ í…ìŠ¤íŠ¸ì—ì„œ ê°ì • ì¶”ì • (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        emotions = {}
        
        # ê°„ë‹¨í•œ ê°ì • í‚¤ì›Œë“œ ë§¤í•‘
        emotion_keywords_en = {
            'angry': (EmotionState.ANGER, 0.7),
            'sad': (EmotionState.SADNESS, 0.7),
            'happy': (EmotionState.JOY, 0.7),
            'scared': (EmotionState.FEAR, 0.7),
            'disgusted': (EmotionState.DISGUST, 0.7),
            'surprised': (EmotionState.SURPRISE, 0.7),
            'frustrated': (EmotionState.ANGER, 0.5),
            'worried': (EmotionState.FEAR, 0.5),
            'disappointed': (EmotionState.SADNESS, 0.5),
            'excited': (EmotionState.JOY, 0.6)
        }
        
        text_lower = text.lower()
        for keyword, (emotion_state, intensity) in emotion_keywords_en.items():
            if keyword in text_lower:
                emotions[emotion_state.name] = intensity
        
        return emotions
    
    def _estimate_literary_period(self, book_title: str) -> str:
        """ì±… ì œëª©ìœ¼ë¡œ ë¬¸í•™ ì‹œëŒ€ ì¶”ì •"""
        title_lower = book_title.lower()
        
        if any(classic in title_lower for classic in ['frankenstein', 'dracula']):
            return 'gothic_romantic'
        elif any(classic in title_lower for classic in ['gatsby', 'mockingbird']):
            return 'modern_american'
        elif any(classic in title_lower for classic in ['shakespeare', 'romeo']):
            return 'elizabethan'
        elif any(classic in title_lower for classic in ['dickens', 'austen']):
            return 'victorian'
        else:
            return 'unknown'
    
    async def _save_conversion_report(self, results: Dict[str, Any]) -> None:
        """ë³€í™˜ ë³´ê³ ì„œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = PROCESSED_DATASETS_DIR / f"conversion_report_{timestamp}.json"
        
        # í†µê³„ ê³„ì‚°
        total_files_created = sum(r.get('files_created', 0) for r in results.values())
        total_scenarios = sum(r.get('total_scenarios', 0) for r in results.values())
        
        report = {
            'conversion_summary': {
                'timestamp': datetime.now().isoformat(),
                'total_datasets': len(results),
                'total_files_processed': self.stats.processed_files,
                'total_files_failed': self.stats.failed_files,
                'total_scenarios_created': total_scenarios,
                'total_output_files': total_files_created,
                'success_rate': (self.stats.processed_files / (self.stats.processed_files + self.stats.failed_files)) * 100 if (self.stats.processed_files + self.stats.failed_files) > 0 else 0
            },
            'dataset_details': results,
            'processing_stats': asdict(self.stats)
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š ë³€í™˜ ë³´ê³ ì„œ ì €ì¥: {report_path}")
        logger.info(f"âœ… ë³€í™˜ ì™„ë£Œ: {total_scenarios}ê°œ ì‹œë‚˜ë¦¬ì˜¤, {total_files_created}ê°œ íŒŒì¼ ìƒì„±")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Red Heart ì¢…í•© ë°ì´í„° ë³€í™˜ê¸° ì‹œì‘")
    
    converter = ComprehensiveDataConverter()
    
    try:
        # ì†ŒìŠ¤ ë°ì´í„° ë””ë ‰í† ë¦¬
        source_dir = "/mnt/d/large_prj/linux_red_heart/for_learn_dataset"
        
        # ë³€í™˜ ì‹¤í–‰
        results = await converter.convert_all_datasets(source_dir)
        
        print("\n" + "="*60)
        print("ğŸ‰ ë°ì´í„° ë³€í™˜ ì™„ë£Œ!")
        print("="*60)
        print(f"ğŸ“Š ì²˜ë¦¬ëœ íŒŒì¼: {converter.stats.processed_files}")
        print(f"âŒ ì‹¤íŒ¨í•œ íŒŒì¼: {converter.stats.failed_files}")
        print(f"ğŸ¯ ìƒì„±ëœ ì‹œë‚˜ë¦¬ì˜¤: {converter.stats.total_scenarios}")
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {converter.stats.split_files}")
        print("="*60)
        
        return results
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(name)s | %(levelname)s | %(message)s'
    )
    
    # ë³€í™˜ ì‹¤í–‰
    asyncio.run(main())