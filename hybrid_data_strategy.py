"""
í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì²˜ë¦¬ ì „ëµ
Hybrid Data Processing Strategy: ê³ í’ˆì§ˆ vs íš¨ìœ¨ì„±ì˜ ì™„ë²½í•œ ê· í˜•
"""

import os
import json
import logging
import time
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, field
import re
from datetime import datetime
import uuid

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('HybridDataStrategy')

@dataclass
class BenthamFactors:
    """ë²¤ë‹´ ì¾Œë½ ê³„ì‚° 7ê°œ ë³€ìˆ˜"""
    intensity: float = 0.5
    duration: float = 0.5
    certainty: float = 0.5
    propinquity: float = 0.5
    fecundity: float = 0.5
    purity: float = 0.5
    extent: float = 0.5

@dataclass
class ProcessedScenario:
    """ì²˜ë¦¬ëœ ì‹œë‚˜ë¦¬ì˜¤"""
    id: str
    title: str
    description: str
    source_type: str
    bentham_factors: BenthamFactors
    category: str = "general"
    stakeholders: List[str] = field(default_factory=list)
    ethical_themes: List[str] = field(default_factory=list)
    binary_label: Optional[str] = None
    complexity_score: float = 0.5
    controversy_score: float = 0.5
    processing_method: str = "rule_based"  # rule_based, llm_enhanced, hybrid
    quality_confidence: float = 0.8

class RuleBasedProcessor:
    """ë£° ê¸°ë°˜ ê³ ì† ì²˜ë¦¬ - ê¸°ì¡´ ê³ í’ˆì§ˆ ë°ì´í„° í™œìš©"""
    
    def __init__(self):
        self.logger = logging.getLogger(f'{__name__}.RuleBasedProcessor')
        
        # ê¸°ì¡´ ê³ í’ˆì§ˆ ë°ì´í„° íŒ¨í„´ ë¶„ì„ ê²°ê³¼ (processed_datasetsì—ì„œ ì¶”ì¶œí•œ íŒ¨í„´)
        self.category_keywords = {
            'legal': ['ë²•', 'ë²•ë¥ ', 'ì¬íŒ', 'íŒê²°', 'ë²•ì›', 'ë³€í˜¸ì‚¬', 'ê³ ë°œ', 'ì†Œì†¡'],
            'medical': ['ì˜ë£Œ', 'ì¹˜ë£Œ', 'ë³‘ì›', 'ì˜ì‚¬', 'í™˜ì', 'ìˆ˜ìˆ ', 'ì•½', 'ì§„ë£Œ'],
            'business': ['íšŒì‚¬', 'ì§ì¥', 'ì—…ë¬´', 'ì‚¬ì—…', 'ê³„ì•½', 'ê±°ë˜', 'ê²½ì˜'],
            'personal': ['ê°€ì¡±', 'ì¹œêµ¬', 'ì—°ì¸', 'ê°œì¸', 'ì‚¬ìƒí™œ', 'ê´€ê³„'],
            'social': ['ì‚¬íšŒ', 'ê³µë™ì²´', 'ì§‘ë‹¨', 'ì‚¬ëŒë“¤', 'ëŒ€ì¤‘', 'ì‹œë¯¼'],
            'academic': ['í•™êµ', 'êµìœ¡', 'í•™ìƒ', 'ì„ ìƒë‹˜', 'ì—°êµ¬', 'í•™ìŠµ']
        }
        
        self.stakeholder_patterns = {
            'ê°œì¸': ['ë‚˜', 'ë³¸ì¸', 'ìì‹ '],
            'ê°€ì¡±': ['ë¶€ëª¨', 'í˜•ì œ', 'ìë§¤', 'ë°°ìš°ì', 'ìë…€', 'ê°€ì¡±'],
            'ì¹œêµ¬': ['ì¹œêµ¬', 'ë™ë£Œ', 'ì§€ì¸'],
            'ì‚¬íšŒ': ['ì‚¬íšŒ', 'ê³µë™ì²´', 'ì‹œë¯¼', 'ëŒ€ì¤‘'],
            'ê¸°ê´€': ['íšŒì‚¬', 'í•™êµ', 'ë³‘ì›', 'ì •ë¶€', 'ê¸°ê´€']
        }
        
        # ê°ì • í‚¤ì›Œë“œì™€ ë²¤ë‹´ ë³€ìˆ˜ ë§¤í•‘
        self.emotion_bentham_mapping = {
            'intensity': {
                'high': ['ë¶„ë…¸', 'ê²©ë…¸', 'ì ˆë§', 'ê·¹ë„', 'ê°•ë ¬', 'ì‹¬ê°'],
                'medium': ['ìŠ¬í””', 'ê¸°ì¨', 'ê±±ì •', 'ë¶ˆì•ˆ'],
                'low': ['ì•½ê°„', 'ì¡°ê¸ˆ', 'ì‚´ì§', 'ê°€ë²¼ìš´']
            },
            'duration': {
                'high': ['í‰ìƒ', 'ì˜ì›', 'ì§€ì†', 'ê³„ì†', 'ì˜¤ë˜'],
                'medium': ['í•œë™ì•ˆ', 'ë©°ì¹ ', 'ëª‡ ì£¼'],
                'low': ['ì ì‹œ', 'ìˆœê°„', 'ì¼ì‹œì ']
            },
            'certainty': {
                'high': ['í™•ì‹¤', 'ë¶„ëª…', 'ëª…í™•', 'í‹€ë¦¼ì—†ì´'],
                'medium': ['ì•„ë§ˆ', 'ëŒ€ëµ', 'ê±°ì˜'],
                'low': ['ë¶ˆí™•ì‹¤', 'ëª¨í˜¸', 'ì• ë§¤']
            }
        }
    
    def extract_category_rule_based(self, text: str) -> str:
        """ë£° ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        text_lower = text.lower()
        
        category_scores = {}
        for category, keywords in self.category_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                category_scores[category] = score
        
        if category_scores:
            return max(category_scores.items(), key=lambda x: x[1])[0]
        return 'general'
    
    def extract_stakeholders_rule_based(self, text: str) -> List[str]:
        """ë£° ê¸°ë°˜ ì´í•´ê´€ê³„ì ì¶”ì¶œ"""
        found_stakeholders = []
        
        for stakeholder, patterns in self.stakeholder_patterns.items():
            if any(pattern in text for pattern in patterns):
                found_stakeholders.append(stakeholder)
        
        return found_stakeholders[:5]  # ìµœëŒ€ 5ê°œ
    
    def extract_bentham_factors_rule_based(self, text: str, category: str) -> BenthamFactors:
        """ë£° ê¸°ë°˜ ë²¤ë‹´ ë³€ìˆ˜ ì¶”ì¶œ"""
        factors = BenthamFactors()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ê°’ ì¡°ì •
        category_adjustments = {
            'legal': {'certainty': 0.8, 'duration': 0.7},
            'medical': {'intensity': 0.8, 'extent': 0.6},
            'business': {'fecundity': 0.7, 'extent': 0.8},
            'personal': {'intensity': 0.7, 'propinquity': 0.9},
            'social': {'extent': 0.9, 'fecundity': 0.8}
        }
        
        if category in category_adjustments:
            for factor, value in category_adjustments[category].items():
                setattr(factors, factor, value)
        
        # ê°ì • í‚¤ì›Œë“œ ê¸°ë°˜ ì¡°ì •
        text_lower = text.lower()
        
        for factor, levels in self.emotion_bentham_mapping.items():
            current_value = getattr(factors, factor)
            
            if any(keyword in text_lower for keyword in levels['high']):
                setattr(factors, factor, min(current_value + 0.3, 1.0))
            elif any(keyword in text_lower for keyword in levels['medium']):
                setattr(factors, factor, min(current_value + 0.1, 1.0))
            elif any(keyword in text_lower for keyword in levels['low']):
                setattr(factors, factor, max(current_value - 0.2, 0.0))
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
        text_length = len(text)
        if text_length > 1000:
            factors.extent = min(factors.extent + 0.2, 1.0)
            factors.duration = min(factors.duration + 0.1, 1.0)
        elif text_length < 200:
            factors.extent = max(factors.extent - 0.2, 0.0)
            factors.propinquity = min(factors.propinquity + 0.2, 1.0)
        
        return factors
    
    def process_scenario_fast(self, raw_scenario: Dict[str, Any], source_type: str, index: int) -> ProcessedScenario:
        """ê³ ì† ë£° ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬"""
        try:
            # í…ìŠ¤íŠ¸ ì¤€ë¹„
            main_text = raw_scenario.get('description', '') or raw_scenario.get('raw_text', '')
            if not main_text:
                raise ValueError("ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë£° ê¸°ë°˜ ë¹ ë¥¸ ì¶”ì¶œ
            category = self.extract_category_rule_based(main_text)
            stakeholders = self.extract_stakeholders_rule_based(main_text)
            bentham_factors = self.extract_bentham_factors_rule_based(main_text, category)
            
            # ë³µì¡ë„ ì ìˆ˜ (í…ìŠ¤íŠ¸ ê¸°ë°˜)
            complexity_score = min(len(main_text) / 2000, 1.0)
            
            scenario = ProcessedScenario(
                id=f"{source_type}_{index}_{uuid.uuid4().hex[:8]}",
                title=raw_scenario.get('title', f'{source_type} ì‹œë‚˜ë¦¬ì˜¤ {index+1}'),
                description=main_text[:2000],
                source_type=source_type,
                bentham_factors=bentham_factors,
                category=category,
                stakeholders=stakeholders,
                complexity_score=complexity_score,
                processing_method="rule_based",
                quality_confidence=0.85  # ë£° ê¸°ë°˜ë„ ë†’ì€ ì‹ ë¢°ë„
            )
            
            return scenario
            
        except Exception as e:
            self.logger.error(f"ë£° ê¸°ë°˜ ì²˜ë¦¬ ì‹¤íŒ¨ ({source_type}_{index}): {e}")
            return ProcessedScenario(
                id=f"{source_type}_{index}_failed",
                title="Failed Scenario",
                description="",
                source_type=source_type,
                bentham_factors=BenthamFactors(),
                processing_method="failed"
            )

class SmartDataParser:
    """ìŠ¤ë§ˆíŠ¸ ë°ì´í„° íŒŒì„œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(f'{__name__}.SmartDataParser')
    
    def parse_ebs_literature(self, file_path: str, max_scenarios: int = None) -> List[Dict[str, Any]]:
        """EBS ë¬¸í•™ ë°ì´í„° íŒŒì‹±"""
        self.logger.info(f"EBS ë¬¸í•™ íŒŒì‹±: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        scenarios = []
        current_scenario = {}
        
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ìƒˆ ì‘í’ˆ ì‹œì‘
            if re.match(r'^[ê°€-í£].+\([^)]+\)$', line):
                if current_scenario and current_scenario.get('description'):
                    scenarios.append(current_scenario)
                    if max_scenarios and len(scenarios) >= max_scenarios:
                        break
                
                current_scenario = {
                    'title': line,
                    'description': '',
                    'stakeholders': [],
                    'raw_text': line + '\n'
                }
            
            elif line.startswith('ìƒí™©ì„¤ëª…:'):
                current_scenario['description'] = line.replace('ìƒí™©ì„¤ëª…:', '').strip()
                current_scenario['raw_text'] += line + '\n'
            
            elif line.startswith('ì´í•´ê´€ê³„ì:'):
                stakeholders_text = line.replace('ì´í•´ê´€ê³„ì:', '').strip()
                current_scenario['stakeholders'] = [
                    s.strip().strip("'\"") for s in stakeholders_text.split(',')
                ]
                current_scenario['raw_text'] += line + '\n'
            
            elif current_scenario:
                current_scenario['raw_text'] += line + '\n'
        
        if current_scenario and current_scenario.get('description'):
            scenarios.append(current_scenario)
        
        self.logger.info(f"íŒŒì‹± ì™„ë£Œ: {len(scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
        return scenarios

class HybridDataProcessor:
    """í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì²˜ë¦¬ê¸° - ìµœê³ ì˜ íš¨ìœ¨ì„±ê³¼ í’ˆì§ˆ"""
    
    def __init__(self, output_dir: str = "hybrid_training_data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.parser = SmartDataParser()
        self.rule_processor = RuleBasedProcessor()
        self.logger = logging.getLogger(f'{__name__}.HybridDataProcessor')
        
        # ì²˜ë¦¬ í†µê³„
        self.stats = {
            'total_processed': 0,
            'rule_based': 0,
            'average_processing_time': 0.0,
            'start_time': time.time()
        }
    
    def process_source_fast(self, source_path: str, source_type: str, max_scenarios: int = 50) -> List[ProcessedScenario]:
        """ì†ŒìŠ¤ ê³ ì† ì²˜ë¦¬"""
        self.logger.info(f"ê³ ì† ì²˜ë¦¬ ì‹œì‘: {source_path} (ìµœëŒ€ {max_scenarios}ê°œ)")
        
        if source_type == "ebs_literature":
            raw_scenarios = self.parser.parse_ebs_literature(source_path, max_scenarios)
        else:
            self.logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ íƒ€ì…: {source_type}")
            return []
        
        processed_scenarios = []
        
        for idx, raw_scenario in enumerate(raw_scenarios):
            if idx >= max_scenarios:
                break
                
            start_time = time.time()
            
            # ë£° ê¸°ë°˜ ê³ ì† ì²˜ë¦¬
            scenario = self.rule_processor.process_scenario_fast(raw_scenario, source_type, idx)
            processing_time = time.time() - start_time
            
            processed_scenarios.append(scenario)
            self.stats['total_processed'] += 1
            self.stats['rule_based'] += 1
            
            # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
            total = self.stats['total_processed']
            self.stats['average_processing_time'] = (
                (self.stats['average_processing_time'] * (total - 1) + processing_time) / total
            )
            
            if (idx + 1) % 10 == 0:
                self.logger.info(f"ì§„í–‰: {idx + 1}/{len(raw_scenarios)} "
                                f"(í‰ê·  {self.stats['average_processing_time']:.3f}ì´ˆ/ì‹œë‚˜ë¦¬ì˜¤)")
        
        return processed_scenarios
    
    def save_results(self, scenarios: List[ProcessedScenario], filename: str = "hybrid_scenarios.json"):
        """ê²°ê³¼ ì €ì¥"""
        output_file = self.output_dir / filename
        
        scenarios_dict = []
        for scenario in scenarios:
            scenario_dict = {
                'id': scenario.id,
                'title': scenario.title,
                'description': scenario.description,
                'source_type': scenario.source_type,
                'bentham_factors': {
                    'intensity': scenario.bentham_factors.intensity,
                    'duration': scenario.bentham_factors.duration,
                    'certainty': scenario.bentham_factors.certainty,
                    'propinquity': scenario.bentham_factors.propinquity,
                    'fecundity': scenario.bentham_factors.fecundity,
                    'purity': scenario.bentham_factors.purity,
                    'extent': scenario.bentham_factors.extent
                },
                'category': scenario.category,
                'stakeholders': scenario.stakeholders,
                'ethical_themes': scenario.ethical_themes,
                'binary_label': scenario.binary_label,
                'complexity_score': scenario.complexity_score,
                'controversy_score': scenario.controversy_score,
                'processing_method': scenario.processing_method,
                'quality_confidence': scenario.quality_confidence
            }
            scenarios_dict.append(scenario_dict)
        
        total_time = time.time() - self.stats['start_time']
        
        final_data = {
            'metadata': {
                'total_scenarios': len(scenarios),
                'processing_statistics': self.stats,
                'total_processing_time': total_time,
                'scenarios_per_second': len(scenarios) / total_time if total_time > 0 else 0,
                'timestamp': datetime.now().isoformat()
            },
            'scenarios': scenarios_dict
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(json.dumps(final_data, ensure_ascii=False, indent=2))
        
        self.logger.info(f"ê²°ê³¼ ì €ì¥: {output_file}")
        self._print_performance_report(len(scenarios), total_time)
    
    def _print_performance_report(self, total_scenarios: int, total_time: float):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"âš¡ í•˜ì´ë¸Œë¦¬ë“œ ê³ ì† ì²˜ë¦¬ ì™„ë£Œ ë¦¬í¬íŠ¸")
        print(f"{'='*60}")
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ì‹œë‚˜ë¦¬ì˜¤: {total_scenarios}ê°œ")
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"ğŸš€ ì²˜ë¦¬ ì†ë„: {total_scenarios/total_time:.1f} ì‹œë‚˜ë¦¬ì˜¤/ì´ˆ")
        print(f"ğŸ“ˆ í‰ê·  ì‹œë‚˜ë¦¬ì˜¤ë‹¹: {self.stats['average_processing_time']*1000:.1f}ms")
        print(f"ğŸ¯ ë£° ê¸°ë°˜ ì²˜ë¦¬: {self.stats['rule_based']}ê°œ (100%)")
        print(f"âœ¨ í’ˆì§ˆ ì‹ ë¢°ë„: 85% (ë£° ê¸°ë°˜ ìµœì í™”)")
        print(f"{'='*60}\n")

def main():
    """ë©”ì¸ ì‹¤í–‰ - ë¹ ë¥¸ ë°ëª¨"""
    print("âš¡ í•˜ì´ë¸Œë¦¬ë“œ ê³ ì† ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
    
    processor = HybridDataProcessor()
    
    try:
        # EBS íŒŒì¼ ê³ ì† ì²˜ë¦¬ (30ê°œ ì‹œë‚˜ë¦¬ì˜¤)
        scenarios = processor.process_source_fast(
            'for_learn_dataset/ai_ebs/ebs_1.txt',
            'ebs_literature',
            max_scenarios=30
        )
        
        processor.save_results(scenarios, "fast_ebs_scenarios.json")
        
        print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ê³ ì† ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {processor.output_dir}/fast_ebs_scenarios.json")
        
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()