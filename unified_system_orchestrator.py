"""
í†µí•© ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - Week 5 í•µì‹¬ êµ¬í˜„
Unified System Orchestrator - Week 5 Core Implementation

ì „ì²´ Red Heart ì‹œìŠ¤í…œ í†µí•© ê´€ë¦¬:
- run_learning.shì™€ Python ì‹œìŠ¤í…œ ì—°ë™
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ëŒ€ì‹œë³´ë“œ
- ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ ì¶”ì  ë° ì•Œë¦¼
- ë¡œê·¸ í†µí•© ê´€ë¦¬ ë° ë¶„ì„
- ìë™ ì¥ì•  ë³µêµ¬ ë° ìµœì í™”
"""

import asyncio
import logging
import time
import subprocess
import signal
import os
import sys
import json
import threading
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque, defaultdict, OrderedDict
from enum import Enum
import numpy as np
import psutil
import socket
import torch
import torch.nn
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from contextlib import asynccontextmanager
import shutil

# í•µì‹¬ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from config import ADVANCED_CONFIG, get_smart_device, get_gpu_memory_info
from head_compatibility_interface import HeadType, HeadProcessingResult, HeadCompatibilityManager
from unified_red_heart_core import RedHeartUnifiedBackbone
from dynamic_swap_manager import RedHeartDynamicSwapManager, SwapPriority
from intelligent_synergy_system import IntelligentSynergySystem
from unified_learning_system import UnifiedLearningSystem, TrainingMetrics
from advanced_usage_pattern_analyzer import AdvancedUsagePatternAnalyzer
from workflow_aware_memory_manager import WorkflowAwareMemoryManager, WorkflowStage

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

def _extract_nn_module(obj):
    """ë˜í¼ ê°ì²´ì—ì„œ ì‹¤ì œ nn.Module ì¶”ì¶œ (NO FALLBACK)
    
    Args:
        obj: ê²€ì‚¬í•  ê°ì²´
        
    Returns:
        nn.Module ë˜ëŠ” None
    """
    import torch.nn as nn
    
    # ì´ë¯¸ nn.Moduleì´ë©´ ë°”ë¡œ ë°˜í™˜
    if isinstance(obj, nn.Module):
        return obj
    
    # get_pytorch_network ë©”ì„œë“œ í™•ì¸ (ìµœìš°ì„ )
    if hasattr(obj, 'get_pytorch_network') and callable(obj.get_pytorch_network):
        try:
            net = obj.get_pytorch_network()
            if isinstance(net, nn.Module):
                logger.debug(f"get_pytorch_network()ì—ì„œ nn.Module ë°œê²¬: {net.__class__.__name__}")
                return net
        except Exception as e:
            logger.debug(f"get_pytorch_network() í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    # HF pipeline ê°ì§€ - .model ë…¸ì¶œ
    if obj.__class__.__name__.endswith("Pipeline") and hasattr(obj, "model"):
        if isinstance(obj.model, nn.Module):
            logger.debug(f"HF Pipelineì—ì„œ nn.Module ë°œê²¬: {obj.model.__class__.__name__}")
            return obj.model
    
    # ë‹¤ì–‘í•œ ì†ì„±ëª… ì²´í¬ (ìš°ì„ ìˆœìœ„ ìˆœ)
    attr_names = [
        'model', '_model', 'network', '_network', 'embedding_model',
        'module', '_module', 'net', '_net',
        'encoder', 'decoder', 'embeddings',
        'backbone', 'head', 'classifier',
        # emotion_analyzer ê´€ë ¨ ì¶”ê°€
        'emotion_moe', 'hierarchical_model', 'default_network',
        '_primary_nn', 'multilingual_direct', 'korean_model',
        # semantic_analyzer ê´€ë ¨ ì¶”ê°€
        'fusion_network', 'semantic_model', 'causal_model',
        'emotion_model', 'fusion_model', 'ethical_model',
        # translator ê´€ë ¨ ì¶”ê°€  
        'translation_model', 'translator_model', 'mbart_model'
    ]
    
    # ê´€ìš© íŒ©í† ë¦¬ ë©”ì„œë“œë„ ì‹œë„ (í™•ì¥)
    factory_methods = [
        'to_torch', 'build_network', 'as_pytorch', 'materialize', 'get_model',
        'create_model', 'build', 'construct', 'get_nn_module', 'to_nn_module',
        'get_network', 'create_network', 'get_pytorch_model'
    ]
    for method_name in factory_methods:
        if hasattr(obj, method_name) and callable(getattr(obj, method_name)):
            try:
                net = getattr(obj, method_name)()
                if isinstance(net, nn.Module):
                    logger.debug(f"{method_name}()ì—ì„œ nn.Module ì¶”ì¶œ")
                    return net
            except Exception as e:
                logger.debug(f"{method_name}() ì‹¤íŒ¨: {e}")
    
    for attr_name in attr_names:
        if hasattr(obj, attr_name):
            attr = getattr(obj, attr_name)
            if isinstance(attr, nn.Module):
                logger.debug(f"{attr_name} ì†ì„±ì—ì„œ nn.Module ë°œê²¬: {attr.__class__.__name__}")
                return attr
    
    # models/_models ë”•ì…”ë„ˆë¦¬ í™•ì¸
    for dict_name in ['models', '_models']:
        if hasattr(obj, dict_name):
            models_dict = getattr(obj, dict_name)
            if isinstance(models_dict, dict):
                for key, m in models_dict.items():
                    if isinstance(m, nn.Module):
                        logger.debug(f"{dict_name}['{key}']ì—ì„œ nn.Module ë°œê²¬: {m.__class__.__name__}")
                        return m
    
    # ì°¾ì„ ìˆ˜ ì—†ìŒ
    logger.debug(f"{obj.__class__.__name__}ì—ì„œ nn.Moduleì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    return None

class SimpleThinAdapter(torch.nn.Module):
    """nn.Moduleì´ ì—†ëŠ” ê°ì²´ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ì–´ëŒ‘í„°"""
    
    def __init__(self, wrapped_obj):
        super().__init__()
        self.wrapped = wrapped_obj
        # ë”ë¯¸ íŒŒë¼ë¯¸í„° ì¶”ê°€ (DSMì´ ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡)
        self.dummy_param = torch.nn.Parameter(torch.zeros(1), requires_grad=False)
        
    def forward(self, *args, **kwargs):
        """wrapped ê°ì²´ê°€ forwardë‚˜ process ë©”ì„œë“œë¥¼ ê°€ì§€ë©´ í˜¸ì¶œ"""
        if hasattr(self.wrapped, 'forward'):
            return self.wrapped.forward(*args, **kwargs)
        elif hasattr(self.wrapped, 'process'):
            return self.wrapped.process(*args, **kwargs)
        elif hasattr(self.wrapped, '__call__'):
            return self.wrapped(*args, **kwargs)
        else:
            # ìµœì†Œí•œ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return args[0] if args else None
    
    def to(self, device):
        """ë””ë°”ì´ìŠ¤ ì´ë™ ì²˜ë¦¬"""
        super().to(device)
        if hasattr(self.wrapped, 'to'):
            self.wrapped.to(device)
        elif hasattr(self.wrapped, 'device'):
            self.wrapped.device = device
        return self

class SystemStatus(Enum):
    """ì‹œìŠ¤í…œ ìƒíƒœ"""
    INITIALIZING = "initializing"
    READY = "ready"
    TRAINING = "training"
    MONITORING = "monitoring"
    ERROR = "error"
    MAINTENANCE = "maintenance"
    SHUTDOWN = "shutdown"

class ComponentStatus(Enum):
    """ì»´í¬ë„ŒíŠ¸ ìƒíƒœ"""
    HEALTHY = "healthy"
    WARNING = "warning"
    CRITICAL = "critical"
    OFFLINE = "offline"
    RECOVERING = "recovering"

@dataclass
class SystemHealthMetrics:
    """ì‹œìŠ¤í…œ ê±´ê°•ë„ ë©”íŠ¸ë¦­"""
    timestamp: datetime = field(default_factory=datetime.now)
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    gpu_usage: float = 0.0
    gpu_memory: float = 0.0
    disk_usage: float = 0.0
    
    # ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­
    network_in: float = 0.0
    network_out: float = 0.0
    
    # ì‹œìŠ¤í…œ ì˜¨ë„ (ê°€ëŠ¥í•œ ê²½ìš°)
    cpu_temp: Optional[float] = None
    gpu_temp: Optional[float] = None
    
    # í”„ë¡œì„¸ìŠ¤ ë©”íŠ¸ë¦­
    active_processes: int = 0
    python_processes: int = 0
    
    # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­
    training_efficiency: float = 0.0
    synergy_performance: float = 0.0
    swap_efficiency: float = 0.0

@dataclass
class ComponentHealth:
    """ì»´í¬ë„ŒíŠ¸ ê±´ê°•ë„"""
    name: str
    status: ComponentStatus
    health_score: float = 1.0  # 0.0 - 1.0
    last_check: datetime = field(default_factory=datetime.now)
    error_count: int = 0
    uptime: timedelta = field(default_factory=lambda: timedelta(0))
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    response_time: float = 0.0
    throughput: float = 0.0
    error_rate: float = 0.0
    
    # ìƒì„¸ ì •ë³´
    details: Dict[str, Any] = field(default_factory=dict)
    recent_errors: List[str] = field(default_factory=lambda: deque(maxlen=10))

class SystemMonitor:
    """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°"""
    
    def __init__(self, check_interval: float = 5.0):
        self.check_interval = check_interval
        self.is_monitoring = False
        self.monitoring_thread = None
        
        # ë©”íŠ¸ë¦­ ì €ì¥ì†Œ
        self.health_history = deque(maxlen=1000)  # ìµœê·¼ 1000ê°œ ë©”íŠ¸ë¦­
        self.component_health = {}
        
        # ì•ŒëŒ ì„¤ì •
        self.alert_thresholds = {
            'cpu_usage': 80.0,      # 80% ì´ìƒ
            'memory_usage': 85.0,   # 85% ì´ìƒ
            'gpu_usage': 90.0,      # 90% ì´ìƒ
            'disk_usage': 95.0,     # 95% ì´ìƒ
            'error_rate': 0.1       # 10% ì´ìƒ
        }
        
        # ì•ŒëŒ ì½œë°±
        self.alert_callbacks = []
        
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_monitoring:
            return
        
        self.is_monitoring = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitoring_thread.start()
        logger.info("ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_monitoring = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=10)
        logger.info("ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.is_monitoring:
            try:
                # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics = self._collect_system_metrics()
                self.health_history.append(metrics)
                
                # ì•ŒëŒ ì²´í¬
                self._check_alerts(metrics)
                
                # ì»´í¬ë„ŒíŠ¸ ê±´ê°•ë„ ì—…ë°ì´íŠ¸
                self._update_component_health()
                
                time.sleep(self.check_interval)
                
            except Exception as e:
                logger.error(f"ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                time.sleep(self.check_interval)
    
    def _collect_system_metrics(self) -> SystemHealthMetrics:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        
        # CPU ë° ë©”ëª¨ë¦¬
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        # ë””ìŠ¤í¬
        disk = psutil.disk_usage('/')
        
        # ë„¤íŠ¸ì›Œí¬
        net_io = psutil.net_io_counters()
        
        # GPU ì •ë³´
        gpu_info = get_gpu_memory_info()
        gpu_usage = 0.0
        gpu_memory = 0.0
        if gpu_info:
            gpu_memory = gpu_info.get('memory_used_gb', 0) / gpu_info.get('memory_total_gb', 1) * 100
            gpu_usage = gpu_info.get('utilization', 0)
        
        # í”„ë¡œì„¸ìŠ¤ ì •ë³´
        all_processes = list(psutil.process_iter(['pid', 'name', 'cmdline']))
        python_processes = len([p for p in all_processes 
                               if p.info['name'] and 'python' in p.info['name'].lower()])
        
        # ì˜¨ë„ ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
        cpu_temp = None
        try:
            temps = psutil.sensors_temperatures()
            if 'coretemp' in temps:
                cpu_temp = max([t.current for t in temps['coretemp']])
        except:
            pass
        
        metrics = SystemHealthMetrics(
            timestamp=datetime.now(),
            cpu_usage=cpu_percent,
            memory_usage=memory.percent,
            gpu_usage=gpu_usage,
            gpu_memory=gpu_memory,
            disk_usage=disk.percent,
            network_in=net_io.bytes_recv / (1024**2),  # MB
            network_out=net_io.bytes_sent / (1024**2),  # MB
            cpu_temp=cpu_temp,
            active_processes=len(all_processes),
            python_processes=python_processes
        )
        
        return metrics
    
    def _check_alerts(self, metrics: SystemHealthMetrics):
        """ì•ŒëŒ ì²´í¬"""
        alerts = []
        
        if metrics.cpu_usage > self.alert_thresholds['cpu_usage']:
            alerts.append(f"ë†’ì€ CPU ì‚¬ìš©ë¥ : {metrics.cpu_usage:.1f}%")
        
        if metrics.memory_usage > self.alert_thresholds['memory_usage']:
            alerts.append(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {metrics.memory_usage:.1f}%")
        
        if metrics.gpu_usage > self.alert_thresholds['gpu_usage']:
            alerts.append(f"ë†’ì€ GPU ì‚¬ìš©ë¥ : {metrics.gpu_usage:.1f}%")
        
        if metrics.disk_usage > self.alert_thresholds['disk_usage']:
            alerts.append(f"ë†’ì€ ë””ìŠ¤í¬ ì‚¬ìš©ë¥ : {metrics.disk_usage:.1f}%")
        
        # ì˜¨ë„ ì•ŒëŒ
        if metrics.cpu_temp and metrics.cpu_temp > 80:
            alerts.append(f"ë†’ì€ CPU ì˜¨ë„: {metrics.cpu_temp:.1f}Â°C")
        
        # ì•ŒëŒ ì½œë°± ì‹¤í–‰
        for alert in alerts:
            for callback in self.alert_callbacks:
                try:
                    callback(alert, metrics)
                except Exception as e:
                    logger.error(f"ì•ŒëŒ ì½œë°± ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
    
    def _update_component_health(self):
        """ì»´í¬ë„ŒíŠ¸ ê±´ê°•ë„ ì—…ë°ì´íŠ¸"""
        # í˜„ì¬ í™œì„± ì»´í¬ë„ŒíŠ¸ë“¤ì˜ ê±´ê°•ë„ ì²´í¬
        components_to_check = [
            'unified_backbone',
            'swap_manager', 
            'synergy_system',
            'learning_system',
            'pattern_analyzer'
        ]
        
        for component_name in components_to_check:
            health = self._assess_component_health(component_name)
            self.component_health[component_name] = health
    
    def _assess_component_health(self, component_name: str) -> ComponentHealth:
        """ì»´í¬ë„ŒíŠ¸ ê±´ê°•ë„ í‰ê°€"""
        
        # ê¸°ì¡´ ê±´ê°•ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        existing_health = self.component_health.get(component_name)
        if existing_health:
            health = existing_health
        else:
            health = ComponentHealth(name=component_name, status=ComponentStatus.HEALTHY)
        
        # ê°„ë‹¨í•œ ê±´ê°•ë„ ì²´í¬ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê° ì»´í¬ë„ŒíŠ¸ë³„ ì„¸ë¶€ ì²´í¬)
        try:
            # ì˜ˆì‹œ: í”„ë¡œì„¸ìŠ¤ ì¡´ì¬ ì—¬ë¶€ë¡œ ê±´ê°•ë„ íŒë‹¨
            if component_name == 'unified_backbone':
                # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ìœ¼ë¡œ í™œì„± ìƒíƒœ íŒë‹¨
                gpu_info = get_gpu_memory_info()
                if gpu_info and gpu_info.get('memory_used_gb', 0) > 1:
                    health.status = ComponentStatus.HEALTHY
                    health.health_score = 0.9
                else:
                    health.status = ComponentStatus.WARNING
                    health.health_score = 0.6
            
            elif component_name in ['swap_manager', 'synergy_system', 'learning_system']:
                # Python í”„ë¡œì„¸ìŠ¤ í™œì„± ìƒíƒœë¡œ íŒë‹¨
                if self.health_history and self.health_history[-1].python_processes > 0:
                    health.status = ComponentStatus.HEALTHY
                    health.health_score = 0.8
                else:
                    health.status = ComponentStatus.WARNING
                    health.health_score = 0.5
            
            else:
                # ê¸°ë³¸ì ìœ¼ë¡œ ê±´ê°•í•œ ìƒíƒœë¡œ ê°€ì •
                health.status = ComponentStatus.HEALTHY
                health.health_score = 0.7
            
            health.last_check = datetime.now()
            
        except Exception as e:
            health.status = ComponentStatus.CRITICAL
            health.health_score = 0.1
            health.recent_errors.append(str(e))
            health.error_count += 1
        
        return health
    
    def get_system_summary(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìš”ì•½ ì •ë³´"""
        if not self.health_history:
            return {"status": "no_data"}
        
        latest_metrics = self.health_history[-1]
        
        # ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ ê²°ì •
        overall_status = SystemStatus.READY
        if latest_metrics.cpu_usage > 90 or latest_metrics.memory_usage > 95:
            overall_status = SystemStatus.ERROR
        elif latest_metrics.cpu_usage > 80 or latest_metrics.memory_usage > 85:
            overall_status = SystemStatus.WARNING
        
        # ì»´í¬ë„ŒíŠ¸ ê±´ê°•ë„ ìš”ì•½
        component_summary = {}
        healthy_components = 0
        total_components = len(self.component_health)
        
        for name, health in self.component_health.items():
            component_summary[name] = {
                'status': health.status.value,
                'health_score': health.health_score,
                'error_count': health.error_count
            }
            if health.status == ComponentStatus.HEALTHY:
                healthy_components += 1
        
        overall_health = healthy_components / max(1, total_components)
        
        return {
            'overall_status': overall_status.value,
            'overall_health': overall_health,
            'current_metrics': {
                'cpu_usage': latest_metrics.cpu_usage,
                'memory_usage': latest_metrics.memory_usage,
                'gpu_usage': latest_metrics.gpu_usage,
                'gpu_memory': latest_metrics.gpu_memory,
                'disk_usage': latest_metrics.disk_usage,
                'python_processes': latest_metrics.python_processes
            },
            'component_health': component_summary,
            'timestamp': latest_metrics.timestamp.isoformat(),
            'uptime_minutes': len(self.health_history) * self.check_interval / 60
        }

class LogManager:
    """ë¡œê·¸ ê´€ë¦¬ì"""
    
    def __init__(self, log_dir: str = "logs/unified_system"):
        self.log_dir = log_dir
        os.makedirs(self.log_dir, exist_ok=True)
        
        # ë¡œê·¸ íŒŒì¼ë“¤ - pathlib ëŒ€ì‹  os.path.join ì‚¬ìš©
        self.system_log = os.path.join(self.log_dir, "system.log")
        self.training_log = os.path.join(self.log_dir, "training.log")
        self.error_log = os.path.join(self.log_dir, "errors.log")
        self.performance_log = os.path.join(self.log_dir, "performance.log")
        
        # ë¡œê·¸ íšŒì „ ì„¤ì •
        self.max_log_size = 100 * 1024 * 1024  # 100MB
        self.max_log_files = 10
        
        self._setup_loggers()
    
    def _setup_loggers(self):
        """ë¡œê±° ì„¤ì •"""
        
        # ì‹œìŠ¤í…œ ë¡œê±°
        self.system_logger = logging.getLogger("red_heart.system")
        self.system_logger.setLevel(logging.INFO)
        
        system_handler = logging.FileHandler(self.system_log)
        system_handler.setFormatter(logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        ))
        self.system_logger.addHandler(system_handler)
        
        # í›ˆë ¨ ë¡œê±°
        self.training_logger = logging.getLogger("red_heart.training")
        self.training_logger.setLevel(logging.INFO)
        
        training_handler = logging.FileHandler(self.training_log)
        training_handler.setFormatter(logging.Formatter(
            '%(asctime)s - TRAINING - %(message)s'
        ))
        self.training_logger.addHandler(training_handler)
        
        # ì—ëŸ¬ ë¡œê±°
        self.error_logger = logging.getLogger("red_heart.errors")
        self.error_logger.setLevel(logging.ERROR)
        
        error_handler = logging.FileHandler(self.error_log)
        error_handler.setFormatter(logging.Formatter(
            '%(asctime)s - ERROR - %(name)s - %(message)s'
        ))
        self.error_logger.addHandler(error_handler)
    
    def log_system_event(self, message: str, level: str = "INFO"):
        """ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ ë¡œê¹…"""
        if level == "INFO":
            self.system_logger.info(message)
        elif level == "WARNING":
            self.system_logger.warning(message)
        elif level == "ERROR":
            self.system_logger.error(message)
            self.error_logger.error(message)
    
    def log_training_metrics(self, metrics: TrainingMetrics):
        """í›ˆë ¨ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        metric_str = (
            f"Epoch: {metrics.epoch}, Step: {metrics.step}, "
            f"Loss: {metrics.total_loss:.4f}, "
            f"Memory: {metrics.memory_usage:.2f}MB, "
            f"Time: {metrics.training_time:.3f}s, "
            f"Samples/s: {metrics.samples_per_second:.1f}"
        )
        self.training_logger.info(metric_str)
    
    def log_performance_data(self, component: str, metrics: Dict[str, Any]):
        """ì„±ëŠ¥ ë°ì´í„° ë¡œê¹…"""
        perf_data = {
            'timestamp': datetime.now().isoformat(),
            'component': component,
            'metrics': metrics
        }
        
        with open(self.performance_log, 'a') as f:
            f.write(json.dumps(perf_data) + '\n')
    
    def rotate_logs(self):
        """ë¡œê·¸ íšŒì „"""
        for log_file in [self.system_log, self.training_log, self.error_log, self.performance_log]:
            # ë¬¸ìì—´ë¡œ ì €ì¥ëœ ë¡œê·¸ íŒŒì¼ì„ os.pathë¡œ ì²˜ë¦¬
            if os.path.exists(log_file) and os.path.getsize(log_file) > self.max_log_size:
                # ê¸°ì¡´ ë¡œê·¸ ë°±ì—…
                base_name = os.path.splitext(log_file)[0]
                ext = os.path.splitext(log_file)[1]
                
                for i in range(self.max_log_files - 1, 0, -1):
                    old_file = f"{base_name}.{i}{ext}"
                    new_file = f"{base_name}.{i+1}{ext}"
                    if os.path.exists(old_file):
                        os.rename(old_file, new_file)
                
                # í˜„ì¬ ë¡œê·¸ë¥¼ .1ë¡œ ì´ë™
                os.rename(log_file, f"{base_name}.1{ext}")

class ProcessManager:
    """í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.processes = {}  # í”„ë¡œì„¸ìŠ¤ ID -> í”„ë¡œì„¸ìŠ¤ ì •ë³´
        self.process_configs = {}  # í”„ë¡œì„¸ìŠ¤ ì„¤ì •
    
    async def start_background_process(self, name: str, command: List[str],
                                     restart_on_failure: bool = True) -> int:
        """ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘"""
        
        try:
            # ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰ì…˜ìœ¼ë¡œ ë¸”ë¡œí‚¹ ë°©ì§€
            process = await asyncio.create_subprocess_exec(
                *command,
                stdout=asyncio.subprocess.DEVNULL,
                stderr=asyncio.subprocess.DEVNULL,
                # í‘œì¤€ ì…ë ¥ë„ ë‹«ìŒ
                stdin=asyncio.subprocess.DEVNULL
            )
            
            self.processes[name] = {
                'process': process,
                'pid': process.pid,
                'command': command,
                'start_time': datetime.now(),
                'restart_on_failure': restart_on_failure,
                'restart_count': 0
            }
            
            logger.info(f"ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘: {name} (PID: {process.pid})")
            
            # í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì‹œì‘
            monitor_task = asyncio.create_task(self._monitor_process(name))
            self.processes[name]['monitor_task'] = monitor_task
            
            return process.pid
            
        except Exception as e:
            logger.error(f"í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì‹¤íŒ¨ - {name}: {str(e)}")
            raise
    
    async def _monitor_process(self, name: str):
        """í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§"""
        if name not in self.processes:
            return
            
        process_info = self.processes[name]
        process = process_info['process']
        
        try:
            # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
            return_code = await process.wait()
            
            logger.info(f"í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ: {name} (ì½”ë“œ: {return_code})")
            
            if return_code != 0:
                logger.warning(f"í”„ë¡œì„¸ìŠ¤ ë¹„ì •ìƒ ì¢…ë£Œ: {name} (ì½”ë“œ: {return_code})")
                
                # ì¬ì‹œì‘ ì—¬ë¶€ ê²°ì •
                if process_info['restart_on_failure'] and process_info['restart_count'] < 3:
                    logger.info(f"í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘: {name}")
                    # ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì •ë³´ ì •ë¦¬ í›„ ì¬ì‹œì‘
                    if name in self.processes:
                        del self.processes[name]
                    
                    process_info['restart_count'] += 1
                    await self.start_background_process(
                        name, process_info['command'], 
                        process_info['restart_on_failure']
                    )
                    return  # ì¬ì‹œì‘ ì‹œì—ëŠ” ì—¬ê¸°ì„œ ì¢…ë£Œ
                else:
                    logger.error(f"í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘ í¬ê¸°: {name}")
            else:
                logger.info(f"í”„ë¡œì„¸ìŠ¤ ì •ìƒ ì™„ë£Œ: {name}")
            
        except asyncio.CancelledError:
            logger.info(f"í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§ ì·¨ì†Œ: {name}")
        except Exception as e:
            logger.error(f"í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜ - {name}: {str(e)}")
        
        finally:
            # í”„ë¡œì„¸ìŠ¤ ì •ë³´ í™•ì‹¤íˆ ì •ë¦¬
            try:
                if name in self.processes:
                    process_info = self.processes[name]
                    
                    # ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì •ë¦¬
                    if 'monitor_task' in process_info:
                        monitor_task = process_info['monitor_task']
                        if not monitor_task.done():
                            monitor_task.cancel()
                    
                    # í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì „ì— ì¶œë ¥ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬ (DEVNULLì´ë¯€ë¡œ ì‹¤ì œë¡œëŠ” ë¶ˆí•„ìš”)
                    if hasattr(process, 'stdout') and process.stdout and not process.stdout.is_closing():
                        process.stdout.close()
                    if hasattr(process, 'stderr') and process.stderr and not process.stderr.is_closing():
                        process.stderr.close()
                    
                    del self.processes[name]
                    logger.info(f"í”„ë¡œì„¸ìŠ¤ ì •ë³´ ì •ë¦¬ ì™„ë£Œ: {name}")
            except Exception as e:
                logger.error(f"í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì˜¤ë¥˜ - {name}: {str(e)}")
    
    async def stop_process(self, name: str) -> bool:
        """í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€"""
        if name not in self.processes:
            logger.warning(f"í”„ë¡œì„¸ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {name}")
            return False
        
        process_info = self.processes[name]
        process = process_info['process']
        
        try:
            if process.returncode is not None:
                # ì´ë¯¸ ì¢…ë£Œëœ í”„ë¡œì„¸ìŠ¤
                logger.info(f"í”„ë¡œì„¸ìŠ¤ ì´ë¯¸ ì¢…ë£Œë¨: {name}")
                if name in self.processes:
                    del self.processes[name]
                return True
            
            # SIGTERM ì‹ í˜¸ ì „ì†¡
            process.terminate()
            logger.info(f"í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡: {name}")
            
            # 5ì´ˆ ëŒ€ê¸° í›„ ê°•ì œ ì¢…ë£Œ
            try:
                await asyncio.wait_for(process.wait(), timeout=5.0)
                logger.info(f"í”„ë¡œì„¸ìŠ¤ ì •ìƒ ì¢…ë£Œ: {name}")
            except asyncio.TimeoutError:
                process.kill()
                logger.warning(f"í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ: {name}")
                try:
                    await asyncio.wait_for(process.wait(), timeout=2.0)
                except asyncio.TimeoutError:
                    logger.error(f"í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ ì‹¤íŒ¨: {name}")
            
            # í”„ë¡œì„¸ìŠ¤ ì •ë³´ ì •ë¦¬
            if name in self.processes:
                del self.processes[name]
            
            return True
            
        except Exception as e:
            logger.error(f"í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€ ì˜¤ë¥˜ - {name}: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ í”„ë¡œì„¸ìŠ¤ ì •ë³´ ì •ë¦¬
            if name in self.processes:
                del self.processes[name]
            return False
    
    def get_process_status(self) -> Dict[str, Any]:
        """í”„ë¡œì„¸ìŠ¤ ìƒíƒœ ì¡°íšŒ"""
        status = {}
        
        for name, info in self.processes.items():
            process = info['process']
            uptime = datetime.now() - info['start_time']
            
            status[name] = {
                'pid': info['pid'],
                'uptime_seconds': uptime.total_seconds(),
                'restart_count': info['restart_count'],
                'is_running': process.returncode is None,
                'command': ' '.join(info['command'])
            }
        
        return status

class UnifiedSystemOrchestrator:
    """
    í†µí•© ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ë©”ì¸ í´ë˜ìŠ¤
    
    ì „ì²´ Red Heart ì‹œìŠ¤í…œì˜ í†µí•© ê´€ë¦¬ ë° ì¡°ì •
    """
    
    # ëª¨ë“ˆ ì˜ì¡´ì„± ê·¸ë˜í”„ ì •ì˜ (í´ë˜ìŠ¤ ìˆ˜ì¤€)
    MODULE_DEP = {
        'emotion_analyzer':  ['bentham_calculator'],  # emotion_empathy_head ì œê±° - ìˆœí™˜ ì˜ì¡´ì„± ë°©ì§€
        'semantic_analyzer': [],  # semantic_surd_head ì œê±° - í—¤ë“œëŠ” ëª¨ë“ˆì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ
        'regret_analyzer': [],
        'translator': [],
        'neural_components': [],
        'surd_analyzer': [],
        'bayesian_engine': [],
        'llm_engine': [],
        'experience_database': [],
        'hierarchical_emotion': [],
        'usage_pattern_analyzer': [],
        'meta_integration': []
    }
    
    # \ubaa8\ub4c8 \uc774\ub984\uacfc DSM \ud5e4\ub4dc \uc774\ub984 \ub9e4\ud551 (\ud074\ub798\uc2a4 \uc218\uc900)
    MODULE_TO_HEAD_NAME = {
        'semantic_analyzer': 'semantic_surd_head',
        'emotion_analyzer': 'emotion_empathy_head',
        'meta_integration': 'meta_integration_head',
        'bentham_calculator': 'bentham_fromm_head',
        'regret_analyzer': 'regret_learning_head',
        # translator\ub294 \ud5e4\ub4dc\uac00 \uc544\ub2c8\ubbc0\ub85c \ub9e4\ud551 \uc5c6\uc74c
    }
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or ADVANCED_CONFIG.get('orchestrator_config', {})
        
        # HF ëª¨ë¸ ë˜í¼ í™œì„±í™” - ëª¨ë“  from_pretrained/pipeline í˜¸ì¶œì´ ìë™ìœ¼ë¡œ ì¶”ì ë¨
        try:
            from hf_model_wrapper import enable_auto_registration, get_hf_wrapper
            enable_auto_registration()
            logger.info("âœ… HF ëª¨ë¸ ìë™ ë“±ë¡ í™œì„±í™” - ëª¨ë“  ëª¨ë¸ ë¡œë”©ì´ ì¶”ì ë©ë‹ˆë‹¤")
        except ImportError:
            logger.warning("âš ï¸ HF ëª¨ë¸ ë˜í¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ëª¨ë¸ ë¡œë”© ì¶”ì  ë¹„í™œì„±í™”")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        self.system_status = SystemStatus.INITIALIZING
        self.start_time = datetime.now()
        self.is_running = False
        
        # ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬ (ì˜ˆì™¸ ê²½ë¡œì—ì„œë„ ì•ˆì „í•˜ê²Œ ì ‘ê·¼ ê°€ëŠ¥)
        self.module_instances: Dict[str, Any] = {}
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤
        self.unified_backbone = None
        self.swap_manager = None
        self.synergy_system = None
        self.learning_system = None
        self.pattern_analyzer = None
        
        # ê´€ë¦¬ ì»´í¬ë„ŒíŠ¸ë“¤
        self.system_monitor = SystemMonitor()
        self.log_manager = LogManager()
        self.process_manager = ProcessManager()
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì
        self.memory_manager = WorkflowAwareMemoryManager(memory_threshold_mb=6500.0)
        
        # run_learning.sh í†µí•© - pathlib ëŒ€ì‹  os.path ì‚¬ìš©
        self.script_path = os.path.join(os.path.dirname(__file__), "run_learning.sh")
        self.script_runner = None
        
        # ì›¹ ëŒ€ì‹œë³´ë“œ (ì„ íƒì )
        self.dashboard_port = self.config.get('dashboard_port', 8080)
        self.dashboard_server = None
        
        # ìë™ ë³µêµ¬ ì„¤ì •
        self.auto_recovery = True
        self.recovery_attempts = 0
        self.max_recovery_attempts = 3
        
        # ë¹ ë¥¸ ì´ˆê¸°í™” ëª¨ë“œ (validation ë“±ì—ì„œ ì‚¬ìš©)
        self.fast_init_mode = self.config.get('fast_init_mode', False)
        
        logger.info("UnifiedSystemOrchestrator ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def initialize_system(self) -> bool:
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        
        try:
            self.log_manager.log_system_event("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
            
            # 1. ëª¨ë‹ˆí„°ë§ ì‹œì‘
            self.system_monitor.start_monitoring()
            self.system_monitor.alert_callbacks.append(self._handle_system_alert)
            
            # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— ë©”ì¸ ë£¨í”„ ì£¼ì…
            self.memory_manager.main_loop = asyncio.get_running_loop()
            logger.info("âœ… ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— ë©”ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ì£¼ì… ì™„ë£Œ")
            
            # 2. í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            try:
                await self._initialize_core_components()
            finally:
                # ë¶€íŒ… ì™„ë£Œ ì‹ í˜¸ - ì´í›„ë¶€í„° GPU í• ë‹¹ í—ˆìš©
                self.memory_manager.set_boot_completed()
            
            # 3. run_learning.sh ìƒíƒœ í™•ì¸
            if not os.path.exists(self.script_path):
                raise FileNotFoundError(f"run_learning.sh not found: {self.script_path}")
            
            # 4. ëŒ€ì‹œë³´ë“œ ì‹œì‘ (ì„ íƒì )
            if self.config.get('enable_dashboard', False):
                await self._start_dashboard()
            
            self.system_status = SystemStatus.READY
            self.is_running = True
            
            self.log_manager.log_system_event("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ", "INFO")
            logger.info("Red Heart í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            self.system_status = SystemStatus.ERROR
            error_msg = f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            self.log_manager.log_system_event(error_msg, "ERROR")
            logger.error(error_msg)
            
            # ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´ ì¶œë ¥
            import traceback
            tb_str = traceback.format_exc()
            logger.error(f"ğŸš¨ ìƒì„¸ ì—ëŸ¬ ìŠ¤íƒ:\n{tb_str}")
            print(f"\n{'='*60}")
            print(f"ğŸš¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨!")
            print(f"{'='*60}")
            print(f"ì—ëŸ¬: {str(e)}")
            print(f"{'='*60}")
            print(f"ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            print(tb_str)
            print(f"{'='*60}\n")
            
            return False
    
    def _ensure_pre_registered_heads(self):
        """í—¤ë“œ ì„ ë“±ë¡ ë³´ì¥ - ëª¨ë“ˆ ì„±ê³µ/ì‹¤íŒ¨ì™€ ë¬´ê´€í•˜ê²Œ í•­ìƒ ë“±ë¡"""
        import torch.nn as nn
        from dynamic_swap_manager import get_swap_manager, SwapPriority
        
        logger.critical("ğŸ“‹ í—¤ë“œ ì„ ë“±ë¡ ë³´ì¥ í•¨ìˆ˜ í˜¸ì¶œë¨")
        
        # self.swap_manager ì§ì ‘ ì‚¬ìš© (get_swap_manager ëŒ€ì‹ )
        swap = self.swap_manager if hasattr(self, 'swap_manager') else get_swap_manager()
        logger.critical(f"ğŸ“‹ swap_manager ìƒíƒœ: {type(swap)}")
        
        if not swap:
            logger.critical("âŒ DSMì´ ì—†ì–´ì„œ ì„ ë“±ë¡ ë¶ˆê°€")
            raise RuntimeError("DSM not available at pre-registration time")
            
        # DSM í˜„ì¬ ìƒíƒœ ë¡œê·¸
        logger.critical(f"ğŸ“Š DSM ë“±ë¡ëœ í‚¤(ì„ ë“±ë¡ ì „): {sorted(list(swap.models.keys()))[:50]}")
        
        # emotion_empathy_head ë¬´ì¡°ê±´ ì„ ë“±ë¡ (ì—†ì„ ë•Œë§Œ)
        if "emotion_empathy_head" not in swap.models:
            class MinimalEmotionHead(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.linear = nn.Linear(768, 7)
                    # gradient íë¦„ ë³´ì¥ì„ ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ requires_grad ì„¤ì •
                    for param in self.parameters():
                        param.requires_grad = True
                
                def forward(self, x):
                    # gradient íë¦„ ë³´ì¥
                    x = x.contiguous()
                    # 1280 -> 768 ë³€í™˜ì€ í•™ìŠµ ì‹œìŠ¤í…œì—ì„œ ì²˜ë¦¬
                    if x.shape[-1] > 768:
                        x = x[..., :768]
                    output = self.linear(x)
                    # gradient íë¦„ ê°•ì œ
                    if not output.requires_grad and self.training:
                        output.requires_grad_(True)
                    return output
            
            try:
                default_head = MinimalEmotionHead()
                # HIGH ìš°ì„ ìˆœìœ„ë¡œ ë“±ë¡ (ë³´í˜¸ í•„ìš”)
                swap.register_model("emotion_empathy_head", default_head, priority=SwapPriority.HIGH)
                logger.info("âœ… emotion_empathy_head ì„ ë“±ë¡ ì™„ë£Œ (MinimalEmotionHead, priority=HIGH)")
                
                # warm load/unload - ë‚˜ì¤‘ì— ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ ìˆ˜í–‰í•˜ë„ë¡ í”Œë˜ê·¸ ì„¤ì •
                # ë™ê¸° í•¨ìˆ˜ì—ì„œëŠ” ì§ì ‘ ì‹¤í–‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í”Œë˜ê·¸ë§Œ ì„¤ì •
                self._needs_warm_load_unload = True
                logger.info("ğŸ”„ warm load/unload ì˜ˆì•½ë¨ (ë‚˜ì¤‘ì— ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰)")
                    
            except Exception as e:
                logger.critical(f"âŒ emotion_empathy_head ì„ ë“±ë¡ ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"emotion_empathy_head ì„ ë“±ë¡ í•„ìˆ˜: {e}")
        else:
            logger.info("â„¹ï¸ emotion_empathy_head ì´ë¯¸ ë“±ë¡ë¨")
        
        # ì„±ê³µ ë³´ì¥ ê²€ì¦
        assert "emotion_empathy_head" in swap.models, "emotion_empathy_head pre-registration must succeed"
        logger.info(f"ğŸ“Š DSM ë“±ë¡ëœ í‚¤(ì„ ë“±ë¡ í›„): {sorted(list(swap.models.keys()))[:50]}")
    
    async def _wait_for_existing_init(self, instance):
        """ì´ë¯¸ ì‹œì‘ëœ ì´ˆê¸°í™”ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        while not getattr(instance, 'fully_initialized', False):
            await asyncio.sleep(0.5)
        return instance
    
    async def _initialize_global_modules_sequential(self):
        """ì „ì—­ ëª¨ë“ˆ ìˆœì°¨ ì´ˆê¸°í™” - MasterMemoryOrchestrator ê´€ë¦¬"""
        
        # ì „ì—­ ìƒíƒœ ì»¨í…Œì´ë„ˆ ë³´ì¥ - ì˜ˆì™¸ ê²½ë¡œì—ì„œë„ ì•ˆì „
        if not hasattr(self, 'module_instances'):
            self.module_instances = {}
        
        # Float16 ì„¤ì • - ì•ˆì „í•˜ê²Œ GPU ë¸”ë¡ì—ì„œë§Œ ì ìš©
        import torch
        # torch.set_default_dtype(torch.float16)  # ì „ì—­ ì„¤ì •ì€ ìœ„í—˜
        # ëŒ€ì‹  GPU ì—°ì‚° ì‹œ torch.cuda.amp.autocast ì‚¬ìš© ê¶Œì¥
        logger.info("ğŸ”§ PyTorch ì •ë°€ë„: GPU ë¸”ë¡ì—ì„œë§Œ float16 ì ìš© ì˜ˆì •")
        
        from config import register_system_module, get_master_memory_orchestrator, get_gpu_memory_info
        from module_specs import MODULE_SPECS
        
        # MasterMemoryOrchestrator ì—°ê²°
        try:
            orchestrator = get_master_memory_orchestrator()
        except Exception as e:
            logger.warning(f"MasterMemoryOrchestrator ì—°ê²° ì‹¤íŒ¨: {e}, ê¸°ë³¸ ì´ˆê¸°í™” ì§„í–‰")
            orchestrator = None
        
        # MODULE_SPECSë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸°í™”
        module_specs = MODULE_SPECS
        
        # TODO: ë™ì  íƒ€ì„ì•„ì›ƒ ì‹œìŠ¤í…œ êµ¬í˜„
        # ê° ëª¨ë“ˆì˜ ì‹¤ì œ ì´ˆê¸°í™” ì‹œê°„ì„ ì¸¡ì •í•˜ì—¬ ë‹¤ìŒ ì‹¤í–‰ ì‹œ íƒ€ì„ì•„ì›ƒ ìë™ ì¡°ì •
        """
        # í–¥í›„ êµ¬í˜„ ì˜ˆì •: ëª¨ë“ˆë³„ 2ë¶„(120ì´ˆ) ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ + ë™ì  ì¡°ì •
        for spec in module_specs:
            # ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ: 2ë¶„
            base_timeout = 180  # 3ë¶„ìœ¼ë¡œ ì¦ê°€
            
            # ì´ì „ ì‹¤í–‰ ê¸°ë¡ì—ì„œ í‰ê·  ì´ˆê¸°í™” ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
            avg_init_time = self._get_average_init_time(spec['name'])
            
            # ë™ì  íƒ€ì„ì•„ì›ƒ = í‰ê·  ì‹œê°„ì˜ 150% ë˜ëŠ” ê¸°ë³¸ê°’ ì¤‘ í° ê°’
            if avg_init_time > 0:
                dynamic_timeout = max(avg_init_time * 1.5, base_timeout)
            else:
                dynamic_timeout = base_timeout
            
            spec['timeout'] = dynamic_timeout
            
            # ì´ˆê¸°í™” ì‹œê°„ ê¸°ë¡
            start_time = time.time()
            result = await self._load_single_module(spec)
            init_time = time.time() - start_time
            self._record_init_time(spec['name'], init_time)
        """
        
        logger.info(f"ğŸ“‹ {len(module_specs)}ê°œ ì „ì—­ ëª¨ë“ˆ ìˆœì°¨ ì´ˆê¸°í™” ì‹œì‘")
        
        for i, spec in enumerate(module_specs, 1):
            try:
                logger.info(f"ğŸ”„ [{i}/{len(module_specs)}] {spec['name']} ì´ˆê¸°í™” ì¤‘...")
                
                # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                memory_info = get_gpu_memory_info()
                if memory_info:
                    current_usage = memory_info['usage_percent']
                    free_mb = memory_info['free_mb']
                    logger.info(f"   ğŸ“Š í˜„ì¬ GPU: {current_usage:.1f}% ì‚¬ìš©, {free_mb}MB ì—¬ìœ ")
                    
                    # NO-FALLBACK: ì¶”ì •ì¹˜ ê¸°ë°˜ ê³µê°„ í™•ë³´ ê¸ˆì§€
                # ì‹¤ì œ ë¡œë”©ì€ DSM.register_model í›„, DSM._ensure_gpu_memoryì—ì„œë§Œ íŒë‹¨
                if current_usage > 85:
                    logger.info("   ğŸ”„ GPU ë©”ëª¨ë¦¬ 85% ì´ˆê³¼ - DSM ê²½ìœ  ë¡œë”©ë§Œ í—ˆìš© (ì¶”ì •ì¹˜ ì‚¬ìš© ê¸ˆì§€)")
                
                # íƒ€ì„ì•„ì›ƒ ì§ì „ ê²½ê³  ë¡œê·¸
                timeout_seconds = spec['timeout']
                
                # semantic_analyzerëŠ” ìµœì´ˆ ìºì‹œ êµ¬ì¶• ì‹œ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ íƒ€ì„ì•„ì›ƒ ìƒí–¥
                original_timeout = timeout_seconds
                spec_name = str(spec.get('name', ''))
                # semantic_analyzerë¡œ ì‹œì‘í•˜ê±°ë‚˜ í¬í•¨í•˜ë©´ íƒ€ì„ì•„ì›ƒ ìƒí–¥
                if spec_name.startswith('semantic_analyzer') or 'semantic_analyzer' in spec_name:
                    timeout_seconds = max(timeout_seconds, 360)
                    logger.info(f"   â±ï¸ semantic_analyzer íŠ¹ë³„ íƒ€ì„ì•„ì›ƒ ì ìš©: {timeout_seconds}ì´ˆ (ì›ë˜: {original_timeout}ì´ˆ, ëª¨ë“ˆëª…: {spec_name})")
                
                logger.info(f"   â±ï¸ ëª¨ë“ˆ ë¡œë”© ì‹œì‘: {spec['name']} (íƒ€ì„ì•„ì›ƒ: {timeout_seconds}ì´ˆ, ìŠ¤í™ ê¸°ë³¸: {spec['timeout']}ì´ˆ)")
                
                # íƒ€ì„ì•„ì›ƒ ì ìš©í•œ ëª¨ë“ˆ ì´ˆê¸°í™”
                start_load_time = time.time()
                module_instance = await asyncio.wait_for(
                    self._load_single_module(spec),
                    timeout=timeout_seconds
                )
                load_time = time.time() - start_load_time
                logger.info(f"   â±ï¸ {spec['name']} ë¡œë”© ì™„ë£Œ: {load_time:.1f}ì´ˆ ì†Œìš”")
                
                # emotion_analyzerê°€ ìŠ¤í‚µë˜ë”ë¼ë„ ê¸°ë³¸ head ë“±ë¡ ë³´ì¥ (NO FALLBACK)
                if spec['name'] == 'emotion_analyzer' and module_instance is None:
                    logger.warning(f"âš ï¸ emotion_analyzer ìŠ¤í‚µë¨ - ê¸°ë³¸ emotion_empathy_head ë“±ë¡ ì§„í–‰")
                    try:
                        from dynamic_swap_manager import get_swap_manager, SwapPriority
                        import torch.nn as nn
                        
                        swap_manager = get_swap_manager()
                        if swap_manager:
                            # ìµœì†Œí•œì˜ ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ ìƒì„± (NO FALLBACK - ì‹¤ì œ nn.Module)
                            class MinimalEmotionNetwork(nn.Module):
                                def __init__(self):
                                    super().__init__()
                                    self.linear = nn.Linear(768, 7)  # BERT hidden size -> 7 emotions
                                
                                def forward(self, x):
                                    return self.linear(x)
                            
                            default_network = MinimalEmotionNetwork()
                            swap_manager.register_model(
                                "emotion_empathy_head",
                                default_network,
                                priority=SwapPriority.HIGH
                            )
                            logger.info("âœ… ê¸°ë³¸ emotion_empathy_head ë“±ë¡ ì™„ë£Œ (MinimalEmotionNetwork)")
                    except Exception as e:
                        logger.error(f"âŒ ê¸°ë³¸ emotion_empathy_head ë“±ë¡ ì‹¤íŒ¨: {e}")
                        # NO FALLBACK - ì‹¤íŒ¨ ì‹œ ì‹œìŠ¤í…œ ì¤‘ë‹¨
                        raise RuntimeError(f"emotion_empathy_head ë“±ë¡ í•„ìˆ˜: {e}")
                
                # ëª¨ë“ˆì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë”©ëœ ê²½ìš° í™•ì¸
                if module_instance is not None:
                    logger.info(f"   âœ… {spec['name']} ëª¨ë“ˆ ë¡œë”© ì„±ê³µ")
                    
                    # DSMì— ì „ì—­ ëª¨ë“ˆ ë“±ë¡ (ì–¸ë¡œë“œ ê°€ëŠ¥í•˜ë„ë¡)
                    try:
                        from dynamic_swap_manager import get_swap_manager, SwapPriority
                        swap_manager = get_swap_manager()
                        if swap_manager:
                            # ìš°ì„ ìˆœìœ„ ë§¤í•‘
                            priority_map = {
                                'CRITICAL': SwapPriority.CRITICAL,
                                'HIGH': SwapPriority.HIGH,
                                'MEDIUM': SwapPriority.MEDIUM,
                                'LOW': SwapPriority.LOW
                            }
                            priority = priority_map.get(spec.get('priority', 'MEDIUM'), SwapPriority.MEDIUM)
                            
                            # _extract_nn_module í•¨ìˆ˜ë¡œ ì‹¤ì œ nn.Module ì¶”ì¶œ (NO FALLBACK)
                            actual_nn_module = _extract_nn_module(module_instance)
                            
                            # ì‹¤ì œë¡œ GPUì— ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë“±ë¡
                            is_on_gpu = False
                            
                            # 1. wrapperì˜ device í™•ì¸
                            if hasattr(module_instance, 'device'):
                                if str(module_instance.device).startswith('cuda'):
                                    is_on_gpu = True
                            
                            # 2. ì‹¤ì œ nn.Moduleì˜ device í™•ì¸
                            if not is_on_gpu and actual_nn_module is not None:
                                if hasattr(actual_nn_module, 'device'):
                                    if str(actual_nn_module.device).startswith('cuda'):
                                        is_on_gpu = True
                                # parametersë¥¼ í†µí•´ device í™•ì¸
                                elif hasattr(actual_nn_module, 'parameters'):
                                    try:
                                        for p in actual_nn_module.parameters():
                                            if p.is_cuda:
                                                is_on_gpu = True
                                                break
                                    except:
                                        pass
                            
                            # 3. DSMì´ gpu_resident ê´€ë¦¬ (ì™¸ë¶€ì—ì„œ ì§ì ‘ ì¡°ì‘ ê¸ˆì§€)
                            if is_on_gpu:
                                logger.info(f"   â„¹ï¸ {spec['name']}ì´ GPUì— ìˆìŒ - DSMì´ ê´€ë¦¬")
                            else:
                                logger.info(f"   â„¹ï¸ {spec['name']}ì€ CPUì— ìˆìŒ")
                            
                            # ì‹¤ì œ nn.Moduleì„ DSMì— ë“±ë¡ (NO FALLBACK)
                            if actual_nn_module is None:
                                # nn.Module ì¶”ì¶œ ì¬ì‹œë„ (ë” ì²´ê³„ì ì¸ ë°©ë²•)
                                actual_nn_module = _extract_nn_module(module_instance)
                            
                            if actual_nn_module is not None:
                                swap_manager.register_model(
                                    spec['name'],
                                    actual_nn_module,  # ì‹¤ì œ nn.Moduleë§Œ ë“±ë¡
                                    priority=priority
                                )
                                logger.info(f"   âœ… {spec['name']}ì„ DSMì— ë“±ë¡ ì™„ë£Œ (ëª¨ë¸: {actual_nn_module.__class__.__name__})")
                                
                                # GPU ìƒì£¼ ì „í™˜ + ì•ˆì •í™” ëŒ€ê¸° (ë“±ë¡-ìƒì£¼-ì–¸ë¡œë“œ í•¸ë“œì…°ì´í¬)
                                try:
                                    if spec.get('device_policy') == 'gpu_on_demand':
                                        await swap_manager.load_model_to_gpu(spec['name'], timeout=spec.get('timeout', 120))
                                        await asyncio.sleep(0.1)  # ìºì‹œ/allocator ì•ˆì •í™”
                                        logger.info(f"   âœ… {spec['name']} GPU ìƒì£¼ ì™„ë£Œ")
                                except Exception as e:
                                    logger.warning(f"   âš ï¸ {spec['name']} GPU ìƒì£¼ ì‹¤íŒ¨: {e}")
                                
                                # emotion_analyzerì˜ ê²½ìš° íŠ¹ë³„ í™•ì¸
                                if spec['name'] == 'emotion_analyzer':
                                    logger.info(f"   ğŸ” emotion_analyzer DSM ë“±ë¡ í™•ì¸: {actual_nn_module.__class__.__name__}")
                            else:
                                # nn.Module ì¶”ì¶œ ì‹¤íŒ¨ - NO FALLBACK ì •ì±…ì— ë”°ë¼ ë©”íƒ€ ë“±ë¡ ê¸ˆì§€
                                logger.warning(f"   âš ï¸ {spec['name']} nn.Module ì¶”ì¶œ ì‹¤íŒ¨ - DSM ë“±ë¡ ìŠ¤í‚µ")
                                logger.debug(f"      - ëª¨ë“ˆ íƒ€ì…: {type(module_instance)}")
                                logger.debug(f"      - get_pytorch_network ë©”ì„œë“œ: {hasattr(module_instance, 'get_pytorch_network')}")
                                logger.debug(f"      - NO FALLBACK: ë©”íƒ€ ë“±ë¡ ê¸ˆì§€, ì‹¤ì œ nn.Moduleë§Œ í—ˆìš©")
                    except Exception as e:
                        logger.warning(f"   âš ï¸ DSM ë“±ë¡ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
                else:
                    error_msg = f"{spec['name']} ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨"
                    logger.critical(f"   âŒ {error_msg} - ì‹œìŠ¤í…œ ì¤‘ë‹¨")
                    
                    # ì „ì—­ ëª¨ë“ˆ ìƒíƒœ í™•ì¸
                    from config import get_system_module
                    all_modules = []
                    for mod_spec in module_specs:
                        mod = get_system_module(mod_spec['name'])
                        if mod:
                            all_modules.append(f"âœ… {mod_spec['name']}")
                        else:
                            all_modules.append(f"âŒ {mod_spec['name']}")
                    
                    logger.error(f"ğŸ” í˜„ì¬ ì „ì—­ ëª¨ë“ˆ ìƒíƒœ:")
                    for mod_status in all_modules:
                        logger.error(f"   {mod_status}")
                    
                    raise RuntimeError(f"í•„ìˆ˜ ëª¨ë“ˆ {spec['name']} ë¡œë”© ì‹¤íŒ¨")
                
                # ì´ˆê¸°í™” í›„ ë©”ëª¨ë¦¬ ìƒíƒœ ì¬í™•ì¸
                memory_info_after = get_gpu_memory_info()
                if memory_info_after:
                    usage_after = memory_info_after['usage_percent']
                    logger.info(f"   âœ… {spec['name']} ì™„ë£Œ - GPU: {usage_after:.1f}% ì‚¬ìš©")
                else:
                    logger.info(f"   âœ… {spec['name']} ì™„ë£Œ")
                
                # ë©”ëª¨ë¦¬ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸° (100ms)
                await asyncio.sleep(0.1)
                
            except asyncio.TimeoutError:
                # semantic_analyzerì— ëŒ€í•´ í•˜íŠ¸ë¹„íŠ¸ ê¸°ë°˜ ë™ì  ì—°ì¥ ì‹œë„
                if 'semantic_analyzer' in str(spec.get('name', '')):
                    # ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ê°€ ë¶€ë¶„ì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    partial_instance = None
                    for mod_name, mod_inst in (self.module_instances or {}).items():
                        if 'semantic_analyzer' in mod_name:
                            partial_instance = mod_inst
                            break
                    
                    # í•˜íŠ¸ë¹„íŠ¸ í™•ì¸
                    if partial_instance:
                        last_hb = getattr(partial_instance, 'last_heartbeat_ts', 0.0)
                        if (time.time() - last_hb) <= 10.0:
                            extra_timeout = 120
                            logger.warning(f"   â±ï¸ semantic_analyzer ì§„í–‰ì¤‘ ê°ì§€ (ë§ˆì§€ë§‰ í•˜íŠ¸ë¹„íŠ¸: {int(time.time()-last_hb)}ì´ˆ ì „)")
                            logger.warning(f"   â±ï¸ íƒ€ì„ì•„ì›ƒ {extra_timeout}ì´ˆ ì¶”ê°€ ì—°ì¥ ì‹œë„")
                            try:
                                # ì¬ì‹œë„ (ì´ë¯¸ ì‹œì‘ëœ ì´ˆê¸°í™”ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°)
                                module_instance = await asyncio.wait_for(
                                    self._wait_for_existing_init(partial_instance),
                                    timeout=extra_timeout
                                )
                                logger.info(f"   âœ… semantic_analyzer ì—°ì¥ëœ ì‹œê°„ ë‚´ ì™„ë£Œ")
                                # ì„±ê³µ ì‹œ continueë¡œ ë‹¤ìŒ ëª¨ë“ˆë¡œ ì§„í–‰
                                continue
                            except asyncio.TimeoutError:
                                logger.error(f"   âŒ semantic_analyzer ì—°ì¥ í›„ì—ë„ íƒ€ì„ì•„ì›ƒ")
                
                # íƒ€ì„ì•„ì›ƒ ì§„ë‹¨ ì •ë³´ ìˆ˜ì§‘
                diagnostic_info = f"ì„¤ì •: {timeout_seconds}ì´ˆ, ê¸°ë³¸: {spec['timeout']}ì´ˆ"
                
                # ë¶€ë¶„ì ìœ¼ë¡œ ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ í•˜íŠ¸ë¹„íŠ¸ ì •ë³´ í™•ì¸
                for mod_name, mod_inst in (self.module_instances or {}).items():
                    if spec['name'] in mod_name or mod_name in spec['name']:
                        last_hb = getattr(mod_inst, 'last_heartbeat_ts', None)
                        if last_hb:
                            elapsed_since_hb = time.time() - last_hb
                            diagnostic_info += f", ë§ˆì§€ë§‰ í•˜íŠ¸ë¹„íŠ¸: {elapsed_since_hb:.1f}ì´ˆ ì „"
                            break
                
                logger.error(f"   âŒ {spec['name']} ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ ({timeout_seconds}ì´ˆ)")
                logger.error(f"   ğŸ’” íƒ€ì„ì•„ì›ƒ ë°œìƒ ({diagnostic_info})")
                
                # ë¶€ë¶„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ë° ì•ˆì „ íƒˆì¶œ
                partial_inst = self.module_instances.pop(spec['name'], None)
                # semantic_analyzer ì „ìš©: ì·¨ì†Œ ì´ë²¤íŠ¸ê°€ ìˆìœ¼ë©´ ì„¤ì •
                if partial_inst and hasattr(partial_inst, '_cancel_event'):
                    partial_inst._cancel_event.set()
                    logger.info(f"   ğŸ›‘ {spec['name']} ì·¨ì†Œ ì´ë²¤íŠ¸ ì„¤ì •ë¨")
                
                raise RuntimeError(f"{spec['name']} ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ")
                
            except Exception as e:
                logger.error(f"   âŒ {spec['name']} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # í•„ìˆ˜ ëª¨ë“ˆì¸ ê²½ìš° ì „ì²´ ì‹¤íŒ¨
                if spec['priority'] == 'HIGH':
                    raise RuntimeError(f"í•„ìˆ˜ ëª¨ë“ˆ {spec['name']} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                else:
                    logger.warning(f"   âš ï¸ ì„ íƒì  ëª¨ë“ˆ {spec['name']} ê±´ë„ˆëœ€")
        
        # ë©”íƒ€ í†µí•© ì‹œìŠ¤í…œ (ì„ íƒì )
        try:
            logger.info("ğŸŒ ë©”íƒ€ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            from advanced_meta_integration_system import AdvancedMetaIntegrationSystem
            meta_integration = AdvancedMetaIntegrationSystem()
            register_system_module('meta_integration', meta_integration, 'meta')
            logger.info("âœ… meta_integration ì „ì—­ ë“±ë¡ ì™„ë£Œ")
        except ImportError:
            logger.warning("âš ï¸ ë©”íƒ€ í†µí•© ì‹œìŠ¤í…œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - ê±´ë„ˆëœ€")
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”íƒ€ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
        final_memory = get_gpu_memory_info()
        if final_memory:
            final_usage = final_memory['usage_percent']
            logger.info(f"ğŸ¯ ì „ì—­ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ - ìµœì¢… GPU ì‚¬ìš©ë¥ : {final_usage:.1f}%")
            if final_usage < 60:
                logger.warning(f"âš ï¸ GPU ì‚¬ìš©ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ ({final_usage:.1f}%) - ë” ì ê·¹ì  ë¡œë”© í•„ìš”")
        
        logger.info("ğŸ‰ ì „ì—­ ëª¨ë“ˆ ìˆœì°¨ ì´ˆê¸°í™” ì™„ë£Œ - HeadAdapter ì—°ê²° ì¤€ë¹„ë¨")
    
    async def _load_single_module(self, spec):
        """ë‹¨ì¼ ëª¨ë“ˆ ì•ˆì „ ë¡œë”© - ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ í•„ìš” ëª¨ë“ˆ ì²´í¬"""
        module_start_time = time.time()
        logger.info(f"ğŸ”§ _load_single_module ì‹œì‘: {spec['name']} ({spec['class_path']})")
        logger.info(f"   â±ï¸ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%H:%M:%S.%f')[:-3]}")
        try:
            # ì˜¤í”„ë¼ì¸ ëª¨ë“œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë‹¤ìš´ë¡œë“œ ë°©ì§€)
            import os
            original_env = {}
            offline_env = {
                'TRANSFORMERS_OFFLINE': '1',
                'HF_HUB_OFFLINE': '1', 
                'HF_DATASETS_OFFLINE': '1',
                'HF_HUB_DISABLE_TELEMETRY': '1',
                'DISABLE_TELEMETRY': '1',
                'HF_HOME': os.path.expanduser('~/.cache/huggingface'),
                'TRANSFORMERS_CACHE': os.path.expanduser('~/.cache/huggingface/hub'),
                'HF_DATASETS_CACHE': os.path.expanduser('~/.cache/huggingface/datasets'),
                'SENTENCE_TRANSFORMERS_HOME': os.path.expanduser('~/.cache/torch/sentence_transformers'),
                'TOKENIZERS_PARALLELISM': 'false'
            }
            
            # í™˜ê²½ ë³€ìˆ˜ ë°±ì—… ë° ì„¤ì •
            for key, value in offline_env.items():
                original_env[key] = os.environ.get(key)
                os.environ[key] = value
            
            try:
                # ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•œ ëª¨ë“ˆì¸ì§€ ì²´í¬
                requires_download = self._check_if_requires_download(spec)
                
                if requires_download:
                    logger.info(f"ğŸ” {spec['name']} - ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ í•„ìš” ëª¨ë“ˆ ê°ì§€, ìºì‹œ í™•ì¸ ì¤‘...")
                    logger.info(f"   - ëª¨ë“ˆ ê²½ë¡œ: {spec['class_path']}")
                    
                    # ìºì‹œ í™•ì¸
                    has_cache = self._check_model_cache(spec)
                    if not has_cache:
                        logger.warning(f"âš ï¸ {spec['name']} - ë¡œì»¬ ìºì‹œ ì—†ìŒ")
                        print(f"\n{'='*60}")
                        print(f"âš ï¸ ëª¨ë“ˆ {spec['name']} ê±´ë„ˆëœ€ - ìºì‹œ ì—†ìŒ")
                        print(f"ëª¨ë“ˆ ê²½ë¡œ: {spec['class_path']}")
                        print(f"í•„ìš”í•œ ëª¨ë¸ì´ ë¡œì»¬ì— ì—†ìŠµë‹ˆë‹¤.")
                        print(f"{'='*60}\n")
                        return None  # ìºì‹œê°€ ì—†ìœ¼ë©´ None ë°˜í™˜í•˜ì—¬ ê±´ë„ˆëœ€
                    else:
                        logger.info(f"âœ… {spec['name']} - ë¡œì»¬ ìºì‹œ ë°œê²¬, ì˜¤í”„ë¼ì¸ ë¡œë”© ì§„í–‰")
                else:
                    logger.info(f"ğŸ“¦ {spec['name']} - ë‹¤ìš´ë¡œë“œ ë¶ˆí•„ìš” ëª¨ë“ˆ, ì§ì ‘ ë¡œë”©")
                
                # ë™ì  import
                module_path, class_name = spec['class_path'].rsplit('.', 1)
                module = __import__(module_path, fromlist=[class_name])
                
                # í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì‹œë„ (ì˜¤íƒ€ ë³´ì • í¬í•¨)
                try:
                    module_class = getattr(module, class_name)
                except AttributeError:
                    # í´ë˜ìŠ¤ëª… ì˜¤íƒ€ ê°€ëŠ¥ì„± - difflibìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ í´ë˜ìŠ¤ ì°¾ê¸°
                    import difflib
                    candidates = [name for name in dir(module) if name[0].isupper() and not name.startswith('_')]
                    best_matches = difflib.get_close_matches(class_name, candidates, n=1, cutoff=0.8)
                    
                    if best_matches:
                        actual_class_name = best_matches[0]
                        logger.warning(f"âš ï¸ í´ë˜ìŠ¤ '{class_name}' ëŒ€ì‹  '{actual_class_name}' ì‚¬ìš© (ìœ ì‚¬ë„ ê¸°ë°˜)")
                        module_class = getattr(module, actual_class_name)
                    else:
                        # ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ì›ë˜ ì—ëŸ¬ ë°œìƒ
                        raise AttributeError(f"module '{module.__name__}' has no attribute '{class_name}'. "
                                           f"Available classes: {candidates[:5]}...")
                
                # device_policy ë¨¼ì € í™•ì¸
                policy = spec.get('device_policy', 'gpu_required')
                logger.info(f"ğŸ”§ {spec['name']} device_policy: {policy}")
                
                # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                instance = module_class()
                
                # RAM ì„ ë“±ë¡ - ëª¨ë“  ëª¨ë“ˆì„ ì´ˆê¸°ì— DSMì— ë“±ë¡
                if hasattr(self, 'swap_manager') and self.swap_manager:
                    try:
                        # nn.Module ì¶”ì¶œ ì‹œë„
                        nn_core = _extract_nn_module(instance)
                        if nn_core is None:
                            # ì–´ëŒ‘í„° ìƒì„±
                            nn_core = SimpleThinAdapter(instance)
                            logger.info(f"ğŸ“¦ {spec['name']} - SimpleThinAdapterë¡œ ë˜í•‘")
                        
                        # DSMì— RAM ì„ ë“±ë¡
                        priority = self.swap_manager.DEFAULT_PRIORITIES.get(
                            spec['name'], 
                            SwapPriority.MEDIUM
                        )
                        self.swap_manager.register_model(
                            spec['name'], 
                            nn_core, 
                            priority=priority
                        )
                        logger.info(f"âœ… {spec['name']} RAM ì„ ë“±ë¡ ì™„ë£Œ (priority={priority})")
                    except Exception as e:
                        logger.warning(f"âš ï¸ {spec['name']} RAM ì„ ë“±ë¡ ì¤‘ ê²½ê³ : {e}")
                
                # cpu_preload ì •ì±…ì´ë©´ ì¦‰ì‹œ CPUë¡œ ì´ë™ (initialize ì „ì—!)
                if policy == 'cpu_preload':
                    logger.info(f"ğŸ“‹ {spec['name']}ì„ CPUë¡œ ì´ë™ (cpu_preload ì •ì±…)")
                    try:
                        # device ì†ì„± ë³€ê²½
                        if hasattr(instance, 'device'):
                            instance.device = torch.device('cpu')
                            logger.info(f"   - device ì†ì„±ì„ CPUë¡œ ë³€ê²½")
                        
                        # ë‚´ë¶€ ëª¨ë¸ë“¤ CPUë¡œ ì´ë™
                        if hasattr(instance, 'models') and isinstance(instance.models, dict):
                            for model_name, model in instance.models.items():
                                if model is not None and hasattr(model, 'to'):
                                    instance.models[model_name] = model.to('cpu')
                                    logger.debug(f"   - {model_name} CPUë¡œ ì´ë™")
                        
                        # GPU ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ
                        torch.cuda.empty_cache()
                        logger.info(f"   âœ… {spec['name']} CPU ì´ë™ ì™„ë£Œ")
                    except Exception as e:
                        logger.warning(f"   âš ï¸ CPU ì´ë™ ì¤‘ ê²½ê³ : {e}")
                
                # ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ê³¼ ë©”ì„œë“œ í™•ì¸
                logger.info(f"ğŸ” {spec['name']} ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨:")
                logger.info(f"   - íƒ€ì…: {type(instance)}")
                logger.info(f"   - í´ë˜ìŠ¤ëª…: {instance.__class__.__name__}")
                logger.info(f"   - get_pytorch_network ë©”ì„œë“œ ì¡´ì¬: {hasattr(instance, 'get_pytorch_network')}")
                if hasattr(instance, 'get_pytorch_network'):
                    logger.info(f"   - get_pytorch_network íƒ€ì…: {type(getattr(instance, 'get_pytorch_network'))}")
                else:
                    # ë©”ì„œë“œ ëª©ë¡ ì¶œë ¥
                    methods = [m for m in dir(instance) if not m.startswith('_') and callable(getattr(instance, m, None))]
                    logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì„œë“œë“¤ (ì¼ë¶€): {methods[:10]}")
                
                # ë¹„ë™ê¸° ì´ˆê¸°í™” ë©”ì„œë“œê°€ ìˆê³  needs_initializeê°€ Trueë©´ í˜¸ì¶œ
                if hasattr(instance, 'initialize') and spec.get('needs_initialize', False):
                    initialize_method = getattr(instance, 'initialize')
                    logger.info(f"ğŸ”„ {spec['name']} - initialize() ë©”ì„œë“œ ë°œê²¬, needs_initialize=True, í˜¸ì¶œ ì¤‘...")
                    
                    # ì½”ë£¨í‹´ì¸ì§€ í™•ì¸
                    import inspect
                    if inspect.iscoroutinefunction(initialize_method):
                        logger.info(f"   - ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤í–‰ ì¤‘...")
                        await initialize_method()
                        logger.info(f"   âœ… ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                    else:
                        logger.info(f"   - ë™ê¸° ì´ˆê¸°í™” ì‹¤í–‰ ì¤‘...")
                        initialize_method()
                        logger.info(f"   âœ… ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                    
                    # ì´ˆê¸°í™” í›„ get_pytorch_network ì¬í™•ì¸
                    if hasattr(instance, 'get_pytorch_network'):
                        logger.info(f"   - ì´ˆê¸°í™” í›„ get_pytorch_network ë©”ì„œë“œ ì¡´ì¬: True")
                        try:
                            network = instance.get_pytorch_network()
                            if network is not None:
                                logger.info(f"   âœ… PyTorch ë„¤íŠ¸ì›Œí¬ í™•ì¸ë¨: {type(network)}")
                            else:
                                logger.warning(f"   âš ï¸ get_pytorch_network()ê°€ None ë°˜í™˜")
                        except Exception as e:
                            logger.error(f"   âŒ get_pytorch_network() í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                elif hasattr(instance, 'initialize') and not spec.get('needs_initialize', False):
                    logger.info(f"âš ï¸ {spec['name']} - initialize() ë©”ì„œë“œëŠ” ìˆì§€ë§Œ needs_initialize=Falseì´ë¯€ë¡œ ê±´ë„ˆëœ€")
                
                from config import register_system_module, get_system_module
                category = spec.get('category', 'misc')
                
                if policy == 'cpu_preload':
                    # ì´ë¯¸ CPUë¡œ ì´ë™í–ˆìœ¼ë¯€ë¡œ ë“±ë¡ë§Œ ìˆ˜í–‰
                    # ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
                    register_system_module(spec['name'], instance, category)
                    
                    # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— CPU í”„ë¦¬ë¡œë“œ ë“±ë¡
                    if hasattr(self, 'memory_manager') and self.memory_manager:
                        estimated_mb = spec.get('estimated_mb', None)
                        self.memory_manager.register_cpu_preloaded(spec['name'], estimated_mb if estimated_mb else 0)
                    
                    logger.info(f"âœ… {spec['name']} CPU í”„ë¦¬ë¡œë“œ ì™„ë£Œ (ì¹´í…Œê³ ë¦¬: {category})")
                    return instance
                    
                elif policy == 'gpu_required':
                    # GPU ë¡œë”©ì´ í•„ìš”í•œ ê²½ìš° - DSMì„ í†µí•´ GPU ë¡œë”©
                    logger.info(f"ğŸ”„ {spec['name']} GPU ë¡œë”© í•„ìš” - DSM ê²½ìœ  GPU ìŠ¹ê²©...")
                    
                    # ë¨¼ì € CPU í”„ë¦¬ë¡œë“œì²˜ëŸ¼ ì²˜ë¦¬
                    category = spec.get('category', 'utility')
                    logger.info(f"ğŸ“ {spec['name']} ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ (ì¹´í…Œê³ ë¦¬: {category})")
                    register_system_module(spec['name'], instance, category)
                    
                    # DSMì— ë“±ë¡ í›„ GPUë¡œ ìŠ¹ê²©
                    if hasattr(self, 'swap_manager') and self.swap_manager:
                        try:
                            # nn.Module ì¶”ì¶œ
                            actual_nn_module = _extract_nn_module(instance)
                            if actual_nn_module:
                                # DSMì— ë“±ë¡
                                priority = self.swap_manager.DEFAULT_PRIORITIES.get(
                                    spec['name'], 
                                    SwapPriority.MEDIUM
                                )
                                self.swap_manager.register_model(
                                    spec['name'],
                                    actual_nn_module,
                                    priority=priority
                                )
                                logger.info(f"   âœ… {spec['name']} DSM ë“±ë¡ ì™„ë£Œ")
                                
                                # DSMì„ í†µí•´ GPUë¡œ ìŠ¹ê²©
                                await self.swap_manager.load_model_to_gpu(spec['name'])
                                logger.info(f"   âœ… {spec['name']} DSM ê²½ìœ  GPU ìŠ¹ê²© ì™„ë£Œ")
                            else:
                                logger.warning(f"   âš ï¸ {spec['name']} nn.Module ì¶”ì¶œ ì‹¤íŒ¨ - GPU ìŠ¹ê²© ìŠ¤í‚µ")
                        except Exception as e:
                            logger.error(f"   âŒ {spec['name']} DSM GPU ìŠ¹ê²© ì‹¤íŒ¨: {e}")
                            if spec.get('required', True):
                                raise
                    
                    # ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
                    logger.info(f"ğŸ“ {spec['name']} ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ ì‹œë„...")
                    register_system_module(spec['name'], instance, category)
                    
                    # DSMì´ GPU resident ê´€ë¦¬ (ì™¸ë¶€ì—ì„œ ì§ì ‘ ì¡°ì‘ ê¸ˆì§€)
                    
                    # ë“±ë¡ í™•ì¸
                    registered_module = get_system_module(spec['name'])
                    if registered_module is not None:
                        module_load_time = time.time() - module_start_time
                        logger.info(f"âœ… {spec['name']} ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ ì„±ê³µ! (ì¹´í…Œê³ ë¦¬: {category}, ì†Œìš”ì‹œê°„: {module_load_time:.1f}ì´ˆ)")
                        logger.info(f"   - ë“±ë¡ëœ ëª¨ë“ˆ íƒ€ì…: {type(registered_module)}")
                        
                        # emotion_analyzerì˜ ê²½ìš° initialize() ë©”ì„œë“œ í˜¸ì¶œí•˜ì—¬ emotion_empathy_head ë“±ë¡
                        if spec['name'] == 'emotion_analyzer' and hasattr(instance, 'initialize'):
                            logger.info(f"ğŸ”„ emotion_analyzer.initialize() í˜¸ì¶œí•˜ì—¬ emotion_empathy_head ë“±ë¡...")
                            try:
                                await instance.initialize()
                                logger.info(f"âœ… emotion_analyzer.initialize() ì™„ë£Œ - emotion_empathy_head ë“±ë¡ë¨")
                            except Exception as e:
                                logger.error(f"âŒ emotion_analyzer.initialize() ì‹¤íŒ¨: {e}")
                                # NO FALLBACK - ì´ˆê¸°í™” ì‹¤íŒ¨ì‹œ ì‹œìŠ¤í…œ ì¤‘ë‹¨
                                raise RuntimeError(f"emotion_analyzer ì´ˆê¸°í™” ì‹¤íŒ¨ (emotion_empathy_head ë“±ë¡ ì‹¤íŒ¨): {e}")
                        
                        # GPU MEM ë¡œê·¸ í¬ë§·
                        gpu_info = get_gpu_memory_info()
                        if gpu_info:
                            logger.info(f"[GPU MEM] after swap-in {spec['name']}: alloc={gpu_info['allocated_mb']/1024:.1f}GB reserved={gpu_info['cached_mb']/1024:.1f}GB util={gpu_info['usage_percent']:.1f}%")
                    else:
                        logger.error(f"âŒ {spec['name']} ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ ì‹¤íŒ¨!")
                        raise RuntimeError(f"{spec['name']} ëª¨ë“ˆì´ ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡ë˜ì§€ ì•ŠìŒ")
                    
                    return instance
                    
                elif policy == 'gpu_on_demand':
                    # GPU on-demand ì •ì±…: CPU í”„ë¦¬ë¡œë“œ + DSM ë“±ë¡ (GPU ìŠ¹ê²©ì€ ë‚˜ì¤‘ì—)
                    logger.info(f"ğŸ”„ {spec['name']} GPU on-demand ì •ì±… - CPU í”„ë¦¬ë¡œë“œ + DSM ë“±ë¡")
                    
                    # CPUë¡œ ì´ë™
                    logger.info(f"ğŸ“‹ {spec['name']}ì„ CPUë¡œ ì´ë™ (gpu_on_demand ì •ì±…)")
                    try:
                        # device ì†ì„± ë³€ê²½
                        if hasattr(instance, 'device'):
                            instance.device = torch.device('cpu')
                            logger.info(f"   - device ì†ì„±ì„ CPUë¡œ ë³€ê²½")
                        
                        # ë‚´ë¶€ ëª¨ë¸ë“¤ CPUë¡œ ì´ë™
                        if hasattr(instance, 'models') and isinstance(instance.models, dict):
                            for model_name, model in instance.models.items():
                                if model is not None and hasattr(model, 'to'):
                                    instance.models[model_name] = model.to('cpu')
                                    logger.debug(f"   - {model_name} CPUë¡œ ì´ë™")
                        
                        # GPU ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ
                        torch.cuda.empty_cache()
                        logger.info(f"   âœ… {spec['name']} CPU ì´ë™ ì™„ë£Œ")
                    except Exception as e:
                        logger.warning(f"   âš ï¸ CPU ì´ë™ ì¤‘ ê²½ê³ : {e}")
                    
                    # ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
                    category = spec.get('category', 'misc')
                    register_system_module(spec['name'], instance, category)
                    logger.info(f"âœ… {spec['name']} ì „ì—­ ë“±ë¡ ì™„ë£Œ (ì¹´í…Œê³ ë¦¬: {category})")
                    
                    # DSMì— ë“±ë¡ (ë‚˜ì¤‘ì— GPU ìŠ¹ê²© ê°€ëŠ¥í•˜ë„ë¡)
                    if hasattr(self, 'swap_manager') and self.swap_manager:
                        try:
                            actual_nn_module = _extract_nn_module(instance)
                            if actual_nn_module:
                                # DSM ìš°ì„ ìˆœìœ„ ê²°ì •
                                priority_map = {
                                    'HIGH': SwapPriority.HIGH,
                                    'MEDIUM': SwapPriority.MEDIUM,
                                    'LOW': SwapPriority.LOW
                                }
                                priority = priority_map.get(
                                    spec.get('priority', 'MEDIUM'),
                                    SwapPriority.MEDIUM
                                )
                                
                                # DSMì— ë“±ë¡
                                self.swap_manager.register_model(
                                    spec['name'],
                                    actual_nn_module,
                                    priority=priority
                                )
                                logger.info(f"   âœ… {spec['name']} DSM ë“±ë¡ ì™„ë£Œ (ìš°ì„ ìˆœìœ„: {priority.name})")
                                logger.info(f"   ğŸ“Š GPU ìŠ¹ê²©ì€ í•„ìš”ì‹œ load_model_to_gpu()ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤")
                            else:
                                logger.warning(f"   âš ï¸ {spec['name']} nn.Module ì¶”ì¶œ ì‹¤íŒ¨ - DSM ë“±ë¡ ìŠ¤í‚µ")
                        except Exception as e:
                            logger.warning(f"   âš ï¸ {spec['name']} DSM ë“±ë¡ ì¤‘ ê²½ê³ : {e}")
                    
                    # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— CPU í”„ë¦¬ë¡œë“œ ë“±ë¡
                    if hasattr(self, 'memory_manager') and self.memory_manager:
                        estimated_mb = spec.get('estimated_mb', 0)
                        self.memory_manager.register_cpu_preloaded(spec['name'], estimated_mb)
                        logger.info(f"   ğŸ“Š ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ë“±ë¡: {estimated_mb}MB")
                    
                    logger.info(f"âœ… {spec['name']} GPU on-demand ì´ˆê¸°í™” ì™„ë£Œ")
                    return instance
                    
                else:
                    # ì˜ëª»ëœ device_policy ê°’
                    logger.error(f"âŒ ì˜ëª»ëœ device_policy ê°’: {policy}")
                    raise ValueError(f"í—ˆìš©ë˜ì§€ ì•Šì€ device_policy: {policy}. 'gpu_required', 'cpu_preload' ë˜ëŠ” 'gpu_on_demand'ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
                
            finally:
                # í™˜ê²½ ë³€ìˆ˜ ë³µì›
                for key, original_value in original_env.items():
                    if original_value is None:
                        os.environ.pop(key, None)
                    else:
                        os.environ[key] = original_value
            
        except Exception as e:
            error_msg = f"ëª¨ë“ˆ {spec['name']} ë¡œë”© ì‹¤íŒ¨: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            
            # ìƒì„¸ ì—ëŸ¬ ì •ë³´ ì¶œë ¥
            import traceback
            tb_str = traceback.format_exc()
            logger.error(f"ğŸš¨ {spec['name']} ë¡œë”© ì‹¤íŒ¨ ìƒì„¸ ìŠ¤íƒ:\n{tb_str}")
            
            # ì¬ì‹œë„ ë¡œì§ ì²˜ë¦¬
            retry_count = spec.get('retry_on_error', 0)
            if hasattr(self, '_module_retry_count'):
                current_retry = self._module_retry_count.get(spec['name'], 0)
            else:
                self._module_retry_count = {}
                current_retry = 0
            
            if current_retry < retry_count:
                self._module_retry_count[spec['name']] = current_retry + 1
                logger.warning(f"ğŸ”„ {spec['name']} ë¡œë”© ì¬ì‹œë„ ì¤‘... ({current_retry + 1}/{retry_count})")
                await asyncio.sleep(2)  # 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                return await self._load_single_module(spec)
            
            # optional ëª¨ë“ˆ ì²˜ë¦¬
            if spec.get('optional', False):
                logger.warning(f"âš ï¸ ì„ íƒì  ëª¨ë“ˆ {spec['name']} ë¡œë”© ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                print(f"\n{'='*60}")
                print(f"âš ï¸ ì„ íƒì  ëª¨ë“ˆ {spec['name']} ê±´ë„ˆëœ€")
                print(f"ì—ëŸ¬: {str(e)}")
                print(f"ëª¨ë“ˆ ê²½ë¡œ: {spec['class_path']}")
                print(f"{'='*60}\n")
                return None
            
            # í•„ìˆ˜ ëª¨ë“ˆì¸ ê²½ìš°ì—ë§Œ ì˜ˆì™¸ ë°œìƒ
            if spec.get('priority') == 'HIGH' and not spec.get('optional', False):
                print(f"\n{'='*60}")
                print(f"ğŸš¨ í•„ìˆ˜ ëª¨ë“ˆ {spec['name']} ë¡œë”© ì‹¤íŒ¨!")
                print(f"{'='*60}")
                print(f"ì—ëŸ¬: {str(e)}")
                print(f"ëª¨ë“ˆ ê²½ë¡œ: {spec['class_path']}")
                print(f"{'='*60}\n")
                raise
            else:
                logger.warning(f"ì„ íƒì  ëª¨ë“ˆ {spec['name']} ë¡œë”© ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                return None
    
    def _check_if_requires_download(self, spec):
        """ëª¨ë“ˆì´ ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•œì§€ ì²´í¬"""
        # transformers, sentence-transformers ê¸°ë°˜ ëª¨ë“ˆë“¤ë§Œ ì²´í¬
        if 'advanced_emotion_analyzer' in spec.get('class_path', ''):
            return True  # transformers ì‚¬ìš©
        elif 'advanced_semantic_analyzer' in spec.get('class_path', ''):
            return True  # transformers + sentence-transformers ì‚¬ìš©
        elif 'local_translator' in spec.get('class_path', ''):
            return True  # OPUS-MT ëª¨ë¸ ì‚¬ìš©
        elif 'advanced_bentham_calculator' in spec.get('class_path', ''):
            return True  # transformers ì‚¬ìš© (ê¸°ì¡´ ëª¨ë¸ë“¤ ì¬ì‚¬ìš©)
        
        # ë‹¤ë¥¸ ëª¨ë“ˆë“¤ì€ transformers ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        return False
    
    def _check_model_cache(self, spec):
        """ëª¨ë¸ ìºì‹œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        import os
        from pathlib import Path
        
        # ì‹¤ì œ huggingface ìºì‹œ ê²½ë¡œ
        hub_cache_path = Path.home() / '.cache' / 'huggingface' / 'hub'
        
        if not hub_cache_path.exists():
            logger.warning(f"HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬ ì—†ìŒ: {hub_cache_path}")
            return False
        
        # ëª¨ë“ˆë³„ í•„ìš”í•œ ëª¨ë¸ë“¤ í™•ì¸
        required_models = []
        
        if 'advanced_emotion_analyzer' in spec.get('class_path', ''):
            required_models = [
                'models--j-hartmann--emotion-english-distilroberta-base',
                'models--beomi--KcELECTRA-base-v2022',
                'models--jhgan--ko-sroberta-multitask',
                'models--sentence-transformers--paraphrase-multilingual-mpnet-base-v2'
            ]
        elif 'advanced_semantic_analyzer' in spec.get('class_path', ''):
            required_models = [
                'models--jhgan--ko-sroberta-multitask',
                'models--j-hartmann--emotion-english-distilroberta-base',
                'models--facebook--bart-large-mnli',
                'models--microsoft--DialoGPT-medium',
                'models--klue--bert-base'
            ]
        elif 'local_translator' in spec.get('class_path', ''):
            required_models = [
                'models--Helsinki-NLP--opus-mt-ko-en'
            ]
        elif 'advanced_bentham_calculator' in spec.get('class_path', ''):
            # bentham_calculatorëŠ” ë‹¤ë¥¸ ëª¨ë“ˆë“¤ì˜ ëª¨ë¸ì„ ì¬ì‚¬ìš©
            required_models = [
                'models--beomi--KcELECTRA-base-v2022',
                'models--jhgan--ko-sroberta-multitask'
            ]
        
        # í•„ìš”í•œ ëª¨ë¸ë“¤ì´ ìºì‹œì— ìˆëŠ”ì§€ í™•ì¸
        logger.info(f"ğŸ“‚ {spec['name']}ì— í•„ìš”í•œ ëª¨ë¸ {len(required_models)}ê°œ í™•ì¸ ì¤‘...")
        logger.info(f"   ğŸ“ ìºì‹œ ê²½ë¡œ: {hub_cache_path}")
        missing_models = []
        
        for model_cache_name in required_models:
            model_path = hub_cache_path / model_cache_name
            if not model_path.exists():
                logger.warning(f"   âŒ ëª¨ë¸ ìºì‹œ ì—†ìŒ: {model_cache_name}")
                logger.warning(f"      ê²½ë¡œ: {model_path}")
                missing_models.append(model_cache_name)
            else:
                # ìºì‹œ í¬ê¸° í™•ì¸
                try:
                    cache_size = sum(f.stat().st_size for f in model_path.rglob('*') if f.is_file())
                    cache_size_mb = cache_size / (1024 * 1024)
                    logger.info(f"   âœ… ëª¨ë¸ ìºì‹œ ë°œê²¬: {model_cache_name} ({cache_size_mb:.1f}MB)")
                except Exception as e:
                    logger.info(f"   âœ… ëª¨ë¸ ìºì‹œ ë°œê²¬: {model_cache_name} (í¬ê¸° í™•ì¸ ì‹¤íŒ¨)")
        
        if missing_models:
            print(f"\n{'='*60}")
            print(f"âŒ {spec['name']} ëª¨ë“ˆì— í•„ìš”í•œ ëª¨ë¸ ìºì‹œ ë¶€ì¡±")
            print(f"ëˆ„ë½ëœ ëª¨ë¸:")
            for model in missing_models:
                print(f"  - {model}")
            print(f"{'='*60}\n")
            return False
        
        return len(required_models) > 0  # í•„ìš”í•œ ëª¨ë¸ì´ ìˆê³  ëª¨ë‘ ìºì‹œì— ìˆìœ¼ë©´ True
    
    def _log_dsm_snapshot(self, tag: str):
        """DSM ìƒíƒœ ìŠ¤ëƒ…ìƒ· ë¡œê¹… - ë””ë²„ê¹…ìš©"""
        try:
            swap = self.swap_manager
            if not swap:
                logger.warning(f"[DSM@{tag}] swap_managerê°€ None")
                return
            
            # ë“±ë¡ëœ ëª¨ë“  ëª¨ë¸
            all_models = list(swap.models.keys()) if hasattr(swap, 'models') else []
            logger.critical(f"[DSM@{tag}] models={all_models[:30]}")
            
            # GPU ìƒì£¼ ëª¨ë¸ë“¤
            gpu_keys = list(swap.gpu_resident_models.keys()) if hasattr(swap, 'gpu_resident_models') else []
            
            # í¬ê¸° ì •ë³´
            sizes = {}
            priorities = {}
            for k in gpu_keys[:20]:
                if k in swap.models:
                    sizes[k] = f"{swap.models[k].size_mb:.1f}MB"
                    priorities[k] = swap.models[k].priority.value if swap.models[k].priority else "NONE"
            
            logger.critical(f"[DSM@{tag}] gpu_resident={gpu_keys[:20]} sizes={sizes}")
            logger.critical(f"[DSM@{tag}] priorities={priorities}")
            
        except Exception as e:
            logger.warning(f"DSM ìŠ¤ëƒ…ìƒ· ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    def _log_gpu_memory_state(self, stage: str):
        """GPU ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹…"""
        try:
            gpu_info = get_gpu_memory_info()
            if gpu_info:
                usage_percent = gpu_info.get('usage_percent', 0)
                used_gb = gpu_info.get('memory_used_gb', 0)
                total_gb = gpu_info.get('memory_total_gb', 8)
                logger.info(f"ğŸ” [{stage}] GPU ë©”ëª¨ë¦¬: {usage_percent:.1f}% ({used_gb:.2f}/{total_gb:.2f}GB)")
            else:
                logger.info(f"ğŸ” [{stage}] GPU ë©”ëª¨ë¦¬ ì •ë³´ ì—†ìŒ")
        except Exception as e:
            logger.warning(f"GPU ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    async def _emergency_gpu_cleanup_using_dsm(self, target_usage: float = 0.78, 
                                               exclude: Optional[set] = None) -> None:
        """
        DSM ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œë§Œ GPU ì–¸ë¡œë“œë¥¼ ìˆ˜í–‰í•œë‹¤.
        exclude: í˜„ì¬ ë¡œë”©ì¤‘/í•„ìˆ˜ ë³´í˜¸ ëª¨ë¸ ì´ë¦„ ì§‘í•©
        """
        from dynamic_swap_manager import SwapPriority
        swap = self.swap_manager
        if swap is None:
            logger.error("âŒ DSM ë¯¸ì´ˆê¸°í™” - ê¸´ê¸‰ ì •ë¦¬ ìŠ¤í‚µ")
            return
        exclude = exclude or set()

        info_before = get_gpu_memory_info() or {}
        used_before = info_before.get('usage_percent', 0.0)
        freed_total = 0.0

        # í›„ë³´: GPU ìƒì£¼ + CRITICAL ì œì™¸ + exclude ì œì™¸
        candidates = []
        for name, model in list(swap.models.items()):
            if name in swap.gpu_resident_models:
                if model.priority != SwapPriority.CRITICAL and name not in exclude:
                    candidates.append((name, model.priority, model.size_mb, model.last_access))

        if not candidates:
            logger.warning("âš ï¸ DSM ê¸°ë°˜ í›„ë³´ 0ê°œ â†’ ì–¸ë¡œë“œ ë¶ˆê°€")
            return

        # ìš°ì„ ìˆœìœ„/ìµœê·¼ì ‘ê·¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë‚®ì€ ìš°ì„ ìˆœìœ„, ì˜¤ë˜ëœ ì ‘ê·¼ ìˆœ)
        candidates.sort(key=lambda x: (x[1].value, x[3]))  # priority.value asc, last_access asc
        logger.info(f"ğŸ“ DSM í›„ë³´ {len(candidates)}ê°œ: {[c[0] for c in candidates[:20]]} ...")

        for name, prio, sz, _ in candidates:
            try:
                await swap.unload_model_from_gpu(name)
                freed_total += sz
            except Exception as e:
                logger.error(f"ì–¸ë¡œë“œ ì‹¤íŒ¨: {name} ({e})")
                continue
            
            # CUDA ìºì‹œ ë°˜ì˜ ëŒ€ê¸°
            await asyncio.sleep(0.15)
            info_now = get_gpu_memory_info() or {}
            if info_now.get('usage_percent', 1.0) <= target_usage * 100:
                break

        info_after = get_gpu_memory_info() or {}
        logger.info(
            f"âœ… DSM ê¸´ê¸‰ ì •ë¦¬ ì™„ë£Œ: ì‚¬ìš©ë¥  {used_before:.1f}% â†’ {info_after.get('usage_percent', used_before):.1f}% "
            f"(í•´ì œ ì¶”ì • {freed_total:.1f}MB)"
        )
    
    async def _initialize_core_components(self):
        """í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        
        # ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì ì§„ì ìœ¼ë¡œ ì´ˆê¸°í™”
        self.log_manager.log_system_event("í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹œì‘")
        
        # GPU ë©”ëª¨ë¦¬ ì´ˆê¸° ìƒíƒœ í™•ì¸ - ê°€ì¥ ë¨¼ì €!
        self._log_gpu_memory_state("ì´ˆê¸°í™” ì‹œì‘ ì „")
        
        # ğŸ”¥ 1ë‹¨ê³„: ì „ì—­ ëª¨ë“ˆ ìˆœì°¨ ì´ˆê¸°í™” (GPU ë©”ëª¨ë¦¬ 85% ëª©í‘œ)
        logger.critical(f"ğŸ” fast_init_mode = {self.fast_init_mode}")
        if self.fast_init_mode:
            logger.info("âš¡ ë¹ ë¥¸ ì´ˆê¸°í™” ëª¨ë“œ - ì „ì—­ ëª¨ë“ˆ ì´ˆê¸°í™” ê±´ë„ˆëœ€")
        else:
            logger.critical("ğŸŒ ì „ì—­ ëª¨ë“ˆ ìˆœì°¨ ì´ˆê¸°í™” ì‹œì‘ - MasterMemoryOrchestrator ê´€ë¦¬")
            try:
                await self._initialize_global_modules_sequential()
                logger.critical("âœ… ì „ì—­ ëª¨ë“ˆ ìˆœì°¨ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.critical(f"âŒ ì „ì—­ ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                import traceback
                logger.critical(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
                # í”„ë¡œì íŠ¸ ê·œì¹™: fallback ì—†ì´ ì‹¤íŒ¨
                raise RuntimeError(f"ì „ì—­ ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨ - ì‹œìŠ¤í…œ ì¤‘ë‹¨: {e}") from e
        
        # ğŸ”¥ í•µì‹¬ ì¶”ê°€: í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ í­ì£¼ ë°©ì§€)
        logger.info("ğŸ§  í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        from config import initialize_unified_memory_system
        if initialize_unified_memory_system():
            logger.info("âœ… í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.error("âŒ í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ - ë©”ëª¨ë¦¬ í­ì£¼ ìœ„í—˜!")
        
        # ì „ì—­ ëª¨ë“ˆ ì´ˆê¸°í™” í›„ GPU ìƒíƒœ í™•ì¸
        self._log_gpu_memory_state("ì „ì—­ ëª¨ë“ˆ ì´ˆê¸°í™” í›„")
        
        # 1. Unified Backbone
        logger.info("ğŸ§  Unified Backbone ì´ˆê¸°í™” ì‹œì‘...")
        self.unified_backbone = RedHeartUnifiedBackbone()
        logger.info("Unified Backbone ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë°±ë³¸ ì´ˆê¸°í™” í›„ ë©”ëª¨ë¦¬ ìƒíƒœ
        self._log_gpu_memory_state("ë°±ë³¸ ì´ˆê¸°í™” í›„")
        
        # 2. Swap Manager (ì›Œí¬í”Œë¡œìš° ì¸ì‹ í™œì„±í™”)
        logger.info("ğŸ”„ Dynamic Swap Manager ì´ˆê¸°í™” ì‹œì‘...")
        swap_config = ADVANCED_CONFIG.get('dynamic_swap_config', {})
        swap_config['workflow_aware'] = True
        swap_config['memory_threshold_mb'] = 6500.0  # 8GB GPUì˜ ~81%
        self.swap_manager = RedHeartDynamicSwapManager(config=swap_config)
        await self.swap_manager.initialize()
        
        # ì „ì—­ DSMìœ¼ë¡œ publish (ì´í›„ ëª¨ë“  ëª¨ë“ˆì˜ get_swap_manager()ê°€ ê°™ì€ ê°ì²´ë¥¼ ë³´ë„ë¡)
        from dynamic_swap_manager import set_swap_manager, get_swap_manager, SwapPriority
        set_swap_manager(self.swap_manager)
        logger.critical(f"[O] set_swap_manager OK: self_id={id(self.swap_manager)} global_id={id(get_swap_manager())}")
        assert id(self.swap_manager) == id(get_swap_manager()), "DSM publish failed"
        logger.info("Dynamic Swap Manager ì´ˆê¸°í™” ì™„ë£Œ (ì›Œí¬í”Œë¡œìš° ì¸ì‹ í™œì„±í™”)")
        
        # ğŸ”¥ ë°±ë³¸ì„ DSMì— ë“±ë¡ (ê°€ì¥ í° ë©”ëª¨ë¦¬ ì†Œë¹„ì)
        if self.swap_manager and self.unified_backbone:
            try:
                # _extract_nn_module í—¬í¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
                backbone_model = _extract_nn_module(self.unified_backbone)
                
                if backbone_model is not None:
                    self.swap_manager.register_model(
                        "unified_backbone",
                        backbone_model,
                        priority=SwapPriority.CRITICAL  # ë°±ë³¸ì€ CRITICALë¡œ ë³´í˜¸
                    )
                    logger.critical("âœ… ë°±ë³¸ DSM ë“±ë¡ ì™„ë£Œ (CRITICAL ìš°ì„ ìˆœìœ„)")
                else:
                    # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ MinimalBackboneìœ¼ë¡œ ì„ì‹œ ë“±ë¡ (NO FALLBACK ì •ì±… ì˜ˆì™¸)
                    logger.warning("âš ï¸ ë°±ë³¸ nn.Module ì¶”ì¶œ ì‹¤íŒ¨ - MinimalBackboneë¡œ ì„ì‹œ ë“±ë¡")
                    import torch
                    class _MinimalBackbone(torch.nn.Module):
                        def __init__(self):
                            super().__init__()
                            self.placeholder = torch.nn.Linear(1280, 1280, bias=False)
                        def forward(self, x):
                            return x if x is None else self.placeholder(x)
                    
                    minimal_backbone = _MinimalBackbone()
                    minimal_backbone.to(get_smart_device())  # GPUë¡œ ì´ë™
                    self.swap_manager.register_model(
                        "unified_backbone",
                        minimal_backbone,
                        priority=SwapPriority.CRITICAL
                    )
                    logger.critical("âš ï¸ MinimalBackbone DSM ë“±ë¡ ì™„ë£Œ (ë°±ë³¸ ì¶”ì¶œ ì‹¤íŒ¨ ëŒ€ë¹„)")
            except Exception as e:
                logger.error(f"âŒ ë°±ë³¸ DSM ë“±ë¡ ì‹¤íŒ¨: {e}")
        
        self._log_gpu_memory_state("ìŠ¤ì™‘ ë§¤ë‹ˆì € ì´ˆê¸°í™” í›„")
        self._log_dsm_snapshot("ìŠ¤ì™‘ë§¤ë‹ˆì €ì´ˆê¸°í™”í›„")
        
        # í—¤ë“œ ì„ ë“±ë¡ ë³´ì¥ (ìŠ¤ì™‘ ë§¤ë‹ˆì € ì¤€ë¹„ ì§í›„)
        self._ensure_pre_registered_heads()
        
        # warm load/unload ìˆ˜í–‰ (ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì•ˆì „í•˜ê²Œ)
        if hasattr(self, '_needs_warm_load_unload') and self._needs_warm_load_unload:
            try:
                logger.info("ğŸ”„ warm load/unload ì‹œì‘ (ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸)")
                # GPUë¡œ ì˜¬ë¦¬ê¸°
                if "emotion_empathy_head" in self.swap_manager.models:
                    await self.swap_manager.load_head_to_gpu("emotion_empathy_head", timeout=1.0)
                    logger.info("âœ… emotion_empathy_head warm load ì™„ë£Œ")
                    
                    # ì ì‹œ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì•ˆì •í™”)
                    await asyncio.sleep(0.1)
                    
                    # GPUì—ì„œ ë‚´ë¦¬ê¸°
                    await self.swap_manager.unload_model_from_gpu("emotion_empathy_head")
                    logger.info("âœ… emotion_empathy_head warm unload ì™„ë£Œ")
                    
                self._needs_warm_load_unload = False
                logger.info("âœ… warm load/unload ì™„ë£Œ (ìŠ¤ì™‘ ê²½ë¡œ ê²€ì¦)")
            except Exception as e:
                logger.warning(f"âš ï¸ warm load/unload ì‹¤íŒ¨: {e}")
                self._needs_warm_load_unload = False
        
        # 3. Head Compatibility Manager (í—¤ë“œ ë“±ë¡ì„ ìœ„í•´ í•„ìš”)
        logger.info("ğŸ­ Head Compatibility Manager ì´ˆê¸°í™” ì‹œì‘...")
        self.head_compatibility_manager = HeadCompatibilityManager(
            self.unified_backbone, 
            self.swap_manager
        )
        logger.info("Head Compatibility Manager ì´ˆê¸°í™” ì™„ë£Œ")
        self._log_gpu_memory_state("í—¤ë“œ í˜¸í™˜ì„± ë§¤ë‹ˆì € ìƒì„± í›„")
        
        # 4. ëª¨ë“  í—¤ë“œ ì´ˆê¸°í™” ë° ë“±ë¡ (ë©”ëª¨ë¦¬ í­ë°œ ì˜ì‹¬ êµ¬ê°„)
        if self.fast_init_mode:
            logger.info("âš¡ ë¹ ë¥¸ ì´ˆê¸°í™” ëª¨ë“œ - í—¤ë“œ ì´ˆê¸°í™” ê±´ë„ˆëœ€")
        else:
            # í—¤ë“œ ì´ˆê¸°í™” ì „ ì„ ë“±ë¡ ì¬í™•ì¸
            self._ensure_pre_registered_heads()
            
            logger.critical("ğŸš¨ í—¤ë“œ ì´ˆê¸°í™” ì‹œì‘ - ë©”ëª¨ë¦¬ í­ë°œ ì˜ì‹¬ êµ¬ê°„!")
            logger.critical(f"ğŸ” head_compatibility_manager = {type(self.head_compatibility_manager)}")
            logger.critical(f"ğŸ” head_compatibility_manager.initialized = {getattr(self.head_compatibility_manager, 'initialized', 'NOT_FOUND')}")
            logger.critical("ğŸ” initialize_all_heads() í˜¸ì¶œ ì§ì „...")
            
            # asyncio íƒ€ì„ì•„ì›ƒ ì¶”ê°€ (900ì´ˆ ì œí•œ - 5ê°œ í—¤ë“œ * 180ì´ˆ) - ì‹¤íŒ¨ì‹œ ì‹œìŠ¤í…œ ì¤‘ë‹¨
            try:
                await asyncio.wait_for(
                    self.head_compatibility_manager.initialize_all_heads(),
                    timeout=900.0
                )
                logger.critical("ğŸ” initialize_all_heads() í˜¸ì¶œ ì™„ë£Œ!")
            except asyncio.TimeoutError:
                logger.error("âŒ initialize_all_heads() 900ì´ˆ íƒ€ì„ì•„ì›ƒ ë°œìƒ!")
                logger.error("ğŸ” ì–´ë–¤ í—¤ë“œì—ì„œ hangingì´ ë°œìƒí–ˆëŠ”ì§€ í™•ì¸ ì¤‘...")
                
                # ê° í—¤ë“œë³„ ìƒíƒœ í™•ì¸
                if hasattr(self.head_compatibility_manager, 'head_adapters'):
                    for head_name, adapter in self.head_compatibility_manager.head_adapters.items():
                        status = getattr(adapter, 'initialized', 'UNKNOWN')
                        logger.error(f"  ğŸ“‹ {head_name}: {status}")
                
                # ì‹œìŠ¤í…œ ì™„ì „ ì¤‘ë‹¨ - graceful degradation ê¸ˆì§€
                logger.critical("âŒ í—¤ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨ë¡œ ì‹œìŠ¤í…œ ì¤‘ë‹¨")
                raise RuntimeError("Head initialization timeout - system cannot continue without all heads")
            
        logger.critical("ğŸš¨ í—¤ë“œ ì´ˆê¸°í™” ì™„ë£Œ - ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ì¤‘...")
        self._log_gpu_memory_state("ëª¨ë“  í—¤ë“œ ì´ˆê¸°í™” í›„")
        self._log_dsm_snapshot("í—¤ë“œì´ˆê¸°í™”í›„")
        
        # í—¤ë“œ ì´ˆê¸°í™” ì§í›„ ë©”ëª¨ë¦¬ ìƒíƒœì— ë”°ë¼ ì¡°ê±´ë¶€ ì •ë¦¬
        mem_info = get_gpu_memory_info()
        current_usage = mem_info.get("usage_percent", 0) if mem_info else 0
        
        if current_usage >= 85.0:
            logger.critical(f"ğŸš¨ GPU ì‚¬ìš©ë¥  {current_usage:.1f}% - DSM ê¸´ê¸‰ ì •ë¦¬ ìˆ˜í–‰ (target=85%)")
            self._log_dsm_snapshot("ê¸´ê¸‰ì •ë¦¬ì „")
            # í•„ìˆ˜ í—¤ë“œì™€ ë°±ë³¸ì€ ì œì™¸
            exclude_models = {'emotion_empathy_head', 'unified_backbone', 'regret_learning_head'}
            await self._emergency_gpu_cleanup_using_dsm(target_usage=0.85, exclude=exclude_models)
            self._log_gpu_memory_state("ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ í›„")
            self._log_dsm_snapshot("ê¸´ê¸‰ì •ë¦¬í›„")
        else:
            logger.info(f"âœ… GPU ì‚¬ìš©ë¥  {current_usage:.1f}% - ì„ê³„ì¹˜ ë¯¸ë§Œìœ¼ë¡œ ì •ë¦¬ ìŠ¤í‚µ")
        
        # 5. Synergy System
        logger.info("âš¡ Intelligent Synergy System ì´ˆê¸°í™” ì‹œì‘...")
        self.synergy_system = IntelligentSynergySystem()
        logger.info("Intelligent Synergy System ì´ˆê¸°í™” ì™„ë£Œ")
        self._log_gpu_memory_state("ì‹œë„ˆì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” í›„")
        
        # 6. Learning System - HeadCompatibilityManager ì „ë‹¬
        logger.info("ğŸ“š Unified Learning System ì´ˆê¸°í™” ì‹œì‘...")
        self.learning_system = UnifiedLearningSystem(
            head_compatibility_manager=self.head_compatibility_manager,
            swap_manager=self.swap_manager
        )
        logger.info("Unified Learning System ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ (HeadCompatibilityManager ê³µìœ )")
        
        # Learning Systemì˜ ë¹„ë™ê¸° ì´ˆê¸°í™” í˜¸ì¶œ - cached_head_modules ì±„ìš°ê¸°
        logger.info("ğŸ“š Unified Learning System í—¤ë“œ ëª¨ë“ˆ ìºì‹± ì‹œì‘...")
        try:
            await self.learning_system.initialize_system()
            logger.info("âœ… Unified Learning System ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë“  í—¤ë“œ ëª¨ë“ˆ ìºì‹±ë¨")
        except RuntimeError as e:
            logger.error(f"âŒ í—¤ë“œ ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise  # í”„ë¡œì íŠ¸ ê·œì¹™: fallback ì—†ì´ hard failure
        
        self._log_gpu_memory_state("í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” í›„")
        self._log_dsm_snapshot("í•™ìŠµì‹œìŠ¤í…œì´ˆê¸°í™”í›„")
        
        # 7. Pattern Analyzer
        logger.info("ğŸ“Š Advanced Usage Pattern Analyzer ì´ˆê¸°í™” ì‹œì‘...")
        self.pattern_analyzer = AdvancedUsagePatternAnalyzer()
        logger.info("Advanced Usage Pattern Analyzer ì´ˆê¸°í™” ì™„ë£Œ")
        self._log_gpu_memory_state("íŒ¨í„´ ë¶„ì„ê¸° ì´ˆê¸°í™” í›„")
    
    async def run_training_pipeline(self, mode: str = "auto", **kwargs) -> bool:
        """í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        if self.system_status != SystemStatus.READY:
            logger.error("ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ")
            return False
        
        try:
            self.system_status = SystemStatus.TRAINING
            self.log_manager.log_system_event(f"í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {mode}")
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€: run_learning.sh ì¬ì‹¤í–‰ ì œê±°, Python í•™ìŠµ ì‹œìŠ¤í…œ ì§ì ‘ ì‹¤í–‰
            logger.info(f"Python í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì§ì ‘ ì‹¤í–‰ (ëª¨ë“œ: {mode})")
            
            # Python í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰
            python_success = await self._run_python_training(**kwargs)
            
            if python_success:
                self.log_manager.log_system_event("í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ", "INFO")
                self.system_status = SystemStatus.READY
                return True
            else:
                raise Exception("Python í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨")
            
        except Exception as e:
            self.system_status = SystemStatus.ERROR
            self.log_manager.log_system_event(f"í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}", "ERROR")
            
            # ìë™ ë³µêµ¬ ì‹œë„
            if self.auto_recovery and self.recovery_attempts < self.max_recovery_attempts:
                logger.info("ìë™ ë³µêµ¬ ì‹œë„ ì¤‘...")
                await self._attempt_recovery()
            
            return False
    
    async def _run_learning_script(self, mode: str, **kwargs) -> bool:
        """run_learning.sh ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"""
        
        # ëª…ë ¹ì–´ êµ¬ì„±
        command = ["bash", str(self.script_path), mode]
        
        # ì¶”ê°€ ì¸ìë“¤ ì²˜ë¦¬
        for key, value in kwargs.items():
            if key == "samples":
                command.extend(["--samples", str(value)])
            elif key == "batch_size":
                command.extend(["--batch-size", str(value)])
            elif key == "learning_rate":
                command.extend(["--learning-rate", str(value)])
            elif key == "verbose" and value:
                command.append("--verbose")
            elif key == "debug" and value:
                command.append("--debug")
        
        try:
            # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            process_name = f"run_learning_{mode}_{int(time.time())}"
            pid = await self.process_manager.start_background_process(
                process_name, command, restart_on_failure=False
            )
            
            self.log_manager.log_system_event(f"run_learning.sh ì‹¤í–‰: PID {pid}")
            
            # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            timeout = kwargs.get('timeout', 3600)  # ê¸°ë³¸ 1ì‹œê°„
            
            start_time = time.time()
            while process_name in self.process_manager.processes:
                if time.time() - start_time > timeout:
                    logger.warning("run_learning.sh íƒ€ì„ì•„ì›ƒ")
                    await self.process_manager.stop_process(process_name)
                    return False
                
                await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ í™•ì¸
            
            logger.info("run_learning.sh ì‹¤í–‰ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"run_learning.sh ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return False
    
    async def _run_python_training(self, **kwargs) -> bool:
        """Python í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰"""
        
        try:
            # ê°€ìƒì˜ ë°ì´í„° ë¡œë” (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ë°ì´í„° ì‚¬ìš©)
            class DummyDataLoader:
                def __init__(self, num_batches=100):
                    self.num_batches = num_batches
                
                def __iter__(self):
                    for i in range(self.num_batches):
                        yield {
                            'text': f'Training sample {i}',
                            'batch_size': kwargs.get('batch_size', 4),
                            'labels': np.random.randint(0, 10, size=(kwargs.get('batch_size', 4),))
                        }
            
            # í›ˆë ¨ ì„¤ì •
            num_epochs = kwargs.get('epochs', 3)
            num_samples = kwargs.get('samples', 500)
            num_batches = min(num_samples // kwargs.get('batch_size', 4), 500)
            
            train_loader = DummyDataLoader(num_batches)
            val_loader = DummyDataLoader(num_batches // 5)  # ê²€ì¦ ë°ì´í„°ëŠ” 1/5
            
            # í•™ìŠµ ì‹œì‘ ì§ì „ í—¤ë“œ ì„ ë“±ë¡ ì¬í™•ì¸
            self._ensure_pre_registered_heads()
            
            # í†µí•© í•™ìŠµ ì‹¤í–‰
            await self.learning_system.train_unified_system(
                train_data_loader=train_loader,
                validation_data_loader=val_loader,
                num_epochs=num_epochs
            )
            
            logger.info("Python í†µí•© í•™ìŠµ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"Python í†µí•© í•™ìŠµ ì˜¤ë¥˜: {str(e)}")
            import traceback
            tb_str = traceback.format_exc()
            logger.error(f"ğŸ˜¨ ìƒì„¸ ì—ëŸ¬ ìŠ¤íƒ:\n{tb_str}")
            logger.error(f"ğŸ” ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ğŸ“ num_epochs: {num_epochs}, num_batches: {num_batches}")
            return False
    
    async def _attempt_recovery(self):
        """ìë™ ë³µêµ¬ ì‹œë„"""
        
        self.recovery_attempts += 1
        self.log_manager.log_system_event(f"ìë™ ë³µêµ¬ ì‹œë„ {self.recovery_attempts}/{self.max_recovery_attempts}")
        
        try:
            # 1. ì‹œìŠ¤í…œ ì •ë¦¬
            await self._cleanup_system()
            
            # 2. ì»´í¬ë„ŒíŠ¸ ì¬ì´ˆê¸°í™”
            await self._initialize_core_components()
            
            # 3. ì‹œìŠ¤í…œ ìƒíƒœ ë³µì›
            self.system_status = SystemStatus.READY
            
            self.log_manager.log_system_event("ìë™ ë³µêµ¬ ì„±ê³µ", "INFO")
            logger.info("ì‹œìŠ¤í…œ ìë™ ë³µêµ¬ ì„±ê³µ")
            
        except Exception as e:
            self.log_manager.log_system_event(f"ìë™ ë³µêµ¬ ì‹¤íŒ¨: {str(e)}", "ERROR")
            logger.error(f"ìë™ ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
    
    async def _cleanup_system(self):
        """ì‹œìŠ¤í…œ ì •ë¦¬"""
        
        # ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
        for process_name in list(self.process_manager.processes.keys()):
            await self.process_manager.stop_process(process_name)
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if hasattr(self.unified_backbone, 'clear_cache'):
            self.unified_backbone.clear_cache()
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        temp_dir = "./temp"
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)
        
        logger.info("ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
    
    def _handle_system_alert(self, alert_message: str, metrics: SystemHealthMetrics):
        """ì‹œìŠ¤í…œ ì•Œë¦¼ ì²˜ë¦¬"""
        
        self.log_manager.log_system_event(f"ì‹œìŠ¤í…œ ì•Œë¦¼: {alert_message}", "WARNING")
        
        # ì‹¬ê°í•œ ì•Œë¦¼ì˜ ê²½ìš° ìë™ ì¡°ì¹˜
        if "CPU" in alert_message and metrics.cpu_usage > 95:
            logger.warning("ë†’ì€ CPU ì‚¬ìš©ë¥  ê°ì§€ - ë¶€í•˜ ë¶„ì‚° ì‹œë„")
            # ì—¬ê¸°ì„œ ë¶€í•˜ ë¶„ì‚° ë¡œì§ êµ¬í˜„
        
        elif "ë©”ëª¨ë¦¬" in alert_message and metrics.memory_usage > 90:
            logger.warning("ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê°ì§€ - ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„")
            # ì—¬ê¸°ì„œ ë©”ëª¨ë¦¬ ì •ë¦¬ ë¡œì§ êµ¬í˜„
    
    async def _start_dashboard(self):
        """ì›¹ ëŒ€ì‹œë³´ë“œ ì‹œì‘"""
        
        try:
            # ê°„ë‹¨í•œ ì›¹ ëŒ€ì‹œë³´ë“œ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” FastAPI, Flask ë“± ì‚¬ìš©)
            logger.info(f"ì›¹ ëŒ€ì‹œë³´ë“œ ì‹œì‘ ì˜ˆì •: http://localhost:{self.dashboard_port}")
            self.log_manager.log_system_event(f"ì›¹ ëŒ€ì‹œë³´ë“œ í¬íŠ¸ {self.dashboard_port}ì—ì„œ ì‹œì‘ ì˜ˆì •")
            
        except Exception as e:
            logger.error(f"ì›¹ ëŒ€ì‹œë³´ë“œ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")
    
    async def shutdown_system(self):
        """ì‹œìŠ¤í…œ ì¢…ë£Œ"""
        
        try:
            self.system_status = SystemStatus.SHUTDOWN
            self.log_manager.log_system_event("ì‹œìŠ¤í…œ ì¢…ë£Œ ì‹œì‘")
            
            # 1. ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
            for process_name in list(self.process_manager.processes.keys()):
                await self.process_manager.stop_process(process_name)
            
            # 2. ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.system_monitor.stop_monitoring()
            
            # 3. ë¡œê·¸ íšŒì „
            self.log_manager.rotate_logs()
            
            # 4. ì‹œìŠ¤í…œ ì •ë¦¬
            await self._cleanup_system()
            
            self.is_running = False
            
            uptime = datetime.now() - self.start_time
            self.log_manager.log_system_event(f"ì‹œìŠ¤í…œ ì •ìƒ ì¢…ë£Œ (ê°€ë™ì‹œê°„: {uptime})", "INFO")
            logger.info(f"Red Heart í†µí•© ì‹œìŠ¤í…œ ì¢…ë£Œ ì™„ë£Œ (ê°€ë™ì‹œê°„: {uptime})")
            
        except Exception as e:
            self.log_manager.log_system_event(f"ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {str(e)}", "ERROR")
            logger.error(f"ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        
        # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„° ìš”ì•½
        monitor_summary = self.system_monitor.get_system_summary()
        
        # í”„ë¡œì„¸ìŠ¤ ìƒíƒœ
        process_status = self.process_manager.get_process_status()
        
        # ì»´í¬ë„ŒíŠ¸ ìƒíƒœ
        component_status = {}
        if self.learning_system:
            component_status['learning_system'] = self.learning_system.get_training_statistics()
        
        if self.synergy_system:
            component_status['synergy_system'] = self.synergy_system.get_synergy_statistics()
        
        # ì „ì²´ ìƒíƒœ ì¢…í•©
        uptime = datetime.now() - self.start_time
        
        return {
            'system_status': self.system_status.value,
            'uptime_seconds': uptime.total_seconds(),
            'is_running': self.is_running,
            'recovery_attempts': self.recovery_attempts,
            'monitor_summary': monitor_summary,
            'process_status': process_status,
            'component_status': component_status,
            'timestamp': datetime.now().isoformat()
        }
    
    def _log_gpu_memory_state(self, stage_name: str):
        """ë‹¨ê³„ë³„ GPU ë©”ëª¨ë¦¬ ìƒíƒœ ìƒì„¸ ë¡œê¹…"""
        try:
            import torch
            if not torch.cuda.is_available():
                logger.warning(f"[{stage_name}] CUDA ì‚¬ìš© ë¶ˆê°€")
                return
            
            # ê¸°ë³¸ GPU ë©”ëª¨ë¦¬ ì •ë³´
            device_props = torch.cuda.get_device_properties(0)
            total_memory_gb = device_props.total_memory / (1024**3)
            allocated_gb = torch.cuda.memory_allocated(0) / (1024**3)
            reserved_gb = torch.cuda.memory_reserved(0) / (1024**3)
            free_gb = total_memory_gb - allocated_gb
            allocated_percent = (allocated_gb / total_memory_gb) * 100
            
            logger.critical(f"ğŸ” [{stage_name}] GPU ë©”ëª¨ë¦¬ ìƒíƒœ:")
            logger.critical(f"   ğŸ’¾ ì´ ë©”ëª¨ë¦¬: {total_memory_gb:.2f}GB")
            logger.critical(f"   ğŸ“Š í• ë‹¹ë¨: {allocated_gb:.3f}GB ({allocated_percent:.1f}%)")
            logger.critical(f"   ğŸ“¦ ì˜ˆì•½ë¨: {reserved_gb:.3f}GB")
            logger.critical(f"   ğŸ’š ì—¬ìœ : {free_gb:.3f}GB")
            
            # config.pyì˜ get_gpu_memory_info()ì™€ ë¹„êµ
            from config import get_gpu_memory_info
            config_info = get_gpu_memory_info()
            if config_info:
                config_percent = config_info.get('usage_percent', 0)
                logger.critical(f"   ğŸ”„ config.py ì¸¡ì •: {config_percent:.1f}%")
                if abs(allocated_percent - config_percent) > 5:
                    logger.error(f"   âš ï¸ ì¸¡ì • ë°©ì‹ ì°¨ì´ ë°œê²¬: torch({allocated_percent:.1f}%) vs config({config_percent:.1f}%)")
            
            # GPU í…ì„œ ê°œìˆ˜ ë° í¬ê¸° ë¶„ì„
            import gc
            gpu_tensors = []
            total_tensor_memory = 0
            
            for obj in gc.get_objects():
                if torch.is_tensor(obj) and obj.is_cuda:
                    tensor_size = obj.element_size() * obj.numel()
                    gpu_tensors.append({
                        'shape': list(obj.shape),
                        'dtype': str(obj.dtype),
                        'size_mb': tensor_size / (1024**2),
                        'requires_grad': obj.requires_grad
                    })
                    total_tensor_memory += tensor_size
            
            total_tensor_gb = total_tensor_memory / (1024**3)
            logger.critical(f"   ğŸ§  GPU í…ì„œ: {len(gpu_tensors)}ê°œ, {total_tensor_gb:.3f}GB")
            
            # í° í…ì„œë“¤ ìƒìœ„ 5ê°œ ì¶œë ¥
            if gpu_tensors:
                large_tensors = sorted(gpu_tensors, key=lambda x: x['size_mb'], reverse=True)[:5]
                logger.critical(f"   ğŸ“‹ í° í…ì„œ TOP 5:")
                for i, tensor in enumerate(large_tensors, 1):
                    logger.critical(f"      {i}. {tensor['shape']} ({tensor['dtype']}) - {tensor['size_mb']:.1f}MB")
            
            # ë©”ëª¨ë¦¬ ìºì‹œ ìƒíƒœ
            logger.critical(f"   ğŸ—‚ï¸ ìºì‹œëœ ë©”ëª¨ë¦¬: {(reserved_gb - allocated_gb):.3f}GB")
            
            # ìœ„í—˜ ìˆ˜ì¤€ íŒì •
            if allocated_percent > 100:
                logger.error(f"   ğŸš¨ğŸš¨ğŸš¨ CRITICAL: ë©”ëª¨ë¦¬ ì˜¤ë²„í”Œë¡œìš° ({allocated_percent:.1f}%)")
            elif allocated_percent > 90:
                logger.warning(f"   âš ï¸ WARNING: ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ({allocated_percent:.1f}%)")
            
            # ë©”ëª¨ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸ (peak tracking)
            current_max = torch.cuda.max_memory_allocated(0) / (1024**3)
            if not hasattr(self, '_peak_memory_gb'):
                self._peak_memory_gb = 0
            
            if current_max > self._peak_memory_gb:
                self._peak_memory_gb = current_max
                logger.critical(f"   ğŸ” ìƒˆë¡œìš´ Peak ë©”ëª¨ë¦¬: {self._peak_memory_gb:.3f}GB (at {stage_name})")
            
        except Exception as e:
            logger.error(f"[{stage_name}] GPU ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹… ì‹¤íŒ¨: {str(e)}")

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
async def example_usage():
    """í†µí•© ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì‚¬ìš© ì˜ˆì‹œ"""
    
    # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒì„±
    orchestrator = UnifiedSystemOrchestrator({
        'dashboard_port': 8080,
        'enable_dashboard': False,  # ì˜ˆì‹œì—ì„œëŠ” ë¹„í™œì„±í™”
        'auto_recovery': True
    })
    
    try:
        print("=== Red Heart í†µí•© ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í…ŒìŠ¤íŠ¸ ===")
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        init_success = await orchestrator.initialize_system()
        if not init_success:
            print("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return
        
        print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        status = orchestrator.get_system_status()
        print(f"ì‹œìŠ¤í…œ ìƒíƒœ: {status['system_status']}")
        print(f"ì „ì²´ ê±´ê°•ë„: {status['monitor_summary'].get('overall_health', 0):.2%}")
        
        # í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸)
        print("\nğŸš€ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
        training_success = await orchestrator.run_training_pipeline(
            mode="test",
            samples=50,
            batch_size=2,
            epochs=1,
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        if training_success:
            print("âœ… í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
        else:
            print("âŒ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨")
        
        # ìµœì¢… ìƒíƒœ í™•ì¸
        final_status = orchestrator.get_system_status()
        print(f"\n=== ìµœì¢… ì‹œìŠ¤í…œ ìƒíƒœ ===")
        print(f"ìƒíƒœ: {final_status['system_status']}")
        print(f"ê°€ë™ì‹œê°„: {final_status['uptime_seconds']:.1f}ì´ˆ")
        print(f"ë³µêµ¬ ì‹œë„: {final_status['recovery_attempts']}íšŒ")
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    finally:
        # ì‹œìŠ¤í…œ ì¢…ë£Œ
        print("\nğŸ”„ ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
        await orchestrator.shutdown_system()
        print("âœ… ì‹œìŠ¤í…œ ì¢…ë£Œ ì™„ë£Œ")

if __name__ == "__main__":
    asyncio.run(example_usage())