#!/usr/bin/env python3
"""
Claude API ì „ìš© ë…ë¦½ ì¶”ë¡  ì‹œìŠ¤í…œ
DSMê³¼ ì™„ì „íˆ ë¶„ë¦¬ëœ ë…ë¦½ì ì¸ ì›Œí¬í”Œë¡œìš°
"""

import asyncio
import logging
import torch
import numpy as np
from datetime import datetime
from typing import Dict, Any, Optional
from pathlib import Path
import json
import sys
import os

# ê²½ë¡œ ì„¤ì •
sys.path.append('/mnt/c/large_project/linux_red_heart')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('ClaudeInference')

class ClaudeInferenceSystem:
    """Claude API ì „ìš© ì¶”ë¡  ì‹œìŠ¤í…œ - DSM ì—†ì´ ë…ë¦½ ì‹¤í–‰"""
    
    def __init__(self, epoch: int = 50, debug: bool = False):
        self.epoch = epoch
        self.debug = debug
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” í”Œë˜ê·¸
        self.unified_model = None
        self.translator = None
        self.neural_analyzers = {}
        self.advanced_wrappers = {}
        self.llm_engine = None
        self.emotion_hierarchy_processor = None  # Regret Circuit ì¶”ê°€
        
        logger.info("=" * 70)
        logger.info("ğŸš€ Claude API ë…ë¦½ ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        logger.info("=" * 70)
        logger.info(f"ğŸ“Œ ì—í­: {epoch}")
        logger.info(f"ğŸ“Œ ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"ğŸ“Œ ë””ë²„ê·¸ ëª¨ë“œ: {debug}")
        
    async def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™” - ìˆœì°¨ì  ë¡œë“œ"""
        try:
            logger.info("\nğŸ“¦ Phase 1: UnifiedModel ë¡œë“œ...")
            await self._load_unified_model()
            
            logger.info("\nğŸ“¦ Phase 2: ë²ˆì—­ê¸° ë¡œë“œ...")
            await self._load_translator()
            
            logger.info("\nğŸ“¦ Phase 3: Neural Analyzers ë¡œë“œ...")
            await self._load_neural_analyzers()
            
            logger.info("\nğŸ“¦ Phase 4: Advanced Wrappers ë¡œë“œ...")
            await self._load_advanced_wrappers()
            
            logger.info("\nğŸ“¦ Phase 5: Emotion-Ethics-Regret Circuit ì´ˆê¸°í™”...")
            await self._load_emotion_circuit()
            
            logger.info("\nğŸ“¦ Phase 6: Claude API ì—”ì§„ ì´ˆê¸°í™”...")
            await self._init_claude_api()
            
            logger.info("\nâœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
            raise
    
    async def _load_unified_model(self):
        """UnifiedModel ë¡œë“œ - DSM ì—†ì´"""
        from training.unified_training_final import UnifiedModel, UnifiedTrainingConfig
        from training.enhanced_checkpoint_manager import EnhancedCheckpointManager
        
        # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì €
        checkpoint_manager = EnhancedCheckpointManager(
            checkpoint_dir="training/checkpoints_final",
            max_checkpoints=30
        )
        
        # í•™ìŠµ ì„¤ì • ìƒì„±
        train_config = UnifiedTrainingConfig()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.unified_model = UnifiedModel(
            config=train_config,
            device=self.device
        )
        
        # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì € ì—°ê²°
        self.unified_model.checkpoint_manager = checkpoint_manager
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint_path = f"training/checkpoints_final/checkpoint_epoch_{self.epoch:04d}_*.pt"
        checkpoints = list(Path(".").glob(checkpoint_path))
        
        if checkpoints:
            checkpoint_file = str(checkpoints[0])
            logger.info(f"   ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_file}")
            
            checkpoint = torch.load(checkpoint_file, map_location=self.device)
            
            # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ë¡œë“œ
            if 'model_state' in checkpoint:
                self.unified_model.load_state_dict(checkpoint['model_state'], strict=False)
                logger.info("   âœ… UnifiedModel ë¡œë“œ ì™„ë£Œ")
                logger.info(f"   - Epoch: {checkpoint.get('epoch', 'unknown')}")
                logger.info(f"   - LR: {checkpoint.get('lr', 'unknown')}")
            else:
                logger.error("   âŒ ì²´í¬í¬ì¸íŠ¸ì— 'model_state' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                logger.error(f"   ì²´í¬í¬ì¸íŠ¸ í‚¤: {list(checkpoint.keys())}")
                raise KeyError("ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤")
        else:
            logger.warning(f"   âš ï¸ ì—í­ {self.epoch} ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ")
        
        self.unified_model.eval()
        
        # advanced_wrappersë¥¼ UnifiedModelì— ì „ë‹¬ (ì´ê²ƒì„ ë‚˜ì¤‘ì— ë¡œë“œí•œ advanced_wrappersë¡œ ì„¤ì •)
        self.unified_model.advanced_wrappers = None  # ë‚˜ì¤‘ì— _load_advanced_wrappersì—ì„œ ì„¤ì •
        
    async def _load_translator(self):
        """ë²ˆì—­ê¸° ë¡œë“œ - Claude ëª¨ë“œì—ì„œëŠ” ìŠ¤í‚µ"""
        import os
        
        # Claude ëª¨ë“œì—ì„œëŠ” ë²ˆì—­ê¸° ë¶ˆí•„ìš” (Claude APIê°€ ë‹¤êµ­ì–´ ì§€ì›)
        if os.getenv('REDHEART_CLAUDE_MODE') == '1':
            logger.info("   âš ï¸ Claude ëª¨ë“œ - ë²ˆì—­ê¸° ë¡œë“œ ìŠ¤í‚µ (Claude API ë‹¤êµ­ì–´ ì§€ì›)")
            self.translator = None
            return
        
        from config import get_system_module
        
        # main_unified.pyì—ì„œ ì´ë¯¸ ë“±ë¡ëœ translatorë¥¼ ê°€ì ¸ì˜¤ê¸°
        self.translator = get_system_module('translator')
        
        if self.translator is None:
            # í˜¹ì‹œ ë“±ë¡ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì§ì ‘ ìƒì„±
            from local_translator import LocalTranslator
            from config import register_system_module
            
            logger.warning("ì „ì—­ translator ì—†ìŒ - ì§ì ‘ ìƒì„±")
            self.translator = LocalTranslator()
            register_system_module('translator', self.translator)
            logger.info("   âœ… ë²ˆì—­ê¸° ì§ì ‘ ë¡œë“œ ë° ë“±ë¡ ì™„ë£Œ (CPU)")
        else:
            logger.info("   âœ… ì „ì—­ ë²ˆì—­ê¸° ì¬ì‚¬ìš© (CPU)")
    
    async def _load_neural_analyzers(self):
        """Neural Analyzers ë¡œë“œ - CPUì—ì„œë§Œ"""
        from analyzer_neural_modules import (
            NeuralEmotionAnalyzer,
            NeuralBenthamCalculator,
            NeuralRegretAnalyzer,
            NeuralSURDAnalyzer
        )
        
        # ì´ˆê¸°í™” (input_dim ê¸°ë³¸ê°’ ì‚¬ìš©)
        self.neural_analyzers = {
            'emotion': NeuralEmotionAnalyzer(),
            'bentham': NeuralBenthamCalculator(),
            'regret': NeuralRegretAnalyzer(),
            'surd': NeuralSURDAnalyzer()
        }
        
        # CPUë¡œ ì´ë™
        for analyzer in self.neural_analyzers.values():
            analyzer.to('cpu')
        
        logger.info("   âœ… Neural Analyzers ë¡œë“œ ì™„ë£Œ (CPU)")
        
    async def _load_advanced_wrappers(self):
        """Advanced Wrappers ë¡œë“œ - CPUì—ì„œë§Œ"""
        from advanced_analyzer_wrappers import (
            AdvancedEmotionAnalyzerWrapper,
            AdvancedBenthamCalculatorWrapper,
            AdvancedRegretAnalyzerWrapper,
            AdvancedSURDAnalyzerWrapper
        )
        
        # Wrapper ì´ˆê¸°í™” (íŒŒë¼ë¯¸í„° ì—†ì´)
        self.advanced_wrappers = {
            'advanced_emotion': AdvancedEmotionAnalyzerWrapper(),
            'advanced_bentham': AdvancedBenthamCalculatorWrapper(),
            'advanced_regret': AdvancedRegretAnalyzerWrapper(),
            'advanced_surd': AdvancedSURDAnalyzerWrapper()
        }
        
        # Wrappersì— í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ ì„¤ì •
        for wrapper_name, wrapper in self.advanced_wrappers.items():
            # analyzer ë‚´ë¶€ì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìŒ
            if hasattr(wrapper, 'analyzer'):
                # translator ì„¤ì •
                if hasattr(wrapper.analyzer, 'translator'):
                    wrapper.analyzer.translator = self.translator
        
        logger.info("   âœ… Advanced Wrappers ë¡œë“œ ì™„ë£Œ (CPU)")
        
        # UnifiedModelì— advanced_wrappers ì „ë‹¬
        if hasattr(self, 'unified_model'):
            self.unified_model.advanced_wrappers = self.advanced_wrappers
            logger.info("   âœ… UnifiedModelì— advanced_wrappers ì—°ê²° ì™„ë£Œ")
    
    async def _init_claude_api(self):
        """Claude API ì—”ì§„ ì´ˆê¸°í™”"""
        from llm_module.advanced_llm_engine import AdvancedLLMEngine
        
        # Claude API ì—”ì§„ ì´ˆê¸°í™”
        self.llm_engine = AdvancedLLMEngine(use_api='claude')
        # AdvancedLLMEngineì—ëŠ” initialize() ë©”ì„œë“œê°€ ì—†ìŒ - ì œê±°
        
        # Wrappersì— LLM ì—”ì§„ ì—°ê²°
        for wrapper in self.advanced_wrappers.values():
            wrapper.llm_engine = self.llm_engine
        
        logger.info("   âœ… Claude API ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def _load_emotion_circuit(self):
        """Emotion-Ethics-Regret Circuit ë¡œë“œ"""
        try:
            from emotion_ethics_regret_circuit import EmotionEthicsRegretCircuit
            
            # Circuit ì´ˆê¸°í™”
            self.emotion_hierarchy_processor = EmotionEthicsRegretCircuit()
            logger.info("   âœ… Emotion-Ethics-Regret Circuit ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info("      - ê°ì •-ìœ¤ë¦¬-í›„íšŒ í†µí•© ë¶„ì„ ê°€ëŠ¥")
            logger.info("      - Circuit í™œì„±í™” ì¡°ê±´: config.use_emotion_hierarchy=True")
            
        except ImportError as e:
            logger.warning(f"   âš ï¸ Emotion Circuit ë¡œë“œ ì‹¤íŒ¨ (ì„ íƒì ): {e}")
            self.emotion_hierarchy_processor = None
        except Exception as e:
            logger.error(f"   âŒ Emotion Circuit ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.emotion_hierarchy_processor = None
    
    async def inference(self, text: str) -> Dict[str, Any]:
        """ì¶”ë¡  ì‹¤í–‰"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ¯ ì¶”ë¡  ì‹œì‘")
        logger.info("=" * 70)
        logger.info(f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: {text}")
        
        try:
            # 1. ë²ˆì—­ (í•œêµ­ì–´ â†’ ì˜ì–´) - Claude ëª¨ë“œì—ì„œëŠ” ìŠ¤í‚µ
            import os
            if os.getenv('REDHEART_CLAUDE_MODE') == '1':
                # Claude ëª¨ë“œ: ë²ˆì—­ ìŠ¤í‚µ (Claudeê°€ ë‹¤êµ­ì–´ ì§€ì›)
                en_text = text
                logger.info("âš¡ Claude ëª¨ë“œ - ë²ˆì—­ ìŠ¤í‚µ (ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)")
            elif self.translator:
                en_text = await self.translator.translate_async(text)
                logger.info(f"ğŸ”„ ë²ˆì—­ ì™„ë£Œ: {en_text[:100]}...")
            else:
                en_text = text
            
            # 1.5. LLMì„ í†µí•œ ì´ˆê¸° ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìš”ì²­ (í•„ìˆ˜)
            counterfactual_scenarios = []
            if not self.llm_engine:
                logger.error("\nâŒ LLM ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                raise RuntimeError("LLM ì—”ì§„ì´ ì—†ìœ¼ë©´ ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ë¶ˆê°€. ì‹œìŠ¤í…œ ë¬´ê²°ì„± ì˜¤ë¥˜")
            
            if self.llm_engine:
                logger.info("\nğŸ¯ LLM ì´ˆê¸° ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±...")
                from llm_module.advanced_llm_engine import LLMRequest, TaskComplexity
                
                scenario_prompt = f"""ë‹¤ìŒ ìƒí™©ì— ëŒ€í•œ 3ê°€ì§€ ë°˜ì‚¬ì‹¤ì (counterfactual) ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ìƒí™©: {text}

ê° ì‹œë‚˜ë¦¬ì˜¤ëŠ” ë‹¤ë¥¸ ì„ íƒì´ë‚˜ í–‰ë™ì„ í–ˆì„ ë•Œì˜ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤:
1. ë‚™ê´€ì  ì‹œë‚˜ë¦¬ì˜¤: ìµœì„ ì˜ ì„ íƒì„ í–ˆì„ ë•Œì˜ ê²°ê³¼
2. ì¤‘ë¦½ì  ì‹œë‚˜ë¦¬ì˜¤: ì¼ë°˜ì ì¸ ì„ íƒì„ í–ˆì„ ë•Œì˜ ê²°ê³¼  
3. ë¹„ê´€ì  ì‹œë‚˜ë¦¬ì˜¤: ìµœì•…ì˜ ì„ íƒì„ í–ˆì„ ë•Œì˜ ê²°ê³¼

ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”:
[
  {{"type": "optimistic", "action": "êµ¬ì²´ì  í–‰ë™", "outcome": "ì˜ˆìƒ ê²°ê³¼", "ethical_implications": "ìœ¤ë¦¬ì  í•¨ì˜"}},
  {{"type": "neutral", "action": "êµ¬ì²´ì  í–‰ë™", "outcome": "ì˜ˆìƒ ê²°ê³¼", "ethical_implications": "ìœ¤ë¦¬ì  í•¨ì˜"}},
  {{"type": "pessimistic", "action": "êµ¬ì²´ì  í–‰ë™", "outcome": "ì˜ˆìƒ ê²°ê³¼", "ethical_implications": "ìœ¤ë¦¬ì  í•¨ì˜"}}
]

ì¤‘ìš”: JSON í˜•ì‹ë§Œ ë°˜í™˜í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""
                
                scenario_request = LLMRequest(
                    prompt=scenario_prompt,
                    task_type="scenario_generation",
                    complexity=TaskComplexity.MODERATE,
                    max_tokens=2000,  # 3ê°œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì™„ì „íˆ ë‹´ì„ ìˆ˜ ìˆë„ë¡ ì¦ê°€
                    context={'mode': 'counterfactual_initial'}
                )
                
                try:
                    scenario_response = await self.llm_engine.generate_async(scenario_request)
                    if scenario_response and scenario_response.success:
                        # JSON íŒŒì‹± ì‹œë„ (ê°œì„ ëœ ë²„ì „)
                        import json
                        import re
                        text_content = scenario_response.generated_text
                        
                        # ì—¬ëŸ¬ JSON ì¶”ì¶œ ë°©ë²• ì‹œë„
                        json_parsed = False
                        
                        # ë°©ë²• 1: ì „ì²´ í…ìŠ¤íŠ¸ê°€ JSONì¸ ê²½ìš°
                        try:
                            # ë””ë²„ê¹…: ì „ì²´ ì‘ë‹µ ë¡œê¹…
                            logger.debug(f"   ğŸ“ LLM ì „ì²´ ì‘ë‹µ ({len(text_content)}ì): {text_content}")
                            counterfactual_scenarios = json.loads(text_content)
                            json_parsed = True
                            logger.info("   âœ… ë°©ë²• 1(ì „ì²´ í…ìŠ¤íŠ¸ JSON)ë¡œ íŒŒì‹± ì„±ê³µ")
                        except json.JSONDecodeError as e:
                            logger.debug(f"   âš ï¸ ë°©ë²• 1 ì‹¤íŒ¨: {e}")
                            pass
                        
                        # ë°©ë²• 2: JSON ë°°ì—´ íŒ¨í„´ ì°¾ê¸° (ê°œì„ ëœ ì •ê·œì‹)
                        if not json_parsed:
                            # ë” ìœ ì—°í•œ JSON ë°°ì—´ ì¶”ì¶œ (ì¤‘ì²©ëœ ê°ì²´ ê³ ë ¤)
                            json_patterns = [
                                r'\[\s*\{[^[\]]*\}\s*\]',  # ê°„ë‹¨í•œ íŒ¨í„´
                                r'\[[\s\S]*?\](?=\s*$|\s*[^\[\]\{\}])',  # ë¬¸ì„œ ëì˜ JSON
                                r'(?:^|\n)\s*\[[\s\S]*?\]\s*(?:\n|$)'  # ì¤„ë°”ê¿ˆ ì‚¬ì´ì˜ JSON
                            ]
                            
                            for pattern in json_patterns:
                                matches = re.findall(pattern, text_content, re.MULTILINE | re.DOTALL)
                                for match in matches:
                                    try:
                                        # JSON ë¬¸ìì—´ ì •ë¦¬
                                        cleaned_json = match.strip()
                                        # ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ ì œê±° (ì´ë¯¸ ì˜¬ë°”ë¥¸ JSONì´ì–´ì•¼ í•¨)
                                        counterfactual_scenarios = json.loads(cleaned_json)
                                        json_parsed = True
                                        logger.info(f"   âœ… ë°©ë²• 2(ì •ê·œì‹ íŒ¨í„´ {idx+1})ë¡œ íŒŒì‹± ì„±ê³µ")
                                        break
                                    except json.JSONDecodeError:
                                        continue
                                if json_parsed:
                                    break
                        
                        # ë°©ë²• 3: ì½”ë“œ ë¸”ë¡ ë‚´ JSON ì°¾ê¸°
                        if not json_parsed:
                            code_block_match = re.search(r'```(?:json)?\s*([\s\S]*?)```', text_content)
                            if code_block_match:
                                try:
                                    counterfactual_scenarios = json.loads(code_block_match.group(1).strip())
                                    json_parsed = True
                                    logger.info("   âœ… ë°©ë²• 3(ì½”ë“œ ë¸”ë¡)ìœ¼ë¡œ íŒŒì‹± ì„±ê³µ")
                                except json.JSONDecodeError as e:
                                    logger.debug(f"   âš ï¸ ë°©ë²• 3 ì‹¤íŒ¨: {e}")
                                    pass
                        
                        # JSON íŒŒì‹± ì„±ê³µ ì—¬ë¶€ì— ë”°ë¥¸ ì²˜ë¦¬
                        if json_parsed and isinstance(counterfactual_scenarios, list) and len(counterfactual_scenarios) > 0:
                            # ì‹œë‚˜ë¦¬ì˜¤ í˜•ì‹ ê²€ì¦
                            required_fields = ['type', 'action', 'outcome', 'ethical_implications']
                            for idx, scenario in enumerate(counterfactual_scenarios):
                                if not isinstance(scenario, dict):
                                    raise RuntimeError(f"ì‹œë‚˜ë¦¬ì˜¤ {idx}ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(scenario)}")
                                missing_fields = [f for f in required_fields if f not in scenario]
                                if missing_fields:
                                    raise RuntimeError(f"ì‹œë‚˜ë¦¬ì˜¤ {idx}ì— í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {missing_fields}")
                            
                            logger.info(f"   âœ… {len(counterfactual_scenarios)}ê°œ ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ë° ê²€ì¦ ì™„ë£Œ")
                            for scenario in counterfactual_scenarios:
                                logger.info(f"      - {scenario.get('type', 'unknown')}: {scenario.get('action', '')[:50]}...")
                        else:
                            # JSON íŒŒì‹± ì‹¤íŒ¨ - ì‹œìŠ¤í…œ ì¤‘ë‹¨
                            logger.error("   âŒ JSON íŒŒì‹± ì‹¤íŒ¨ - ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ë¶ˆê°€")
                            logger.error(f"   â†’ LLM ì‘ë‹µ: {text_content[:200]}...")
                            raise RuntimeError("ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ JSON íŒŒì‹± ì‹¤íŒ¨: LLMì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì˜ JSONì„ ë°˜í™˜í•˜ì§€ ì•ŠìŒ")
                    else:
                        # LLM ì‘ë‹µ ì‹¤íŒ¨ - ì‹œìŠ¤í…œ ì¤‘ë‹¨
                        logger.error("   âŒ LLM ì‘ë‹µ ì‹¤íŒ¨ - ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ë¶ˆê°€")
                        if scenario_response:
                            logger.error(f"   â†’ ì—ëŸ¬: {scenario_response.error}")
                        raise RuntimeError(f"LLM ì‘ë‹µ ì‹¤íŒ¨: {scenario_response.error if scenario_response else 'Unknown error'}")
                except Exception as e:
                    logger.error(f"   âŒ ì´ˆê¸° ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì‹¤íŒ¨: {e}")
                    import traceback
                    logger.error(f"   â†’ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
                    raise RuntimeError(f"ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            
            # 2. UnifiedModel ì¶”ë¡  (GPU)
            logger.info("\nğŸ“Š UnifiedModel ì¶”ë¡ ...")
            with torch.no_grad():
                # GPUë¡œ ì„ì‹œ ì´ë™
                self.unified_model = self.unified_model.to(self.device)
                
                # ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
                from sentence_transformer_singleton import SentenceTransformerManager
                stm = SentenceTransformerManager()
                
                # get_model()ì„ í†µí•´ ëª¨ë¸ í”„ë¡ì‹œ íšë“
                logger.info("   ğŸ“ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
                model_proxy = stm.get_model('sentence-transformers/paraphrase-multilingual-mpnet-base-v2', device=str(self.device))
                # convert_to_tensor=Falseë¡œ ì„¤ì •í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ê³  ì§ì ‘ í…ì„œ ë³€í™˜
                embeddings = model_proxy.encode(en_text, convert_to_tensor=False)
                # ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
                embeddings = torch.tensor(embeddings).to(self.device)
                embeddings = embeddings.unsqueeze(0) if len(embeddings.shape) == 1 else embeddings
                
                # ê° íƒœìŠ¤í¬ë³„ë¡œ ì‹¤ì œ ì¶”ë¡  ìˆ˜í–‰
                logger.info("   ğŸ­ ê°ì • ë¶„ì„ ì¶”ë¡ ...")
                emotion_outputs = self.unified_model(embeddings, task='emotion', return_all=True)
                emotion_output = emotion_outputs.get('head', list(emotion_outputs.values())[0]).cpu().numpy()
                
                logger.info("   âš–ï¸ ë²¤ë‹´ ê³„ì‚° ì¶”ë¡ ...")
                bentham_outputs = self.unified_model(embeddings, task='bentham', return_all=True)
                bentham_output = bentham_outputs.get('head', list(bentham_outputs.values())[0]).cpu().numpy()
                
                logger.info("   ğŸ˜” í›„íšŒ ë¶„ì„ ì¶”ë¡ ...")
                regret_outputs = self.unified_model(embeddings, task='regret', return_all=True)
                regret_output = regret_outputs.get('head', list(regret_outputs.values())[0]).cpu().numpy()
                
                logger.info("   ğŸ” SURD ë¶„ì„ ì¶”ë¡ ...")
                surd_outputs = self.unified_model(embeddings, task='surd', return_all=True)
                surd_output = surd_outputs.get('head', list(surd_outputs.values())[0]).cpu().numpy()
                
                # ì„ë² ë”©ì„ CPUë¡œ ì´ë™í•˜ì—¬ ë³´ì¡´ (Advanced Analysisì—ì„œ ì‚¬ìš©)
                embeddings_cpu = embeddings.cpu()
                
                # ëª¨ë¸ì„ ë‹¤ì‹œ CPUë¡œ
                self.unified_model = self.unified_model.to('cpu')
                torch.cuda.empty_cache()
                
            logger.info("   âœ… UnifiedModel ì¶”ë¡  ì™„ë£Œ")
            
            # 3. Advanced Wrappers ë¶„ì„ (CPU)
            logger.info("\nğŸ§  Advanced Analysis...")
            
            results = {
                'text': text,
                'translated': en_text,
                'unified_outputs': {
                    'emotion': emotion_output.tolist(),
                    'bentham': bentham_output.tolist(),
                    'regret': regret_output.tolist(),
                    'surd': surd_output.tolist()
                },
                'advanced_analysis': {}
            }
            
            # ê° Wrapper ì‹¤í–‰ - nn.Moduleì˜ forward ë©”ì„œë“œ í˜¸ì¶œ
            # ëª¨ë“  wrapperì— ì„ë² ë”©(896ì°¨ì›)ì„ ì „ë‹¬í•´ì•¼ í•¨
            if self.advanced_wrappers.get('advanced_emotion'):
                try:
                    # ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì „ë‹¬
                    emotion_result = self.advanced_wrappers['advanced_emotion'](embeddings_cpu, text=text, language='ko')
                    results['advanced_analysis']['emotion'] = {k: v.tolist() if torch.is_tensor(v) else v for k, v in emotion_result.items()}
                    logger.info("   âœ… ê°ì • ë¶„ì„ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"   âŒ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
                    import traceback
                    logger.error(f"   â†’ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
                    # ê°ì • ë¶„ì„ì€ Circuitì˜ í•„ìˆ˜ ì˜ì¡´ì„±ì´ë¯€ë¡œ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                    raise RuntimeError(f"Advanced Emotion Analyzer ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            if self.advanced_wrappers.get('advanced_bentham'):
                try:
                    # ì„ë² ë”©ì„ ì „ë‹¬ (896ì°¨ì›)
                    bentham_result = self.advanced_wrappers['advanced_bentham'](embeddings_cpu)
                    results['advanced_analysis']['bentham'] = {k: v.tolist() if torch.is_tensor(v) else v for k, v in bentham_result.items()}
                    logger.info("   âœ… ë²¤ë‹´ ê³„ì‚° ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"   âŒ ë²¤ë‹´ ê³„ì‚° ì‹¤íŒ¨: {e}")
                    import traceback
                    logger.error(f"   â†’ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
                    # ë²¤ë‹´ ê³„ì‚°ì€ ìœ¤ë¦¬ì  í‰ê°€ì˜ í•µì‹¬ì´ë¯€ë¡œ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                    raise RuntimeError(f"Advanced Bentham Calculator ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # Regret Wrapper í˜¸ì¶œ ì¶”ê°€ (ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ í¬í•¨)
            if self.advanced_wrappers.get('advanced_regret'):
                logger.info("   ğŸ”„ Regret ë¶„ì„ ì¤‘...")
                # ì„ë² ë”©ì„ ì „ë‹¬ (896ì°¨ì›)
                try:
                    # ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ê°€ ìˆìœ¼ë©´ ì „ë‹¬
                    if counterfactual_scenarios:
                        regret_result = self.advanced_wrappers['advanced_regret'](
                            embeddings_cpu, 
                            scenarios=counterfactual_scenarios,
                            text=text
                        )
                        logger.info(f"      - {len(counterfactual_scenarios)}ê°œ ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ì™€ í•¨ê»˜ ë¶„ì„")
                    else:
                        regret_result = self.advanced_wrappers['advanced_regret'](embeddings_cpu)
                    
                    results['advanced_analysis']['regret'] = {k: v.tolist() if torch.is_tensor(v) else v for k, v in regret_result.items()}
                    
                    # ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ì €ì¥
                    if counterfactual_scenarios:
                        results['counterfactual_analysis'] = {
                            'initial_scenarios': counterfactual_scenarios,
                            'regret_based_evaluation': regret_result.get('counterfactual_evaluation', {})
                        }
                    
                    logger.info("   âœ… Regret ë¶„ì„ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"   âŒ Regret ë¶„ì„ ì‹¤íŒ¨: {e}")
                    import traceback
                    logger.error(f"   â†’ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
                    # Regret ë¶„ì„ì€ ë°˜ì‚¬ì‹¤ì  ì¶”ë¡ ì˜ í•µì‹¬ì´ë¯€ë¡œ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                    raise RuntimeError(f"Advanced Regret Analyzer ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # SURD Wrapper í˜¸ì¶œ ì¶”ê°€
            if self.advanced_wrappers.get('advanced_surd'):
                logger.info("   ğŸ”„ SURD ë¶„ì„ ì¤‘...")
                # ì„ë² ë”©ì„ ì „ë‹¬ (896ì°¨ì›)
                try:
                    surd_result = self.advanced_wrappers['advanced_surd'](embeddings_cpu)
                    results['advanced_analysis']['surd'] = {k: v.tolist() if torch.is_tensor(v) else v for k, v in surd_result.items()}
                    logger.info("   âœ… SURD ë¶„ì„ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"   âŒ SURD ë¶„ì„ ì‹¤íŒ¨: {e}")
                    import traceback
                    logger.error(f"   â†’ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
                    # SURD ë¶„ì„ì€ ë¶ˆí™•ì‹¤ì„± í‰ê°€ì˜ í•µì‹¬ì´ë¯€ë¡œ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                    raise RuntimeError(f"Advanced SURD Analyzer ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # Three View Analysis Resultë¥¼ dictë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
            def three_view_to_dict(obj):
                """ThreeViewAnalysisResult ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜"""
                from dataclasses import fields, is_dataclass
                from enum import Enum
                
                if is_dataclass(obj):
                    # dataclassë¥¼ dictë¡œ ë³€í™˜ (ì¬ê·€ì ìœ¼ë¡œ)
                    result = {}
                    for field in fields(obj):
                        field_value = getattr(obj, field.name)
                        result[field.name] = three_view_to_dict(field_value)
                    return result
                elif isinstance(obj, Enum):
                    return obj.value
                elif isinstance(obj, dict):
                    return {k: three_view_to_dict(v) for k, v in obj.items()}
                elif isinstance(obj, (list, tuple)):
                    return [three_view_to_dict(item) for item in obj]
                else:
                    return obj
            
            # 3.4. Three View System ë¶„ì„ (ì¶”ê°€)
            three_view_results = None
            try:
                from three_view_scenario_system import ThreeViewScenarioSystem
                logger.info("\nğŸ”® Three View Scenario System ë¶„ì„...")
                
                three_view_system = ThreeViewScenarioSystem()
                
                # ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ë‚˜ í…ìŠ¤íŠ¸ë¡œ 3ë·° ë¶„ì„ ìˆ˜í–‰
                if counterfactual_scenarios:
                    # ê° ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ 3ë·° ë¶„ì„
                    logger.info(f"   ğŸ“Š {len(counterfactual_scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ 3ë·° ë¶„ì„ ì¤‘...")
                    scenario_analyses = []
                    for scenario in counterfactual_scenarios:
                        scenario_text = f"{scenario.get('action', '')} - {scenario.get('outcome', '')}"
                        analysis = await three_view_system.analyze_three_view_scenarios({
                            'text': scenario_text,
                            'scenario_type': scenario.get('type', 'unknown')
                        })
                        scenario_analyses.append({
                            'scenario_type': scenario.get('type'),
                            'three_views': three_view_to_dict(analysis)
                        })
                    three_view_results = scenario_analyses
                    logger.info("   âœ… ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ 3ë·° ë¶„ì„ ì™„ë£Œ")
                else:
                    # ì›ë³¸ í…ìŠ¤íŠ¸ì— ëŒ€í•œ 3ë·° ë¶„ì„
                    logger.info("   ğŸ“Š ì›ë³¸ í…ìŠ¤íŠ¸ì— ëŒ€í•œ 3ë·° ë¶„ì„ ì¤‘...")
                    analysis_result = await three_view_system.analyze_three_view_scenarios({
                        'text': text,
                        'mode': 'direct_analysis'
                    })
                    three_view_results = three_view_to_dict(analysis_result)
                    logger.info("   âœ… ì›ë³¸ í…ìŠ¤íŠ¸ 3ë·° ë¶„ì„ ì™„ë£Œ")
                
                # ê²°ê³¼ ì €ì¥
                if three_view_results:
                    results['three_view_analysis'] = three_view_results
                    logger.info(f"   ğŸ’¡ 3ë·° ì‹œìŠ¤í…œ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
                    
            except ImportError as e:
                logger.warning(f"   âš ï¸ Three View System ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
            except Exception as e:
                logger.error(f"   âŒ Three View System ë¶„ì„ ì‹¤íŒ¨: {e}")
                # 3ë·° ì‹œìŠ¤í…œ ì‹¤íŒ¨ëŠ” ì „ì²´ ì‹¤íŒ¨ê°€ ì•„ë‹ˆë¯€ë¡œ ê³„ì† ì§„í–‰
            
            # 3.5. Emotion-Ethics-Regret Circuit ì‹¤í–‰ (ì¶”ê°€)
            circuit_result = None
            if self.emotion_hierarchy_processor and results.get('advanced_analysis', {}).get('emotion'):
                logger.info("\nğŸ­ Emotion-Ethics-Regret Circuit ì²˜ë¦¬...")
                try:
                    from emotion_ethics_regret_circuit import CircuitDecisionContext
                    from data_models import EmotionData, EmotionState, EmotionIntensity
                    
                    # ê°ì • ë°ì´í„° ì¤€ë¹„
                    emotion_analysis = results['advanced_analysis']['emotion']
                    emotions_array = emotion_analysis.get('emotions', [[0,0,0,0,0,0,0.5]])[0]
                    
                    # ì£¼ìš” ê°ì • ì‹ë³„
                    emotion_labels = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'neutral']
                    max_idx = np.argmax(emotions_array)
                    primary_emotion = EmotionState(max_idx if max_idx < 7 else 6)
                    
                    # EmotionData ìƒì„±
                    emotion_data = EmotionData(
                        primary_emotion=primary_emotion,
                        intensity=EmotionIntensity.MODERATE,
                        arousal=float(np.mean(emotions_array[:6])),  # ì¤‘ë¦½ ì œì™¸ í‰ê· 
                        valence=float(emotions_array[0] - emotions_array[1]),  # ê¸°ì¨ - ìŠ¬í””
                        dominance=float(emotions_array[2]),  # ë¶„ë…¸ë¥¼ dominanceë¡œ
                        confidence=float(np.max(emotions_array)),
                        language='ko'
                    )
                    
                    # ì´í•´ê´€ê³„ì ì¶”ì¶œ
                    stakeholders = []
                    if "ì¹œêµ¬" in text:
                        stakeholders.append("ì¹œêµ¬")
                    if "ê°€ì¡±" in text:
                        stakeholders.append("ê°€ì¡±")
                    if "ë™ë£Œ" in text or "íšŒì‚¬" in text:
                        stakeholders.append("ë™ë£Œ")
                    if not stakeholders:
                        stakeholders = ["íƒ€ì¸", "ì‚¬íšŒ"]
                    
                    # Circuit ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                    circuit_context = CircuitDecisionContext(
                        scenario_text=text,
                        proposed_action="ìƒí™© ë¶„ì„ ë° ìœ¤ë¦¬ì  í‰ê°€",
                        stakeholders=stakeholders,
                        social_context={
                            'impact_scope': 'personal' if len(stakeholders) < 3 else 'community',
                            'keywords': text.split()[:5],
                            'urgency': 0.5
                        },
                        temporal_urgency=0.5,
                        self_emotion=emotion_data
                    )
                    
                    # Circuit ì‹¤í–‰ (ì§§ì€ íƒ€ì„ì•„ì›ƒ)
                    import asyncio
                    circuit_result = await asyncio.wait_for(
                        self.emotion_hierarchy_processor.process_ethical_decision(circuit_context),
                        timeout=3.0  # 3ì´ˆ íƒ€ì„ì•„ì›ƒ
                    )
                    
                    if circuit_result:
                        # Circuit ê²°ê³¼ë¥¼ resultsì— ì¶”ê°€
                        results['circuit_analysis'] = {
                            'integrated_emotion': {
                                'primary': circuit_result.integrated_emotion.primary_emotion.value if hasattr(circuit_result, 'integrated_emotion') else None,
                                'intensity': circuit_result.integrated_emotion.intensity.value if hasattr(circuit_result, 'integrated_emotion') else None,
                                'confidence': circuit_result.integrated_emotion.confidence if hasattr(circuit_result, 'integrated_emotion') else 0
                            } if hasattr(circuit_result, 'integrated_emotion') else {},
                            'ethical_values': circuit_result.ethical_values if hasattr(circuit_result, 'ethical_values') else {},
                            'predicted_regret': circuit_result.predicted_regret if hasattr(circuit_result, 'predicted_regret') else {},
                            'regret_metrics': circuit_result.regret_metrics if hasattr(circuit_result, 'regret_metrics') else {},
                            'reasoning_trace': circuit_result.reasoning_trace if hasattr(circuit_result, 'reasoning_trace') else []
                        }
                        logger.info(f"   âœ… Circuit ì²˜ë¦¬ ì™„ë£Œ (ì‹ ë¢°ë„: {circuit_result.confidence if hasattr(circuit_result, 'confidence') else 0:.2f})")
                        
                        # Circuit ê²°ê³¼ ë¡œê¹…
                        if hasattr(circuit_result, 'integrated_emotion'):
                            logger.info(f"      - í†µí•© ê°ì •: {circuit_result.integrated_emotion.primary_emotion.name}")
                        if hasattr(circuit_result, 'predicted_regret'):
                            logger.info(f"      - ì˜ˆì¸¡ í›„íšŒ: {circuit_result.predicted_regret}")
                        if hasattr(circuit_result, 'ethical_values'):
                            logger.info(f"      - ìœ¤ë¦¬ì  ê°€ì¹˜: {circuit_result.ethical_values}")
                    else:
                        logger.warning("   âš ï¸ Circuit ê²°ê³¼ ì—†ìŒ")
                        
                except asyncio.TimeoutError:
                    logger.warning("   â±ï¸ Circuit ì²˜ë¦¬ íƒ€ì„ì•„ì›ƒ (3ì´ˆ ì´ˆê³¼)")
                except ImportError as e:
                    logger.warning(f"   âš ï¸ Circuit ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
                except Exception as e:
                    logger.error(f"   âŒ Circuit ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    # Circuit ì‹¤íŒ¨ëŠ” ì „ì²´ ì‹¤íŒ¨ê°€ ì•„ë‹ˆë¯€ë¡œ ê³„ì† ì§„í–‰
            else:
                # ê°ì • ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ì„œ Circuit ì‹¤í–‰ ë¶ˆê°€
                if self.emotion_hierarchy_processor:
                    if not results.get('advanced_analysis'):
                        logger.error("   âŒ Circuit ì‹¤í–‰ ë¶ˆê°€: Advanced Analysis ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                        logger.error("      â†’ Advanced Emotion Analyzerê°€ ì‹¤íŒ¨í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤")
                    elif not results.get('advanced_analysis', {}).get('emotion'):
                        logger.error("   âŒ Circuit ì‹¤í–‰ ë¶ˆê°€: ê°ì • ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                        logger.error("      â†’ emotion í•„ë“œê°€ ë¹„ì–´ìˆê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
                        logger.error(f"      â†’ Advanced Analysis í‚¤: {list(results.get('advanced_analysis', {}).keys())}")
                    # Circuitì€ í•„ìˆ˜ êµ¬ì„±ìš”ì†Œì´ë¯€ë¡œ ì‹¤í–‰ ë¶ˆê°€ ì‹œ ì‹œìŠ¤í…œ ì¤‘ë‹¨
                    raise RuntimeError("Circuit ì‹¤í–‰ ë¶ˆê°€: ê°ì • ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŒ. Advanced Emotion Analyzer ì‹¤íŒ¨ë¡œ ì¸í•œ ì—°ì‡„ ì‹¤íŒ¨")
                else:
                    logger.info("   â„¹ï¸ Circuit í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            
            # 4. Claude API ë³´ê°•
            if self.llm_engine:
                logger.info("\nğŸŒ Claude API ë³´ê°•...")
                from llm_module.advanced_llm_engine import LLMRequest, TaskComplexity
                
                # Red Heart ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                analysis_summary = []
                
                # UnifiedModel ê²°ê³¼ í¬í•¨
                if results.get('unified_outputs'):
                    unified_outputs = results['unified_outputs']
                    if 'emotion' in unified_outputs:
                        analysis_summary.append(f"ê°ì • ë¶„ì„: {unified_outputs['emotion']}")
                    if 'bentham' in unified_outputs:
                        analysis_summary.append(f"ê³µë¦¬ì£¼ì˜ ì ìˆ˜: {unified_outputs['bentham']}")
                    if 'regret' in unified_outputs:
                        analysis_summary.append(f"í›„íšŒ ì˜ˆì¸¡: {unified_outputs['regret']}")
                    if 'surd' in unified_outputs:
                        analysis_summary.append(f"SURD ë©”íŠ¸ë¦­: {unified_outputs['surd']}")
                
                # Advanced Wrapper ê²°ê³¼ í¬í•¨ (ëª¨ë“  ë¶„ì„ì´ ì„±ê³µí•´ì•¼ í•¨)
                if not results.get('advanced_analysis'):
                    raise RuntimeError("Advanced Analysis ê²°ê³¼ê°€ ì—†ìŒ: ì‹œìŠ¤í…œ ë¬´ê²°ì„± ì˜¤ë¥˜")
                
                adv = results['advanced_analysis']
                required_analyses = ['emotion', 'bentham', 'regret', 'surd']
                missing_analyses = [a for a in required_analyses if a not in adv or adv[a] is None]
                
                if missing_analyses:
                    raise RuntimeError(f"í•„ìˆ˜ ë¶„ì„ ëˆ„ë½: {', '.join(missing_analyses)}. ì‹œìŠ¤í…œ ë¬´ê²°ì„± ì˜¤ë¥˜")
                
                analysis_summary.append(f"ê³ ê¸‰ ê°ì • ë¶„ì„: {adv['emotion']}")
                analysis_summary.append(f"ê³ ê¸‰ ë²¤ë‹´ ë¶„ì„: {adv['bentham']}")
                analysis_summary.append(f"ê³ ê¸‰ í›„íšŒ ë¶„ì„: {adv['regret']}")
                analysis_summary.append(f"ê³ ê¸‰ SURD ë¶„ì„: {adv['surd']}")
                
                # Circuit ë¶„ì„ ê²°ê³¼ í¬í•¨
                if results.get('circuit_analysis'):
                    circuit = results['circuit_analysis']
                    if 'integrated_emotion' in circuit and circuit['integrated_emotion']:
                        analysis_summary.append(f"Circuit í†µí•© ê°ì •: {circuit['integrated_emotion']}")
                    if 'ethical_values' in circuit and circuit['ethical_values']:
                        analysis_summary.append(f"Circuit ìœ¤ë¦¬ ê°€ì¹˜: {circuit['ethical_values']}")
                    if 'predicted_regret' in circuit and circuit['predicted_regret']:
                        analysis_summary.append(f"Circuit ì˜ˆì¸¡ í›„íšŒ: {circuit['predicted_regret']}")
                
                # ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ê²°ê³¼ í¬í•¨
                if results.get('counterfactual_analysis'):
                    cf_analysis = results['counterfactual_analysis']
                    if 'initial_scenarios' in cf_analysis:
                        scenarios_text = []
                        for scenario in cf_analysis['initial_scenarios']:
                            scenarios_text.append(f"- {scenario.get('type', '')}: {scenario.get('action', '')[:50]}...")
                        analysis_summary.append(f"ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤: {chr(10).join(scenarios_text)}")
                
                # 3ë·° ì‹œìŠ¤í…œ ë¶„ì„ ê²°ê³¼ í¬í•¨
                if results.get('three_view_analysis'):
                    analysis_summary.append(f"3ë·° ì‹œìŠ¤í…œ ë¶„ì„: ë‚™ê´€ì /ì¤‘ë¦½ì /ë¹„ê´€ì  ê´€ì  ë¶„ì„ ì™„ë£Œ")
                
                # í†µí•©ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
                integrated_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì™€ Red Heart AIì˜ ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ìœ¤ë¦¬ì  í‰ê°€ë¥¼ ì œê³µí•˜ì„¸ìš”.

í…ìŠ¤íŠ¸: {text}

Red Heart AI ë‹¤ì¸µ ë¶„ì„ ê²°ê³¼:
{chr(10).join(analysis_summary) if analysis_summary else 'ë¶„ì„ ê²°ê³¼ ì—†ìŒ'}

ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ë¶„ì„:
{len(counterfactual_scenarios)}ê°œì˜ ëŒ€ì•ˆì  í–‰ë™ ì‹œë‚˜ë¦¬ì˜¤ê°€ ìƒì„±ë˜ê³  í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.

ìœ„ ëª¨ë“  ë¶„ì„ì„ í†µí•©í•˜ì—¬ ë‹¤ìŒì„ í¬í•¨í•œ ìµœì¢… ìœ¤ë¦¬ì  í‰ê°€ë¥¼ ì œê³µí•˜ì„¸ìš”:
1. ì£¼ìš” ìœ¤ë¦¬ì  ìŸì ê³¼ ë”œë ˆë§ˆ
2. ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ ì ì¬ì  ê²°ê³¼ì™€ ì˜í–¥
3. ìµœì ì˜ í–‰ë™ ë°©í–¥ ê¶Œì¥ (3ë·° ë¶„ì„ ê¸°ë°˜)
4. ê³ ë ¤í•´ì•¼ í•  ê°€ì¹˜ ì¶©ëŒê³¼ trade-off
5. ë°˜ì‚¬ì‹¤ì  ë¶„ì„ì—ì„œ ë„ì¶œëœ í•µì‹¬ í†µì°°"""
                
                request = LLMRequest(
                    prompt=integrated_prompt,
                    task_type="ethical_analysis",
                    complexity=TaskComplexity.COMPLEX,
                    max_tokens=500,
                    context={'red_heart_analysis': results}  # ì»¨í…ìŠ¤íŠ¸ì— Red Heart ë¶„ì„ ê²°ê³¼ í¬í•¨
                )
                
                llm_response = await self.llm_engine.generate_async(request)
                
                if llm_response and llm_response.success:
                    results['claude_analysis'] = {
                        'text': llm_response.generated_text,
                        'confidence': llm_response.confidence,
                        'processing_time': llm_response.processing_time
                    }
                    logger.info("   âœ… Claude API ë¶„ì„ ì™„ë£Œ")
                else:
                    logger.warning("   âš ï¸ Claude API ì‘ë‹µ ì‹¤íŒ¨")
            
            # 5. ê²°ê³¼ ì €ì¥ (ë§ˆì´í¬ë¡œì´ˆ ë° PID í¬í•¨í•˜ì—¬ ìœ ì¼ì„± ë³´ì¥)
            import os
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ë°€ë¦¬ì´ˆê¹Œì§€ í¬í•¨
            pid = os.getpid()
            output_file = f"claude_inference_result_{timestamp}_pid{pid}.json"
            self.output_file = output_file  # ë‚˜ì¤‘ì— ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
            
            # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” ì»¤ìŠ¤í…€ JSON encoder
            class DateTimeEncoder(json.JSONEncoder):
                def default(self, obj):
                    if isinstance(obj, datetime):
                        return obj.isoformat()
                    return super().default(obj)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, cls=DateTimeEncoder)
            
            logger.info(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
            
            # ìµœì‹  ê²°ê³¼ë¥¼ ê°€ë¦¬í‚¤ëŠ” ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± (ì„ íƒì )
            try:
                latest_link = "claude_inference_result_latest.json"
                if os.path.exists(latest_link) or os.path.islink(latest_link):
                    os.remove(latest_link)
                os.symlink(output_file, latest_link)
                logger.info(f"   ğŸ”— ìµœì‹  ê²°ê³¼ ë§í¬: {latest_link} -> {output_file}")
            except (OSError, NotImplementedError) as e:
                # WSLì´ë‚˜ ê¶Œí•œ ë¬¸ì œë¡œ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
                logger.debug(f"   âš ï¸ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
            
            # 6. ìš”ì•½ ì¶œë ¥
            logger.info("\n" + "=" * 70)
            logger.info("ğŸ“Š ì¶”ë¡  ì™„ë£Œ ìš”ì•½")
            logger.info("=" * 70)
            
            if 'emotion' in results['advanced_analysis']:
                emotions = results['advanced_analysis']['emotion'].get('emotions', [])
                if emotions and len(emotions) > 0 and len(emotions[0]) > 0:
                    # ê°ì • ë ˆì´ë¸” ë§¤í•‘ (7ì°¨ì›)
                    emotion_labels = ['ê¸°ì¨', 'ìŠ¬í””', 'ë¶„ë…¸', 'ë‘ë ¤ì›€', 'ë†€ëŒ', 'í˜ì˜¤', 'ì¤‘ë¦½']
                    emotion_values = emotions[0] if isinstance(emotions[0], list) else emotions
                    
                    # ê°€ì¥ ë†’ì€ ê°’ì˜ ê°ì • ì°¾ê¸°
                    max_idx = np.argmax(emotion_values)
                    max_score = emotion_values[max_idx]
                    primary_emotion = emotion_labels[max_idx]
                    
                    logger.info(f"ğŸ­ ì£¼ìš” ê°ì •: {primary_emotion} (ì ìˆ˜: {max_score:.3f})")
                else:
                    logger.info(f"ğŸ­ ì£¼ìš” ê°ì •: N/A")
            
            if 'bentham' in results['advanced_analysis']:
                bentham_scores = results['advanced_analysis']['bentham'].get('bentham_scores', [])
                if bentham_scores and len(bentham_scores) > 0 and len(bentham_scores[0]) > 0:
                    # ë²¤ë‹´ ì ìˆ˜ë“¤ì˜ í‰ê· ì„ ê³µë¦¬ì£¼ì˜ ì ìˆ˜ë¡œ ì‚¬ìš©
                    scores = bentham_scores[0] if isinstance(bentham_scores[0], list) else bentham_scores
                    utility_score = np.mean(scores)
                    logger.info(f"âš–ï¸ ê³µë¦¬ì£¼ì˜ ì ìˆ˜: {utility_score:.3f}")
                else:
                    logger.info(f"âš–ï¸ ê³µë¦¬ì£¼ì˜ ì ìˆ˜: N/A")
            
            # Circuit ê²°ê³¼ ì¶œë ¥ ì¶”ê°€
            if 'circuit_analysis' in results:
                circuit = results['circuit_analysis']
                if circuit.get('integrated_emotion'):
                    ie = circuit['integrated_emotion']
                    logger.info(f"ğŸ­ Circuit í†µí•© ê°ì •: ì£¼ìš”={ie.get('primary', 'N/A')}, ê°•ë„={ie.get('intensity', 'N/A')}, ì‹ ë¢°ë„={ie.get('confidence', 0):.3f}")
                if circuit.get('predicted_regret'):
                    logger.info(f"ğŸ˜” Circuit ì˜ˆì¸¡ í›„íšŒ: {circuit['predicted_regret']}")
                if circuit.get('ethical_values'):
                    logger.info(f"âš–ï¸ Circuit ìœ¤ë¦¬ ê°€ì¹˜: {circuit['ethical_values']}")
            
            # Regret ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ì¶”ê°€
            if 'regret' in results['advanced_analysis']:
                regret_scores = results['advanced_analysis']['regret']
                if regret_scores:
                    logger.info(f"ğŸ˜” ê³ ê¸‰ í›„íšŒ ë¶„ì„: {regret_scores}")
            
            if 'claude_analysis' in results:
                claude_text = results['claude_analysis']['text'][:200]
                logger.info(f"ğŸ¤– Claude ë¶„ì„:\n{claude_text}...")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}", exc_info=True)
            return {'error': str(e)}
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("\nğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        # ëª¨ë¸ ì •ë¦¬
        if self.unified_model:
            del self.unified_model
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("âœ… ì •ë¦¬ ì™„ë£Œ")


async def main(args):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    system = ClaudeInferenceSystem(
        epoch=args.epoch,
        debug=args.debug
    )
    
    try:
        # ì´ˆê¸°í™”
        await system.initialize()
        
        # ì¶”ë¡  ì‹¤í–‰
        results = await system.inference(args.text)
        
        # ì •ë¦¬
        await system.cleanup()
        
        logger.info("\nğŸ‰ Claude API ì¶”ë¡  ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Claude API ë…ë¦½ ì¶”ë¡  ì‹œìŠ¤í…œ')
    parser.add_argument('--text', type=str, required=True, help='ì¶”ë¡ í•  í…ìŠ¤íŠ¸')
    parser.add_argument('--epoch', type=int, default=50, help='ì²´í¬í¬ì¸íŠ¸ ì—í­')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
    
    args = parser.parse_args()
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main(args))