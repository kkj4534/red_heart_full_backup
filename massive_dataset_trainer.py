"""
ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ í•™ìŠµ ì‹œìŠ¤í…œ
Massive Dataset Learning System for Red Heart AI

ìš”êµ¬ì‚¬í•­:
- ì „ì²´ ë°ì´í„°ì…‹ (~266MB, 130ê°œ JSON íŒŒì¼) í™œìš©
- ë°ì´í„° 1ê°œë‹¹ 7íšŒ í›„íšŒ ë¶„ì„ + 21íšŒ ë²¤ë‹´ ì¾Œë½ ê³„ì‚°
- ì´ 3ë²ˆ ì„ íšŒ í•™ìŠµ
- 20íšŒ í•™ìŠµë§ˆë‹¤ ë¡œê·¸ ì €ì¥
- 200GB ìŠ¤í† ë¦¬ì§€ ì œí•œ ì¤€ìˆ˜
- Adaptive gradient ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë°ì´í„° ì…”í”Œë§
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import logging
import time
import random
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict
import asyncio
import gc
from collections import defaultdict

# í†µí•© ì‹œìŠ¤í…œ ì„í¬íŠ¸
try:
    from config import DEVICE, ADVANCED_CONFIG, DATA_DIR
    from data_models import EmotionData, EmotionState, EmotionIntensity
    from emotion_ethics_regret_circuit import EmotionEthicsRegretCircuit, CircuitDecisionContext
    from ethics_policy_updater import EthicsPolicyUpdater, EthicsExperience
    from phase_controller import PhaseController, PhaseDecisionContext, Phase
    from xai_feedback_integrator import XAIFeedbackIntegrator, XAIInterpretation
    from fuzzy_emotion_ethics_mapper import FuzzyEmotionEthicsMapper
    from deep_multi_dimensional_ethics_system import DeepMultiDimensionalEthicsSystem, EthicalDilemma, StakeholderPerspective
    from temporal_event_propagation_analyzer import TemporalEventPropagationAnalyzer, TemporalEvent
    from integrated_system_orchestrator import IntegratedSystemOrchestrator, IntegrationContext
    from dynamic_gpu_manager import get_gpu_manager, allocate_gpu_memory, optimize_gpu_for_learning
    from robust_logging_system import get_robust_logger, test_session, add_performance_sample
    
    print("âœ… ëª¨ë“  í†µí•© ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('MassiveDatasetTrainer')

@dataclass
class MassiveTrainingConfig:
    """ëŒ€ê·œëª¨ í•™ìŠµ ì„¤ì •"""
    dataset_path: str = "/mnt/c/large_project/linux_red_heart/processed_datasets"
    regret_iterations_per_data: int = 7  # ë°ì´í„°ë‹¹ í›„íšŒ ê³„ì‚° íšŸìˆ˜
    bentham_calculations_per_regret: int = 3  # í›„íšŒë‹¹ ë²¤ë‹´ ê³„ì‚° íšŸìˆ˜ (7*3=21)
    training_cycles: int = 3  # ì´ ì„ íšŒ íšŸìˆ˜
    log_interval: int = 50  # 50íšŒë§ˆë‹¤ ë¡œê·¸ ì €ì¥ (ì—°ì‚° ì‹œê°„ ìµœì í™”)
    max_storage_gb: float = 200.0  # ìµœëŒ€ ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ (GB)
    batch_size: int = 4
    learning_rate: float = 0.001
    device: str = str(DEVICE)
    use_gpu: bool = ADVANCED_CONFIG.get('enable_gpu', False)
    shuffle_strategy: str = "balanced_hash"  # adaptive gradient ëŒ€ì‘ ì…”í”Œë§

@dataclass
class DatasetMetrics:
    """ë°ì´í„°ì…‹ ë©”íŠ¸ë¦­"""
    total_files: int = 0
    total_scenarios: int = 0
    processed_scenarios: int = 0
    regret_calculations: int = 0
    bentham_calculations: int = 0
    current_cycle: int = 0
    storage_used_gb: float = 0.0

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    cycle_accuracy: float = 0.0
    cycle_loss: float = 0.0
    avg_regret_score: float = 0.0
    avg_bentham_score: float = 0.0
    processing_time_per_scenario: float = 0.0
    gpu_utilization: float = 0.0
    memory_efficiency: float = 0.0

class MassiveDatasetTrainer:
    """ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ í•™ìŠµê¸°"""
    
    def __init__(self, config: MassiveTrainingConfig):
        self.config = config
        self.logger = logger
        self.session_id = f"massive_training_{int(time.time())}"
        
        # ì—…ê·¸ë ˆì´ë“œëœ ì‹œìŠ¤í…œ ì—°ë™
        self.gpu_manager = get_gpu_manager()
        self.robust_logger = get_robust_logger()
        
        # GPU ìµœì í™”
        if config.use_gpu and torch.cuda.is_available():
            optimization_success = optimize_gpu_for_learning()
            gpu_status = self.gpu_manager.get_memory_status()
            self.logger.info(f"ğŸš€ GPU ëŒ€ê·œëª¨ í•™ìŠµ í™˜ê²½ ì´ˆê¸°í™”: {torch.cuda.get_device_name()}")
            self.logger.info(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_status['total_gb']:.1f}GB")
        
        # í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.initialize_integrated_systems()
        
        # ê°ì • ê²½í—˜ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
        self.emotional_experience_memory = {
            'regret_patterns': defaultdict(list),  # í›„íšŒ íŒ¨í„´ë³„ ê°ì • ê²½í—˜
            'ethical_emotions': defaultdict(list),  # ìœ¤ë¦¬ì  ìƒí™©ë³„ ê°ì • ë°˜ì‘
            'decision_outcomes': [],  # ê²°ì •-ê²°ê³¼-ê°ì • ê²½í—˜
            'emotional_learning': []  # ê°ì • ê¸°ë°˜ í•™ìŠµ ë‚´ì—­
        }
        
        # ë°ì´í„°ì…‹ ë©”íŠ¸ë¦­
        self.dataset_metrics = DatasetMetrics()
        self.performance_history = []
        
        # ë¡œê·¸ ê´€ë¦¬
        self.log_dir = Path(f"logs/massive_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # ìŠ¤í† ë¦¬ì§€ ëª¨ë‹ˆí„°ë§
        self.storage_monitor = StorageMonitor(self.config.max_storage_gb)
        
        self.logger.info(f"ğŸ¯ ëŒ€ê·œëª¨ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: {self.session_id}")
    
    def initialize_integrated_systems(self):
        """í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.logger.info("ğŸ—ï¸ í†µí•© ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”...")
        
        self.emotion_circuit = EmotionEthicsRegretCircuit()
        self.policy_updater = EthicsPolicyUpdater()
        self.phase_controller = PhaseController()
        self.xai_integrator = XAIFeedbackIntegrator()
        self.fuzzy_mapper = FuzzyEmotionEthicsMapper()
        self.ethics_system = DeepMultiDimensionalEthicsSystem()
        self.temporal_analyzer = TemporalEventPropagationAnalyzer()
        self.orchestrator = IntegratedSystemOrchestrator()
        
        self.logger.info("âœ… ëª¨ë“  í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def discover_datasets(self) -> List[Dict[str, Any]]:
        """ë°ì´í„°ì…‹ íƒìƒ‰ ë° ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        self.logger.info(f"ğŸ“‚ ë°ì´í„°ì…‹ íƒìƒ‰ ì‹œì‘: {self.config.dataset_path}")
        
        datasets = []
        dataset_folders = [
            "scruples", "academic", "augmented", "classic_literature", 
            "ebs_korean_literature", "ethical_scenarios", "literature"
        ]
        
        # í´ë”ë³„ ë°ì´í„° ìˆ˜ì§‘
        for folder in dataset_folders:
            folder_path = Path(self.config.dataset_path) / folder
            if folder_path.exists():
                json_files = list(folder_path.glob("*.json"))
                for json_file in json_files:
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                        scenario_count = 0
                        if 'scenarios' in data:
                            scenario_count = len(data['scenarios'])
                        elif 'metadata' in data and 'scenario_count' in data['metadata']:
                            scenario_count = data['metadata']['scenario_count']
                        
                        datasets.append({
                            'file_path': str(json_file),
                            'folder': folder,
                            'scenario_count': scenario_count,
                            'file_size': json_file.stat().st_size,
                            'dataset_type': folder
                        })
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {json_file}: {e}")
        
        # ë£¨íŠ¸ ë ˆë²¨ JSON íŒŒì¼ë“¤
        root_files = list(Path(self.config.dataset_path).glob("*.json"))
        for json_file in root_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                scenario_count = 0
                if isinstance(data, list):
                    scenario_count = len(data)
                elif 'scenarios' in data:
                    scenario_count = len(data['scenarios'])
                
                datasets.append({
                    'file_path': str(json_file),
                    'folder': 'root',
                    'scenario_count': scenario_count,
                    'file_size': json_file.stat().st_size,
                    'dataset_type': 'integrated'
                })
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ ë£¨íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {json_file}: {e}")
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.dataset_metrics.total_files = len(datasets)
        self.dataset_metrics.total_scenarios = sum(d['scenario_count'] for d in datasets)
        
        self.logger.info(f"ğŸ“Š íƒìƒ‰ ì™„ë£Œ: {len(datasets)}ê°œ íŒŒì¼, {self.dataset_metrics.total_scenarios}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
        return datasets
    
    def create_balanced_shuffle_order(self, datasets: List[Dict]) -> List[Dict]:
        """Adaptive gradient ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ê· í˜•ì¡íŒ ì…”í”Œë§"""
        self.logger.info("ğŸ”€ ê· í˜•ì¡íŒ ë°ì´í„° ì…”í”Œë§ ìˆ˜í–‰...")
        
        # ë°ì´í„°ì…‹ íƒ€ì…ë³„ ê·¸ë£¹í™”
        type_groups = defaultdict(list)
        for dataset in datasets:
            type_groups[dataset['dataset_type']].append(dataset)
        
        # ê° ê·¸ë£¹ ë‚´ì—ì„œ ì…”í”Œë§
        shuffled_datasets = []
        for type_name, type_datasets in type_groups.items():
            # í•´ì‹œ ê¸°ë°˜ ì•ˆì •ì  ì…”í”Œë§
            type_datasets_with_hash = []
            for dataset in type_datasets:
                hash_value = hashlib.md5(dataset['file_path'].encode()).hexdigest()
                type_datasets_with_hash.append((hash_value, dataset))
            
            # í•´ì‹œê°’ìœ¼ë¡œ ì •ë ¬ (ì˜ì‚¬ ëœë¤ì´ì§€ë§Œ ì¬í˜„ ê°€ëŠ¥)
            type_datasets_with_hash.sort(key=lambda x: x[0])
            type_datasets = [item[1] for item in type_datasets_with_hash]
            
            self.logger.info(f"  ğŸ“¦ {type_name}: {len(type_datasets)}ê°œ íŒŒì¼ ì…”í”Œë§")
            shuffled_datasets.extend(type_datasets)
        
        # ì „ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ íƒ€ì…ë³„ë¡œ ê· ë“± ë¶„ì‚°
        final_order = []
        type_iterators = {k: iter(v) for k, v in type_groups.items()}
        
        while type_iterators:
            for type_name in list(type_iterators.keys()):
                try:
                    dataset = next(type_iterators[type_name])
                    final_order.append(dataset)
                except StopIteration:
                    del type_iterators[type_name]
        
        self.logger.info(f"âœ… ê· í˜•ì¡íŒ ì…”í”Œë§ ì™„ë£Œ: {len(final_order)}ê°œ íŒŒì¼")
        return final_order
    
    async def process_single_scenario(self, scenario_data: Dict, 
                                    file_info: Dict) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬ (7íšŒ í›„íšŒ + 21íšŒ ë²¤ë‹´ ê³„ì‚°)"""
        
        # ê°ì • ê²½í—˜ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ - ì´ì „ ê²½í—˜ì„ í•™ìŠµì— í™œìš©
        await self._update_emotional_experience_context(scenario_data)
        
        results = {
            'regret_scores': [],
            'bentham_scores': [],
            'integrated_predictions': [],
            'processing_times': []
        }
        
        # ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬
        try:
            processed_scenario = self.preprocess_scenario(scenario_data)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì‹œë‚˜ë¦¬ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return results
        
        # 7íšŒ í›„íšŒ ê³„ì‚° ë£¨í”„
        for regret_iteration in range(self.config.regret_iterations_per_data):
            iteration_start = time.time()
            
            try:
                # í›„íšŒ ë¶„ì„ ìˆ˜í–‰
                regret_result = await self.emotion_circuit.process_ethical_decision(
                    processed_scenario['circuit_context']
                )
                results['regret_scores'].append(regret_result.predicted_regret)
                
                # ê° í›„íšŒë‹¹ 3íšŒ ë²¤ë‹´ ê³„ì‚°
                bentham_scores = []
                for bentham_iteration in range(self.config.bentham_calculations_per_regret):
                    # ë‹¤ì°¨ì› ìœ¤ë¦¬ ë¶„ì„ì„ í†µí•œ ë²¤ë‹´ ê³„ì‚°
                    ethics_result = self.ethics_system.comprehensive_ethical_analysis(
                        processed_scenario['ethical_dilemma']
                    )
                    bentham_score = self.calculate_bentham_pleasure(
                        ethics_result, processed_scenario
                    )
                    bentham_scores.append(bentham_score)
                
                results['bentham_scores'].append(bentham_scores)
                
                # í†µí•© ì˜ˆì¸¡ ìˆ˜í–‰
                integrated_prediction = await self.run_integrated_prediction(
                    processed_scenario
                )
                results['integrated_predictions'].append(integrated_prediction)
                
                # ê°ì • ê²½í—˜ ë©”ëª¨ë¦¬ì— ì €ì¥
                await self._store_emotional_experience(
                    scenario_data, regret_result, bentham_scores, integrated_prediction
                )
                
                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                self.dataset_metrics.regret_calculations += 1
                self.dataset_metrics.bentham_calculations += self.config.bentham_calculations_per_regret
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ í›„íšŒ/ë²¤ë‹´ ê³„ì‚° ì‹¤íŒ¨ (ë°˜ë³µ {regret_iteration}): {e}")
                continue
            
            iteration_time = time.time() - iteration_start
            results['processing_times'].append(iteration_time)
        
        return results
    
    def preprocess_scenario(self, scenario_data: Dict) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬"""
        # ê¸°ë³¸ ê°ì • ë°ì´í„° ì„¤ì •
        emotion_data = EmotionData(
            primary_emotion=EmotionState.NEUTRAL,
            intensity=EmotionIntensity.MODERATE,
            valence=0.5,
            arousal=0.5,
            confidence=0.7,
            dominance=0.5,
            language='ko',
            processing_method='massive_training'
        )
        
        # ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ê°ì • ì •ë³´ ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
        if 'emotions' in scenario_data:
            emotions = scenario_data['emotions']
            if 'primary_emotion' in emotions:
                try:
                    emotion_data.primary_emotion = EmotionState[emotions['primary_emotion']]
                except KeyError:
                    pass
            if 'intensity' in emotions:
                emotion_data.intensity = EmotionIntensity[emotions.get('intensity', 'MODERATE')]
            if 'valence' in emotions:
                emotion_data.valence = emotions['valence']
            if 'arousal' in emotions:
                emotion_data.arousal = emotions['arousal']
        
        # ì´í•´ê´€ê³„ì êµ¬ì„±
        stakeholders = []
        if 'context' in scenario_data and 'people_involved' in scenario_data['context']:
            for person in scenario_data['context']['people_involved']:
                stakeholder = StakeholderPerspective(
                    stakeholder_id=person.replace(' ', '_'),
                    name=person,
                    role='participant',
                    power_level=0.5,
                    vulnerability=0.5
                )
                stakeholders.append(stakeholder)
        
        # ìœ¤ë¦¬ì  ë”œë ˆë§ˆ êµ¬ì„±
        ethical_dilemma = EthicalDilemma(
            dilemma_id=scenario_data.get('id', f"scenario_{int(time.time())}"),
            scenario=scenario_data.get('description', ''),
            context=scenario_data.get('context', {}).get('location', ''),
            stakeholders=stakeholders,
            available_options=[
                opt.get('text', opt) if isinstance(opt, dict) else str(opt) 
                for opt in scenario_data.get('options', [])
            ]
        )
        
        # íšŒë¡œ ê²°ì • ì»¨í…ìŠ¤íŠ¸
        circuit_context = CircuitDecisionContext(
            scenario_text=ethical_dilemma.scenario,
            proposed_action="ìœ¤ë¦¬ì  ì˜ì‚¬ê²°ì •",
            self_emotion=emotion_data,
            stakeholders=[s.name for s in stakeholders]
        )
        
        return {
            'emotion_data': emotion_data,
            'ethical_dilemma': ethical_dilemma,
            'circuit_context': circuit_context,
            'original_scenario': scenario_data
        }
    
    def calculate_bentham_pleasure(self, ethics_result, processed_scenario: Dict) -> float:
        """ë²¤ë‹´ ì¾Œë½ ê³„ì‚°"""
        # ë‹¤ì°¨ì› ìœ¤ë¦¬ ê²°ê³¼ë¥¼ ë²¤ë‹´ ì¾Œë½ ì ìˆ˜ë¡œ ë³€í™˜
        school_scores = list(ethics_result.school_reasonings.values())
        if school_scores:
            base_score = np.mean([score.confidence for score in school_scores])
        else:
            base_score = 0.5
        
        # ê°ì • ë°ì´í„° ë°˜ì˜
        emotion_influence = (
            processed_scenario['emotion_data'].valence * 0.4 +
            processed_scenario['emotion_data'].arousal * 0.3 +
            processed_scenario['emotion_data'].confidence * 0.3
        )
        
        bentham_score = base_score * 0.7 + emotion_influence * 0.3
        return float(bentham_score)
    
    async def run_integrated_prediction(self, processed_scenario: Dict) -> Dict[str, Any]:
        """í†µí•© ì‹œìŠ¤í…œ ì˜ˆì¸¡ ì‹¤í–‰"""
        try:
            # í¼ì§€ ê°ì •-ìœ¤ë¦¬ ë§¤í•‘
            fuzzy_result = self.fuzzy_mapper.map_emotion_to_ethics(
                processed_scenario['emotion_data']
            )
            
            # ì‹œê³„ì—´ ì´ë²¤íŠ¸ ë“±ë¡
            temporal_event = TemporalEvent(
                event_id=f"massive_event_{int(time.time())}",
                timestamp=time.time(),
                event_type="massive_training",
                description=processed_scenario['ethical_dilemma'].scenario,
                emotion_state=processed_scenario['emotion_data']
            )
            self.temporal_analyzer.register_event(temporal_event)
            
            return {
                'fuzzy_ethics_weights': fuzzy_result.ethics_weights,
                'temporal_event_id': temporal_event.event_id,
                'prediction_confidence': 0.8
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í†µí•© ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    async def train_on_datasets(self, datasets: List[Dict]) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµ ì‹¤í–‰"""
        self.logger.info(f"ğŸš€ ëŒ€ê·œëª¨ í•™ìŠµ ì‹œì‘: {len(datasets)}ê°œ íŒŒì¼, {self.config.training_cycles}ë²ˆ ì„ íšŒ")
        
        total_start_time = time.time()
        cycle_results = []
        
        # 3ë²ˆ ì„ íšŒ í•™ìŠµ
        for cycle in range(self.config.training_cycles):
            cycle_start_time = time.time()
            self.dataset_metrics.current_cycle = cycle + 1
            
            self.logger.info(f"ğŸ”„ í•™ìŠµ ì„ íšŒ {cycle + 1}/{self.config.training_cycles} ì‹œì‘")
            
            # ê° ì„ íšŒë§ˆë‹¤ ë°ì´í„° ì¬ì…”í”Œë§
            shuffled_datasets = self.create_balanced_shuffle_order(datasets)
            
            cycle_performance = PerformanceMetrics()
            scenarios_processed_in_cycle = 0
            log_counter = 0
            
            # íŒŒì¼ë³„ ì²˜ë¦¬
            for file_idx, dataset_info in enumerate(shuffled_datasets):
                file_start_time = time.time()
                
                # ìŠ¤í† ë¦¬ì§€ ì²´í¬
                if not self.storage_monitor.check_storage_limit():
                    self.logger.warning("âš ï¸ ìŠ¤í† ë¦¬ì§€ í•œê³„ ë„ë‹¬, í•™ìŠµ ì¤‘ë‹¨")
                    break
                
                # íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬
                try:
                    scenarios = self.load_scenarios_from_file(dataset_info['file_path'])
                    
                    for scenario_idx, scenario in enumerate(scenarios):
                        scenario_start_time = time.time()
                        
                        # ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬ (7íšŒ í›„íšŒ + 21íšŒ ë²¤ë‹´)
                        scenario_results = await self.process_single_scenario(
                            scenario, dataset_info
                        )
                        
                        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                        scenario_time = time.time() - scenario_start_time
                        cycle_performance.processing_time_per_scenario += scenario_time
                        
                        # ì ìˆ˜ ê³„ì‚°
                        if scenario_results['regret_scores']:
                            cycle_performance.avg_regret_score += np.mean([
                                list(scores.values())[0] if isinstance(scores, dict) else 0.5
                                for scores in scenario_results['regret_scores']
                            ])
                        
                        if scenario_results['bentham_scores']:
                            all_bentham_scores = []
                            for bentham_list in scenario_results['bentham_scores']:
                                all_bentham_scores.extend(bentham_list)
                            if all_bentham_scores:
                                cycle_performance.avg_bentham_score += np.mean(all_bentham_scores)
                        
                        scenarios_processed_in_cycle += 1
                        self.dataset_metrics.processed_scenarios += 1
                        
                        # 20íšŒë§ˆë‹¤ ë¡œê·¸ ì €ì¥
                        log_counter += 1
                        if log_counter % self.config.log_interval == 0:
                            await self.save_intermediate_log(cycle, scenarios_processed_in_cycle, cycle_performance)
                        
                        # ê°•í™”ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬ - ëˆ„ìˆ˜ ë°©ì§€
                        if self.config.use_gpu:
                            if scenario_idx % 5 == 0:  # ë” ìì£¼ ì •ë¦¬ (10 â†’ 5)
                                torch.cuda.empty_cache()
                                gc.collect()
                                
                            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
                            if scenario_idx % 20 == 0:
                                memory_allocated = torch.cuda.memory_allocated() / 1e9
                                memory_reserved = torch.cuda.memory_reserved() / 1e9
                                
                                if memory_reserved > 7.0:  # 7GB ì´ìƒ ì ìœ  ì‹œ ê²½ê³ 
                                    self.logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ê³¼ì ìœ : {memory_reserved:.1f}GB")
                                    # ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
                                    torch.cuda.empty_cache()
                                    torch.cuda.synchronize()
                                    gc.collect()
                
                except Exception as e:
                    self.logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {dataset_info['file_path']}: {e}")
                    continue
                
                file_time = time.time() - file_start_time
                self.logger.info(f"ğŸ“„ íŒŒì¼ ì™„ë£Œ ({file_idx+1}/{len(shuffled_datasets)}): "
                               f"{dataset_info['scenario_count']}ê°œ ì‹œë‚˜ë¦¬ì˜¤, {file_time:.2f}ì´ˆ")
            
            # ì„ íšŒ ì™„ë£Œ ì²˜ë¦¬
            cycle_time = time.time() - cycle_start_time
            
            # í‰ê·  ê³„ì‚°
            if scenarios_processed_in_cycle > 0:
                cycle_performance.processing_time_per_scenario /= scenarios_processed_in_cycle
                cycle_performance.avg_regret_score /= scenarios_processed_in_cycle
                cycle_performance.avg_bentham_score /= scenarios_processed_in_cycle
            
            cycle_performance.cycle_accuracy = 0.75  # ì„ì‹œ ì •í™•ë„
            cycle_performance.cycle_loss = 0.25      # ì„ì‹œ ì†ì‹¤
            
            cycle_results.append({
                'cycle': cycle + 1,
                'scenarios_processed': scenarios_processed_in_cycle,
                'cycle_time': cycle_time,
                'performance': asdict(cycle_performance)
            })
            
            self.logger.info(f"âœ… ì„ íšŒ {cycle + 1} ì™„ë£Œ: {scenarios_processed_in_cycle}ê°œ ì‹œë‚˜ë¦¬ì˜¤, "
                           f"{cycle_time:.2f}ì´ˆ, í‰ê·  í›„íšŒ: {cycle_performance.avg_regret_score:.4f}")
        
        # ì „ì²´ í•™ìŠµ ì™„ë£Œ
        total_time = time.time() - total_start_time
        
        final_results = {
            'session_id': self.session_id,
            'total_training_time': total_time,
            'cycles_completed': len(cycle_results),
            'total_scenarios_processed': self.dataset_metrics.processed_scenarios,
            'total_regret_calculations': self.dataset_metrics.regret_calculations,
            'total_bentham_calculations': self.dataset_metrics.bentham_calculations,
            'cycle_results': cycle_results,
            'final_metrics': asdict(self.dataset_metrics),
            'storage_used_gb': self.storage_monitor.get_current_usage()
        }
        
        return final_results
    
    async def _update_emotional_experience_context(self, scenario_data: Dict):
        """ê°ì • ê²½í—˜ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ - ê³¼ê±° ê²½í—˜ì„ í˜„ì¬ íŒë‹¨ì— ë°˜ì˜"""
        
        # í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤ì˜ ìœ¤ë¦¬ì  íŠ¹ì„± ë¶„ì„
        scenario_type = self._classify_ethical_scenario(scenario_data)
        
        # ê³¼ê±° ìœ ì‚¬í•œ ìœ¤ë¦¬ì  ìƒí™©ì—ì„œì˜ ê°ì • ê²½í—˜ ì¡°íšŒ
        similar_experiences = self.emotional_experience_memory['ethical_emotions'][scenario_type]
        
        # ê°ì • ì‹œìŠ¤í…œì— ê³¼ê±° ê²½í—˜ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
        if similar_experiences:
            # ê³¼ê±° ê²½í—˜ì—ì„œ í•™ìŠµëœ ê°ì • íŒ¨í„´ì„ í˜„ì¬ ë¶„ì„ì— ë°˜ì˜
            avg_past_regret = np.mean([exp['regret_intensity'] for exp in similar_experiences[-5:]])  # ìµœê·¼ 5ê°œ
            avg_past_confidence = np.mean([exp['confidence'] for exp in similar_experiences[-5:]])
            
            # í†µí•© ì‹œìŠ¤í…œì— ê²½í—˜ ì»¨í…ìŠ¤íŠ¸ ì„¤ì • (ì§ì ‘ ì†ì„± ì„¤ì •)
            self.emotion_circuit.experience_context = {
                'similar_scenario_count': len(similar_experiences),
                'average_past_regret': float(avg_past_regret),
                'average_past_confidence': float(avg_past_confidence),
                'learning_progression': self._calculate_emotional_learning_curve(similar_experiences)
            }
            
            self.logger.debug(f"ğŸ“š ê°ì • ê²½í—˜ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ: {scenario_type} - {len(similar_experiences)}ê°œ ê³¼ê±° ê²½í—˜")
    
    def _classify_ethical_scenario(self, scenario_data: Dict) -> str:
        """ìœ¤ë¦¬ì  ì‹œë‚˜ë¦¬ì˜¤ ë¶„ë¥˜"""
        # ì‹œë‚˜ë¦¬ì˜¤ì˜ ìœ¤ë¦¬ì  íŠ¹ì„±ì— ë”°ë¼ ë¶„ë¥˜
        description = scenario_data.get('description', '').lower()
        
        if any(word in description for word in ['ë°°ì‹ ', 'betrayal', 'ê±°ì§“ë§', 'lie']):
            return 'trust_violation'
        elif any(word in description for word in ['ë„ì›€', 'help', 'êµ¬ì¡°', 'rescue']):
            return 'helping_dilemma'
        elif any(word in description for word in ['ê³µì •', 'fair', 'ë¶ˆê³µí‰', 'unfair']):
            return 'fairness_issue'
        elif any(word in description for word in ['ê°€ì¡±', 'family', 'ì¹œêµ¬', 'friend']):
            return 'relationship_conflict'
        else:
            return 'general_ethical'
    
    def _calculate_emotional_learning_curve(self, experiences: List[Dict]) -> float:
        """ê°ì • í•™ìŠµ ê³¡ì„  ê³„ì‚°"""
        if len(experiences) < 2:
            return 0.0
        
        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì‹ ë¢°ë„ ë³€í™” ì¶”ì 
        confidences = [exp['confidence'] for exp in experiences]
        
        # í•™ìŠµ ì§„í–‰ë„ = ìµœê·¼ ì‹ ë¢°ë„ì™€ ì´ˆê¸° ì‹ ë¢°ë„ì˜ ì°¨ì´
        if len(confidences) >= 3:
            recent_avg = np.mean(confidences[-3:])
            initial_avg = np.mean(confidences[:3])
            return recent_avg - initial_avg
        
        return 0.0
    
    async def _store_emotional_experience(self, scenario_data: Dict, regret_result: Dict, 
                                        bentham_scores: List[float], integrated_prediction: Dict):
        """ê°ì • ê²½í—˜ì„ ë©”ëª¨ë¦¬ì— ì €ì¥"""
        
        scenario_type = self._classify_ethical_scenario(scenario_data)
        
        # ê²½í—˜ ë°ì´í„° êµ¬ì„± - CircuitDecisionResult ê°ì²´ ì†ì„± ì§ì ‘ ì ‘ê·¼
        experience = {
            'timestamp': time.time(),
            'scenario_type': scenario_type,
            'scenario_id': scenario_data.get('id', f"scenario_{int(time.time())}"),
            'regret_intensity': regret_result.predicted_regret.get('anticipated', 0.0) if hasattr(regret_result, 'predicted_regret') else 0.0,
            'confidence': regret_result.confidence if hasattr(regret_result, 'confidence') else 0.5,
            'bentham_average': np.mean(bentham_scores) if bentham_scores else 0.5,
            'emotional_complexity': len(regret_result.predicted_regret.get('emotion_vector', [])) if hasattr(regret_result, 'predicted_regret') and isinstance(regret_result.predicted_regret, dict) else 0,
            'decision_quality': integrated_prediction.get('prediction_confidence', 0.5)
        }
        
        # ê²½í—˜ ì €ì¥
        self.emotional_experience_memory['ethical_emotions'][scenario_type].append(experience)
        self.emotional_experience_memory['decision_outcomes'].append(experience)
        
        # ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œ (ìµœëŒ€ 1000ê°œ ê²½í—˜)
        if len(self.emotional_experience_memory['decision_outcomes']) > 1000:
            self.emotional_experience_memory['decision_outcomes'] = \
                self.emotional_experience_memory['decision_outcomes'][-1000:]
        
        # íƒ€ì…ë³„ ê²½í—˜ë„ ì œí•œ
        for scenario_type, experiences in self.emotional_experience_memory['ethical_emotions'].items():
            if len(experiences) > 200:
                self.emotional_experience_memory['ethical_emotions'][scenario_type] = experiences[-200:]
    
    def load_scenarios_from_file(self, file_path: str) -> List[Dict]:
        """íŒŒì¼ì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                return data
            elif 'scenarios' in data:
                return data['scenarios']
            else:
                return [data]  # ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ì¸ ê²½ìš°
                
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
            return []
    
    async def save_intermediate_log(self, cycle: int, scenarios_count: int, 
                                  performance: PerformanceMetrics):
        """ì¤‘ê°„ ë¡œê·¸ ì €ì¥"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'cycle': cycle + 1,
            'scenarios_processed': scenarios_count,
            'regret_calculations': self.dataset_metrics.regret_calculations,
            'bentham_calculations': self.dataset_metrics.bentham_calculations,
            'performance': asdict(performance),
            'gpu_status': self.gpu_manager.get_memory_status() if self.config.use_gpu else {},
            'storage_used_gb': self.storage_monitor.get_current_usage()
        }
        
        log_file = self.log_dir / f"cycle_{cycle+1}_checkpoint_{scenarios_count}.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ’¾ ì¤‘ê°„ ë¡œê·¸ ì €ì¥: {scenarios_count}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬ ì™„ë£Œ")


class StorageMonitor:
    """ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, max_gb: float):
        self.max_gb = max_gb
        self.base_usage = self.get_current_usage()
    
    def get_current_usage(self) -> float:
        """í˜„ì¬ ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ (GB)"""
        import shutil
        total, used, free = shutil.disk_usage("/mnt/c/large_project/linux_red_heart")
        return used / (1024**3)
    
    def check_storage_limit(self) -> bool:
        """ìŠ¤í† ë¦¬ì§€ í•œê³„ ì²´í¬"""
        current_usage = self.get_current_usage()
        usage_increase = current_usage - self.base_usage
        return usage_increase < self.max_gb


async def run_massive_training():
    """ëŒ€ê·œëª¨ í•™ìŠµ ì‹¤í–‰"""
    logger.info("ğŸ¯ Red Heart ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ í•™ìŠµ ì‹œì‘")
    
    # ì„¤ì •
    config = MassiveTrainingConfig()
    
    # í•™ìŠµê¸° ì´ˆê¸°í™”
    trainer = MassiveDatasetTrainer(config)
    
    # ë°ì´í„°ì…‹ íƒìƒ‰
    datasets = trainer.discover_datasets()
    if not datasets:
        logger.error("âŒ ì²˜ë¦¬í•  ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ëŒ€ê·œëª¨ í•™ìŠµ ì‹¤í–‰
    results = await trainer.train_on_datasets(datasets)
    
    # ê²°ê³¼ ì €ì¥
    final_results_file = trainer.log_dir / "massive_training_final_results.json"
    with open(final_results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    logger.info("ğŸ“ˆ ëŒ€ê·œëª¨ í•™ìŠµ ì™„ë£Œ!")
    logger.info(f"- ì´ í•™ìŠµ ì‹œê°„: {results['total_training_time']:.2f}ì´ˆ")
    logger.info(f"- ì²˜ë¦¬ëœ ì‹œë‚˜ë¦¬ì˜¤: {results['total_scenarios_processed']}ê°œ")
    logger.info(f"- í›„íšŒ ê³„ì‚°: {results['total_regret_calculations']}íšŒ")
    logger.info(f"- ë²¤ë‹´ ê³„ì‚°: {results['total_bentham_calculations']}íšŒ")
    logger.info(f"- ì™„ë£Œëœ ì„ íšŒ: {results['cycles_completed']}íšŒ")
    logger.info(f"- ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©: {results['storage_used_gb']:.2f}GB")
    
    return results


if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    results = asyncio.run(run_massive_training())