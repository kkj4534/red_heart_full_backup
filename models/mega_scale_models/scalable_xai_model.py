"""
í™•ì¥ ê°€ëŠ¥í•œ XAI ëª¨ë¸ (2ì–µ íŒŒë¼ë¯¸í„°)
Scalable XAI Model (200M Parameters)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import math
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import time
from datetime import datetime

# XAI ë° LLM ëª¨ë“ˆ import
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from xai_core.xai_logging_system import xai_logger, xai_trace, xai_decision_point
from llm_module import llm_tracker, register_llm, ask_llm, LLMConfig

@dataclass
class MegaScaleConfig:
    """ë©”ê°€ ìŠ¤ì¼€ì¼ ëª¨ë¸ ì„¤ì •"""
    # ê¸°ë³¸ ì°¨ì› ì„¤ì •
    input_dim: int = 1024
    hidden_dim: int = 2048
    num_layers: int = 48
    num_attention_heads: int = 32
    intermediate_size: int = 8192
    
    # ì „ë¬¸í™” ëª¨ë“ˆ ì„¤ì •
    emotion_layers: int = 12
    semantic_layers: int = 12
    reasoning_layers: int = 12
    integration_layers: int = 12
    
    # ì„±ëŠ¥ ìµœì í™”
    use_gradient_checkpointing: bool = True
    use_mixed_precision: bool = True
    dropout_rate: float = 0.02  # Loss NaN ë°©ì§€ë¥¼ ìœ„í•œ Dropout ê°ì†Œ
    layer_norm_eps: float = 1e-6  # Loss NaN ë°©ì§€ë¥¼ ìœ„í•œ ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ 
    
    # XAI ì„¤ì •
    enable_xai_tracking: bool = True
    explanation_depth: int = 5
    llm_integration: bool = True
    
    def get_total_params(self) -> int:
        """ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ë“¤
        transformer_params = self.num_layers * (
            4 * self.hidden_dim * self.hidden_dim +  # attention
            3 * self.hidden_dim * self.intermediate_size +  # FFN
            4 * self.hidden_dim  # layer norms and biases
        )
        
        # ì…ì¶œë ¥ ë ˆì´ì–´ë“¤
        io_params = (
            self.input_dim * self.hidden_dim +  # input projection
            self.hidden_dim * 6 +  # emotion output
            self.hidden_dim * 1000 +  # semantic output
            self.hidden_dim * 128 +  # reasoning output
            self.hidden_dim * 512  # integration output
        )
        
        return transformer_params + io_params

class MultiHeadAttentionXL(nn.Module):
    """í™•ì¥ëœ ë©€í‹°í—¤ë“œ ì–´í…ì…˜"""
    
    def __init__(self, config: MegaScaleConfig):
        super().__init__()
        self.config = config
        self.hidden_dim = config.hidden_dim
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_dim // self.num_heads
        
        assert self.head_dim * self.num_heads == self.hidden_dim
        
        # Query, Key, Value í”„ë¡œì ì…˜
        self.q_proj = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)
        self.k_proj = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)
        self.v_proj = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)
        self.o_proj = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)
        
        # ìƒëŒ€ì  ìœ„ì¹˜ ì¸ì½”ë”©
        self.relative_position_encoding = nn.Parameter(
            torch.randn(2 * 512 - 1, self.head_dim)
        )
        
        # ì–´í…ì…˜ ë“œë¡­ì•„ì›ƒ
        self.attention_dropout = nn.Dropout(config.dropout_rate)
        self.output_dropout = nn.Dropout(config.dropout_rate)
        
        # ìŠ¤ì¼€ì¼ë§ íŒ©í„°
        self.scale = math.sqrt(self.head_dim)
    
    def forward(self, hidden_states: torch.Tensor, 
                attention_mask: Optional[torch.Tensor] = None,
                return_attention: bool = False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        
        batch_size, seq_len, hidden_dim = hidden_states.shape
        
        # Q, K, V ê³„ì‚°
        query = self.q_proj(hidden_states)
        key = self.k_proj(hidden_states)
        value = self.v_proj(hidden_states)
        
        # í—¤ë“œë³„ë¡œ ë¶„í• 
        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°
        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale
        
        # ìƒëŒ€ì  ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        if seq_len <= 512:
            rel_pos_bias = self._get_relative_position_bias(seq_len)
            attention_scores = attention_scores + rel_pos_bias.unsqueeze(0).unsqueeze(0)
        
        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì ìš©
        if attention_mask is not None:
            # ë§ˆìŠ¤í¬ ì°¨ì› ë§ì¶”ê¸°
            if attention_mask.dim() == 3:  # [batch, seq_len, seq_len]
                attention_mask = attention_mask.unsqueeze(1)  # [batch, 1, seq_len, seq_len]
            attention_scores = attention_scores + attention_mask
        
        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_weights = self.attention_dropout(attention_weights)
        
        # ì–´í…ì…˜ ì ìš©
        context = torch.matmul(attention_weights, value)
        
        # í—¤ë“œ í•©ì¹˜ê¸°
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.hidden_dim
        )
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        output = self.o_proj(context)
        output = self.output_dropout(output)
        
        if return_attention:
            return output, attention_weights
        return output, None
    
    def _get_relative_position_bias(self, seq_len: int) -> torch.Tensor:
        """ìƒëŒ€ì  ìœ„ì¹˜ í¸í–¥ ê³„ì‚°"""
        positions = torch.arange(seq_len, device=self.relative_position_encoding.device)
        relative_positions = positions.unsqueeze(0) - positions.unsqueeze(1)
        relative_positions = relative_positions + 512 - 1  # ìµœëŒ€ ê¸¸ì´ ê¸°ì¤€
        relative_positions = torch.clamp(relative_positions, 0, 2 * 512 - 2)
        
        bias = self.relative_position_encoding[relative_positions]
        # [seq_len, seq_len, head_dim] -> [seq_len, seq_len]ë¡œ ì¶•ì†Œ
        return bias.mean(dim=-1)

class FeedForwardXL(nn.Module):
    """í™•ì¥ëœ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, config: MegaScaleConfig):
        super().__init__()
        self.config = config
        
        # Gated Linear Unit ì‚¬ìš©
        self.gate_proj = nn.Linear(config.hidden_dim, config.intermediate_size, bias=False)
        self.up_proj = nn.Linear(config.hidden_dim, config.intermediate_size, bias=False)
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_dim, bias=False)
        
        self.dropout = nn.Dropout(config.dropout_rate)
        
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        gate = F.silu(self.gate_proj(hidden_states))  # SwiGLU í™œì„±í™”
        up = self.up_proj(hidden_states)
        intermediate = gate * up
        intermediate = self.dropout(intermediate)
        output = self.down_proj(intermediate)
        return output

class TransformerLayerXL(nn.Module):
    """í™•ì¥ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´"""
    
    def __init__(self, config: MegaScaleConfig, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        
        # ì–´í…ì…˜
        self.attention = MultiHeadAttentionXL(config)
        self.attention_norm = nn.LayerNorm(config.hidden_dim, eps=config.layer_norm_eps)
        
        # í”¼ë“œí¬ì›Œë“œ
        self.feed_forward = FeedForwardXL(config)
        self.ffn_norm = nn.LayerNorm(config.hidden_dim, eps=config.layer_norm_eps)
        
        # ì „ë¬¸í™” ëª¨ë“ˆ (ì¼ë¶€ ë ˆì´ì–´ì—ë§Œ)
        self.specialized_module = None
        if layer_idx % 4 == 0:  # 4ë²ˆì§¸ë§ˆë‹¤ ì „ë¬¸í™” ëª¨ë“ˆ
            self.specialized_module = self._create_specialized_module()
    
    def _create_specialized_module(self) -> nn.Module:
        """ì „ë¬¸í™” ëª¨ë“ˆ ìƒì„±"""
        if self.layer_idx < self.config.emotion_layers:
            return EmotionSpecializedModule(self.config)
        elif self.layer_idx < self.config.emotion_layers + self.config.semantic_layers:
            return SemanticSpecializedModule(self.config)
        elif self.layer_idx < self.config.emotion_layers + self.config.semantic_layers + self.config.reasoning_layers:
            return ReasoningSpecializedModule(self.config)
        else:
            return IntegrationSpecializedModule(self.config)
    
    def forward(self, hidden_states: torch.Tensor, 
                attention_mask: Optional[torch.Tensor] = None,
                return_attention: bool = False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        
        # ì–´í…ì…˜ ë¸”ë¡ (Post-norm ì•ˆì •ì„± ê°œì„ )
        attention_output, attention_weights = self.attention(
            hidden_states, attention_mask, return_attention
        )
        hidden_states = hidden_states + attention_output
        hidden_states = self.attention_norm(hidden_states)
        
        # í”¼ë“œí¬ì›Œë“œ ë¸”ë¡ (Post-norm ì•ˆì •ì„± ê°œì„ )
        ffn_output = self.feed_forward(hidden_states)
        hidden_states = hidden_states + ffn_output
        hidden_states = self.ffn_norm(hidden_states)
        
        # ì „ë¬¸í™” ëª¨ë“ˆ (ìˆëŠ” ê²½ìš°)
        if self.specialized_module is not None:
            specialized_output = self.specialized_module(hidden_states)
            hidden_states = hidden_states + specialized_output
        
        return hidden_states, attention_weights

class EmotionSpecializedModule(nn.Module):
    """ê°ì • ì „ë¬¸í™” ëª¨ë“ˆ"""
    
    def __init__(self, config: MegaScaleConfig):
        super().__init__()
        self.emotion_projector = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim // 2),
            nn.GELU(),
            nn.Linear(config.hidden_dim // 2, config.hidden_dim),
            nn.Dropout(config.dropout_rate)
        )
        
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return self.emotion_projector(hidden_states) * 0.1  # ì”ì°¨ ì—°ê²°ì„ ìœ„í•´ ì‘ì€ ìŠ¤ì¼€ì¼

class SemanticSpecializedModule(nn.Module):
    """ì˜ë¯¸ ì „ë¬¸í™” ëª¨ë“ˆ"""
    
    def __init__(self, config: MegaScaleConfig):
        super().__init__()
        self.semantic_analyzer = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.Tanh(),
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.Dropout(config.dropout_rate)
        )
        
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return self.semantic_analyzer(hidden_states) * 0.1

class ReasoningSpecializedModule(nn.Module):
    """ì¶”ë¡  ì „ë¬¸í™” ëª¨ë“ˆ"""
    
    def __init__(self, config: MegaScaleConfig):
        super().__init__()
        self.reasoning_network = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim * 2),
            nn.SiLU(),
            nn.Linear(config.hidden_dim * 2, config.hidden_dim),
            nn.Dropout(config.dropout_rate)
        )
        
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return self.reasoning_network(hidden_states) * 0.1

class IntegrationSpecializedModule(nn.Module):
    """í†µí•© ì „ë¬¸í™” ëª¨ë“ˆ"""
    
    def __init__(self, config: MegaScaleConfig):
        super().__init__()
        self.integration_hub = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.LayerNorm(config.hidden_dim),
            nn.GELU(),
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.Dropout(config.dropout_rate)
        )
        
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return self.integration_hub(hidden_states) * 0.1

class MegaScaleXAIModel(nn.Module):
    """ë©”ê°€ ìŠ¤ì¼€ì¼ XAI ëª¨ë¸ (2ì–µ íŒŒë¼ë¯¸í„°)"""
    
    def __init__(self, config: MegaScaleConfig):
        super().__init__()
        self.config = config
        
        # ì…ë ¥ ì„ë² ë”©
        self.input_projection = nn.Linear(config.input_dim, config.hidden_dim)
        self.input_norm = nn.LayerNorm(config.hidden_dim, eps=config.layer_norm_eps)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤
        self.transformer_layers = nn.ModuleList([
            TransformerLayerXL(config, i) for i in range(config.num_layers)
        ])
        
        # ìµœì¢… ì •ê·œí™”
        self.final_norm = nn.LayerNorm(config.hidden_dim, eps=config.layer_norm_eps)
        
        # ì¶œë ¥ í—¤ë“œë“¤
        self.emotion_head = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.hidden_dim // 2, 6),  # 6ì°¨ì› ê°ì •
            nn.Tanh()
        )
        
        self.semantic_head = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.GELU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.hidden_dim, 1000),  # ì˜ë¯¸ í´ë˜ìŠ¤
            nn.Softmax(dim=-1)
        )
        
        self.reasoning_head = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim // 4),
            nn.GELU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.hidden_dim // 4, 128)  # ì¶”ë¡  íŠ¹ì§•
        )
        
        self.integration_head = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim),
            nn.GELU(),
            nn.Dropout(config.dropout_rate),
            nn.Linear(config.hidden_dim, 512),  # í†µí•© íŠ¹ì§•
            nn.Tanh()
        )
        
        # XAI ì¶”ì ê¸°
        if config.enable_xai_tracking:
            self.xai_tracker = XAITracker(config)
        
        # LLM í†µí•©
        if config.llm_integration:
            self.llm_integration_layer = LLMIntegrationLayer(config)
        
        # íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
        self.apply(self._init_weights)
        
        print(f"âœ… MegaScaleXAIModel ì´ˆê¸°í™” ì™„ë£Œ: {self.get_parameter_count():,}ê°œ íŒŒë¼ë¯¸í„°")
    
    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            # Xavier uniform ì´ˆê¸°í™”
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.constant_(module.bias, 0)
            torch.nn.init.constant_(module.weight, 1.0)
        elif isinstance(module, nn.Parameter):
            torch.nn.init.normal_(module, mean=0.0, std=0.02)
    
    def get_parameter_count(self) -> int:
        """íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        return sum(p.numel() for p in self.parameters())
    
    @xai_trace("MegaScaleXAI", "inference")
    def forward(self, input_embeddings: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                return_hidden_states: bool = False,
                return_attention: bool = False) -> Dict[str, torch.Tensor]:
        
        batch_size, seq_len, input_dim = input_embeddings.shape
        
        # ì…ë ¥ í”„ë¡œì ì…˜
        hidden_states = self.input_projection(input_embeddings)
        hidden_states = self.input_norm(hidden_states)
        
        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì¤€ë¹„
        if attention_mask is None:
            attention_mask = None  # Noneìœ¼ë¡œ ìœ ì§€
        elif attention_mask.dim() == 2:
            # [batch, seq_len] -> [batch, seq_len, seq_len]
            attention_mask = attention_mask.unsqueeze(1).expand(batch_size, seq_len, seq_len)
            attention_mask = (1.0 - attention_mask) * -10000.0
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤ í†µê³¼
        all_hidden_states = []
        all_attention_weights = []
        
        for i, layer in enumerate(self.transformer_layers):
            if self.config.use_gradient_checkpointing and self.training:
                # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš©
                hidden_states, attention_weights = torch.utils.checkpoint.checkpoint(
                    layer, hidden_states, attention_mask, return_attention
                )
            else:
                hidden_states, attention_weights = layer(
                    hidden_states, attention_mask, return_attention
                )
            
            if return_hidden_states:
                all_hidden_states.append(hidden_states)
            if return_attention and attention_weights is not None:
                all_attention_weights.append(attention_weights)
            
            # XAI ì¶”ì  (ì¼ë¶€ ë ˆì´ì–´ë§Œ)
            if self.config.enable_xai_tracking and i % 8 == 0:
                self.xai_tracker.track_layer_output(i, hidden_states)
        
        # ìµœì¢… ì •ê·œí™”
        hidden_states = self.final_norm(hidden_states)
        
        # í‰ê·  í’€ë§ìœ¼ë¡œ ë¬¸ì¥ í‘œí˜„ ìƒì„±
        pooled_output = hidden_states.mean(dim=1)  # ë‹¨ìˆœ í‰ê·  í’€ë§
        
        # ê° í—¤ë“œë³„ ì¶œë ¥ ê³„ì‚°
        emotion_output = self.emotion_head(pooled_output)
        semantic_output = self.semantic_head(pooled_output)
        reasoning_output = self.reasoning_head(pooled_output)
        integration_output = self.integration_head(pooled_output)
        
        results = {
            'emotion_predictions': emotion_output,
            'semantic_predictions': semantic_output,
            'reasoning_features': reasoning_output,
            'integration_features': integration_output,
            'pooled_output': pooled_output
        }
        
        # LLM í†µí•© (í™œì„±í™”ëœ ê²½ìš°)
        if self.config.llm_integration and hasattr(self, 'llm_integration_layer'):
            llm_output = self.llm_integration_layer(pooled_output, results)
            results['llm_analysis'] = llm_output
        
        # XAI ë¶„ì„ ì¶”ê°€
        if self.config.enable_xai_tracking:
            xai_analysis = self.xai_tracker.generate_explanation(results)
            results['xai_explanation'] = xai_analysis
        
        # ì„ íƒì  ë°˜í™˜
        if return_hidden_states:
            results['hidden_states'] = all_hidden_states
        if return_attention:
            results['attention_weights'] = all_attention_weights
        
        return results

class XAITracker(nn.Module):
    """XAI ì¶”ì  ëª¨ë“ˆ"""
    
    def __init__(self, config: MegaScaleConfig):
        super().__init__()
        self.config = config
        self.layer_outputs = {}
        
        # ì„¤ëª… ìƒì„± ë„¤íŠ¸ì›Œí¬
        self.explanation_generator = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(config.hidden_dim // 2, config.explanation_depth),
            nn.Softmax(dim=-1)
        )
        
    def track_layer_output(self, layer_idx: int, hidden_states: torch.Tensor):
        """ë ˆì´ì–´ ì¶œë ¥ ì¶”ì """
        self.layer_outputs[layer_idx] = {
            'mean_activation': hidden_states.mean().item(),
            'std_activation': hidden_states.std().item(),
            'max_activation': hidden_states.max().item(),
            'min_activation': hidden_states.min().item(),
            'shape': hidden_states.shape
        }
    
    def generate_explanation(self, model_outputs: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """ì„¤ëª… ìƒì„±"""
        pooled_output = model_outputs['pooled_output']
        
        # ì„¤ëª… ê°€ì¤‘ì¹˜ ìƒì„±
        explanation_weights = self.explanation_generator(pooled_output)
        
        # íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„
        feature_importance = {
            'emotion_contribution': torch.abs(model_outputs['emotion_predictions']).mean().item(),
            'semantic_contribution': torch.abs(model_outputs['semantic_predictions']).mean().item(),
            'reasoning_contribution': torch.abs(model_outputs['reasoning_features']).mean().item(),
            'integration_contribution': torch.abs(model_outputs['integration_features']).mean().item()
        }
        
        return {
            'explanation_weights': explanation_weights,
            'feature_importance': feature_importance,
            'layer_activations': self.layer_outputs,
            'confidence_score': self._calculate_confidence(model_outputs),
            'decision_path': self._extract_decision_path(model_outputs)
        }
    
    def _calculate_confidence(self, outputs: Dict[str, torch.Tensor]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        # ê° ì¶œë ¥ì˜ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ì‹ ë¢°ë„
        emotion_entropy = -torch.sum(torch.abs(outputs['emotion_predictions']) * 
                                   torch.log(torch.abs(outputs['emotion_predictions']) + 1e-8), dim=-1)
        semantic_entropy = -torch.sum(outputs['semantic_predictions'] * 
                                    torch.log(outputs['semantic_predictions'] + 1e-8), dim=-1)
        
        avg_entropy = (emotion_entropy + semantic_entropy).mean().item()
        confidence = 1.0 / (1.0 + avg_entropy)  # ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„
        
        return confidence
    
    def _extract_decision_path(self, outputs: Dict[str, torch.Tensor]) -> List[str]:
        """ì˜ì‚¬ê²°ì • ê²½ë¡œ ì¶”ì¶œ"""
        path = []
        
        # ê°ì • ê¸°ë°˜ ê²°ì •
        emotion_max = torch.argmax(torch.abs(outputs['emotion_predictions']), dim=-1)
        if emotion_max.numel() == 1:
            path.append(f"ì£¼ìš” ê°ì • ì°¨ì›: {emotion_max.item()}")
        else:
            path.append(f"ì£¼ìš” ê°ì • ì°¨ì›: {emotion_max[0].item()}")
        
        # ì˜ë¯¸ ê¸°ë°˜ ê²°ì •
        semantic_max = torch.argmax(outputs['semantic_predictions'], dim=-1)
        if semantic_max.numel() == 1:
            path.append(f"ì£¼ìš” ì˜ë¯¸ í´ë˜ìŠ¤: {semantic_max.item()}")
        else:
            path.append(f"ì£¼ìš” ì˜ë¯¸ í´ë˜ìŠ¤: {semantic_max[0].item()}")
        
        # ì¶”ë¡  ê¸°ë°˜ ê²°ì •
        reasoning_norm = torch.norm(outputs['reasoning_features'], dim=-1)
        path.append(f"ì¶”ë¡  ê°•ë„: {reasoning_norm.mean().item():.3f}")
        
        return path

class LLMIntegrationLayer(nn.Module):
    """LLM í†µí•© ë ˆì´ì–´"""
    
    def __init__(self, config: MegaScaleConfig):
        super().__init__()
        self.config = config
        
        # LLM ì¿¼ë¦¬ ìƒì„±ê¸°
        self.query_generator = nn.Sequential(
            nn.Linear(config.hidden_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256)
        )
        
        # LLM ì‘ë‹µ ì¸ì½”ë”
        self.response_encoder = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, config.hidden_dim)
        )
        
    @xai_decision_point()
    def forward(self, pooled_output: torch.Tensor, 
                model_outputs: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        
        # ì¿¼ë¦¬ ìƒì„±
        query_features = self.query_generator(pooled_output)
        
        # LLMì—ê²Œ ë³´ë‚¼ ì¿¼ë¦¬ êµ¬ì„±
        emotion_summary = self._summarize_emotions(model_outputs['emotion_predictions'])
        semantic_summary = self._summarize_semantics(model_outputs['semantic_predictions'])
        
        llm_query = f"""
        ê°ì • ë¶„ì„ ê²°ê³¼: {emotion_summary}
        ì˜ë¯¸ ë¶„ì„ ê²°ê³¼: {semantic_summary}
        
        ì´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ì„¤ëª…í•´ì£¼ì„¸ìš”:
        1. ì£¼ìš” ê°ì • íŒ¨í„´
        2. ì˜ë¯¸ì  íŠ¹ì§•
        3. ì¶”ë¡  ê³¼ì •
        """
        
        # LLM í˜¸ì¶œ (ë¹„ë™ê¸°ì ìœ¼ë¡œ)
        try:
            if 'gpt2' in llm_tracker.llm_models:
                llm_response = ask_llm('gpt2', llm_query)
            else:
                # ê¸°ë³¸ LLM ë“±ë¡
                register_llm('gpt2', model_name='gpt2', max_tokens=200)
                llm_response = ask_llm('gpt2', llm_query)
        except Exception as e:
            llm_response = f"[LLM ì˜¤ë¥˜] {str(e)}"
        
        # LLM ì‘ë‹µ ì¸ì½”ë”©
        # ë”ë¯¸ ë²¡í„° (ì‹¤ì œë¡œëŠ” LLM ì‘ë‹µì„ ì„ë² ë”©í•´ì•¼ í•¨)
        response_embedding = torch.randn_like(query_features)
        encoded_response = self.response_encoder(response_embedding)
        
        return {
            'llm_query': llm_query,
            'llm_response': llm_response,
            'encoded_response': encoded_response,
            'query_features': query_features
        }
    
    def _summarize_emotions(self, emotion_predictions: torch.Tensor) -> str:
        """ê°ì • ì˜ˆì¸¡ ìš”ì•½"""
        emotions = ['valence', 'arousal', 'dominance', 'certainty', 'surprise', 'anticipation']
        values = emotion_predictions[0].detach().cpu().numpy()
        
        summary_parts = []
        for i, (emotion, value) in enumerate(zip(emotions, values)):
            if abs(value) > 0.3:  # ì„ê³„ê°’ ì´ìƒì¸ ê°ì •ë§Œ
                summary_parts.append(f"{emotion}: {value:.2f}")
        
        return ", ".join(summary_parts) if summary_parts else "ì¤‘ì„±ì  ê°ì •"
    
    def _summarize_semantics(self, semantic_predictions: torch.Tensor) -> str:
        """ì˜ë¯¸ ì˜ˆì¸¡ ìš”ì•½"""
        top_indices = torch.topk(semantic_predictions[0], 3).indices
        confidence_scores = torch.topk(semantic_predictions[0], 3).values
        
        summary_parts = []
        for idx, conf in zip(top_indices, confidence_scores):
            summary_parts.append(f"í´ë˜ìŠ¤{idx.item()}: {conf.item():.3f}")
        
        return ", ".join(summary_parts)

def create_mega_scale_model(target_params: int = 200_000_000) -> MegaScaleXAIModel:
    """íƒ€ê²Ÿ íŒŒë¼ë¯¸í„° ìˆ˜ì— ë§ëŠ” ë©”ê°€ ìŠ¤ì¼€ì¼ ëª¨ë¸ ìƒì„±"""
    
    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹œì‘
    config = MegaScaleConfig()
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¡°ì •
    estimated_params = config.get_total_params()
    
    if estimated_params < target_params:
        # íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ì°¨ì› ì¦ê°€
        scale_factor = (target_params / estimated_params) ** 0.5
        
        config.hidden_dim = int(config.hidden_dim * scale_factor)
        config.intermediate_size = int(config.intermediate_size * scale_factor)
        
        # 16ì˜ ë°°ìˆ˜ë¡œ ì¡°ì • (íš¨ìœ¨ì„±ì„ ìœ„í•´)
        config.hidden_dim = (config.hidden_dim // 16) * 16
        config.intermediate_size = (config.intermediate_size // 16) * 16
        
        # ì–´í…ì…˜ í—¤ë“œ ìˆ˜ë„ ì¡°ì • (hidden_dimì˜ ì•½ìˆ˜ê°€ ë˜ë„ë¡)
        head_candidates = [8, 16, 32, 64]
        for heads in head_candidates:
            if config.hidden_dim % heads == 0:
                config.num_attention_heads = heads
                break
        else:
            config.num_attention_heads = 8  # ê¸°ë³¸ê°’
    
    model = MegaScaleXAIModel(config)
    actual_params = model.get_parameter_count()
    
    print(f"âœ… ë©”ê°€ ìŠ¤ì¼€ì¼ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"   - íƒ€ê²Ÿ íŒŒë¼ë¯¸í„°: {target_params:,}")
    print(f"   - ì‹¤ì œ íŒŒë¼ë¯¸í„°: {actual_params:,}")
    print(f"   - íˆë“  ì°¨ì›: {config.hidden_dim}")
    print(f"   - ë ˆì´ì–´ ìˆ˜: {config.num_layers}")
    print(f"   - ì–´í…ì…˜ í—¤ë“œ: {config.num_attention_heads}")
    
    return model

def optimize_model_for_inference(model: MegaScaleXAIModel) -> MegaScaleXAIModel:
    """ì¶”ë¡ ì„ ìœ„í•œ ëª¨ë¸ ìµœì í™”"""
    model.eval()
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ë¹„í™œì„±í™”
    model.config.use_gradient_checkpointing = False
    
    # ë“œë¡­ì•„ì›ƒ ë¹„í™œì„±í™”
    for module in model.modules():
        if isinstance(module, nn.Dropout):
            module.p = 0.0
    
    print("âœ… ëª¨ë¸ ì¶”ë¡  ìµœì í™” ì™„ë£Œ")
    return model

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸš€ ë©”ê°€ ìŠ¤ì¼€ì¼ XAI ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    
    model = create_mega_scale_model(target_params=200_000_000)
    model = optimize_model_for_inference(model)
    
    # ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    batch_size = 2
    seq_len = 128
    input_dim = 1024
    
    dummy_input = torch.randn(batch_size, seq_len, input_dim)
    
    with torch.no_grad():
        outputs = model(dummy_input)
    
    print(f"âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print(f"   - ê°ì • ì¶œë ¥: {outputs['emotion_predictions'].shape}")
    print(f"   - ì˜ë¯¸ ì¶œë ¥: {outputs['semantic_predictions'].shape}")
    print(f"   - ì¶”ë¡  ì¶œë ¥: {outputs['reasoning_features'].shape}")
    print(f"   - í†µí•© ì¶œë ¥: {outputs['integration_features'].shape}")
    
    if 'xai_explanation' in outputs:
        print(f"   - XAI ì‹ ë¢°ë„: {outputs['xai_explanation']['confidence_score']:.3f}")