"""
ÌÜµÌï©Îêú Red Heart ÏãúÏä§ÌÖú GPU ÌïôÏäµ ÌÖåÏä§Ìä∏
Integrated Red Heart System GPU Learning Test

ÌòÑÏû¨ ÌÜµÌï©Îêú Î™®Îì† Î™®ÎìàÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµ Î∞è ÌÖåÏä§Ìä∏ ÏßÑÌñâ
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
import asyncio

# ÌòÑÏû¨ ÌîÑÎ°úÏ†ùÌä∏Ïùò ÌÜµÌï©Îêú Î™®ÎìàÎì§ ÏûÑÌè¨Ìä∏
try:
    from config import DEVICE, ADVANCED_CONFIG, DATA_DIR
    from data_models import EmotionData, EmotionState, EmotionIntensity
    from emotion_ethics_regret_circuit import EmotionEthicsRegretCircuit, CircuitDecisionContext
    from ethics_policy_updater import EthicsPolicyUpdater, EthicsExperience
    from phase_controller import PhaseController, PhaseDecisionContext, Phase
    from xai_feedback_integrator import XAIFeedbackIntegrator, XAIInterpretation
    from fuzzy_emotion_ethics_mapper import FuzzyEmotionEthicsMapper
    from deep_multi_dimensional_ethics_system import DeepMultiDimensionalEthicsSystem, EthicalDilemma, StakeholderPerspective
    from temporal_event_propagation_analyzer import TemporalEventPropagationAnalyzer, TemporalEvent
    from integrated_system_orchestrator import IntegratedSystemOrchestrator, IntegrationContext
    
    # ÏóÖÍ∑∏Î†àÏù¥ÎìúÎêú ÏãúÏä§ÌÖú ÏûÑÌè¨Ìä∏
    from dynamic_gpu_manager import get_gpu_manager, allocate_gpu_memory, optimize_gpu_for_learning
    from robust_logging_system import get_robust_logger, test_session, add_performance_sample
    
    print("‚úÖ Î™®Îì† ÌÜµÌï© Î™®Îìà ÏûÑÌè¨Ìä∏ ÏÑ±Í≥µ")
    print("‚úÖ ÎèôÏ†Å GPU Í¥ÄÎ¶¨Ïûê Î∞è Í≤¨Í≥†Ìïú Î°úÍπÖ ÏãúÏä§ÌÖú Ïó∞Îèô ÏôÑÎ£å")
except ImportError as e:
    print(f"‚ùå Î™®Îìà ÏûÑÌè¨Ìä∏ Ïã§Ìå®: {e}")
    sys.exit(1)

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('IntegratedLearningTest')

@dataclass
class LearningConfig:
    """ÌïôÏäµ ÏÑ§Ï†ï"""
    batch_size: int = 8
    learning_rate: float = 0.001
    num_epochs: int = 10
    device: str = str(DEVICE)
    use_gpu: bool = ADVANCED_CONFIG.get('enable_gpu', False)
    validation_split: float = 0.2
    early_stopping_patience: int = 3

@dataclass
class TestResults:
    """ÌÖåÏä§Ìä∏ Í≤∞Í≥º"""
    accuracy: float
    loss: float
    prediction_error: float
    processing_time: float
    gpu_utilization: float
    memory_usage: float

class IntegratedLearningFramework:
    """ÌÜµÌï© ÌïôÏäµ ÌîÑÎ†àÏûÑÏõåÌÅ¨ - ÏóÖÍ∑∏Î†àÏù¥ÎìúÎêú GPU Í¥ÄÎ¶¨ Î∞è Î°úÍπÖ"""
    
    def __init__(self, config: LearningConfig):
        self.config = config
        self.logger = logger
        
        # ÏóÖÍ∑∏Î†àÏù¥ÎìúÎêú GPU Í¥ÄÎ¶¨Ïûê Î∞è Î°úÍπÖ ÏãúÏä§ÌÖú Ïó∞Îèô
        self.gpu_manager = get_gpu_manager()
        self.robust_logger = get_robust_logger()
        
        # GPU ÏÉÅÌÉú ÌôïÏù∏ Î∞è ÏµúÏ†ÅÌôî
        self.device = torch.device(config.device)
        
        if config.use_gpu and torch.cuda.is_available():
            # GPU ÌïôÏäµ ÏµúÏ†ÅÌôî ÌôúÏÑ±Ìôî
            optimization_success = optimize_gpu_for_learning()
            
            gpu_status = self.gpu_manager.get_memory_status()
            self.logger.info(f"üöÄ GPU ÏÇ¨Ïö©: {torch.cuda.get_device_name()}")
            self.logger.info(f"üíæ Ï¥ù GPU Î©îÎ™®Î¶¨: {gpu_status['total_gb']:.1f}GB")
            self.logger.info(f"üíæ ÏÇ¨Ïö© Í∞ÄÎä• Î©îÎ™®Î¶¨: {gpu_status['available_gb']:.1f}GB")
            self.logger.info(f"‚ö° ÌïôÏäµ ÏµúÏ†ÅÌôî: {'ÌôúÏÑ±Ìôî' if optimization_success else 'Ï†úÌïúÎê®'}")
            
            self.robust_logger.log("INFO", "GPU_Manager", 
                                 f"GPU ÌïôÏäµ ÌôòÍ≤Ω Ï¥àÍ∏∞Ìôî ÏôÑÎ£å - ÏµúÏ†ÅÌôî: {optimization_success}",
                                 {"gpu_status": gpu_status})
        else:
            self.logger.info("üíª CPU Î™®ÎìúÎ°ú Ïã§Ìñâ")
            self.robust_logger.log("INFO", "GPU_Manager", "CPU Î™®ÎìúÎ°ú Ïã§Ìñâ")
        
        # ÌÜµÌï©Îêú ÏãúÏä§ÌÖú Ïª¥Ìè¨ÎÑåÌä∏Îì§ Ï¥àÍ∏∞Ìôî
        self.initialize_components()
        
        # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•ÏÜå
        self.training_data = []
        self.validation_data = []
        
        # ÏÑ±Îä• Ï∂îÏ†Å - Ìñ•ÏÉÅÎêú Ï∂îÏ†Å
        self.training_history = {
            'loss': [],
            'accuracy': [],
            'gpu_memory': [],
            'processing_time': [],
            'gpu_utilization': [],
            'memory_efficiency': []
        }
    
    def initialize_components(self):
        """ÌÜµÌï© ÏãúÏä§ÌÖú Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî"""
        self.logger.info("üèóÔ∏è ÌÜµÌï© ÏãúÏä§ÌÖú Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî...")
        
        try:
            # ÌïµÏã¨ ÌöåÎ°ú
            self.emotion_circuit = EmotionEthicsRegretCircuit()
            self.logger.info("‚úÖ Í∞êÏ†ï-Ïú§Î¶¨-ÌõÑÌöå ÏÇºÍ∞ÅÌöåÎ°ú Ï¥àÍ∏∞Ìôî")
            
            # Ï†ïÏ±Ö ÏóÖÎç∞Ïù¥ÌÑ∞
            self.policy_updater = EthicsPolicyUpdater()
            self.logger.info("‚úÖ Ïú§Î¶¨ Ï†ïÏ±Ö ÏóÖÎç∞Ïù¥ÌÑ∞ Ï¥àÍ∏∞Ìôî")
            
            # ÌéòÏù¥Ï¶à Ïª®Ìä∏Î°§Îü¨
            self.phase_controller = PhaseController()
            self.logger.info("‚úÖ ÌéòÏù¥Ï¶à Ïª®Ìä∏Î°§Îü¨ Ï¥àÍ∏∞Ìôî")
            
            # XAI ÌîºÎìúÎ∞± ÌÜµÌï©Í∏∞
            self.xai_integrator = XAIFeedbackIntegrator()
            self.logger.info("‚úÖ XAI ÌîºÎìúÎ∞± ÌÜµÌï©Í∏∞ Ï¥àÍ∏∞Ìôî")
            
            # ÌçºÏßÄ Îß§Ìïë
            self.fuzzy_mapper = FuzzyEmotionEthicsMapper()
            self.logger.info("‚úÖ ÌçºÏßÄ Í∞êÏ†ï-Ïú§Î¶¨ Îß§Ìïë Ï¥àÍ∏∞Ìôî")
            
            # Îã§Ï∞®Ïõê Ïú§Î¶¨ ÏãúÏä§ÌÖú
            self.ethics_system = DeepMultiDimensionalEthicsSystem()
            self.logger.info("‚úÖ Îã§Ï∞®Ïõê Ïú§Î¶¨ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî")
            
            # ÏãúÍ≥ÑÏó¥ Î∂ÑÏÑùÍ∏∞
            self.temporal_analyzer = TemporalEventPropagationAnalyzer()
            self.logger.info("‚úÖ ÏãúÍ≥ÑÏó¥ ÏÇ¨Í±¥ Ï†ÑÌåå Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî")
            
            # ÌÜµÌï© Ïò§ÏºÄÏä§Ìä∏Î†àÏù¥ÌÑ∞
            self.orchestrator = IntegratedSystemOrchestrator()
            self.logger.info("‚úÖ ÌÜµÌï© ÏãúÏä§ÌÖú Ïò§ÏºÄÏä§Ìä∏Î†àÏù¥ÌÑ∞ Ï¥àÍ∏∞Ìôî")
            
            self.logger.info("üéØ Î™®Îì† Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            
        except Exception as e:
            self.logger.error(f"‚ùå Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            raise
    
    def load_training_data(self, data_dir: Path = None) -> int:
        """ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
        if data_dir is None:
            data_dir = DATA_DIR / 'decision_logs'
        
        self.logger.info(f"üìÇ Îç∞Ïù¥ÌÑ∞ Î°úÎî© ÏãúÏûë: {data_dir}")
        
        json_files = list(data_dir.glob("*.json"))
        self.logger.info(f"üìÑ Î∞úÍ≤¨Îêú JSON ÌååÏùº: {len(json_files)}Í∞ú")
        
        loaded_count = 0
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù Î∞è Î≥ÄÌôò
                if self.validate_data_format(data):
                    processed_data = self.preprocess_data(data)
                    self.training_data.append(processed_data)
                    loaded_count += 1
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è ÌååÏùº Î°úÎî© Ïã§Ìå® {json_file.name}: {e}")
        
        # ÌïôÏäµ/Í≤ÄÏ¶ù Î∂ÑÌï†
        if self.training_data:
            split_idx = int(len(self.training_data) * (1 - self.config.validation_split))
            self.validation_data = self.training_data[split_idx:]
            self.training_data = self.training_data[:split_idx]
        
        self.logger.info(f"‚úÖ Îç∞Ïù¥ÌÑ∞ Î°úÎî© ÏôÑÎ£å: ÌïôÏäµ {len(self.training_data)}Í∞ú, Í≤ÄÏ¶ù {len(self.validation_data)}Í∞ú")
        return loaded_count
    
    def validate_data_format(self, data: Dict) -> bool:
        """Îç∞Ïù¥ÌÑ∞ ÌòïÏãù Í≤ÄÏ¶ù"""
        required_fields = ['situation', 'decision', 'actual_outcome']
        return all(field in data for field in required_fields)
    
    def preprocess_data(self, raw_data: Dict) -> Dict:
        """Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨"""
        situation = raw_data['situation']
        decision = raw_data['decision']
        outcome = raw_data['actual_outcome']
        
        # Í∞êÏ†ï Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò
        emotion_data = EmotionData(
            primary_emotion=EmotionState[raw_data['emotions']['primary_emotion']],
            intensity=EmotionIntensity[raw_data['emotions']['intensity']],
            valence=raw_data['emotions']['valence'],
            arousal=raw_data['emotions']['arousal'],
            confidence=raw_data['emotions']['confidence']
        )
        
        # Ïù¥Ìï¥Í¥ÄÍ≥ÑÏûê Ï∂îÏ∂ú
        stakeholders = []
        for person in situation['context']['people_involved']:
            stakeholder = StakeholderPerspective(
                stakeholder_id=person.replace(' ', '_'),
                name=person,
                role='participant',
                power_level=0.5,
                vulnerability=0.5
            )
            stakeholders.append(stakeholder)
        
        # Ïú§Î¶¨Ï†Å ÎîúÎ†àÎßà Íµ¨ÏÑ±
        ethical_dilemma = EthicalDilemma(
            dilemma_id=situation['id'],
            scenario=situation['description'],
            context=situation['context']['location'],
            stakeholders=stakeholders,
            available_options=[opt['text'] for opt in situation['options']]
        )
        
        return {
            'input': {
                'emotion': emotion_data,
                'dilemma': ethical_dilemma,
                'situation': situation,
                'hedonic_values': raw_data['hedonic_values']
            },
            'target': {
                'chosen_option': decision['choice'],
                'hedonic_prediction': decision['predicted_outcome']['hedonic_value'],
                'actual_hedonic': outcome['hedonic_value'],
                'actual_emotion': EmotionState[outcome['primary_emotion']],
                'regret': raw_data.get('regret_data', {}).get('intensity', 0.0)
            }
        }
    
    async def train_integrated_system(self) -> Dict[str, Any]:
        """ÌÜµÌï© ÏãúÏä§ÌÖú ÌïôÏäµ"""
        self.logger.info("üöÄ ÌÜµÌï© ÏãúÏä§ÌÖú ÌïôÏäµ ÏãúÏûë")
        
        start_time = time.time()
        best_loss = float('inf')
        patience_counter = 0
        
        # ÌïôÏäµ Î£®ÌîÑ
        for epoch in range(self.config.num_epochs):
            epoch_start = time.time()
            
            # GPU Î©îÎ™®Î¶¨ Î™®ÎãàÌÑ∞ÎßÅ
            if self.config.use_gpu:
                torch.cuda.empty_cache()
                initial_memory = torch.cuda.memory_allocated()
            
            # Î∞∞Ïπò ÌïôÏäµ
            train_loss, train_accuracy = await self.train_epoch(epoch)
            val_loss, val_accuracy = await self.validate_epoch(epoch)
            
            # GPU Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Í∏∞Î°ù
            if self.config.use_gpu:
                memory_used = torch.cuda.memory_allocated() - initial_memory
                self.training_history['gpu_memory'].append(memory_used / 1e6)  # MB
            
            epoch_time = time.time() - epoch_start
            
            # ÌûàÏä§ÌÜ†Î¶¨ Í∏∞Î°ù
            self.training_history['loss'].append(train_loss)
            self.training_history['accuracy'].append(train_accuracy)
            self.training_history['processing_time'].append(epoch_time)
            
            self.logger.info(
                f"Epoch {epoch+1}/{self.config.num_epochs} - "
                f"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, "
                f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, "
                f"Time: {epoch_time:.2f}s"
            )
            
            # Early stopping
            if val_loss < best_loss:
                best_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= self.config.early_stopping_patience:
                    self.logger.info(f"üõë Early stopping at epoch {epoch+1}")
                    break
        
        total_time = time.time() - start_time
        
        # ÌïôÏäµ Í≤∞Í≥º ÏöîÏïΩ
        results = {
            'total_training_time': total_time,
            'final_train_loss': train_loss,
            'final_train_accuracy': train_accuracy,
            'final_val_loss': val_loss,
            'final_val_accuracy': val_accuracy,
            'best_loss': best_loss,
            'epochs_completed': epoch + 1,
            'training_history': self.training_history
        }
        
        self.logger.info(f"‚úÖ ÌïôÏäµ ÏôÑÎ£å - Ï¥ù ÏãúÍ∞Ñ: {total_time:.2f}Ï¥à, ÏµúÏ¢Ö Ï†ïÌôïÎèÑ: {train_accuracy:.4f}")
        return results
    
    async def train_epoch(self, epoch: int) -> Tuple[float, float]:
        """ÏóêÌè¨ÌÅ¨ ÌïôÏäµ"""
        total_loss = 0.0
        total_accuracy = 0.0
        batch_count = 0
        
        # Î∞∞Ïπò Ï≤òÎ¶¨
        for i in range(0, len(self.training_data), self.config.batch_size):
            batch_data = self.training_data[i:i + self.config.batch_size]
            
            batch_loss, batch_acc = await self.process_batch(batch_data, training=True)
            
            total_loss += batch_loss
            total_accuracy += batch_acc
            batch_count += 1
        
        return total_loss / batch_count, total_accuracy / batch_count
    
    async def validate_epoch(self, epoch: int) -> Tuple[float, float]:
        """ÏóêÌè¨ÌÅ¨ Í≤ÄÏ¶ù"""
        if not self.validation_data:
            return 0.0, 0.0
        
        total_loss = 0.0
        total_accuracy = 0.0
        batch_count = 0
        
        # Í≤ÄÏ¶ù Î∞∞Ïπò Ï≤òÎ¶¨
        for i in range(0, len(self.validation_data), self.config.batch_size):
            batch_data = self.validation_data[i:i + self.config.batch_size]
            
            batch_loss, batch_acc = await self.process_batch(batch_data, training=False)
            
            total_loss += batch_loss
            total_accuracy += batch_acc
            batch_count += 1
        
        return total_loss / batch_count, total_accuracy / batch_count
    
    async def process_batch(self, batch_data: List[Dict], training: bool = True) -> Tuple[float, float]:
        """Î∞∞Ïπò Ï≤òÎ¶¨"""
        batch_losses = []
        batch_accuracies = []
        
        for data_point in batch_data:
            try:
                # ÌÜµÌï© ÏãúÏä§ÌÖúÏúºÎ°ú ÏòàÏ∏° ÏàòÌñâ
                prediction = await self.run_integrated_prediction(data_point['input'])
                
                # ÏÜêÏã§ Í≥ÑÏÇ∞
                loss = self.calculate_loss(prediction, data_point['target'])
                
                # Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞
                accuracy = self.calculate_accuracy(prediction, data_point['target'])
                
                batch_losses.append(loss)
                batch_accuracies.append(accuracy)
                
                # ÌïôÏäµ Îã®Í≥ÑÏóêÏÑú ÏãúÏä§ÌÖú ÏóÖÎç∞Ïù¥Ìä∏
                if training:
                    await self.update_system_components(data_point, prediction)
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Î∞∞Ïπò Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {e}")
                continue
        
        return (
            np.mean(batch_losses) if batch_losses else 0.0,
            np.mean(batch_accuracies) if batch_accuracies else 0.0
        )
    
    async def run_integrated_prediction(self, input_data: Dict) -> Dict:
        """ÌÜµÌï© ÏãúÏä§ÌÖú ÏòàÏ∏° Ïã§Ìñâ"""
        
        # 1. Í∞êÏ†ï-Ïú§Î¶¨-ÌõÑÌöå ÌöåÎ°ú Ï≤òÎ¶¨
        circuit_context = CircuitDecisionContext(
            scenario_text=input_data['dilemma'].scenario,
            proposed_action="Ïú§Î¶¨Ï†Å ÏùòÏÇ¨Í≤∞Ï†ï",
            self_emotion=input_data['emotion'],
            stakeholders=[s.name for s in input_data['dilemma'].stakeholders]
        )
        
        circuit_result = await self.emotion_circuit.process_ethical_decision(circuit_context)
        
        # 2. Îã§Ï∞®Ïõê Ïú§Î¶¨ Î∂ÑÏÑù
        ethics_result = self.ethics_system.comprehensive_ethical_analysis(input_data['dilemma'])
        
        # 3. ÌçºÏßÄ Í∞êÏ†ï-Ïú§Î¶¨ Îß§Ìïë
        fuzzy_result = self.fuzzy_mapper.map_emotion_to_ethics(input_data['emotion'])
        
        # 4. ÏãúÍ≥ÑÏó¥ Ïù¥Î≤§Ìä∏ Îì±Î°ù Î∞è ÏòàÏ∏°
        temporal_event = TemporalEvent(
            event_id=f"decision_{int(time.time())}",
            timestamp=time.time(),
            event_type="ethical_decision",
            description=input_data['dilemma'].scenario,
            emotion_state=input_data['emotion']
        )
        self.temporal_analyzer.register_event(temporal_event)
        
        # ÌÜµÌï© ÏòàÏ∏° Í≤∞Í≥º
        prediction = {
            'ethical_score': circuit_result.final_ethical_score,
            'confidence': circuit_result.confidence,
            'ethics_weights': ethics_result.school_reasonings,
            'fuzzy_mapping': fuzzy_result.ethics_weights,
            'predicted_regret': circuit_result.predicted_regret.get('anticipated_regret', 0.0),
            'temporal_prediction': 0.5  # Í∞ÑÎã®ÌôîÎêú ÏòàÏ∏°Í∞í
        }
        
        return prediction
    
    def calculate_loss(self, prediction: Dict, target: Dict) -> float:
        """ÏÜêÏã§ Í≥ÑÏÇ∞"""
        # Îã§Ï§ë Î™©Ìëú ÏÜêÏã§ Ìï®Ïàò
        ethical_score_loss = abs(prediction['ethical_score'] - target['actual_hedonic'])
        regret_loss = abs(prediction['predicted_regret'] - target['regret'])
        
        # Í∞ÄÏ§ë ÌèâÍ∑†
        total_loss = ethical_score_loss * 0.7 + regret_loss * 0.3
        return total_loss
    
    def calculate_accuracy(self, prediction: Dict, target: Dict) -> float:
        """Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞"""
        # Ïú§Î¶¨Ï†Å Ï†êÏàò ÏòàÏ∏° Ï†ïÌôïÎèÑ
        score_accuracy = 1.0 - abs(prediction['ethical_score'] - target['actual_hedonic'])
        
        # ÌõÑÌöå ÏòàÏ∏° Ï†ïÌôïÎèÑ
        regret_accuracy = 1.0 - abs(prediction['predicted_regret'] - target['regret'])
        
        # ÌèâÍ∑† Ï†ïÌôïÎèÑ
        return (score_accuracy + regret_accuracy) / 2.0
    
    async def update_system_components(self, data_point: Dict, prediction: Dict):
        """ÏãúÏä§ÌÖú Ïª¥Ìè¨ÎÑåÌä∏ ÏóÖÎç∞Ïù¥Ìä∏"""
        try:
            # Ïú§Î¶¨ Ï†ïÏ±Ö ÏóÖÎç∞Ïù¥Ìä∏
            experience = EthicsExperience(
                experience_id=f"exp_{int(time.time())}",
                scenario=data_point['input']['dilemma'].scenario,
                decision_made=data_point['target']['chosen_option'],
                outcome_rating=data_point['target']['actual_hedonic'],
                emotion_state=data_point['input']['emotion'],
                stakeholders=[s.name for s in data_point['input']['dilemma'].stakeholders],
                cultural_context="korean",
                decision_urgency=0.5,
                actual_regret=data_point['target']['regret'],
                user_satisfaction=0.7,
                moral_correctness=0.8
            )
            
            self.policy_updater.add_experience(experience)
            
            # XAI ÌîºÎìúÎ∞± ÌÜµÌï©
            xai_interpretation = XAIInterpretation(
                interpretation_id=f"interp_{int(time.time())}",
                decision_id=f"decision_{int(time.time())}",
                feature_importance={
                    'ethical_score': prediction['ethical_score'],
                    'confidence': prediction['confidence']
                },
                explanation_confidence=prediction['confidence']
            )
            
            # ÏãúÏä§ÌÖú Ïª¥Ìè¨ÎÑåÌä∏Îì§
            system_components = {
                'emotion_circuit': self.emotion_circuit,
                'policy_updater': self.policy_updater,
                'fuzzy_mapper': self.fuzzy_mapper
            }
            
            self.xai_integrator.integrate_xai_feedback(xai_interpretation, system_components)
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Ïª¥Ìè¨ÎÑåÌä∏ ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {e}")
    
    def get_system_analytics(self) -> Dict[str, Any]:
        """ÏãúÏä§ÌÖú Î∂ÑÏÑù Ï†ïÎ≥¥"""
        return {
            'emotion_circuit_status': self.emotion_circuit.get_circuit_status(),
            'policy_analytics': self.policy_updater.get_analytics(),
            'phase_analytics': self.phase_controller.get_analytics(),
            'xai_analytics': self.xai_integrator.get_feedback_analytics(),
            'fuzzy_analytics': self.fuzzy_mapper.get_mapping_analytics(),
            'ethics_analytics': self.ethics_system.get_ethics_analytics(),
            'temporal_analytics': self.temporal_analyzer.get_analytics_dashboard(),
            'training_history': self.training_history
        }

async def run_learning_test():
    """ÌïôÏäµ ÌÖåÏä§Ìä∏ Ïã§Ìñâ"""
    logger.info("üéØ ÌÜµÌï© Red Heart ÏãúÏä§ÌÖú GPU ÌïôÏäµ ÌÖåÏä§Ìä∏ ÏãúÏûë")
    
    # ÏÑ§Ï†ï
    config = LearningConfig(
        batch_size=4,  # Îç∞Ïù¥ÌÑ∞Í∞Ä Ï†ÅÏúºÎØÄÎ°ú ÏûëÏùÄ Î∞∞Ïπò
        learning_rate=0.001,
        num_epochs=10,
        use_gpu=ADVANCED_CONFIG.get('enable_gpu', False)
    )
    
    # ÌïôÏäµ ÌîÑÎ†àÏûÑÏõåÌÅ¨ Ï¥àÍ∏∞Ìôî
    framework = IntegratedLearningFramework(config)
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    data_count = framework.load_training_data()
    if data_count == 0:
        logger.error("‚ùå ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
        return
    
    logger.info(f"üìä Î°úÎìúÎêú Îç∞Ïù¥ÌÑ∞: {data_count}Í∞ú")
    
    # ÌïôÏäµ Ïã§Ìñâ
    results = await framework.train_integrated_system()
    
    # Í≤∞Í≥º Ï∂úÎ†•
    logger.info("üìà ÌïôÏäµ Í≤∞Í≥º:")
    logger.info(f"- Ï¥ù ÌïôÏäµ ÏãúÍ∞Ñ: {results['total_training_time']:.2f}Ï¥à")
    logger.info(f"- ÏµúÏ¢Ö Ï†ïÌôïÎèÑ: {results['final_train_accuracy']:.4f}")
    logger.info(f"- ÏµúÏ¢Ö ÏÜêÏã§: {results['final_train_loss']:.4f}")
    logger.info(f"- ÏôÑÎ£åÎêú ÏóêÌè¨ÌÅ¨: {results['epochs_completed']}")
    
    # ÏãúÏä§ÌÖú Î∂ÑÏÑù
    analytics = framework.get_system_analytics()
    logger.info("üîç ÏãúÏä§ÌÖú Î∂ÑÏÑù:")
    
    for component, data in analytics.items():
        if isinstance(data, dict) and 'total_decisions' in data:
            logger.info(f"- {component}: {data.get('total_decisions', 0)}Í∞ú Í≤∞Ï†ï")
        elif isinstance(data, dict) and 'total_mappings' in data:
            logger.info(f"- {component}: {data.get('total_mappings', 0)}Í∞ú Îß§Ìïë")
    
    # GPU ÏÇ¨Ïö©Îüâ Ï†ïÎ≥¥
    if config.use_gpu and torch.cuda.is_available():
        max_memory = max(results['training_history']['gpu_memory']) if results['training_history']['gpu_memory'] else 0
        logger.info(f"üî• ÏµúÎåÄ GPU Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ: {max_memory:.1f} MB")
        logger.info(f"‚ö° GPU ÌôúÏö©Î•†: ÎÜíÏùå" if max_memory > 100 else "‚ö° GPU ÌôúÏö©Î•†: Î≥¥ÌÜµ")
    
    return results, analytics

if __name__ == "__main__":
    # ÎπÑÎèôÍ∏∞ Ïã§Ìñâ
    results = asyncio.run(run_learning_test())