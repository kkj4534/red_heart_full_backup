#!/usr/bin/env python3
"""
ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì•ˆì „ì¥ì¹˜
Real-time Memory Monitoring Safety Guard
"""

import psutil
import torch
import time
import threading
import logging
from typing import Dict, Optional, Callable

class MemoryMonitor:
    """ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ì•ˆì „ì¥ì¹˜"""
    
    def __init__(self, max_memory_gb: float = 70.0, warning_threshold: float = 0.9, critical_threshold: float = 0.95):
        self.max_memory_gb = max_memory_gb
        self.max_memory_bytes = max_memory_gb * 1024**3
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
        
        self.monitoring = False
        self.monitor_thread = None
        self.callback_functions = []
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def add_callback(self, callback: Callable[[Dict], None]):
        """ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì´ˆê³¼ì‹œ í˜¸ì¶œí•  ì½œë°± í•¨ìˆ˜ ì¶”ê°€"""
        self.callback_functions.append(callback)
    
    def get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (GB ë‹¨ìœ„)"""
        # ì‹œìŠ¤í…œ RAM
        memory = psutil.virtual_memory()
        ram_used_gb = memory.used / 1024**3
        ram_total_gb = memory.total / 1024**3
        ram_percent = memory.percent
        
        # GPU ë©”ëª¨ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
        gpu_used_gb = 0.0
        gpu_total_gb = 0.0
        gpu_percent = 0.0
        
        if torch.cuda.is_available():
            gpu_used_bytes = torch.cuda.memory_allocated()
            gpu_total_bytes = torch.cuda.get_device_properties(0).total_memory
            gpu_used_gb = gpu_used_bytes / 1024**3
            gpu_total_gb = gpu_total_bytes / 1024**3
            gpu_percent = (gpu_used_gb / gpu_total_gb) * 100 if gpu_total_gb > 0 else 0
        
        return {
            'ram_used_gb': ram_used_gb,
            'ram_total_gb': ram_total_gb,
            'ram_percent': ram_percent,
            'gpu_used_gb': gpu_used_gb,
            'gpu_total_gb': gpu_total_gb,
            'gpu_percent': gpu_percent,
            'total_used_gb': ram_used_gb + gpu_used_gb,
            'budget_used_percent': (ram_used_gb / self.max_memory_gb) * 100
        }
    
    def check_memory_safety(self) -> Dict[str, any]:
        """ë©”ëª¨ë¦¬ ì•ˆì „ì„± ê²€ì‚¬"""
        usage = self.get_memory_usage()
        
        status = {
            'safe': True,
            'level': 'safe',
            'message': 'Memory usage within safe limits',
            'usage': usage
        }
        
        budget_usage = usage['budget_used_percent'] / 100
        
        if budget_usage >= self.critical_threshold:
            status.update({
                'safe': False,
                'level': 'critical',
                'message': f"CRITICAL: Memory usage {budget_usage:.1%} exceeds critical threshold {self.critical_threshold:.1%}"
            })
        elif budget_usage >= self.warning_threshold:
            status.update({
                'safe': True,
                'level': 'warning',
                'message': f"WARNING: Memory usage {budget_usage:.1%} exceeds warning threshold {self.warning_threshold:.1%}"
            })
        
        return status
    
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring:
            try:
                status = self.check_memory_safety()
                
                if status['level'] in ['warning', 'critical']:
                    self.logger.warning(status['message'])
                    usage = status['usage']
                    self.logger.info(f"RAM: {usage['ram_used_gb']:.2f}/{usage['ram_total_gb']:.2f}GB ({usage['ram_percent']:.1f}%)")
                    
                    if torch.cuda.is_available():
                        self.logger.info(f"GPU: {usage['gpu_used_gb']:.2f}/{usage['gpu_total_gb']:.2f}GB ({usage['gpu_percent']:.1f}%)")
                    
                    # ì½œë°± í•¨ìˆ˜ë“¤ í˜¸ì¶œ
                    for callback in self.callback_functions:
                        try:
                            callback(status)
                        except Exception as e:
                            self.logger.error(f"Callback error: {e}")
                
                time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ê²€ì‚¬
                
            except Exception as e:
                self.logger.error(f"Monitor loop error: {e}")
                time.sleep(5)
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.monitoring:
            self.logger.warning("Monitoring already started")
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        
        self.logger.info(f"ğŸ” Memory monitoring started (Budget: {self.max_memory_gb}GB, Warning: {self.warning_threshold:.1%}, Critical: {self.critical_threshold:.1%})")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2)
        
        self.logger.info("ğŸ” Memory monitoring stopped")
    
    def emergency_cleanup(self):
        """ì‘ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        self.logger.warning("ğŸš¨ Emergency memory cleanup initiated!")
        
        # PyTorch ìºì‹œ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            self.logger.info("âœ… GPU cache cleared")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        import gc
        gc.collect()
        self.logger.info("âœ… Garbage collection completed")
        
        # ì •ë¦¬ í›„ ìƒíƒœ í™•ì¸
        usage = self.get_memory_usage()
        self.logger.info(f"ğŸ“Š After cleanup - RAM: {usage['ram_used_gb']:.2f}GB, Budget usage: {usage['budget_used_percent']:.1f}%")

def create_memory_guard(max_memory_gb: float = 70.0) -> MemoryMonitor:
    """ë©”ëª¨ë¦¬ ê°€ë“œ ìƒì„± ë° ê¸°ë³¸ ì½œë°± ì„¤ì •"""
    monitor = MemoryMonitor(max_memory_gb)
    
    def emergency_callback(status):
        """ì‘ê¸‰ ìƒí™© ì½œë°±"""
        if status['level'] == 'critical':
            monitor.emergency_cleanup()
            
            # ì—¬ì „íˆ ìœ„í—˜í•˜ë©´ ê²½ê³ 
            new_status = monitor.check_memory_safety()
            if new_status['level'] == 'critical':
                monitor.logger.error("ğŸ†˜ CRITICAL: Emergency cleanup failed to reduce memory usage!")
                monitor.logger.error("ğŸ›‘ Consider stopping training immediately to prevent system crash!")
    
    monitor.add_callback(emergency_callback)
    return monitor

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("ğŸ§ª Memory Monitor Test")
    monitor = create_memory_guard(70.0)
    monitor.start_monitoring()
    
    try:
        # 5ì´ˆê°„ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸
        time.sleep(5)
        status = monitor.check_memory_safety()
        print(f"ğŸ“Š Current status: {status['level']} - {status['message']}")
        print(f"ğŸ“ˆ Memory usage: {status['usage']}")
    finally:
        monitor.stop_monitoring()