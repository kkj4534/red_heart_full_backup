#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Simple Learning Test Script
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import time
import math
import random
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('SimpleLearningTest')

def check_dependencies():
    """ì˜ì¡´ì„± í™•ì¸"""
    dependencies = {
        'builtin_only': True,
        'torch': False,
        'transformers': False,
        'sklearn': False
    }
    
    logger.info("âœ… Python ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš© (fallback ëª¨ë“œ)")
    
    try:
        import torch
        dependencies['torch'] = True
        logger.info(f"âœ… PyTorch {torch.__version__} ì‚¬ìš© ê°€ëŠ¥")
    except ImportError:
        logger.warning("âŒ PyTorch ì‚¬ìš© ë¶ˆê°€")
    
    try:
        import transformers
        dependencies['transformers'] = True
        logger.info(f"âœ… Transformers {transformers.__version__} ì‚¬ìš© ê°€ëŠ¥")
    except ImportError:
        logger.warning("âŒ Transformers ì‚¬ìš© ë¶ˆê°€")
    
    try:
        import sklearn
        dependencies['sklearn'] = True
        logger.info(f"âœ… Scikit-learn {sklearn.__version__} ì‚¬ìš© ê°€ëŠ¥")
    except ImportError:
        logger.warning("âŒ Scikit-learn ì‚¬ìš© ë¶ˆê°€")
    
    return dependencies

def load_dataset_sample(dataset_path: Path, max_samples: int = 5) -> List[Dict[str, Any]]:
    """ë°ì´í„°ì…‹ ìƒ˜í”Œ ë¡œë“œ"""
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        scenarios = data.get('scenarios', [])
        sample_scenarios = scenarios[:max_samples]
        
        logger.info(f"âœ… {dataset_path.name}ì—ì„œ {len(sample_scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ")
        return sample_scenarios
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨ {dataset_path}: {e}")
        return []

def extract_features_simple(scenario: Dict[str, Any]) -> Dict[str, float]:
    """ê°„ë‹¨í•œ íŠ¹ì§• ì¶”ì¶œ"""
    features = {}
    
    # í…ìŠ¤íŠ¸ ê¸°ë³¸ íŠ¹ì§•
    description = scenario.get('description', '')
    features['text_length'] = len(description)
    features['word_count'] = len(description.split())
    features['exclamation_count'] = description.count('!')
    features['question_count'] = description.count('?')
    
    # ê°ì • íŠ¹ì§•
    emotions = scenario.get('context', {}).get('emotions', {})
    for emotion, value in emotions.items():
        features[f'emotion_{emotion.lower()}'] = float(value)
    
    # ê¸°íƒ€ íŠ¹ì§•
    stakeholders = scenario.get('context', {}).get('stakeholders', [])
    features['stakeholder_count'] = len(stakeholders)
    
    return features

def simple_emotion_analysis(text: str) -> Dict[str, float]:
    """ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
    
    # ê¸°ë³¸ í•œêµ­ì–´ ê°ì • í‚¤ì›Œë“œ
    emotion_keywords = {
        'joy': ['ê¸°ì˜', 'í–‰ë³µ', 'ì¦ê±°', 'ì¢‹', 'ì›ƒ', 'ë¯¸ì†Œ', 'ê¸°ë»'],
        'sadness': ['ìŠ¬í”„', 'ìš°ìš¸', 'ëˆˆë¬¼', 'ì•„í”„', 'ê´´ë¡œ', 'ìŠ¬í””'],
        'anger': ['í™”ë‚˜', 'ë¶„ë…¸', 'ì§œì¦', 'ì–µìš¸', 'í™”ê°€', 'ì—´ë°›'],
        'fear': ['ë¬´ì„œ', 'ë‘ë ¤', 'ê±±ì •', 'ë¶ˆì•ˆ', 'ë–¨ë¦¬', 'ê²ë‚˜'],
        'surprise': ['ë†€ë¼', 'ê¹œì§', 'ì–´ë¨¸', 'í—‰', 'ì™€', 'ì–´?'],
        'disgust': ['ì—­ê²¨', 'ì‹«ì–´', 'ì§€ê²¨', 'í˜ì˜¤', 'êµ¬ì—­']
    }
    
    text_lower = text.lower()
    emotion_scores = {}
    
    for emotion, keywords in emotion_keywords.items():
        score = 0
        for keyword in keywords:
            score += text_lower.count(keyword)
        
        # ì •ê·œí™” (0-1 ë²”ìœ„)
        emotion_scores[emotion] = min(score / 10, 1.0)
    
    return emotion_scores

def simple_regret_analysis(scenario: Dict[str, Any]) -> float:
    """ê°„ë‹¨í•œ í›„íšŒ ë¶„ì„"""
    regret_keywords = ['í›„íšŒ', 'ì•„ì‰½', 'ì‹¤ìˆ˜', 'ì˜ëª»', 'ë¯¸ì•ˆ', 'ì£„ì†¡']
    
    text = scenario.get('description', '') + ' ' + scenario.get('context', {}).get('regret_info', '')
    text_lower = text.lower()
    
    regret_score = 0
    for keyword in regret_keywords:
        regret_score += text_lower.count(keyword)
    
    return min(regret_score / 5, 1.0)

def simple_surd_analysis(features: Dict[str, float]) -> Dict[str, float]:
    """ê°„ë‹¨í•œ SURD ë¶„ì„ (ìƒê´€ê´€ê³„ ê¸°ë°˜)"""
    
    # íŠ¹ì§•ë“¤ì„ ë°°ì—´ë¡œ ë³€í™˜
    feature_values = list(features.values())
    
    if len(feature_values) < 2:
        return {'synergy': 0.0, 'redundancy': 0.0, 'unique': 0.5}
    
    # ê°„ë‹¨í•œ ìƒê´€ê´€ê³„ ê³„ì‚° (numpy ì—†ì´)
    mean_val = sum(feature_values) / len(feature_values)
    variance = sum((x - mean_val) ** 2 for x in feature_values) / len(feature_values)
    std_val = math.sqrt(variance)
    
    # SURD ì§€í‘œ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    synergy = min(std_val / (mean_val + 0.001), 1.0)
    redundancy = max(0, 1 - synergy)
    unique = abs(mean_val - 0.5)
    
    return {
        'synergy': synergy,
        'redundancy': redundancy,
        'unique': unique
    }

class SimpleLearningSystem:
    """ê°„ë‹¨í•œ í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.emotion_weights = [random.random() * 0.1 + 0.5 for _ in range(6)]
        self.regret_weights = [random.random() * 0.1 + 0.5 for _ in range(3)]
        self.learning_rate = 0.01
        self.training_history = []
    
    def predict_emotion(self, features: Dict[str, float]) -> Dict[str, float]:
        """ê°ì • ì˜ˆì¸¡"""
        # ë‹¨ìˆœí•œ ì„ í˜• ê²°í•©
        emotion_features = [v for k, v in features.items() if k.startswith('emotion_')]
        
        if len(emotion_features) >= 3:
            prediction = sum(a * b for a, b in zip(emotion_features[:3], self.emotion_weights[:3]))
        else:
            feature_values = list(features.values())[:3]
            prediction = sum(feature_values) / len(feature_values) if feature_values else 0.5
        
        return {
            'predicted_valence': math.tanh(prediction),
            'confidence': min(abs(prediction), 1.0)
        }
    
    def predict_regret(self, features: Dict[str, float]) -> float:
        """í›„íšŒ ì˜ˆì¸¡"""
        text_features = [
            features.get('text_length', 0) / 1000,
            features.get('word_count', 0) / 100,
            features.get('stakeholder_count', 0) / 10
        ]
        
        prediction = sum(a * b for a, b in zip(text_features, self.regret_weights))
        return 1 / (1 + math.exp(-prediction))  # sigmoid function
    
    def train_step(self, features: Dict[str, float], target_emotion: float, target_regret: float):
        """ë‹¨ì¼ í›ˆë ¨ ìŠ¤í…"""
        
        # ì˜ˆì¸¡
        emotion_pred = self.predict_emotion(features)
        regret_pred = self.predict_regret(features)
        
        # ì˜¤ì°¨ ê³„ì‚°
        emotion_error = target_emotion - emotion_pred['predicted_valence']
        regret_error = target_regret - regret_pred
        
        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ê°„ë‹¨í•œ ê·¸ë˜ë””ì–¸íŠ¸)
        for i in range(len(self.emotion_weights)):
            self.emotion_weights[i] += self.learning_rate * emotion_error * 0.1
        for i in range(len(self.regret_weights)):
            self.regret_weights[i] += self.learning_rate * regret_error * 0.1
        
        # ì´ë ¥ ì €ì¥
        self.training_history.append({
            'emotion_error': abs(emotion_error),
            'regret_error': abs(regret_error),
            'emotion_weights': self.emotion_weights[:],  # copy list
            'regret_weights': self.regret_weights[:]     # copy list
        })
    
    def get_training_stats(self) -> Dict[str, Any]:
        """í›ˆë ¨ í†µê³„"""
        if not self.training_history:
            return {}
        
        emotion_errors = [h['emotion_error'] for h in self.training_history]
        regret_errors = [h['regret_error'] for h in self.training_history]
        
        return {
            'total_steps': len(self.training_history),
            'avg_emotion_error': sum(emotion_errors) / len(emotion_errors),
            'avg_regret_error': sum(regret_errors) / len(regret_errors),
            'final_emotion_weights': self.emotion_weights,
            'final_regret_weights': self.regret_weights,
            'improvement': emotion_errors[0] - emotion_errors[-1] if len(emotion_errors) > 1 else 0
        }

def run_simple_learning_test():
    """ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("ğŸš€ ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # 1. ì˜ì¡´ì„± í™•ì¸
    logger.info("ğŸ“‹ 1. ì˜ì¡´ì„± í™•ì¸")
    deps = check_dependencies()
    
    # 2. ë°ì´í„°ì…‹ ë¡œë“œ
    logger.info("ğŸ“‚ 2. ë°ì´í„°ì…‹ ë¡œë“œ")
    datasets_dir = project_root / 'processed_datasets'
    
    # ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ í…ŒìŠ¤íŠ¸
    test_datasets = [
        datasets_dir / 'scruples' / 'scruples_batch_001_of_100_20250622_013432.json',
        datasets_dir / 'scruples' / 'scruples_batch_002_of_100_20250622_013432.json',
        datasets_dir / 'classic_literature' / 'classic_literature_batch_001_of_001_20250622_013442.json',
        datasets_dir / 'korean_cultural_scenarios.json'
    ]
    
    all_scenarios = []
    for dataset_path in test_datasets:
        scenarios = load_dataset_sample(dataset_path, 2)  # ê° ë°ì´í„°ì…‹ì—ì„œ 2ê°œì”©
        all_scenarios.extend(scenarios)
    
    if not all_scenarios:
        logger.error("âŒ ë¡œë“œí•  ìˆ˜ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    logger.info(f"âœ… ì´ {len(all_scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ ì™„ë£Œ")
    
    # 3. íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ì„
    logger.info("ğŸ”¬ 3. íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ì„")
    learning_system = SimpleLearningSystem()
    
    for i, scenario in enumerate(all_scenarios):
        logger.info(f"\n--- ì‹œë‚˜ë¦¬ì˜¤ {i+1}: {scenario.get('title', 'Unknown')} ---")
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = extract_features_simple(scenario)
        logger.info(f"ì¶”ì¶œëœ íŠ¹ì§•: {len(features)}ê°œ")
        
        # ê°ì • ë¶„ì„
        emotion_analysis = simple_emotion_analysis(scenario.get('description', ''))
        logger.info(f"ê°ì • ë¶„ì„: {emotion_analysis}")
        
        # í›„íšŒ ë¶„ì„
        regret_score = simple_regret_analysis(scenario)
        logger.info(f"í›„íšŒ ì ìˆ˜: {regret_score:.3f}")
        
        # SURD ë¶„ì„
        surd_analysis = simple_surd_analysis(features)
        logger.info(f"SURD ë¶„ì„: {surd_analysis}")
        
        # í•™ìŠµ (ì‹¤ì œ ê°’ì„ ëŒ€ëµì ìœ¼ë¡œ ì¶”ì •)
        target_emotion = list(emotion_analysis.values())[0] if emotion_analysis else 0.5
        target_regret = regret_score
        
        learning_system.train_step(features, target_emotion, target_regret)
        
        # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
        emotion_pred = learning_system.predict_emotion(features)
        regret_pred = learning_system.predict_regret(features)
        
        logger.info(f"ê°ì • ì˜ˆì¸¡: {emotion_pred}")
        logger.info(f"í›„íšŒ ì˜ˆì¸¡: {regret_pred:.3f}")
    
    # 4. í•™ìŠµ ê²°ê³¼
    logger.info("\nğŸ“Š 4. í•™ìŠµ ê²°ê³¼")
    training_stats = learning_system.get_training_stats()
    
    for key, value in training_stats.items():
        if isinstance(value, float):
            logger.info(f"{key}: {value:.4f}")
        elif isinstance(value, list) and len(value) < 10:
            logger.info(f"{key}: {[f'{v:.3f}' for v in value]}")
        else:
            logger.info(f"{key}: {value}")
    
    # 5. ê²°ê³¼ ì €ì¥
    logger.info("ğŸ’¾ 5. ê²°ê³¼ ì €ì¥")
    results = {
        'test_info': {
            'timestamp': datetime.now().isoformat(),
            'scenarios_processed': len(all_scenarios),
            'dependencies': deps
        },
        'training_stats': training_stats,
        'scenarios_analyzed': [
            {
                'title': s.get('title', 'Unknown'),
                'source': s.get('context', {}).get('source', 'unknown')
            } for s in all_scenarios
        ]
    }
    
    results_path = project_root / 'logs' / f'simple_learning_test_{int(time.time())}.json'
    results_path.parent.mkdir(exist_ok=True)
    
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_path}")
    logger.info("ğŸ‰ ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    return results

if __name__ == "__main__":
    try:
        results = run_simple_learning_test()
        print("\n" + "="*50)
        print("âœ… í•™ìŠµ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print("="*50)
    except Exception as e:
        print(f"\nâŒ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()