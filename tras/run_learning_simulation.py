#!/usr/bin/env python3
"""
Red Heart í•™ìŠµ ë° ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
Learning and Simulation Runner with Advanced Logging

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Red Heart ì‹œìŠ¤í…œì˜ í•™ìŠµ ê³¼ì •ì„ ì‹¤í–‰í•˜ê³ 
ìƒì„¸í•œ ë¡œê·¸ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import asyncio
import logging
import time
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

from main import (
    RedHeartSystem, AnalysisRequest, setup_advanced_logging,
    log_regret_progress, log_performance_metric, get_learning_logger
)
from advanced_hierarchical_emotion_system import (
    AdvancedHierarchicalEmotionSystem, EmotionVector, EmotionDimension
)
from advanced_regret_learning_system import (
    AdvancedRegretLearningSystem, RegretMemory, RegretType, LearningPhase
)
from advanced_bayesian_inference_module import (
    AdvancedBayesianInference, Evidence, BeliefType
)
from config import LOGS_DIR


class LearningSimulationRunner:
    """í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.logger = get_learning_logger("SimulationRunner")
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results = {
            'session_id': self.session_id,
            'start_time': datetime.now().isoformat(),
            'phases': [],
            'regret_timeline': [],
            'performance_metrics': {},
            'learning_statistics': {}
        }
        
        # ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ë“¤
        self.main_system = None
        self.emotion_system = None
        self.regret_system = None
        self.bayesian_system = None
        
        self.logger.info(f"í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì„¸ì…˜ ì‹œì‘: {self.session_id}", extra={'phase': 'INIT', 'regret': 0.0})
    
    async def initialize_systems(self):
        """ì‹œìŠ¤í…œë“¤ ì´ˆê¸°í™”"""
        self.logger.info("=== ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë‹¨ê³„ ===", extra={'phase': 'INIT', 'regret': 0.0})
        
        # ë©”ì¸ ì‹œìŠ¤í…œ
        self.logger.info("ë©”ì¸ Red Heart ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...", extra={'phase': 'INIT', 'regret': 0.0})
        self.main_system = RedHeartSystem()
        await self.main_system.initialize()
        
        # ê°œë³„ ê³ ê¸‰ ì‹œìŠ¤í…œë“¤ (ë” ìƒì„¸í•œ ì œì–´ë¥¼ ìœ„í•´)
        self.logger.info("ê³„ì¸µì  ê°ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...", extra={'phase': 'INIT', 'regret': 0.0})
        self.emotion_system = AdvancedHierarchicalEmotionSystem()
        
        self.logger.info("í›„íšŒ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...", extra={'phase': 'INIT', 'regret': 0.0})
        self.regret_system = AdvancedRegretLearningSystem()
        
        self.logger.info("ë² ì´ì§€ì•ˆ ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...", extra={'phase': 'INIT', 'regret': 0.0})
        self.bayesian_system = AdvancedBayesianInference()
        
        self.logger.info("ëª¨ë“  ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ", extra={'phase': 'INIT', 'regret': 0.0})
        log_performance_metric("Initialization", "systems_ready", 1.0, "ì „ì²´ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
    
    async def run_hierarchical_emotion_learning(self, num_episodes: int = 150):
        """ê³„ì¸µì  ê°ì • í•™ìŠµ ì‹¤í–‰"""
        self.logger.info(f"=== ê³„ì¸µì  ê°ì • í•™ìŠµ ì‹œì‘ ({num_episodes}íšŒ) ===", extra={'phase': 'EMOTION_LEARNING', 'regret': 0.0})
        
        # í…ŒìŠ¤íŠ¸ìš© ë¬¸í•™ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        literary_scenarios = self._generate_literary_scenarios()
        
        phase_timeline = []
        
        for episode in range(num_episodes):
            scenario = literary_scenarios[episode % len(literary_scenarios)]
            
            # í•™ìŠµ ìˆ˜í–‰
            result = await self.emotion_system.process_literary_emotion_sequence(
                [scenario], time_series_mode=True
            )
            
            # í˜„ì¬ í˜ì´ì¦ˆ ìƒíƒœ í™•ì¸
            current_phase = "PHASE_0"  # ê¸°ë³¸ê°’
            
            # ê²°ê³¼ ë¡œê¹…
            if result.get('learning_summary'):
                summary = result['learning_summary']
                
                # í›„íšŒ ìˆ˜ì¤€ ê³„ì‚° (Phase 1 í•™ìŠµì´ ìˆëŠ” ê²½ìš°)
                regret_level = 0.0
                if result.get('phase1_learnings'):
                    regret_levels = [l.regret_intensity for l in result['phase1_learnings']]
                    regret_level = sum(regret_levels) / len(regret_levels) if regret_levels else 0.0
                
                # ìƒì„¸ ë¡œê¹…
                log_regret_progress(
                    current_phase, 
                    regret_level,
                    f"ì—í”¼ì†Œë“œ {episode+1}: ê°ì • í•™ìŠµ ì™„ë£Œ",
                    episode=episode+1,
                    scenario_type=scenario.get('genre', 'unknown'),
                    calibrations=summary.get('total_calibrations', 0),
                    empathy_learnings=summary.get('total_empathy_learnings', 0)
                )
                
                # íƒ€ì„ë¼ì¸ì— ì¶”ê°€
                phase_timeline.append({
                    'episode': episode + 1,
                    'phase': current_phase,
                    'regret_level': regret_level,
                    'timestamp': datetime.now().isoformat(),
                    'scenario': scenario.get('genre', 'unknown'),
                    'metrics': summary
                })
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (episode + 1) % 10 == 0:
                    progress = (episode + 1) / num_episodes * 100
                    self.logger.info(f"ê°ì • í•™ìŠµ ì§„í–‰ë¥ : {progress:.1f}% ({episode+1}/{num_episodes})", extra={'phase': current_phase, 'regret': 0.0})
                    log_performance_metric(
                        "EmotionLearning", "progress_percent", progress,
                        f"ì—í”¼ì†Œë“œ {episode+1} ì™„ë£Œ"
                    )
        
        self.results['emotion_learning'] = {
            'episodes': num_episodes,
            'timeline': phase_timeline,
            'final_metrics': result.get('learning_summary', {})
        }
        
        self.logger.info(f"ê³„ì¸µì  ê°ì • í•™ìŠµ ì™„ë£Œ: {num_episodes}íšŒ ì—í”¼ì†Œë“œ", extra={'phase': 'EMOTION_COMPLETE', 'regret': 0.0})
        return phase_timeline
    
    async def run_regret_learning_simulation(self, num_scenarios: int = 120):
        """í›„íšŒ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰"""
        self.logger.info(f"=== í›„íšŒ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ({num_scenarios}íšŒ) ===", extra={'phase': 'REGRET_LEARNING', 'regret': 0.0})
        
        # í›„íšŒ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        regret_scenarios = self._generate_regret_scenarios()
        
        regret_timeline = []
        phase_transitions = []
        
        for scenario_idx in range(num_scenarios):
            scenario = regret_scenarios[scenario_idx % len(regret_scenarios)]
            
            # í›„íšŒ í•™ìŠµ ìˆ˜í–‰
            regret_memory = await self.regret_system.process_regret(
                situation=scenario['situation'],
                outcome=scenario['outcome'],
                alternatives=scenario['alternatives'],
                literary_context=scenario.get('literary_context')
            )
            
            # í˜„ì¬ í•™ìŠµ ìƒíƒœ í™•ì¸
            current_phase = self.regret_system.learning_state.current_phase.name
            regret_level = regret_memory.intensity
            
            # í˜ì´ì¦ˆ ì „í™˜ ì²´í¬
            if len(self.regret_system.learning_state.phase_transitions) > len(phase_transitions):
                new_transition = self.regret_system.learning_state.phase_transitions[-1]
                phase_transitions.append({
                    'from_phase': new_transition.from_phase.name,
                    'to_phase': new_transition.to_phase.name,
                    'scenario_number': scenario_idx + 1,
                    'trigger': new_transition.trigger_condition,
                    'metrics': new_transition.metrics_at_transition,
                    'timestamp': new_transition.transition_time.isoformat()
                })
                
                self.logger.info(f"ğŸ”„ í˜ì´ì¦ˆ ì „í™˜ ë°œìƒ: {new_transition.from_phase.name} â†’ {new_transition.to_phase.name}", extra={'phase': new_transition.to_phase.name, 'regret': current_regret})
                log_regret_progress(
                    new_transition.to_phase.name,
                    regret_level,
                    f"í˜ì´ì¦ˆ ì „í™˜: {new_transition.from_phase.name} â†’ {new_transition.to_phase.name}",
                    transition_trigger=new_transition.trigger_condition,
                    scenario_number=scenario_idx + 1
                )
            
            # ìƒì„¸ ë¡œê¹…
            log_regret_progress(
                current_phase,
                regret_level,
                f"ì‹œë‚˜ë¦¬ì˜¤ {scenario_idx+1}: {regret_memory.regret_type.value} í›„íšŒ í•™ìŠµ",
                scenario_number=scenario_idx + 1,
                regret_type=regret_memory.regret_type.value,
                learning_phase=current_phase,
                confidence=regret_memory.confidence_level if hasattr(regret_memory, 'confidence_level') else 0.5
            )
            
            # íƒ€ì„ë¼ì¸ì— ì¶”ê°€
            regret_timeline.append({
                'scenario': scenario_idx + 1,
                'phase': current_phase,
                'regret_type': regret_memory.regret_type.value,
                'regret_intensity': regret_level,
                'timestamp': datetime.now().isoformat(),
                'situation_type': scenario['situation'].get('type', 'unknown')
            })
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if (scenario_idx + 1) % 10 == 0:
                progress = (scenario_idx + 1) / num_scenarios * 100
                self.logger.info(f"í›„íšŒ í•™ìŠµ ì§„í–‰ë¥ : {progress:.1f}% ({scenario_idx+1}/{num_scenarios})", extra={'phase': current_phase.name if hasattr(current_phase, 'name') else 'REGRET_LEARNING', 'regret': current_regret})
                log_performance_metric(
                    "RegretLearning", "progress_percent", progress,
                    f"ì‹œë‚˜ë¦¬ì˜¤ {scenario_idx+1} ì™„ë£Œ, í˜„ì¬ í˜ì´ì¦ˆ: {current_phase}"
                )
        
        # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        final_report = await self.regret_system.generate_regret_report()
        
        self.results['regret_learning'] = {
            'scenarios': num_scenarios,
            'timeline': regret_timeline,
            'phase_transitions': phase_transitions,
            'final_report': final_report
        }
        
        self.logger.info(f"í›„íšŒ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ: {num_scenarios}íšŒ ì‹œë‚˜ë¦¬ì˜¤, {len(phase_transitions)}íšŒ í˜ì´ì¦ˆ ì „í™˜", extra={'phase': 'REGRET_COMPLETE', 'regret': 0.0})
        return regret_timeline, phase_transitions
    
    async def run_bayesian_inference_learning(self, num_inferences: int = 80):
        """ë² ì´ì§€ì•ˆ ì¶”ë¡  í•™ìŠµ ì‹¤í–‰"""
        self.logger.info(f"=== ë² ì´ì§€ì•ˆ ì¶”ë¡  í•™ìŠµ ì‹œì‘ ({num_inferences}íšŒ) ===", extra={'phase': 'BAYESIAN_LEARNING', 'regret': 0.0})
        
        # ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        inference_scenarios = self._generate_inference_scenarios()
        
        inference_timeline = []
        
        for inference_idx in range(num_inferences):
            scenario = inference_scenarios[inference_idx % len(inference_scenarios)]
            
            # ì¦ê±° ì¶”ê°€
            for evidence_data in scenario['evidences']:
                evidence = Evidence(
                    node_id=evidence_data['node'],
                    value=evidence_data['value'],
                    strength=evidence_data['strength'],
                    source="simulation"
                )
                await self.bayesian_system.add_evidence(evidence)
            
            # ì¶”ë¡  ì‹¤í–‰
            result = await self.bayesian_system.infer(
                query_node=scenario['query_node'],
                given_evidence=scenario.get('given_evidence', {}),
                context=scenario.get('context', {})
            )
            
            # ê²°ê³¼ í•™ìŠµ
            if scenario.get('actual_outcome'):
                await self.bayesian_system.update_from_outcome(
                    scenario['query_node'],
                    max(result.posterior_distribution.items(), key=lambda x: x[1])[0],
                    scenario['actual_outcome'],
                    scenario.get('context', {})
                )
            
            # ë¡œê¹…
            uncertainty = result.uncertainty
            confidence = result.confidence
            
            log_performance_metric(
                "BayesianInference", "uncertainty", uncertainty,
                f"ì¶”ë¡  {inference_idx+1}: ë¶ˆí™•ì‹¤ì„± {uncertainty:.3f}"
            )
            
            # íƒ€ì„ë¼ì¸ì— ì¶”ê°€
            inference_timeline.append({
                'inference': inference_idx + 1,
                'query_node': scenario['query_node'],
                'uncertainty': uncertainty,
                'confidence': confidence,
                'posterior': result.posterior_distribution,
                'timestamp': datetime.now().isoformat()
            })
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if (inference_idx + 1) % 10 == 0:
                progress = (inference_idx + 1) / num_inferences * 100
                self.logger.info(f"ë² ì´ì§€ì•ˆ ì¶”ë¡  ì§„í–‰ë¥ : {progress:.1f}% ({inference_idx+1}/{num_inferences})", extra={'phase': 'BAYESIAN_LEARNING', 'regret': 0.0})
        
        self.results['bayesian_learning'] = {
            'inferences': num_inferences,
            'timeline': inference_timeline
        }
        
        self.logger.info(f"ë² ì´ì§€ì•ˆ ì¶”ë¡  í•™ìŠµ ì™„ë£Œ: {num_inferences}íšŒ ì¶”ë¡ ", extra={'phase': 'BAYESIAN_COMPLETE', 'regret': 0.0})
        return inference_timeline
    
    async def run_integrated_analysis(self, num_analyses: int = 50):
        """í†µí•© ë¶„ì„ ì‹¤í–‰"""
        self.logger.info(f"=== í†µí•© ë¶„ì„ ì‹¤í–‰ ì‹œì‘ ({num_analyses}íšŒ) ===", extra={'phase': 'INTEGRATION', 'regret': 0.0})
        
        # ë³µí•© ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        integrated_scenarios = self._generate_integrated_scenarios()
        
        analysis_timeline = []
        
        for analysis_idx in range(num_analyses):
            scenario = integrated_scenarios[analysis_idx % len(integrated_scenarios)]
            
            # í†µí•© ë¶„ì„ ì‹¤í–‰
            request = AnalysisRequest(
                text=scenario['text'],
                language=scenario.get('language', 'ko'),
                scenario_type=scenario.get('type', 'general'),
                additional_context=scenario.get('context', {})
            )
            
            start_time = time.time()
            result = await self.main_system.analyze_async(request)
            processing_time = time.time() - start_time
            
            # ì„±ëŠ¥ ë¡œê¹…
            log_performance_metric(
                "IntegratedAnalysis", "processing_time", processing_time,
                f"ë¶„ì„ {analysis_idx+1}: {processing_time:.3f}ì´ˆ"
            )
            
            log_performance_metric(
                "IntegratedAnalysis", "integrated_score", result.integrated_score,
                f"í†µí•© ì ìˆ˜: {result.integrated_score:.3f}"
            )
            
            # íƒ€ì„ë¼ì¸ì— ì¶”ê°€
            analysis_timeline.append({
                'analysis': analysis_idx + 1,
                'scenario_type': scenario.get('type', 'general'),
                'processing_time': processing_time,
                'integrated_score': result.integrated_score,
                'confidence': result.confidence,
                'components_used': len([c for c in [
                    result.emotion_analysis, result.bentham_analysis,
                    result.semantic_analysis, result.surd_analysis
                ] if c is not None]),
                'timestamp': datetime.now().isoformat()
            })
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if (analysis_idx + 1) % 10 == 0:
                progress = (analysis_idx + 1) / num_analyses * 100
                avg_time = sum(a['processing_time'] for a in analysis_timeline) / len(analysis_timeline)
                self.logger.info(f"í†µí•© ë¶„ì„ ì§„í–‰ë¥ : {progress:.1f}% ({analysis_idx+1}/{num_analyses}), í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_time:.3f}ì´ˆ", extra={'phase': 'INTEGRATION', 'regret': 0.0})
        
        self.results['integrated_analysis'] = {
            'analyses': num_analyses,
            'timeline': analysis_timeline
        }
        
        self.logger.info(f"í†µí•© ë¶„ì„ ì™„ë£Œ: {num_analyses}íšŒ ë¶„ì„", extra={'phase': 'INTEGRATION_COMPLETE', 'regret': 0.0})
        return analysis_timeline
    
    def _generate_literary_scenarios(self) -> List[Dict[str, Any]]:
        """ë¬¸í•™ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        return [
            {
                'character_emotion': {
                    'valence': -0.8, 'arousal': 0.7, 'dominance': -0.5,
                    'source': 'hamlet'
                },
                'reader_emotion': {
                    'valence': -0.5, 'arousal': 0.5, 'dominance': 0.0
                },
                'context': {
                    'situation_type': 'tragedy',
                    'cultural_context': 'western'
                },
                'genre': 'tragedy'
            },
            {
                'character_emotion': {
                    'valence': 0.3, 'arousal': 0.4, 'dominance': -0.2,
                    'source': 'chunhyang'
                },
                'reader_emotion': {
                    'valence': 0.5, 'arousal': 0.3, 'dominance': 0.1
                },
                'community_emotions': {
                    'reader1': {'valence': 0.5, 'arousal': 0.3},
                    'reader2': {'valence': 0.4, 'arousal': 0.4},
                    'reader3': {'valence': 0.6, 'arousal': 0.2}
                },
                'context': {
                    'situation_type': 'romance',
                    'cultural_context': 'korean_traditional'
                },
                'cultural_context': 'korean_traditional',
                'genre': 'romance'
            },
            {
                'character_emotion': {
                    'valence': 0.7, 'arousal': 0.8, 'dominance': 0.3,
                    'source': 'don_quixote'
                },
                'reader_emotion': {
                    'valence': 0.6, 'arousal': 0.6, 'dominance': 0.2
                },
                'context': {
                    'situation_type': 'comedy',
                    'cultural_context': 'western'
                },
                'genre': 'comedy'
            }
        ]
    
    def _generate_regret_scenarios(self) -> List[Dict[str, Any]]:
        """í›„íšŒ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        return [
            {
                'situation': {
                    'type': 'moral_dilemma',
                    'emotion_type': 'conflict',
                    'chosen_action': {'type': 'compromise'},
                    'is_critical': False
                },
                'outcome': {
                    'negativity': 0.6,
                    'actual_value': 0.3,
                    'empathy_failure': False
                },
                'alternatives': [
                    {'action': 'stand_firm', 'expected_value': 0.7},
                    {'action': 'full_cooperation', 'expected_value': 0.5}
                ],
                'literary_context': {
                    'source': 'les_miserables',
                    'genre': 'moral_literature'
                }
            },
            {
                'situation': {
                    'type': 'relationship_conflict',
                    'emotion_type': 'sadness',
                    'chosen_action': {'type': 'avoidance'},
                    'is_critical': True
                },
                'outcome': {
                    'negativity': 0.8,
                    'actual_value': 0.2,
                    'empathy_failure': True
                },
                'alternatives': [
                    {'action': 'direct_communication', 'expected_value': 0.8},
                    {'action': 'seek_mediation', 'expected_value': 0.6}
                ]
            },
            {
                'situation': {
                    'type': 'career_decision',
                    'emotion_type': 'anxiety',
                    'chosen_action': {'type': 'safe_choice'},
                    'is_critical': False
                },
                'outcome': {
                    'negativity': 0.4,
                    'actual_value': 0.6,
                    'prediction_failure': True
                },
                'alternatives': [
                    {'action': 'risk_taking', 'expected_value': 0.9},
                    {'action': 'wait_and_see', 'expected_value': 0.4}
                ]
            }
        ]
    
    def _generate_inference_scenarios(self) -> List[Dict[str, Any]]:
        """ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        return [
            {
                'query_node': 'action_tendency',
                'evidences': [
                    {'node': 'emotional_state', 'value': 'negative', 'strength': 0.8}
                ],
                'given_evidence': {'moral_judgment': 'right'},
                'context': {'literary_context': {'belief_type': 'tragic_fate', 'source': 'hamlet'}},
                'actual_outcome': 'avoid'
            },
            {
                'query_node': 'outcome_prediction',
                'evidences': [
                    {'node': 'emotional_state', 'value': 'positive', 'strength': 0.7},
                    {'node': 'social_approval', 'value': 'approve', 'strength': 0.6}
                ],
                'context': {'literary_context': {'belief_type': 'redemption', 'source': 'les_miserables'}},
                'actual_outcome': 'success'
            }
        ]
    
    def _generate_integrated_scenarios(self) -> List[Dict[str, Any]]:
        """í†µí•© ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        return [
            {
                'text': 'ì´ ê²°ì •ì€ ë§ì€ ì‚¬ëŒë“¤ì˜ ìƒëª…ê³¼ ì•ˆì „ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ë©°, ìš°ë¦¬ëŠ” ì •ì˜ë¡­ê³  ê³µì •í•œ ì„ íƒì„ í•´ì•¼ í•©ë‹ˆë‹¤.',
                'type': 'ethical_dilemma',
                'context': {'urgency': 'high', 'stakeholders': 'many'}
            },
            {
                'text': 'ìƒˆë¡œìš´ ê¸°ìˆ  ë„ì…ìœ¼ë¡œ íš¨ìœ¨ì„±ì€ ë†’ì•„ì§€ì§€ë§Œ ì¼ë¶€ ì§ì›ë“¤ì´ ì¼ìë¦¬ë¥¼ ìƒì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
                'type': 'technology_ethics',
                'context': {'impact': 'economic', 'timeframe': 'long_term'}
            },
            {
                'text': 'ê°œì¸ì •ë³´ ë³´í˜¸ì™€ ê³µìµì„ ìœ„í•œ ì •ë³´ ê³µê°œ ì‚¬ì´ì—ì„œ ê· í˜•ì ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.',
                'type': 'privacy_vs_public',
                'context': {'legal_framework': 'gdpr', 'public_interest': 'high'}
            },
            {
                'text': 'í™˜ê²½ ë³´í˜¸ë¥¼ ìœ„í•´ ê²½ì œ ì„±ì¥ì„ ì¼ë¶€ ì œí•œí•´ì•¼ í• ì§€ ê³ ë¯¼ì´ ë©ë‹ˆë‹¤.',
                'type': 'environment_vs_economy',
                'context': {'urgency': 'medium', 'global_impact': True}
            }
        ]
    
    async def generate_final_report(self):
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        self.logger.info("=== ìµœì¢… í•™ìŠµ ë¦¬í¬íŠ¸ ìƒì„± ===", extra={'phase': 'REPORT', 'regret': 0.0})
        
        self.results['end_time'] = datetime.now().isoformat()
        self.results['duration'] = (
            datetime.fromisoformat(self.results['end_time']) - 
            datetime.fromisoformat(self.results['start_time'])
        ).total_seconds()
        
        # ì¢…í•© í†µê³„ ê³„ì‚°
        total_episodes = (
            self.results.get('emotion_learning', {}).get('episodes', 0) +
            self.results.get('regret_learning', {}).get('scenarios', 0) +
            self.results.get('bayesian_learning', {}).get('inferences', 0) +
            self.results.get('integrated_analysis', {}).get('analyses', 0)
        )
        
        self.results['summary'] = {
            'total_learning_episodes': total_episodes,
            'phase_transitions': len(self.results.get('regret_learning', {}).get('phase_transitions', [])),
            'final_phase': (
                self.results.get('regret_learning', {})
                .get('phase_transitions', [{}])[-1]
                .get('to_phase', 'PHASE_0') if self.results.get('regret_learning', {}).get('phase_transitions') else 'PHASE_0'
            ),
            'avg_regret_final': self._calculate_average_final_regret(),
            'performance_improvement': self._calculate_performance_improvement()
        }
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        report_file = Path(LOGS_DIR) / f"learning_simulation_report_{self.session_id}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ìµœì¢… ë¦¬í¬íŠ¸ ì €ì¥: {report_file}", extra={'phase': 'REPORT', 'regret': 0.0})
        
        # ìš”ì•½ ì¶œë ¥
        self.logger.info("=== í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ ìš”ì•½ ===", extra={'phase': 'COMPLETE', 'regret': 0.0})
        self.logger.info(f"ì„¸ì…˜ ID: {self.session_id}", extra={'phase': 'COMPLETE', 'regret': 0.0})
        self.logger.info(f"ì´ í•™ìŠµ ì—í”¼ì†Œë“œ: {total_episodes}", extra={'phase': 'COMPLETE', 'regret': 0.0})
        self.logger.info(f"í˜ì´ì¦ˆ ì „í™˜ íšŸìˆ˜: {self.results['summary']['phase_transitions']}", extra={'phase': 'COMPLETE', 'regret': 0.0})
        self.logger.info(f"ìµœì¢… ë„ë‹¬ í˜ì´ì¦ˆ: {self.results['summary']['final_phase']}", extra={'phase': 'COMPLETE', 'regret': 0.0})
        self.logger.info(f"í‰ê·  ìµœì¢… í›„íšŒ ìˆ˜ì¤€: {self.results['summary']['avg_regret_final']:.3f}", extra={'phase': 'COMPLETE', 'regret': self.results['summary']['avg_regret_final']})
        self.logger.info(f"ì „ì²´ ì‹¤í–‰ ì‹œê°„: {self.results['duration']:.1f}ì´ˆ", extra={'phase': 'COMPLETE', 'regret': 0.0})
        
        return self.results
    
    def _calculate_average_final_regret(self) -> float:
        """ìµœì¢… í›„íšŒ ìˆ˜ì¤€ í‰ê·  ê³„ì‚°"""
        regret_timeline = self.results.get('regret_learning', {}).get('timeline', [])
        if not regret_timeline:
            return 0.0
        
        # ë§ˆì§€ë§‰ 20ê°œ ì—í”¼ì†Œë“œì˜ í‰ê· 
        recent_regrets = [r['regret_intensity'] for r in regret_timeline[-20:]]
        return sum(recent_regrets) / len(recent_regrets) if recent_regrets else 0.0
    
    def _calculate_performance_improvement(self) -> float:
        """ì„±ëŠ¥ ê°œì„ ë„ ê³„ì‚°"""
        regret_timeline = self.results.get('regret_learning', {}).get('timeline', [])
        if len(regret_timeline) < 20:
            return 0.0
        
        # ì´ˆê¸° 20ê°œì™€ ë§ˆì§€ë§‰ 20ê°œ ë¹„êµ
        initial_regrets = [r['regret_intensity'] for r in regret_timeline[:20]]
        final_regrets = [r['regret_intensity'] for r in regret_timeline[-20:]]
        
        initial_avg = sum(initial_regrets) / len(initial_regrets)
        final_avg = sum(final_regrets) / len(final_regrets)
        
        if initial_avg > 0:
            return (initial_avg - final_avg) / initial_avg
        return 0.0


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”´â¤ï¸ Red Heart í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘")
    print("=" * 60)
    
    # ê³ ê¸‰ ë¡œê¹… ì„¤ì •
    setup_advanced_logging()
    
    # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ê¸° ìƒì„±
    runner = LearningSimulationRunner()
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        await runner.initialize_systems()
        
        # í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        print("\nğŸ“š ê³„ì¸µì  ê°ì • í•™ìŠµ ì‹¤í–‰ ì¤‘...")
        await runner.run_hierarchical_emotion_learning(num_episodes=100)
        
        print("\nğŸ’­ í›„íšŒ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì¤‘...")
        await runner.run_regret_learning_simulation(num_scenarios=80)
        
        print("\nğŸ”® ë² ì´ì§€ì•ˆ ì¶”ë¡  í•™ìŠµ ì‹¤í–‰ ì¤‘...")
        await runner.run_bayesian_inference_learning(num_inferences=60)
        
        print("\nğŸ”„ í†µí•© ë¶„ì„ ì‹¤í–‰ ì¤‘...")
        await runner.run_integrated_analysis(num_analyses=40)
        
        # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        print("\nğŸ“Š ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        final_results = await runner.generate_final_report()
        
        print("\nâœ… í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: logs/learning_simulation_report_{runner.session_id}.json")
        print(f"ğŸ“ ìƒì„¸ ë¡œê·¸: logs/learning_progress_{runner.session_id}.log")
        
    except Exception as e:
        print(f"\nâŒ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        logging.exception("ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì‹¤íŒ¨")


if __name__ == "__main__":
    asyncio.run(main())