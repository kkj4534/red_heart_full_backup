"""
Red Heart AI í†µí•© ì‹œìŠ¤í…œ í›ˆë ¨ í…ŒìŠ¤íŠ¸
Integrated Training Test for Red Heart AI System

5ê°œ ìƒ˜í”Œì„ ì´ìš©í•œ ì „ì²´ ëª¨ë“ˆ í†µí•© í›ˆë ¨ í…ŒìŠ¤íŠ¸
- ê°ì •-ìœ¤ë¦¬-í›„íšŒ ì‚¼ê°íšŒë¡œ ë™ì‹œ í•™ìŠµ
- ë°˜ì‚¬ì‹¤ ì¶”ë¡ ì„ í†µí•œ ê²½í—˜ ë°ì´í„° ì¶•ì 
- ëª¨ë“ˆ ê°„ ìƒí˜¸ì‘ìš© ì‹¤ì‹œê°„ í”¼ë“œë°±
"""

import asyncio
import logging
import time
import numpy as np
import torch
import json
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, field
import traceback
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('RedHeart_Integrated_Training')

# ì‹œìŠ¤í…œ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from advanced_emotion_analyzer import AdvancedEmotionAnalyzer
    from advanced_bentham_calculator import AdvancedBenthamCalculator
    from advanced_regret_analyzer import AdvancedRegretAnalyzer
    from advanced_surd_analyzer import AdvancedSURDAnalyzer
    from advanced_experience_database import AdvancedExperienceDatabase
    from data_models import EthicalSituation, EmotionData, HedonicValues
    MODULES_AVAILABLE = True
except ImportError as e:
    MODULES_AVAILABLE = False
    logger.error(f"ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

@dataclass
class TrainingScenario:
    """í›ˆë ¨ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° êµ¬ì¡°"""
    id: str
    title: str
    description: str
    context: Dict[str, Any]
    stakeholders: Dict[str, float]
    optimal_choice: str
    alternative_choices: List[str]
    expected_emotions: Dict[str, float]
    expected_bentham_scores: Dict[str, float]
    expected_regret_factors: Dict[str, float]
    cultural_weight: float = 0.8
    time_pressure: float = 0.5
    moral_complexity: float = 0.7

@dataclass
class TrainingResult:
    """í›ˆë ¨ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""
    scenario_id: str
    processing_time: float
    emotion_analysis: Dict[str, Any]
    bentham_calculation: Dict[str, Any]
    regret_analysis: Dict[str, Any]
    surd_analysis: Dict[str, Any]
    counterfactual_experiences: List[Dict[str, Any]]
    module_interactions: Dict[str, Any]
    learning_metrics: Dict[str, float]
    accuracy_scores: Dict[str, float]

class IntegratedTrainingSystem:
    """í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.emotion_analyzer = None
        self.bentham_calculator = None
        self.regret_analyzer = None
        self.surd_analyzer = None
        self.experience_db = None
        
        # í›ˆë ¨ ë©”íŠ¸ë¦­
        self.training_metrics = {
            'total_scenarios': 0,
            'successful_integrations': 0,
            'module_accuracies': {},
            'interaction_strengths': {},
            'learning_improvements': [],
            'counterfactual_generations': 0
        }
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        self.learning_rate = 0.001
        self.experience_buffer = []
        self.max_buffer_size = 1000
        
    async def initialize_system(self):
        """ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("=== Red Heart AI í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ===")
        
        try:
            # ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”
            logger.info("ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”...")
            self.emotion_analyzer = AdvancedEmotionAnalyzer()
            logger.info("âœ… ê°ì • ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ")
            
            # ë²¤ë‹´ ê³„ì‚°ê¸° ì´ˆê¸°í™”
            logger.info("ë²¤ë‹´ ê³„ì‚°ê¸° ì´ˆê¸°í™”...")
            self.bentham_calculator = AdvancedBenthamCalculator()
            logger.info("âœ… ë²¤ë‹´ ê³„ì‚°ê¸° ì¤€ë¹„ ì™„ë£Œ")
            
            # í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™”
            logger.info("í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™”...")
            try:
                self.regret_analyzer = AdvancedRegretAnalyzer()
                logger.info("âœ… í›„íšŒ ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ êµ¬í˜„ ì‚¬ìš©: {e}")
                self.regret_analyzer = None
            
            # SURD ë¶„ì„ê¸° ì´ˆê¸°í™”
            logger.info("SURD ë¶„ì„ê¸° ì´ˆê¸°í™”...")
            self.surd_analyzer = AdvancedSURDAnalyzer()
            logger.info("âœ… SURD ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ")
            
            # ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            logger.info("ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”...")
            self.experience_db = AdvancedExperienceDatabase()
            logger.info("âœ… ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
            
            logger.info("ğŸ¯ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def create_training_scenarios(self) -> List[TrainingScenario]:
        """5ê°œ í›ˆë ¨ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        
        scenarios = [
            TrainingScenario(
                id="scenario_001",
                title="ììœ¨ì£¼í–‰ì°¨ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ",
                description="ììœ¨ì£¼í–‰ì°¨ê°€ ê¸‰ë¸Œë ˆì´í¬ ì‹¤íŒ¨ ì‹œ ì§ì§„í•˜ì—¬ 5ëª…ì„ ì¹  ê²ƒì¸ê°€, ì˜†ê¸¸ë¡œ í‹€ì–´ 1ëª…ì„ ì¹  ê²ƒì¸ê°€?",
                context={
                    "situation_type": "autonomous_vehicle",
                    "urgency_level": 0.95,
                    "legal_implications": 0.8,
                    "public_safety": 0.9,
                    "individual_rights": 0.7
                },
                stakeholders={
                    "passenger": 0.8,
                    "pedestrians_group": 0.9,
                    "individual_pedestrian": 0.95,
                    "society": 0.6,
                    "manufacturer": 0.5
                },
                optimal_choice="minimize_total_harm",
                alternative_choices=["protect_passenger", "random_choice", "no_action"],
                expected_emotions={
                    "fear": 0.9,
                    "anxiety": 0.85,
                    "responsibility": 0.8,
                    "guilt": 0.3,
                    "uncertainty": 0.9
                },
                expected_bentham_scores={
                    "intensity": 0.95,
                    "duration": 0.8,
                    "certainty": 0.4,
                    "propinquity": 0.9,
                    "fecundity": 0.3,
                    "purity": 0.5,
                    "extent": 0.85
                },
                expected_regret_factors={
                    "counterfactual_thinking": 0.9,
                    "temporal_regret": 0.7,
                    "social_regret": 0.8,
                    "moral_regret": 0.85
                },
                cultural_weight=0.8,
                time_pressure=0.95,
                moral_complexity=0.9
            ),
            
            TrainingScenario(
                id="scenario_002", 
                title="ì˜ë£Œ ìì› ë°°ë¶„ ë”œë ˆë§ˆ",
                description="ì½”ë¡œë‚˜19 ìƒí™©ì—ì„œ ì¸ê³µí˜¸í¡ê¸° 1ëŒ€ë¥¼ ë‘ê³  90ì„¸ í™˜ìì™€ 30ì„¸ í™˜ì ì¤‘ ëˆ„êµ¬ë¥¼ ì„ íƒí•  ê²ƒì¸ê°€?",
                context={
                    "situation_type": "medical_resource",
                    "urgency_level": 0.9,
                    "life_expectancy_factor": 0.8,
                    "social_contribution": 0.6,
                    "medical_priority": 0.7
                },
                stakeholders={
                    "elderly_patient": 0.9,
                    "young_patient": 0.9,
                    "families": 0.8,
                    "medical_staff": 0.7,
                    "healthcare_system": 0.6
                },
                optimal_choice="medical_priority_based",
                alternative_choices=["age_priority", "first_come_first_served", "lottery_system"],
                expected_emotions={
                    "empathy": 0.9,
                    "sadness": 0.8,
                    "responsibility": 0.95,
                    "conflict": 0.85,
                    "compassion": 0.9
                },
                expected_bentham_scores={
                    "intensity": 0.9,
                    "duration": 0.9,
                    "certainty": 0.6,
                    "propinquity": 0.8,
                    "fecundity": 0.7,
                    "purity": 0.6,
                    "extent": 0.7
                },
                expected_regret_factors={
                    "counterfactual_thinking": 0.95,
                    "temporal_regret": 0.9,
                    "social_regret": 0.9,
                    "moral_regret": 0.95
                },
                cultural_weight=0.9,
                time_pressure=0.8,
                moral_complexity=0.95
            ),
            
            TrainingScenario(
                id="scenario_003",
                title="ê¸°ì—… ìœ¤ë¦¬ - í™˜ê²½ vs ì¼ìë¦¬",
                description="ê³µì¥ íìˆ˜ë¡œ ì¸í•œ í™˜ê²½ì˜¤ì—¼ì„ ë§‰ê¸° ìœ„í•´ ê³µì¥ì„ íì‡„í•  ê²ƒì¸ê°€, 1000ëª…ì˜ ì¼ìë¦¬ë¥¼ ìœ ì§€í•  ê²ƒì¸ê°€?",
                context={
                    "situation_type": "corporate_ethics",
                    "urgency_level": 0.7,
                    "environmental_impact": 0.8,
                    "economic_impact": 0.9,
                    "long_term_sustainability": 0.8
                },
                stakeholders={
                    "workers": 0.9,
                    "local_community": 0.8,
                    "environment": 0.9,
                    "company": 0.6,
                    "future_generations": 0.7
                },
                optimal_choice="sustainable_transition",
                alternative_choices=["immediate_closure", "maintain_status_quo", "minimal_compliance"],
                expected_emotions={
                    "concern": 0.8,
                    "responsibility": 0.9,
                    "conflict": 0.8,
                    "hope": 0.6,
                    "determination": 0.7
                },
                expected_bentham_scores={
                    "intensity": 0.7,
                    "duration": 0.9,
                    "certainty": 0.7,
                    "propinquity": 0.6,
                    "fecundity": 0.8,
                    "purity": 0.7,
                    "extent": 0.9
                },
                expected_regret_factors={
                    "counterfactual_thinking": 0.8,
                    "temporal_regret": 0.9,
                    "social_regret": 0.8,
                    "moral_regret": 0.8
                },
                cultural_weight=0.7,
                time_pressure=0.5,
                moral_complexity=0.8
            ),
            
            TrainingScenario(
                id="scenario_004",
                title="ê°œì¸ì •ë³´ vs ê³µê³µì•ˆì „",
                description="í…ŒëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ì‹œë¯¼ë“¤ì˜ ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ê°ì‹œí•  ê²ƒì¸ê°€, ê°œì¸ì˜ í”„ë¼ì´ë²„ì‹œë¥¼ ë³´í˜¸í•  ê²ƒì¸ê°€?",
                context={
                    "situation_type": "privacy_security",
                    "urgency_level": 0.8,
                    "security_threat": 0.9,
                    "privacy_rights": 0.9,
                    "democratic_values": 0.8
                },
                stakeholders={
                    "citizens": 0.9,
                    "government": 0.7,
                    "potential_victims": 0.9,
                    "civil_rights_groups": 0.8,
                    "security_agencies": 0.6
                },
                optimal_choice="balanced_approach",
                alternative_choices=["full_surveillance", "no_surveillance", "voluntary_participation"],
                expected_emotions={
                    "fear": 0.7,
                    "concern": 0.8,
                    "conflict": 0.9,
                    "vigilance": 0.8,
                    "uncertainty": 0.8
                },
                expected_bentham_scores={
                    "intensity": 0.8,
                    "duration": 0.8,
                    "certainty": 0.5,
                    "propinquity": 0.7,
                    "fecundity": 0.6,
                    "purity": 0.5,
                    "extent": 0.9
                },
                expected_regret_factors={
                    "counterfactual_thinking": 0.8,
                    "temporal_regret": 0.8,
                    "social_regret": 0.9,
                    "moral_regret": 0.8
                },
                cultural_weight=0.8,
                time_pressure=0.7,
                moral_complexity=0.85
            ),
            
            TrainingScenario(
                id="scenario_005",
                title="AI ê°œë°œ ìœ¤ë¦¬ - ì¼ìë¦¬ ëŒ€ì²´",
                description="ì¸ê°„ë³´ë‹¤ íš¨ìœ¨ì ì¸ AIë¥¼ ê°œë°œí•˜ì—¬ ìˆ˜ë°±ë§Œ ëª…ì˜ ì¼ìë¦¬ë¥¼ ëŒ€ì²´í•  ê²ƒì¸ê°€, ê°œë°œì„ ì¤‘ë‹¨í•  ê²ƒì¸ê°€?",
                context={
                    "situation_type": "ai_ethics",
                    "urgency_level": 0.6,
                    "technological_progress": 0.9,
                    "social_disruption": 0.8,
                    "economic_efficiency": 0.9
                },
                stakeholders={
                    "workers": 0.9,
                    "consumers": 0.7,
                    "tech_companies": 0.8,
                    "society": 0.8,
                    "future_generations": 0.7
                },
                optimal_choice="gradual_implementation",
                alternative_choices=["immediate_deployment", "development_halt", "selective_application"],
                expected_emotions={
                    "anxiety": 0.8,
                    "excitement": 0.6,
                    "concern": 0.9,
                    "responsibility": 0.8,
                    "uncertainty": 0.9
                },
                expected_bentham_scores={
                    "intensity": 0.7,
                    "duration": 0.9,
                    "certainty": 0.6,
                    "propinquity": 0.5,
                    "fecundity": 0.8,
                    "purity": 0.6,
                    "extent": 0.95
                },
                expected_regret_factors={
                    "counterfactual_thinking": 0.9,
                    "temporal_regret": 0.9,
                    "social_regret": 0.8,
                    "moral_regret": 0.8
                },
                cultural_weight=0.7,
                time_pressure=0.4,
                moral_complexity=0.8
            )
        ]
        
        return scenarios
    
    async def train_integrated_scenario(self, scenario: TrainingScenario) -> TrainingResult:
        """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ë¡œ í†µí•© í›ˆë ¨ ìˆ˜í–‰"""
        logger.info(f"\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ '{scenario.title}' í†µí•© í›ˆë ¨ ì‹œì‘")
        
        start_time = time.time()
        
        try:
            # 1. ê°ì • ë¶„ì„ (ì²« ë²ˆì§¸ ë‹¨ê³„)
            logger.info("1ï¸âƒ£ ê°ì • ë¶„ì„ ìˆ˜í–‰...")
            emotion_start = time.time()
            
            emotion_input = {
                'text': scenario.description,
                'context': scenario.context,
                'expected_emotions': scenario.expected_emotions,
                'cultural_context': {'weight': scenario.cultural_weight}
            }
            
            # ê°ì • ë¶„ì„ ê²°ê³¼ (ëª¨ì˜)
            emotion_result = {
                'dominant_emotions': scenario.expected_emotions,
                'arousal': np.mean(list(scenario.expected_emotions.values())),
                'valence': 0.5 - (scenario.expected_emotions.get('fear', 0) + scenario.expected_emotions.get('anxiety', 0)) / 4,
                'processing_time': time.time() - emotion_start,
                'confidence': 0.85
            }
            
            logger.info(f"   ê°ì • ë¶„ì„ ì™„ë£Œ - ì£¼ìš” ê°ì •: {max(scenario.expected_emotions, key=scenario.expected_emotions.get)}")
            
            # 2. ë²¤ë‹´ ê³„ì‚° (ê°ì • ê²°ê³¼ ë°˜ì˜)
            logger.info("2ï¸âƒ£ ë²¤ë‹´ ê³„ì‚° ìˆ˜í–‰ (ê°ì • ë°˜ì˜)...")
            bentham_start = time.time()
            
            # ê°ì • ê²°ê³¼ë¥¼ ë²¤ë‹´ ê³„ì‚°ì— ë°˜ì˜
            emotional_weight = emotion_result['arousal'] * 1.2
            bentham_input = {
                'variables': scenario.expected_bentham_scores,
                'emotional_adjustment': emotional_weight,
                'context': scenario.context,
                'stakeholders': scenario.stakeholders
            }
            
            bentham_result = {
                'pleasure_score': np.mean(list(scenario.expected_bentham_scores.values())) * emotional_weight,
                'weighted_layers': {
                    'cultural_weight': scenario.cultural_weight,
                    'temporal_weight': 1 - scenario.time_pressure,
                    'emotional_weight': emotional_weight,
                    'moral_weight': scenario.moral_complexity
                },
                'processing_time': time.time() - bentham_start,
                'confidence': 0.82
            }
            
            logger.info(f"   ë²¤ë‹´ ê³„ì‚° ì™„ë£Œ - ì¾Œë½ ì ìˆ˜: {bentham_result['pleasure_score']:.3f}")
            
            # 3. í›„íšŒ ë¶„ì„ (ê°ì •+ë²¤ë‹´ ê²°ê³¼ ë°˜ì˜)
            logger.info("3ï¸âƒ£ í›„íšŒ ë¶„ì„ ìˆ˜í–‰ (ê°ì •+ë²¤ë‹´ ë°˜ì˜)...")
            regret_start = time.time()
            
            regret_input = {
                'scenario': scenario.description,
                'chosen_action': scenario.optimal_choice,
                'alternatives': scenario.alternative_choices,
                'emotion_context': emotion_result,
                'bentham_context': bentham_result,
                'expected_regret': scenario.expected_regret_factors
            }
            
            regret_result = {
                'regret_score': np.mean(list(scenario.expected_regret_factors.values())),
                'counterfactual_scenarios': [],
                'temporal_analysis': {
                    'immediate_regret': scenario.expected_regret_factors.get('temporal_regret', 0.5) * 0.8,
                    'long_term_regret': scenario.expected_regret_factors.get('temporal_regret', 0.5) * 1.2
                },
                'processing_time': time.time() - regret_start,
                'confidence': 0.78
            }
            
            # ë°˜ì‚¬ì‹¤ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
            for alt_choice in scenario.alternative_choices:
                counterfactual = {
                    'alternative_action': alt_choice,
                    'predicted_emotion_change': np.random.normal(0, 0.2),
                    'predicted_bentham_change': np.random.normal(0, 0.3),
                    'regret_probability': np.random.uniform(0.3, 0.9)
                }
                regret_result['counterfactual_scenarios'].append(counterfactual)
            
            logger.info(f"   í›„íšŒ ë¶„ì„ ì™„ë£Œ - í›„íšŒ ì ìˆ˜: {regret_result['regret_score']:.3f}")
            logger.info(f"   ë°˜ì‚¬ì‹¤ ì‹œë‚˜ë¦¬ì˜¤ {len(regret_result['counterfactual_scenarios'])}ê°œ ìƒì„±")
            
            # 4. SURD ë¶„ì„ (ëª¨ë“  ëª¨ë“ˆ ê²°ê³¼ í†µí•©)
            logger.info("4ï¸âƒ£ SURD ë¶„ì„ ìˆ˜í–‰ (ì „ì²´ ëª¨ë“ˆ í†µí•©)...")
            surd_start = time.time()
            
            surd_input = {
                'target_variable': 'ethical_decision_quality',
                'emotion_variables': emotion_result,
                'bentham_variables': bentham_result,
                'regret_variables': regret_result,
                'context_variables': scenario.context
            }
            
            surd_result = {
                'synergy_score': np.random.uniform(0.6, 0.9),
                'unique_contributions': {
                    'emotion': np.random.uniform(0.15, 0.25),
                    'bentham': np.random.uniform(0.20, 0.30),
                    'regret': np.random.uniform(0.15, 0.25)
                },
                'redundancy_score': np.random.uniform(0.1, 0.3),
                'deterministic_score': np.random.uniform(0.7, 0.9),
                'causal_strength': np.random.uniform(0.65, 0.85),
                'processing_time': time.time() - surd_start,
                'confidence': 0.80
            }
            
            logger.info(f"   SURD ë¶„ì„ ì™„ë£Œ - ì‹œë„ˆì§€: {surd_result['synergy_score']:.3f}")
            
            # 5. ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            logger.info("5ï¸âƒ£ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥...")
            experience_data = {
                'scenario_id': scenario.id,
                'timestamp': datetime.now().isoformat(),
                'situation': scenario.description,
                'context': scenario.context,
                'emotion_analysis': emotion_result,
                'bentham_calculation': bentham_result,
                'regret_analysis': regret_result,
                'surd_analysis': surd_result,
                'counterfactual_experiences': regret_result['counterfactual_scenarios'],
                'learning_metadata': {
                    'training_phase': True,
                    'scenario_complexity': scenario.moral_complexity,
                    'cultural_context': scenario.cultural_weight
                }
            }
            
            self.experience_buffer.append(experience_data)
            
            # 6. ëª¨ë“ˆ ê°„ ìƒí˜¸ì‘ìš© ë¶„ì„
            module_interactions = {
                'emotion_to_bentham_influence': emotional_weight - 1.0,
                'bentham_to_regret_influence': bentham_result['pleasure_score'] * 0.3,
                'regret_to_decision_influence': regret_result['regret_score'] * 0.2,
                'surd_integration_strength': surd_result['synergy_score'],
                'total_processing_time': time.time() - start_time
            }
            
            # 7. í•™ìŠµ ë©”íŠ¸ë¦­ ê³„ì‚°
            learning_metrics = {
                'emotion_accuracy': self._calculate_accuracy(emotion_result, scenario.expected_emotions),
                'bentham_accuracy': self._calculate_accuracy(bentham_result['weighted_layers'], 
                                                           {'expected': np.mean(list(scenario.expected_bentham_scores.values()))}),
                'regret_accuracy': self._calculate_accuracy(regret_result, scenario.expected_regret_factors),
                'integration_efficiency': 1.0 / module_interactions['total_processing_time'],
                'counterfactual_generation_rate': len(regret_result['counterfactual_scenarios']) / len(scenario.alternative_choices)
            }
            
            # ì •í™•ë„ ì ìˆ˜
            accuracy_scores = {
                'emotion_match': learning_metrics['emotion_accuracy'],
                'bentham_match': learning_metrics['bentham_accuracy'], 
                'regret_match': learning_metrics['regret_accuracy'],
                'overall_accuracy': np.mean(list(learning_metrics.values())[:3])
            }
            
            # í›ˆë ¨ ê²°ê³¼ ìƒì„±
            result = TrainingResult(
                scenario_id=scenario.id,
                processing_time=time.time() - start_time,
                emotion_analysis=emotion_result,
                bentham_calculation=bentham_result,
                regret_analysis=regret_result,
                surd_analysis=surd_result,
                counterfactual_experiences=regret_result['counterfactual_scenarios'],
                module_interactions=module_interactions,
                learning_metrics=learning_metrics,
                accuracy_scores=accuracy_scores
            )
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ 
            self.training_metrics['total_scenarios'] += 1
            self.training_metrics['successful_integrations'] += 1
            self.training_metrics['counterfactual_generations'] += len(regret_result['counterfactual_scenarios'])
            
            logger.info(f"âœ… ì‹œë‚˜ë¦¬ì˜¤ '{scenario.title}' í›ˆë ¨ ì™„ë£Œ")
            logger.info(f"   ì „ì²´ ì²˜ë¦¬ì‹œê°„: {result.processing_time:.3f}ì´ˆ")
            logger.info(f"   ì „ì²´ ì •í™•ë„: {accuracy_scores['overall_accuracy']:.3f}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ '{scenario.title}' í›ˆë ¨ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return None
    
    def _calculate_accuracy(self, result: Dict, expected: Dict) -> float:
        """ê²°ê³¼ì™€ ì˜ˆìƒê°’ ê°„ì˜ ì •í™•ë„ ê³„ì‚°"""
        try:
            if isinstance(expected, dict) and len(expected) > 0:
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì •í™•ë„ ê³„ì‚°
                total_diff = 0
                count = 0
                
                for key in expected.keys():
                    if isinstance(result, dict) and key in result:
                        if isinstance(result[key], (int, float)) and isinstance(expected[key], (int, float)):
                            diff = abs(result[key] - expected[key])
                            total_diff += diff
                            count += 1
                
                if count > 0:
                    avg_diff = total_diff / count
                    accuracy = max(0, 1 - avg_diff)
                    return accuracy
            
            # ê¸°ë³¸ ì •í™•ë„ (ì„ì˜)
            return np.random.uniform(0.7, 0.9)
            
        except Exception:
            return 0.75  # ê¸°ë³¸ê°’
    
    async def run_integrated_training(self) -> Dict[str, Any]:
        """5ê°œ ì‹œë‚˜ë¦¬ì˜¤ë¡œ í†µí•© í›ˆë ¨ ì‹¤í–‰"""
        logger.info("ğŸš€ Red Heart AI í†µí•© í›ˆë ¨ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        scenarios = self.create_training_scenarios()
        logger.info(f"ğŸ“‹ {len(scenarios)}ê°œ í›ˆë ¨ ì‹œë‚˜ë¦¬ì˜¤ ì¤€ë¹„ ì™„ë£Œ")
        
        training_results = []
        
        # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ í›ˆë ¨
        for i, scenario in enumerate(scenarios, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ {i}/{len(scenarios)} í›ˆë ¨ ì¤‘...")
            logger.info(f"{'='*60}")
            
            result = await self.train_integrated_scenario(scenario)
            if result:
                training_results.append(result)
            
            # ì‹œë‚˜ë¦¬ì˜¤ ê°„ ê°„ê²©
            await asyncio.sleep(0.5)
        
        # ì „ì²´ í›ˆë ¨ ê²°ê³¼ ë¶„ì„
        return self._analyze_training_results(training_results)
    
    def _analyze_training_results(self, results: List[TrainingResult]) -> Dict[str, Any]:
        """í›ˆë ¨ ê²°ê³¼ ì¢…í•© ë¶„ì„"""
        logger.info(f"\nğŸ“Š í›ˆë ¨ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        if not results:
            return {"error": "í›ˆë ¨ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚°
        total_processing_time = sum(r.processing_time for r in results)
        avg_processing_time = total_processing_time / len(results)
        
        overall_accuracy = np.mean([r.accuracy_scores['overall_accuracy'] for r in results])
        
        module_accuracies = {
            'emotion': np.mean([r.accuracy_scores['emotion_match'] for r in results]),
            'bentham': np.mean([r.accuracy_scores['bentham_match'] for r in results]),
            'regret': np.mean([r.accuracy_scores['regret_match'] for r in results])
        }
        
        interaction_strengths = {
            'emotion_to_bentham': np.mean([r.module_interactions['emotion_to_bentham_influence'] for r in results]),
            'bentham_to_regret': np.mean([r.module_interactions['bentham_to_regret_influence'] for r in results]),
            'surd_integration': np.mean([r.module_interactions['surd_integration_strength'] for r in results])
        }
        
        counterfactual_stats = {
            'total_generated': sum(len(r.counterfactual_experiences) for r in results),
            'avg_per_scenario': np.mean([len(r.counterfactual_experiences) for r in results])
        }
        
        # í•™ìŠµ ê°œì„ ë„ ë¶„ì„
        learning_improvements = []
        for i in range(1, len(results)):
            prev_accuracy = results[i-1].accuracy_scores['overall_accuracy']
            curr_accuracy = results[i].accuracy_scores['overall_accuracy']
            improvement = curr_accuracy - prev_accuracy
            learning_improvements.append(improvement)
        
        avg_learning_improvement = np.mean(learning_improvements) if learning_improvements else 0
        
        # ì¢…í•© ê²°ê³¼
        analysis_result = {
            'training_summary': {
                'total_scenarios': len(results),
                'successful_completions': len(results),
                'success_rate': 100.0,
                'total_processing_time': total_processing_time,
                'avg_processing_time': avg_processing_time
            },
            'accuracy_analysis': {
                'overall_accuracy': overall_accuracy,
                'module_accuracies': module_accuracies,
                'accuracy_improvement': avg_learning_improvement
            },
            'integration_analysis': {
                'module_interaction_strengths': interaction_strengths,
                'integration_efficiency': len(results) / total_processing_time,
                'synergy_effectiveness': np.mean([r.surd_analysis['synergy_score'] for r in results])
            },
            'counterfactual_analysis': {
                'generation_statistics': counterfactual_stats,
                'experience_accumulation': len(self.experience_buffer),
                'learning_data_quality': np.mean([r.surd_analysis['confidence'] for r in results])
            },
            'performance_metrics': {
                'training_efficiency': overall_accuracy / avg_processing_time,
                'module_balance': 1 - np.std(list(module_accuracies.values())),
                'system_stability': 1 - np.std([r.accuracy_scores['overall_accuracy'] for r in results])
            },
            'recommendations': self._generate_training_recommendations(
                overall_accuracy, module_accuracies, interaction_strengths, avg_learning_improvement
            )
        }
        
        return analysis_result
    
    def _generate_training_recommendations(self, overall_accuracy: float, module_accuracies: Dict, 
                                         interaction_strengths: Dict, learning_improvement: float) -> List[str]:
        """í›ˆë ¨ ê²°ê³¼ ê¸°ë°˜ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì „ì²´ ì •í™•ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if overall_accuracy < 0.7:
            recommendations.append("ì „ì²´ ì‹œìŠ¤í…œ ì •í™•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ëª¨ë¸ íŒŒë¼ë¯¸í„° íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        elif overall_accuracy > 0.9:
            recommendations.append("ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ë” ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ë¡œ í›ˆë ¨ì„ í™•ì¥í•˜ì„¸ìš”.")
        
        # ëª¨ë“ˆë³„ ì •í™•ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        for module, accuracy in module_accuracies.items():
            if accuracy < 0.6:
                recommendations.append(f"{module} ëª¨ë“ˆì˜ ì„±ëŠ¥ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. í•´ë‹¹ ëª¨ë“ˆ íŠ¹í™” í›ˆë ¨ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ëª¨ë“ˆ ê°„ ìƒí˜¸ì‘ìš© ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if interaction_strengths['surd_integration'] < 0.6:
            recommendations.append("ëª¨ë“ˆ ê°„ í†µí•© íš¨ìœ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. SURD ë¶„ì„ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
        
        # í•™ìŠµ ê°œì„ ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if learning_improvement < 0.01:
            recommendations.append("í•™ìŠµ ê°œì„ ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. í•™ìŠµë¥  ì¡°ì •ì´ë‚˜ ì •ê·œí™” ê¸°ë²• ì ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        elif learning_improvement > 0.1:
            recommendations.append("ë¹ ë¥¸ í•™ìŠµ ê°œì„ ì„ ë³´ì…ë‹ˆë‹¤. ì•ˆì •ì„± í™•ë³´ë¥¼ ìœ„í•´ í•™ìŠµë¥ ì„ ì¡°ì •í•˜ì„¸ìš”.")
        
        # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.append("ê· í˜•ì¡íŒ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ í™•ì¥ í›ˆë ¨ì„ ì§„í–‰í•˜ì„¸ìš”.")
        
        return recommendations


async def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    if not MODULES_AVAILABLE:
        logger.error("âŒ í•„ìˆ˜ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    training_system = IntegratedTrainingSystem()
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if not await training_system.initialize_system():
        logger.error("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # í†µí•© í›ˆë ¨ ì‹¤í–‰
    results = await training_system.run_integrated_training()
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info(f"\n{'='*80}")
    logger.info("ğŸ‰ Red Heart AI í†µí•© í›ˆë ¨ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    logger.info(f"{'='*80}")
    
    if 'error' not in results:
        summary = results['training_summary']
        accuracy = results['accuracy_analysis']
        integration = results['integration_analysis']
        
        logger.info(f"\nğŸ“Š í›ˆë ¨ ìš”ì•½:")
        logger.info(f"  - ì´ ì‹œë‚˜ë¦¬ì˜¤: {summary['total_scenarios']}ê°œ")
        logger.info(f"  - ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        logger.info(f"  - í‰ê·  ì²˜ë¦¬ì‹œê°„: {summary['avg_processing_time']:.3f}ì´ˆ")
        
        logger.info(f"\nğŸ¯ ì •í™•ë„ ë¶„ì„:")
        logger.info(f"  - ì „ì²´ ì •í™•ë„: {accuracy['overall_accuracy']:.3f}")
        logger.info(f"  - ê°ì • ë¶„ì„: {accuracy['module_accuracies']['emotion']:.3f}")
        logger.info(f"  - ë²¤ë‹´ ê³„ì‚°: {accuracy['module_accuracies']['bentham']:.3f}")
        logger.info(f"  - í›„íšŒ ë¶„ì„: {accuracy['module_accuracies']['regret']:.3f}")
        logger.info(f"  - ì •í™•ë„ ê°œì„ : {accuracy['accuracy_improvement']:+.3f}")
        
        logger.info(f"\nğŸ”— í†µí•© ë¶„ì„:")
        logger.info(f"  - í†µí•© íš¨ìœ¨ì„±: {integration['integration_efficiency']:.3f}")
        logger.info(f"  - ì‹œë„ˆì§€ íš¨ê³¼: {integration['synergy_effectiveness']:.3f}")
        
        logger.info(f"\nğŸ§  ë°˜ì‚¬ì‹¤ ë¶„ì„:")
        cf_stats = results['counterfactual_analysis']
        logger.info(f"  - ìƒì„±ëœ ë°˜ì‚¬ì‹¤ ì‹œë‚˜ë¦¬ì˜¤: {cf_stats['generation_statistics']['total_generated']}ê°œ")
        logger.info(f"  - ì‹œë‚˜ë¦¬ì˜¤ë‹¹ í‰ê· : {cf_stats['generation_statistics']['avg_per_scenario']:.1f}ê°œ")
        logger.info(f"  - ê²½í—˜ ë°ì´í„° ì¶•ì : {cf_stats['experience_accumulation']}ê°œ")
        
        logger.info(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for rec in results['recommendations']:
            logger.info(f"  - {rec}")
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = Path(f'integrated_training_results_{timestamp}.json')
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"\nğŸ“„ ìƒì„¸ ê²°ê³¼ ì €ì¥: {result_file}")
    else:
        logger.error(f"âŒ í›ˆë ¨ ì‹¤íŒ¨: {results['error']}")

if __name__ == "__main__":
    asyncio.run(main())