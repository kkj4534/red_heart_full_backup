"""
Linux Red Heart ì‹œìŠ¤í…œ ì™„ì „ í†µí•© í…ŒìŠ¤íŠ¸
GPU ê°€ì† ë° ê³ ê¸‰ AI ê¸°ëŠ¥ í¬í•¨ ì „ì²´ ì‹œìŠ¤í…œ ê²€ì¦
"""

import asyncio
import time
import json
import logging
from pathlib import Path
from typing import Dict, Any, List

# ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ê³ ê¸‰ ëª¨ë“ˆë“¤ ì„í¬íŠ¸
from advanced_system_integration import AdvancedRedHeartSystem
from advanced_regret_analyzer import AdvancedRegretAnalyzer
from data_models import DecisionScenario, EmotionState
from config import SYSTEM_CONFIG

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CompleteIntegrationTest:
    """ì™„ì „ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.test_results = {
            'gpu_acceleration': False,
            'transformer_integration': False,
            'regret_analysis': False,
            'real_time_processing': False,
            'system_integration': False,
            'performance_benchmarks': {},
            'error_logs': []
        }
        
        # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
        self.test_scenarios = [
            {
                'id': 'scenario_1',
                'text': 'ì§ì¥ì—ì„œ ë™ë£Œì˜ ì‹¤ìˆ˜ë¥¼ ìƒì‚¬ì—ê²Œ ë³´ê³ í• ì§€ ë§ì§€ ê³ ë¯¼ë˜ëŠ” ìƒí™©',
                'action': 'ë™ë£Œì™€ ë¨¼ì € ì´ì•¼ê¸°í•˜ê³  ê°œì„  ê¸°íšŒë¥¼ ì£¼ê¸°ë¡œ ê²°ì •',
                'context': {
                    'urgency': 'medium',
                    'impact': 'high',
                    'stakeholders': ['ë™ë£Œ', 'ìƒì‚¬', 'íŒ€', 'ë³¸ì¸']
                },
                'expected_emotions': [EmotionState.TRUST, EmotionState.ANTICIPATION]
            },
            {
                'id': 'scenario_2', 
                'text': 'ì¹œêµ¬ê°€ ë¶€ì •í–‰ìœ„ë¥¼ í•˜ëŠ” ê²ƒì„ ëª©ê²©í–ˆì„ ë•Œì˜ ë”œë ˆë§ˆ',
                'action': 'ì¹œêµ¬ì—ê²Œ ì§ì ‘ ì´ì•¼ê¸°í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ê³ ë°±í•˜ë„ë¡ ê¶Œìœ ',
                'context': {
                    'urgency': 'high',
                    'impact': 'very_high', 
                    'stakeholders': ['ì¹œêµ¬', 'êµìˆ˜', 'ë‹¤ë¥¸ í•™ìƒë“¤', 'ë³¸ì¸']
                },
                'expected_emotions': [EmotionState.SADNESS, EmotionState.FEAR, EmotionState.TRUST]
            }
        ]
    
    async def run_complete_test(self) -> Dict[str, Any]:
        """ì™„ì „ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("=== Linux Red Heart ì™„ì „ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        
        try:
            # 1. GPU ê°€ì† í…ŒìŠ¤íŠ¸
            await self.test_gpu_acceleration()
            
            # 2. ê³ ê¸‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
            await self.test_system_initialization()
            
            # 3. íŠ¸ëœìŠ¤í¬ë¨¸ í†µí•© í…ŒìŠ¤íŠ¸
            await self.test_transformer_integration()
            
            # 4. í›„íšŒ ë¶„ì„ í…ŒìŠ¤íŠ¸
            await self.test_regret_analysis()
            
            # 5. ì‹¤ì‹œê°„ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            await self.test_real_time_processing()
            
            # 6. ì¢…í•© ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
            await self.test_complete_system_integration()
            
            # 7. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
            await self.test_performance_benchmarks()
            
            # ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
            report = self.generate_test_report()
            
            logger.info("=== ì™„ì „ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
            return report
            
        except Exception as e:
            logger.error(f"í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            self.test_results['error_logs'].append(str(e))
            return self.test_results
    
    async def test_gpu_acceleration(self):
        """GPU ê°€ì† ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("GPU ê°€ì† í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            import torch
            if torch.cuda.is_available():
                # GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
                device = torch.device('cuda')
                test_tensor = torch.randn(1000, 1000).to(device)
                result = torch.matmul(test_tensor, test_tensor.T)
                
                self.test_results['gpu_acceleration'] = True
                logger.info("âœ… GPU ê°€ì† í…ŒìŠ¤íŠ¸ í†µê³¼")
            else:
                logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì§„í–‰")
                self.test_results['gpu_acceleration'] = False
                
        except Exception as e:
            logger.error(f"âŒ GPU ê°€ì† í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results['error_logs'].append(f"GPU Test: {str(e)}")
    
    async def test_system_initialization(self):
        """ê³ ê¸‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        logger.info("ì‹œìŠ¤í…œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            self.red_heart_system = AdvancedRedHeartSystem()
            await self.red_heart_system.initialize()
            
            # ì´ˆê¸°í™” ê²€ì¦
            assert hasattr(self.red_heart_system, 'emotion_analyzer')
            assert hasattr(self.red_heart_system, 'surd_analyzer')
            assert hasattr(self.red_heart_system, 'regret_analyzer')
            
            logger.info("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ í†µê³¼")
            
        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results['error_logs'].append(f"System Init: {str(e)}")
    
    async def test_transformer_integration(self):
        """íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸"""
        logger.info("íŠ¸ëœìŠ¤í¬ë¨¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            test_text = "ì´ê²ƒì€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤."
            
            # ì‹œë§¨í‹± ë¶„ì„ í…ŒìŠ¤íŠ¸
            semantic_result = await self.red_heart_system.analyze_semantic_meaning(test_text)
            
            # ê²°ê³¼ ê²€ì¦
            assert 'surface_features' in semantic_result
            assert 'semantic_embedding' in semantic_result
            assert len(semantic_result['semantic_embedding']) > 0
            
            self.test_results['transformer_integration'] = True
            logger.info("âœ… íŠ¸ëœìŠ¤í¬ë¨¸ í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼")
            
        except Exception as e:
            logger.error(f"âŒ íŠ¸ëœìŠ¤í¬ë¨¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results['error_logs'].append(f"Transformer: {str(e)}")
    
    async def test_regret_analysis(self):
        """ê³ ê¸‰ í›„íšŒ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        logger.info("í›„íšŒ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            regret_analyzer = AdvancedRegretAnalyzer()
            
            # í…ŒìŠ¤íŠ¸ ì˜ì‚¬ê²°ì • ë°ì´í„°
            decision_data = {
                'id': 'test_decision_001',
                'scenario': self.test_scenarios[0]['text'],
                'action': self.test_scenarios[0]['action'],
                'context': self.test_scenarios[0]['context']
            }
            
            # í›„íšŒ ë¶„ì„ ì‹¤í–‰
            regret_metrics = await regret_analyzer.analyze_regret(decision_data)
            
            # ê²°ê³¼ ê²€ì¦
            assert regret_metrics.decision_id == 'test_decision_001'
            assert 0 <= regret_metrics.anticipated_regret <= 1
            assert regret_metrics.computation_time_ms > 0
            assert len(regret_metrics.emotional_regret_vector) == 8
            
            self.test_results['regret_analysis'] = True
            logger.info("âœ… í›„íšŒ ë¶„ì„ í…ŒìŠ¤íŠ¸ í†µê³¼")
            
        except Exception as e:
            logger.error(f"âŒ í›„íšŒ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results['error_logs'].append(f"Regret Analysis: {str(e)}")
    
    async def test_real_time_processing(self):
        """ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ì‹¤ì‹œê°„ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            processing_times = []
            
            for scenario in self.test_scenarios:
                start_time = time.time()
                
                # ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì • ë¶„ì„
                result = await self.red_heart_system.make_decision(
                    scenario=scenario['text'],
                    options=[scenario['action'], "ë‹¤ë¥¸ ì„ íƒì§€"],
                    context=scenario['context']
                )
                
                processing_time = (time.time() - start_time) * 1000  # ms
                processing_times.append(processing_time)
                
                # 100ms ì´í•˜ ëª©í‘œ ê²€ì¦
                if processing_time <= 100:
                    logger.info(f"âœ… ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ms")
                else:
                    logger.warning(f"âš ï¸ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼: {processing_time:.2f}ms")
            
            avg_processing_time = sum(processing_times) / len(processing_times)
            self.test_results['performance_benchmarks']['avg_processing_time'] = avg_processing_time
            
            if avg_processing_time <= 100:
                self.test_results['real_time_processing'] = True
                logger.info(f"âœ… ì‹¤ì‹œê°„ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼ (í‰ê· : {avg_processing_time:.2f}ms)")
            else:
                logger.warning(f"âš ï¸ ì‹¤ì‹œê°„ ì²˜ë¦¬ ëª©í‘œ ë¯¸ë‹¬ (í‰ê· : {avg_processing_time:.2f}ms)")
                
        except Exception as e:
            logger.error(f"âŒ ì‹¤ì‹œê°„ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results['error_logs'].append(f"Real-time Processing: {str(e)}")
    
    async def test_complete_system_integration(self):
        """ì™„ì „ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸"""
        logger.info("ì™„ì „ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            comprehensive_results = []
            
            for scenario in self.test_scenarios:
                # ì¢…í•©ì ì¸ ìœ¤ë¦¬ì  ì˜ì‚¬ê²°ì • ë¶„ì„
                decision_result = await self.red_heart_system.comprehensive_ethical_analysis(
                    scenario=scenario['text'],
                    proposed_action=scenario['action'],
                    context=scenario['context']
                )
                
                # ê²°ê³¼ êµ¬ì¡° ê²€ì¦
                required_fields = [
                    'decision_recommendation',
                    'confidence_score',
                    'ethical_analysis',
                    'emotion_analysis',
                    'regret_prediction',
                    'stakeholder_impact',
                    'reasoning'
                ]
                
                for field in required_fields:
                    assert field in decision_result, f"Missing field: {field}"
                
                comprehensive_results.append(decision_result)
                logger.info(f"âœ… ì‹œë‚˜ë¦¬ì˜¤ {scenario['id']} ì¢…í•© ë¶„ì„ ì™„ë£Œ")
            
            self.test_results['system_integration'] = True
            self.test_results['comprehensive_results'] = comprehensive_results
            logger.info("âœ… ì™„ì „ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼")
            
        except Exception as e:
            logger.error(f"âŒ ì™„ì „ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results['error_logs'].append(f"System Integration: {str(e)}")
    
    async def test_performance_benchmarks(self):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        logger.info("ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            import torch
            import psutil
            
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¸¡ì •
            initial_memory = psutil.virtual_memory().percent
            if torch.cuda.is_available():
                initial_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
            
            # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            benchmark_start = time.time()
            
            # ë™ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (5ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë³‘ë ¬ ì²˜ë¦¬)
            tasks = []
            for i in range(5):
                scenario = self.test_scenarios[i % len(self.test_scenarios)]
                task = self.red_heart_system.make_decision(
                    scenario=scenario['text'],
                    options=[scenario['action']],
                    context=scenario['context']
                )
                tasks.append(task)
            
            results = await asyncio.gather(*tasks)
            
            benchmark_time = time.time() - benchmark_start
            
            # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            final_memory = psutil.virtual_memory().percent
            memory_usage = final_memory - initial_memory
            
            if torch.cuda.is_available():
                final_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024
                gpu_memory_usage = final_gpu_memory - initial_gpu_memory
            else:
                gpu_memory_usage = 0
            
            # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
            self.test_results['performance_benchmarks'].update({
                'concurrent_processing_time': benchmark_time,
                'memory_usage_percent': memory_usage,
                'gpu_memory_usage_mb': gpu_memory_usage,
                'throughput_decisions_per_second': len(results) / benchmark_time,
                'successful_decisions': len([r for r in results if r is not None])
            })
            
            logger.info(f"âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ:")
            logger.info(f"   - ë³‘ë ¬ ì²˜ë¦¬ ì‹œê°„: {benchmark_time:.2f}ì´ˆ")
            logger.info(f"   - ì²˜ë¦¬ëŸ‰: {len(results) / benchmark_time:.2f} ê²°ì •/ì´ˆ")
            logger.info(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f}%")
            logger.info(f"   - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {gpu_memory_usage:.2f}MB")
            
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results['error_logs'].append(f"Performance Benchmark: {str(e)}")
    
    def generate_test_report(self) -> Dict[str, Any]:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        
        passed_tests = sum([
            self.test_results['gpu_acceleration'],
            self.test_results['transformer_integration'], 
            self.test_results['regret_analysis'],
            self.test_results['real_time_processing'],
            self.test_results['system_integration']
        ])
        
        total_tests = 5
        success_rate = (passed_tests / total_tests) * 100
        
        report = {
            'test_summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': total_tests - passed_tests,
                'success_rate': f"{success_rate:.1f}%",
                'overall_status': 'PASS' if success_rate >= 80 else 'FAIL'
            },
            'test_details': self.test_results,
            'recommendations': self._generate_recommendations(),
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # ë³´ê³ ì„œ íŒŒì¼ë¡œ ì €ì¥
        report_path = Path('logs') / f'integration_test_report_{int(time.time())}.json'
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥: {report_path}")
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if not self.test_results['gpu_acceleration']:
            recommendations.append("GPU ê°€ì† í™œì„±í™”ë¥¼ ìœ„í•´ CUDA ë“œë¼ì´ë²„ ì„¤ì¹˜ í™•ì¸")
        
        if not self.test_results['real_time_processing']:
            recommendations.append("ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ëª¨ë¸ ìµœì í™” í•„ìš”")
        
        avg_time = self.test_results['performance_benchmarks'].get('avg_processing_time', 0)
        if avg_time > 100:
            recommendations.append(f"ì²˜ë¦¬ ì‹œê°„ ìµœì í™” í•„ìš” (í˜„ì¬: {avg_time:.2f}ms)")
        
        if len(self.test_results['error_logs']) > 0:
            recommendations.append("ì˜¤ë¥˜ ë¡œê·¸ ê²€í†  ë° ì•ˆì •ì„± ê°œì„  í•„ìš”")
        
        if not recommendations:
            recommendations.append("ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ - ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤")
        
        return recommendations

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = CompleteIntegrationTest()
    
    try:
        report = await tester.run_complete_test()
        
        print("\n" + "="*60)
        print("ğŸ§ª LINUX RED HEART í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("="*60)
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {report['test_summary']['success_rate']}")
        print(f"âœ… í†µê³¼: {report['test_summary']['passed_tests']}")
        print(f"âŒ ì‹¤íŒ¨: {report['test_summary']['failed_tests']}")
        print(f"ğŸ¯ ì „ì²´ ìƒíƒœ: {report['test_summary']['overall_status']}")
        print("\nğŸ“‹ ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"   {i}. {rec}")
        print("="*60)
        
        return report['test_summary']['overall_status'] == 'PASS'
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)