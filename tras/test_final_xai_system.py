#!/usr/bin/env python3
"""
ìµœì¢… XAI ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ - 2ì–µ íŒŒë¼ë¯¸í„° + XAI + LLM í†µí•©
Final XAI System Test - 200M Parameters + XAI + LLM Integration
"""

import os
import sys
import json
import logging
import time
import traceback
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# PATHì— pip ì¶”ê°€
os.environ['PATH'] = os.environ.get('PATH', '') + ':/home/kkj/.local/bin'

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('FinalXAITest')

def test_dependencies():
    """ì˜ì¡´ì„± í™•ì¸"""
    logger.info("ğŸ”§ ìµœì¢… ì˜ì¡´ì„± í™•ì¸")
    
    deps = {}
    try:
        import numpy as np
        deps['numpy'] = np.__version__
        logger.info(f"âœ… NumPy {np.__version__}")
    except ImportError as e:
        deps['numpy'] = f"ERROR: {e}"
        logger.error(f"âŒ NumPy: {e}")
    
    try:
        import torch
        deps['torch'] = torch.__version__
        logger.info(f"âœ… PyTorch {torch.__version__}")
    except ImportError as e:
        deps['torch'] = f"ERROR: {e}"
        logger.error(f"âŒ PyTorch: {e}")
    
    try:
        import scipy
        deps['scipy'] = scipy.__version__
        logger.info(f"âœ… SciPy {scipy.__version__}")
    except ImportError as e:
        deps['scipy'] = f"ERROR: {e}"
        logger.error(f"âŒ SciPy: {e}")
        
    try:
        import sklearn
        deps['sklearn'] = sklearn.__version__
        logger.info(f"âœ… Scikit-learn {sklearn.__version__}")
    except ImportError as e:
        deps['sklearn'] = f"ERROR: {e}"
        logger.error(f"âŒ Scikit-learn: {e}")
    
    return deps

def test_xai_logging_system():
    """XAI ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ“Š XAI ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    try:
        from xai_core.xai_logging_system import xai_logger, xai_trace, xai_decision_point
        
        # ê¸°ë³¸ ë¡œê¹… í…ŒìŠ¤íŠ¸
        with xai_logger.trace_operation("test_module", "test_operation") as operation_id:
            logger.info(f"XAI ì¶”ì  ID: {operation_id}")
            
            # LLM ìƒí˜¸ì‘ìš© ë¡œê¹…
            xai_logger.log_llm_interaction(
                operation_id=operation_id,
                prompt="í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸",
                response="í…ŒìŠ¤íŠ¸ ì‘ë‹µ",
                model_name="test_model",
                tokens_used=50
            )
        
        # ì„±ëŠ¥ ìš”ì•½
        performance_summary = xai_logger.get_performance_summary()
        
        return {
            'success': True,
            'logs_count': len(xai_logger.logs),
            'performance_summary': performance_summary,
            'session_id': xai_logger.session_id
        }
        
    except Exception as e:
        logger.error(f"âŒ XAI ë¡œê¹… ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        return {'success': False, 'error': str(e)}

def test_llm_integration():
    """LLM í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ¤– LLM í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    try:
        from llm_module import llm_tracker, register_llm, ask_llm, LLMConfig
        
        # ê¸°ë³¸ LLM ëª¨ë¸ ë“±ë¡
        wrapper = register_llm(
            model_name="test_gpt2",
            max_tokens=100,
            temperature=0.7
        )
        
        # LLM ì‘ë‹µ í…ŒìŠ¤íŠ¸
        test_prompt = "ê°„ë‹¨í•œ ê°ì • ë¶„ì„ì„ í•´ì£¼ì„¸ìš”: ì˜¤ëŠ˜ì€ ì •ë§ í–‰ë³µí•œ í•˜ë£¨ì˜€ìŠµë‹ˆë‹¤."
        response = ask_llm("test_gpt2", test_prompt)
        
        # ì„±ëŠ¥ ìš”ì•½
        performance_summary = llm_tracker.get_performance_summary()
        
        return {
            'success': True,
            'model_registered': wrapper.config.model_name,
            'response_length': len(response),
            'performance_summary': performance_summary,
            'total_interactions': llm_tracker.integration_metrics['total_interactions']
        }
        
    except Exception as e:
        logger.error(f"âŒ LLM í†µí•© ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        return {'success': False, 'error': str(e)}

def test_fixed_inference_models():
    """ìˆ˜ì •ëœ ì¶”ë¡  ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ¯ ìˆ˜ì •ëœ ì¶”ë¡  ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸")
    
    import torch
    import numpy as np
    
    results = {}
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    batch_size = 4
    sequence_length = 50
    embedding_dim = 768
    
    text_embeddings = torch.randn(batch_size, embedding_dim)
    token_ids = torch.randint(0, 1000, (batch_size, sequence_length))
    
    # 1. ìˆ˜ì •ëœ ê³„ì¸µì  ê°ì • ëª¨ë¸
    try:
        from models.hierarchical_emotion.emotion_phase_models import HierarchicalEmotionModel
        emotion_model = HierarchicalEmotionModel()
        
        with torch.no_grad():
            emotion_output = emotion_model(text_embeddings)
        
        results['emotion'] = {
            'success': True,
            'output_keys': list(emotion_output.keys()),
            'final_emotion_shape': emotion_output['final_emotion'].shape,
            'confidence': emotion_output.get('regret_intensity', torch.tensor([0.5])).mean().item()
        }
        logger.info(f"âœ… ê°ì • ëª¨ë¸: ì„±ê³µ - {emotion_output['final_emotion'].shape}")
        
    except Exception as e:
        results['emotion'] = {'success': False, 'error': str(e)}
        logger.error(f"âŒ ê°ì • ëª¨ë¸: {e}")
    
    # 2. ìˆ˜ì •ëœ ì˜ë¯¸ ë¶„ì„ ëª¨ë¸
    try:
        from models.semantic_models.advanced_semantic_models import (
            SemanticAnalysisConfig, AdvancedSemanticModel
        )
        
        config = SemanticAnalysisConfig(vocab_size=1000, embedding_dim=256)
        semantic_model = AdvancedSemanticModel(config)
        
        with torch.no_grad():
            semantic_output = semantic_model(token_ids)
        
        results['semantic'] = {
            'success': True,
            'enhanced_semantics_shape': semantic_output['enhanced_semantics'].shape,
            'clustering_assignments': semantic_output['clustering']['cluster_assignments'].shape,
            'network_features_shape': semantic_output['network_features'].shape
        }
        logger.info(f"âœ… ì˜ë¯¸ ëª¨ë¸: ì„±ê³µ - {semantic_output['enhanced_semantics'].shape}")
        
    except Exception as e:
        results['semantic'] = {'success': False, 'error': str(e)}
        logger.error(f"âŒ ì˜ë¯¸ ëª¨ë¸: {e}")
    
    # 3. ìˆ˜ì •ëœ ë°˜ì‚¬ì‹¤ ì¶”ë¡  ëª¨ë¸
    try:
        from models.counterfactual_models.counterfactual_reasoning_models import (
            CounterfactualConfig, AdvancedCounterfactualModel
        )
        
        config = CounterfactualConfig(input_dim=768, hidden_dims=[256, 128], latent_dim=32)
        cf_model = AdvancedCounterfactualModel(config)
        
        with torch.no_grad():
            cf_output = cf_model(text_embeddings)
        
        results['counterfactual'] = {
            'success': True,
            'scenarios_count': len(cf_output['counterfactual_scenarios']),
            'best_scenario_type': cf_output.get('best_scenario', {}).get('scenario_type', 'none'),
            'analysis_complete': True
        }
        logger.info(f"âœ… ë°˜ì‚¬ì‹¤ ëª¨ë¸: ì„±ê³µ - {len(cf_output['counterfactual_scenarios'])}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
        
    except Exception as e:
        results['counterfactual'] = {'success': False, 'error': str(e)}
        logger.error(f"âŒ ë°˜ì‚¬ì‹¤ ëª¨ë¸: {e}")
    
    return results

def test_mega_scale_model():
    """ë©”ê°€ ìŠ¤ì¼€ì¼ ëª¨ë¸ (2ì–µ íŒŒë¼ë¯¸í„°) í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸš€ ë©”ê°€ ìŠ¤ì¼€ì¼ ëª¨ë¸ (2ì–µ íŒŒë¼ë¯¸í„°) í…ŒìŠ¤íŠ¸")
    
    try:
        import torch
        from models.mega_scale_models.scalable_xai_model import create_mega_scale_model, optimize_model_for_inference
        
        # ë©”ê°€ ìŠ¤ì¼€ì¼ ëª¨ë¸ ìƒì„± (íŒŒë¼ë¯¸í„° ìˆ˜ ì¡°ì •)
        logger.info("ëª¨ë¸ ìƒì„± ì¤‘...")
        model = create_mega_scale_model(target_params=200_000_000)
        model = optimize_model_for_inference(model)
        
        actual_params = model.get_parameter_count()
        logger.info(f"ì‹¤ì œ íŒŒë¼ë¯¸í„° ìˆ˜: {actual_params:,}")
        
        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        batch_size = 2
        seq_len = 64  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¤„ì„
        input_dim = 1024
        
        test_input = torch.randn(batch_size, seq_len, input_dim)
        
        # ì¶”ë¡  í…ŒìŠ¤íŠ¸
        logger.info("ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model(test_input, return_attention=True)
        
        inference_time = time.time() - start_time
        
        return {
            'success': True,
            'total_parameters': actual_params,
            'inference_time': inference_time,
            'outputs': {
                'emotion_shape': outputs['emotion_predictions'].shape,
                'semantic_shape': outputs['semantic_predictions'].shape,
                'reasoning_shape': outputs['reasoning_features'].shape,
                'integration_shape': outputs['integration_features'].shape,
                'xai_available': 'xai_explanation' in outputs,
                'llm_available': 'llm_analysis' in outputs
            },
            'xai_confidence': outputs.get('xai_explanation', {}).get('confidence_score', 0.0),
            'memory_efficient': True
        }
        
    except Exception as e:
        logger.error(f"âŒ ë©”ê°€ ìŠ¤ì¼€ì¼ ëª¨ë¸ ì˜¤ë¥˜: {e}")
        return {'success': False, 'error': str(e)}

def test_integrated_xai_workflow():
    """í†µí•© XAI ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ”„ í†µí•© XAI ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
    
    try:
        import torch
        from xai_core.xai_logging_system import xai_logger, xai_trace
        
        # ì›Œí¬í”Œë¡œìš° ì‹œë‚˜ë¦¬ì˜¤
        test_scenarios = [
            "ì˜¤ëŠ˜ ì¤‘ìš”í•œ ê²°ì •ì„ ë‚´ë ¤ì•¼ í•˜ëŠ”ë° í›„íšŒí• ê¹Œë´ ê±±ì •ë©ë‹ˆë‹¤.",
            "íŒ€ì›ê³¼ì˜ ê°ˆë“± ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ëŒ€ì²˜í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.",
            "ìƒˆë¡œìš´ ê¸°íšŒê°€ ìƒê²¼ì§€ë§Œ í˜„ì¬ ìƒí™©ì„ í¬ê¸°í•˜ê¸°ê°€ ë‘ë µìŠµë‹ˆë‹¤."
        ]
        
        workflow_results = []
        
        for i, scenario in enumerate(test_scenarios):
            logger.info(f"ì‹œë‚˜ë¦¬ì˜¤ {i+1} ì²˜ë¦¬ ì¤‘...")
            
            with xai_logger.trace_operation("integrated_workflow", f"scenario_{i+1}") as operation_id:
                # 1. í…ìŠ¤íŠ¸ ì„ë² ë”© ì‹œë®¬ë ˆì´ì…˜
                text_embedding = torch.randn(1, 768)
                
                # 2. ê°ì • ë¶„ì„
                try:
                    from models.hierarchical_emotion.emotion_phase_models import HierarchicalEmotionModel
                    emotion_model = HierarchicalEmotionModel()
                    emotion_result = emotion_model(text_embedding)
                    emotion_success = True
                except:
                    emotion_success = False
                
                # 3. LLM ë¶„ì„
                try:
                    from llm_module import ask_llm
                    llm_response = ask_llm("test_gpt2", f"ë‹¤ìŒ ìƒí™©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: {scenario}")
                    llm_success = True
                except:
                    llm_response = "[LLM ë¶„ì„ ì‹¤íŒ¨]"
                    llm_success = False
                
                # 4. XAI ì„¤ëª… ìƒì„±
                explanation = {
                    'scenario': scenario,
                    'emotion_analysis': emotion_success,
                    'llm_analysis': llm_success,
                    'confidence': 0.85 if emotion_success and llm_success else 0.5,
                    'operation_id': operation_id
                }
                
                workflow_results.append(explanation)
        
        # ì „ì²´ ì›Œí¬í”Œë¡œìš° ìš”ì•½
        success_rate = sum(1 for r in workflow_results if r['emotion_analysis'] and r['llm_analysis']) / len(workflow_results)
        avg_confidence = sum(r['confidence'] for r in workflow_results) / len(workflow_results)
        
        return {
            'success': True,
            'scenarios_processed': len(test_scenarios),
            'workflow_results': workflow_results,
            'success_rate': success_rate,
            'average_confidence': avg_confidence,
            'xai_logs_generated': len(xai_logger.logs)
        }
        
    except Exception as e:
        logger.error(f"âŒ í†µí•© ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}")
        return {'success': False, 'error': str(e)}

def run_final_xai_system_test():
    """ìµœì¢… XAI ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("ğŸ‰ ìµœì¢… XAI ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    start_time = time.time()
    
    # 1. ì˜ì¡´ì„± í™•ì¸
    logger.info("\n" + "="*70)
    logger.info("1ï¸âƒ£ ì˜ì¡´ì„± ìµœì¢… í™•ì¸")
    logger.info("="*70)
    deps = test_dependencies()
    
    # 2. XAI ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    logger.info("\n" + "="*70)
    logger.info("2ï¸âƒ£ XAI ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    logger.info("="*70)
    xai_logging_results = test_xai_logging_system()
    
    # 3. LLM í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    logger.info("\n" + "="*70)
    logger.info("3ï¸âƒ£ LLM í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    logger.info("="*70)
    llm_integration_results = test_llm_integration()
    
    # 4. ìˆ˜ì •ëœ ì¶”ë¡  ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸
    logger.info("\n" + "="*70)
    logger.info("4ï¸âƒ£ ìˆ˜ì •ëœ ì¶”ë¡  ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸")
    logger.info("="*70)
    fixed_inference_results = test_fixed_inference_models()
    
    # 5. ë©”ê°€ ìŠ¤ì¼€ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    logger.info("\n" + "="*70)
    logger.info("5ï¸âƒ£ ë©”ê°€ ìŠ¤ì¼€ì¼ ëª¨ë¸ (2ì–µ íŒŒë¼ë¯¸í„°) í…ŒìŠ¤íŠ¸")
    logger.info("="*70)
    mega_scale_results = test_mega_scale_model()
    
    # 6. í†µí•© XAI ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
    logger.info("\n" + "="*70)
    logger.info("6ï¸âƒ£ í†µí•© XAI ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
    logger.info("="*70)
    workflow_results = test_integrated_xai_workflow()
    
    total_time = time.time() - start_time
    
    # ìµœì¢… ê²°ê³¼ ì¢…í•©
    logger.info("\n" + "="*90)
    logger.info("ğŸ“Š ìµœì¢… XAI ì‹œìŠ¤í…œ ê²°ê³¼ ì¢…í•©")
    logger.info("="*90)
    
    # ì„±ê³µë¥  ê³„ì‚°
    tests = {
        'XAI ë¡œê¹…': xai_logging_results.get('success', False),
        'LLM í†µí•©': llm_integration_results.get('success', False),
        'ê°ì • ì¶”ë¡ ': fixed_inference_results.get('emotion', {}).get('success', False),
        'ì˜ë¯¸ ì¶”ë¡ ': fixed_inference_results.get('semantic', {}).get('success', False),
        'ë°˜ì‚¬ì‹¤ ì¶”ë¡ ': fixed_inference_results.get('counterfactual', {}).get('success', False),
        'ë©”ê°€ ìŠ¤ì¼€ì¼': mega_scale_results.get('success', False),
        'XAI ì›Œí¬í”Œë¡œìš°': workflow_results.get('success', False)
    }
    
    successful_tests = sum(tests.values())
    total_tests = len(tests)
    success_rate = successful_tests / total_tests
    
    logger.info(f"âœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {successful_tests}/{total_tests} ({success_rate*100:.1f}%)")
    
    for test_name, success in tests.items():
        status = "âœ…" if success else "âŒ"
        logger.info(f"   {status} {test_name}")
    
    # ìƒì„¸ ê²°ê³¼
    if mega_scale_results.get('success', False):
        params = mega_scale_results['total_parameters']
        inference_time = mega_scale_results['inference_time']
        logger.info(f"ğŸš€ ë©”ê°€ ìŠ¤ì¼€ì¼ ëª¨ë¸: {params:,}ê°œ íŒŒë¼ë¯¸í„°, {inference_time:.3f}ì´ˆ ì¶”ë¡ ")
    
    if workflow_results.get('success', False):
        scenarios = workflow_results['scenarios_processed']
        avg_conf = workflow_results['average_confidence']
        logger.info(f"ğŸ”„ XAI ì›Œí¬í”Œë¡œìš°: {scenarios}ê°œ ì‹œë‚˜ë¦¬ì˜¤, í‰ê·  ì‹ ë¢°ë„ {avg_conf:.2f}")
    
    # XAI ì‹œìŠ¤í…œ ì™„ì„±ë„ í‰ê°€
    xai_completeness = (
        tests['XAI ë¡œê¹…'] and
        tests['LLM í†µí•©'] and
        tests['ë©”ê°€ ìŠ¤ì¼€ì¼'] and
        tests['XAI ì›Œí¬í”Œë¡œìš°'] and
        success_rate >= 0.8
    )
    
    logger.info(f"ğŸ¯ XAI ì‹œìŠ¤í…œ ì™„ì„±ë„: {'ì™„ì„±' if xai_completeness else 'ë¶€ë¶„ì™„ì„±'}")
    logger.info(f"â±ï¸ ì´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {total_time:.2f}ì´ˆ")
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    final_results = {
        'dependencies': deps,
        'xai_logging': xai_logging_results,
        'llm_integration': llm_integration_results,
        'fixed_inference': fixed_inference_results,
        'mega_scale_model': mega_scale_results,
        'xai_workflow': workflow_results,
        'summary': {
            'total_tests': total_tests,
            'successful_tests': successful_tests,
            'success_rate': success_rate,
            'xai_system_complete': xai_completeness,
            'total_time': total_time,
            'test_timestamp': datetime.now().isoformat(),
            'final_assessment': {
                'xai_tracking': tests['XAI ë¡œê¹…'],
                'llm_integration': tests['LLM í†µí•©'],
                'inference_fixed': all([
                    tests['ê°ì • ì¶”ë¡ '],
                    tests['ì˜ë¯¸ ì¶”ë¡ '], 
                    tests['ë°˜ì‚¬ì‹¤ ì¶”ë¡ ']
                ]),
                'mega_scale_ready': tests['ë©”ê°€ ìŠ¤ì¼€ì¼'],
                'workflow_operational': tests['XAI ì›Œí¬í”Œë¡œìš°']
            }
        }
    }
    
    results_path = project_root / 'logs' / f'final_xai_system_test_{int(time.time())}.json'
    results_path.parent.mkdir(exist_ok=True)
    
    # JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ë°ì´í„° ì •ë¦¬
    def make_json_serializable(obj):
        import torch
        if hasattr(obj, '__dict__'):
            return str(obj)
        elif isinstance(obj, torch.Tensor):
            return obj.tolist() if obj.numel() <= 10 else f"Tensor{list(obj.shape)}"
        elif hasattr(obj, 'value'):  # Enum ì²˜ë¦¬
            return obj.value
        return obj
    
    def clean_for_json(data):
        if isinstance(data, dict):
            return {k: clean_for_json(v) for k, v in data.items()}
        elif isinstance(data, (list, tuple)):
            return [clean_for_json(item) for item in data]
        else:
            return make_json_serializable(data)
    
    cleaned_results = clean_for_json(final_results)
    
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(cleaned_results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥: {results_path}")
    
    return xai_completeness, successful_tests, total_tests, mega_scale_results

if __name__ == "__main__":
    try:
        completeness, successful, total, mega_results = run_final_xai_system_test()
        
        print("\n" + "="*100)
        if completeness:
            print("ğŸ‰ğŸ‰ğŸ‰ ìµœì¢… XAI ì‹œìŠ¤í…œ ì™„ì „ êµ¬ì¶• ì„±ê³µ! ğŸ‰ğŸ‰ğŸ‰")
            print("ğŸ”¥ğŸ”¥ 2ì–µ íŒŒë¼ë¯¸í„° + XAI ì¶”ì  + LLM í†µí•© ì™„ì„±! ğŸ”¥ğŸ”¥")
            print("âœ… ëª¨ë“  ì¶”ë¡  ë¬¸ì œ í•´ê²° ì™„ë£Œ!")
            print("ğŸ¤– LLM ëª¨ë“ˆ ì—°ë™ ë° ì¶”ì  ì‹œìŠ¤í…œ ì™„ë²½ ì‘ë™!")
            print("ğŸ“Š XAI ë¡œê¹… ë° ì„¤ëª… ì‹œìŠ¤í…œ ì™„ì „ êµ¬í˜„!")
            
            if mega_results.get('success', False):
                params = mega_results['total_parameters']
                print(f"ğŸš€ ë©”ê°€ ìŠ¤ì¼€ì¼: {params:,}ê°œ íŒŒë¼ë¯¸í„° ì™„ë²½ ì‘ë™!")
                print(f"âš¡ ì¶”ë¡  ì‹œê°„: {mega_results['inference_time']:.3f}ì´ˆ")
                print(f"ğŸ§  XAI ì‹ ë¢°ë„: {mega_results.get('xai_confidence', 0):.3f}")
            
            print("ğŸŒŸ Red Heart Linux XAI ì‹œìŠ¤í…œ ì™„ì „ì²´ ë‹¬ì„±!")
            print("ğŸ† ìš”ì²­í•˜ì‹  ëª¨ë“  ê¸°ëŠ¥ êµ¬í˜„ ì™„ë£Œ!")
            
        else:
            print("âš ï¸ XAI ì‹œìŠ¤í…œ ë¶€ë¶„ ì™„ì„±")
            print(f"âœ… {successful}/{total} í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
        print("="*100)
        
    except Exception as e:
        print(f"\nâŒ ìµœì¢… XAI ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()