#!/usr/bin/env python3
"""
NaN Loss ë¬¸ì œ í•´ê²° íŒ¨ì¹˜
Fix NaN Loss Issues in Unified Learning System

ë¬¸ì œ ì›ì¸:
1. ì™„ì „ ëœë¤ íƒ€ê²Ÿìœ¼ë¡œ ì¸í•œ í•™ìŠµ ë¶ˆì•ˆì •ì„±
2. cosine_embedding_loss ë¶ˆì•ˆì •ì„±
3. ì°¨ì› ë¶ˆì¼ì¹˜ ë° ì •ë³´ ì†ì‹¤
4. Loss ìŠ¤ì¼€ì¼ ë¶ˆê· í˜•

í•´ê²° ë°©ì•ˆ:
1. ì•ˆì •ì ì¸ synthetic íƒ€ê²Ÿ ìƒì„±
2. ì•ˆì „í•œ loss í•¨ìˆ˜ ì‚¬ìš©
3. ì°¨ì› ê²€ì¦ ë° ì•ˆì „í•œ ë³€í™˜
4. Loss ì •ê·œí™” ë° í´ë¦¬í•‘
"""

import torch
import torch.nn.functional as F
from typing import Dict, Any, List
import logging

logger = logging.getLogger(__name__)

class StableLossCalculator:
    """ì•ˆì •ì ì¸ Loss ê³„ì‚°ê¸°"""
    
    def __init__(self):
        self.loss_history = {}  # ê° í—¤ë“œë³„ loss íˆìŠ¤í† ë¦¬
        self.synthetic_targets = {}  # í•©ì„± íƒ€ê²Ÿ ìºì‹œ
        
    def calculate_stable_losses(self, outputs: Dict[str, torch.Tensor], 
                              batch_data: Dict[str, Any], 
                              active_heads: List, 
                              head_configs: Dict) -> Dict[str, torch.Tensor]:
        """ì•ˆì •ì ì¸ ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°"""
        losses = {}
        batch_size = batch_data.get('batch_size', 1)
        
        for head_type in active_heads:
            config = head_configs[head_type]
            head_output = outputs['head_outputs'][head_type.value]
            
            # ì¶œë ¥ ê²€ì¦ ë° ì •ê·œí™”
            head_output = self._validate_and_normalize_output(head_output, head_type)
            
            # ì•ˆì •ì ì¸ íƒ€ê²Ÿ ìƒì„±
            target = self._generate_stable_target(head_output, head_type, batch_size)
            
            # ì•ˆì „í•œ loss ê³„ì‚°
            loss = self._calculate_safe_loss(head_output, target, head_type, config)
            
            # Loss í´ë¦¬í•‘ ë° ê²€ì¦
            loss = self._clip_and_validate_loss(loss, head_type)
            
            losses[head_type.value] = loss
            
        return losses
    
    def _validate_and_normalize_output(self, output: torch.Tensor, head_type) -> torch.Tensor:
        """ì¶œë ¥ ê²€ì¦ ë° ì •ê·œí™”"""
        # NaN/Inf ì²´í¬
        if torch.isnan(output).any() or torch.isinf(output).any():
            logger.warning(f"{head_type.value} í—¤ë“œ ì¶œë ¥ì— NaN/Inf ë°œê²¬ - 0ìœ¼ë¡œ ëŒ€ì²´")
            output = torch.nan_to_num(output, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # ê·¹ê°’ í´ë¦¬í•‘
        output = torch.clamp(output, min=-10.0, max=10.0)
        
        # L2 ì •ê·œí™” (í•„ìš”ì‹œ)
        if output.dim() > 1 and output.size(-1) > 1:
            output = F.normalize(output, p=2, dim=-1)
            
        return output
    
    def _generate_stable_target(self, output: torch.Tensor, head_type, batch_size: int) -> torch.Tensor:
        """ì•ˆì •ì ì¸ í•©ì„± íƒ€ê²Ÿ ìƒì„±"""
        device = output.device
        output_shape = output.shape
        
        # í—¤ë“œ íƒ€ì…ë³„ ì•ˆì •ì ì¸ íƒ€ê²Ÿ ìƒì„±
        if 'EMOTION' in head_type.value.upper():
            # ê°ì •: ì†Œí”„íŠ¸ ì›-í•« ì¸ì½”ë”©
            num_classes = min(output_shape[-1], 10)
            target_classes = torch.randint(0, num_classes, (batch_size,), device=device)
            target = F.one_hot(target_classes, num_classes=num_classes).float()
            
            # ë¼ë²¨ ìŠ¤ë¬´ë”© ì ìš©
            target = target * 0.9 + 0.1 / num_classes
            
        elif 'BENTHAM' in head_type.value.upper():
            # ë²¤ë‹´: 0.5 ì¤‘ì‹¬ì˜ ì‘ì€ ë³€ë™
            target = 0.5 + 0.1 * torch.randn(batch_size, 1, device=device)
            target = torch.clamp(target, 0.0, 1.0)
            
        elif 'SEMANTIC' in head_type.value.upper():
            # ì˜ë¯¸: ì •ê·œí™”ëœ ëœë¤ ë²¡í„° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì•ˆì •í™”)
            target = torch.randn(output_shape, device=device)
            target = F.normalize(target, p=2, dim=-1)
            
        elif 'REGRET' in head_type.value.upper():
            # í›„íšŒ: 0 ì¤‘ì‹¬ì˜ ì‘ì€ ê°’
            target = 0.1 * torch.randn(batch_size, 1, device=device)
            target = torch.clamp(target, -1.0, 1.0)
            
        else:
            # ê¸°ë³¸: ì¶œë ¥ê³¼ ìœ ì‚¬í•œ ìŠ¤ì¼€ì¼ì˜ ì•ˆì •ì ì¸ íƒ€ê²Ÿ
            target = 0.1 * torch.randn_like(output)
            target = torch.clamp(target, -1.0, 1.0)
            
        return target
    
    def _calculate_safe_loss(self, output: torch.Tensor, target: torch.Tensor, 
                           head_type, config) -> torch.Tensor:
        """ì•ˆì „í•œ loss ê³„ì‚°"""
        
        try:
            if 'EMOTION' in head_type.value.upper():
                # ê°ì •: ì•ˆì „í•œ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼
                if output.dim() > 1:
                    # ì†Œí”„íŠ¸ë§¥ìŠ¤ + í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ëŒ€ì‹  KL divergence ì‚¬ìš©
                    output_probs = F.softmax(output, dim=-1)
                    target_probs = target
                    loss = F.kl_div(F.log_softmax(output, dim=-1), target_probs, reduction='batchmean')
                else:
                    loss = F.mse_loss(output, target.mean(dim=-1, keepdim=True))
                    
            elif 'BENTHAM' in head_type.value.upper():
                # ë²¤ë‹´: Smooth L1 Loss (ë” ì•ˆì •ì )
                loss = F.smooth_l1_loss(output[:, :1] if output.dim() > 1 else output, target)
                
            elif 'SEMANTIC' in head_type.value.upper():
                # ì˜ë¯¸: ì•ˆì „í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ëŒ€ì‹  MSE ì‚¬ìš©
                loss = F.mse_loss(output, target)
                
            elif 'REGRET' in head_type.value.upper():
                # í›„íšŒ: Huber Loss ì‚¬ìš©
                loss = F.smooth_l1_loss(output[:, :1] if output.dim() > 1 else output, target)
                
            else:
                # ê¸°ë³¸: MSE
                loss = F.mse_loss(output, target)
                
        except Exception as e:
            logger.warning(f"{head_type.value} loss ê³„ì‚° ì‹¤íŒ¨: {e}, fallback MSE ì‚¬ìš©")
            loss = F.mse_loss(output, target)
            
        return loss
    
    def _clip_and_validate_loss(self, loss: torch.Tensor, head_type) -> torch.Tensor:
        """Loss í´ë¦¬í•‘ ë° ê²€ì¦"""
        
        # NaN/Inf ì²´í¬
        if torch.isnan(loss) or torch.isinf(loss):
            logger.warning(f"{head_type.value} lossê°€ NaN/Infì…ë‹ˆë‹¤. 1.0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return torch.tensor(1.0, device=loss.device, requires_grad=True)
        
        # ê·¹ê°’ í´ë¦¬í•‘
        loss = torch.clamp(loss, min=0.0, max=100.0)
        
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        if head_type.value not in self.loss_history:
            self.loss_history[head_type.value] = []
        
        self.loss_history[head_type.value].append(float(loss.item()))
        
        # ìµœê·¼ 10ê°œë§Œ ìœ ì§€
        if len(self.loss_history[head_type.value]) > 10:
            self.loss_history[head_type.value] = self.loss_history[head_type.value][-10:]
        
        return loss


def patch_unified_learning_system():
    """unified_learning_system.pyì— íŒ¨ì¹˜ ì ìš©"""
    
    print("ğŸ”§ NaN Loss ë¬¸ì œ í•´ê²° íŒ¨ì¹˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤...")
    
    # unified_learning_system.py ì½ê¸°
    try:
        with open('/mnt/c/large_project/linux_red_heart/unified_learning_system.py', 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ê¸°ì¡´ _calculate_losses í•¨ìˆ˜ë¥¼ ì•ˆì „í•œ ë²„ì „ìœ¼ë¡œ êµì²´
        old_function = """    def _calculate_losses(self, outputs: Dict[str, torch.Tensor],
                         batch_data: Dict[str, Any],
                         active_heads: List[HeadType]) -> Dict[str, torch.Tensor]:
        \"\"\"ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°\"\"\"
        losses = {}
        
        # ê° í—¤ë“œë³„ ì†ì‹¤ ê³„ì‚°
        for head_type in active_heads:
            config = self.head_configs[head_type]
            
            # ê°€ìƒì˜ íƒ€ê²Ÿ ìƒì„± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” batch_dataì—ì„œ ì¶”ì¶œ)
            batch_size = batch_data.get('batch_size', 1)
            
            if head_type == HeadType.EMOTION_EMPATHY:
                # ê°ì • ë¶„ë¥˜ ì†ì‹¤
                target = torch.randint(0, 10, (batch_size,), device=outputs['head_outputs'][head_type.value].device)
                logits = outputs['head_outputs'][head_type.value][:, :10]  # 10ê°œ ê°ì • í´ë˜ìŠ¤
                loss = F.cross_entropy(logits, target, label_smoothing=config.label_smoothing)
                
            elif head_type == HeadType.BENTHAM_FROMM:
                # ìœ¤ë¦¬ ì ìˆ˜ íšŒê·€ ì†ì‹¤
                target = torch.rand(batch_size, 1, device=outputs['head_outputs'][head_type.value].device)
                pred = outputs['head_outputs'][head_type.value][:, :1]
                loss = F.mse_loss(pred, target)
                
            elif head_type == HeadType.SEMANTIC_SURD:
                # ì˜ë¯¸ ìœ ì‚¬ë„ ì†ì‹¤
                target = torch.rand(batch_size, 768, device=outputs['head_outputs'][head_type.value].device)
                pred = outputs['head_outputs'][head_type.value]
                loss = F.cosine_embedding_loss(pred, target, torch.ones(batch_size, device=pred.device))
                
            elif head_type == HeadType.REGRET_LEARNING:
                # í›„íšŒ ì˜ˆì¸¡ ì†ì‹¤
                target = torch.rand(batch_size, 1, device=outputs['head_outputs'][head_type.value].device)
                pred = outputs['head_outputs'][head_type.value][:, :1]
                loss = F.smooth_l1_loss(pred, target)
                
            else:
                # ê¸°ë³¸ ì†ì‹¤
                target = torch.randn_like(outputs['head_outputs'][head_type.value])
                loss = F.mse_loss(outputs['head_outputs'][head_type.value], target)
            
            # ê°€ì¤‘ì¹˜ ì ìš©
            losses[head_type.value] = loss * config.loss_weight
        
        return losses"""
        
        new_function = """    def _calculate_losses(self, outputs: Dict[str, torch.Tensor],
                         batch_data: Dict[str, Any],
                         active_heads: List[HeadType]) -> Dict[str, torch.Tensor]:
        \"\"\"ì•ˆì •ì ì¸ ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚° (NaN ë°©ì§€)\"\"\"
        
        # ì•ˆì •ì ì¸ loss ê³„ì‚°ê¸° ì´ˆê¸°í™”
        if not hasattr(self, '_stable_loss_calculator'):
            from fix_nan_loss import StableLossCalculator
            self._stable_loss_calculator = StableLossCalculator()
        
        # ì•ˆì •ì ì¸ loss ê³„ì‚°
        losses = self._stable_loss_calculator.calculate_stable_losses(
            outputs, batch_data, active_heads, self.head_configs
        )
        
        # ê°€ì¤‘ì¹˜ ì ìš© ë° ì¶”ê°€ ì•ˆì „ ê²€ì‚¬
        weighted_losses = {}
        for head_type in active_heads:
            config = self.head_configs[head_type]
            loss = losses[head_type.value]
            
            # ê°€ì¤‘ì¹˜ ì ìš©
            weighted_loss = loss * config.loss_weight
            
            # ìµœì¢… ì•ˆì „ ê²€ì‚¬
            if torch.isnan(weighted_loss) or torch.isinf(weighted_loss):
                logger.warning(f"{head_type.value} ê°€ì¤‘ loss NaN/Inf ê°ì§€, 1.0ìœ¼ë¡œ ëŒ€ì²´")
                weighted_loss = torch.tensor(1.0, device=loss.device, requires_grad=True)
            
            weighted_losses[head_type.value] = weighted_loss
        
        return weighted_losses"""
        
        # í•¨ìˆ˜ êµì²´
        if old_function in content:
            content = content.replace(old_function, new_function)
            print("âœ… _calculate_losses í•¨ìˆ˜ íŒ¨ì¹˜ ì™„ë£Œ")
        else:
            print("âš ï¸  ê¸°ì¡´ _calculate_losses í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        # íŒŒì¼ì— import ì¶”ê°€
        import_line = "from fix_nan_loss import StableLossCalculator"
        if import_line not in content:
            # import ì„¹ì…˜ì— ì¶”ê°€
            import_insertion_point = "from intelligent_synergy_system import IntelligentSynergySystem"
            if import_insertion_point in content:
                content = content.replace(import_insertion_point, f"{import_insertion_point}\n{import_line}")
                print("âœ… import êµ¬ë¬¸ ì¶”ê°€ ì™„ë£Œ")
        
        # ë°±ì—… íŒŒì¼ ìƒì„±
        backup_path = '/mnt/c/large_project/linux_red_heart/unified_learning_system_backup.py'
        with open(backup_path, 'w', encoding='utf-8') as f:
            with open('/mnt/c/large_project/linux_red_heart/unified_learning_system.py', 'r', encoding='utf-8') as original:
                f.write(original.read())
        print(f"âœ… ë°±ì—… íŒŒì¼ ìƒì„±: {backup_path}")
        
        # ìˆ˜ì •ëœ ë‚´ìš© ì €ì¥
        with open('/mnt/c/large_project/linux_red_heart/unified_learning_system.py', 'w', encoding='utf-8') as f:
            f.write(content)
        
        print("ğŸ‰ NaN Loss ë¬¸ì œ í•´ê²° íŒ¨ì¹˜ ì ìš© ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    patch_unified_learning_system()