# ğŸ¯ Hierarchical Learning Rate Sweep Strategy
## 5-5-5-5 Coarse-to-Fine Optimization

### ğŸ“Š ì „ì²´ ê°œìš”
ì ì§„ì  ë²”ìœ„ ì¶•ì†Œë¥¼ í†µí•œ íš¨ìœ¨ì ì¸ í•™ìŠµë¥  íƒìƒ‰ ì „ëµ

```
Stage 0 (Base): 5 points  â†’ 4 intervals
Stage 1: 5 points â†’ Select top 2 intervals â†’ 2 refined intervals  
Stage 2: 5 points â†’ Select top 2 intervals â†’ 2 refined intervals
Stage 3: 5 points â†’ Select top 2 intervals â†’ 2 refined intervals
Stage 4: 5 points â†’ Final precision tuning
---
Total: 25 data points (vs 500+ for full grid search)
```

### ğŸ” ë‹¨ê³„ë³„ ìƒì„¸ ì „ëµ

#### Stage 0: Initial Sweep (ì™„ë£Œ)
```python
initial_lrs = [1e-5, 5.6e-5, 3.2e-4, 1.8e-3, 1e-2]
# ê²°ê³¼: 5ê°œ ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ì§‘
```

#### Stage 1: First Refinement
```python
# ìƒìœ„ 2ê°œ êµ¬ê°„ ì„ íƒ (ì˜ˆ: [5.6e-5, 3.2e-4], [3.2e-4, 1.8e-3])
# ê° êµ¬ê°„ì„ 2-3ê°œë¡œ ì„¸ë¶„í™”
stage1_lrs = np.logspace(
    np.log10(5.6e-5), 
    np.log10(1.8e-3), 
    5
)
# ê²½ê³„ ì¤‘ë³µ ì œê±° í›„ 5ê°œ ìƒˆ í¬ì¸íŠ¸
```

#### Stage 2-4: Progressive Refinement
```python
def generate_next_stage(best_intervals, n_points=5):
    """
    ìƒìœ„ ì„±ëŠ¥ êµ¬ê°„ì—ì„œ ìƒˆë¡œìš´ íƒìƒ‰ì  ìƒì„±
    - ê²½ê³„ ì¤‘ë³µ ë°©ì§€
    - ë¡œê·¸ ìŠ¤ì¼€ì¼ ê· ë“± ë¶„í¬
    """
    new_points = []
    for (low, high) in best_intervals:
        points = np.logspace(
            np.log10(low),
            np.log10(high),
            n_points // len(best_intervals) + 1
        )[1:-1]  # ê²½ê³„ ì œì™¸
        new_points.extend(points)
    return new_points[:n_points]
```

### ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

ê° ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘í•  ë°ì´í„°:
1. **Loss Metrics**
   - Train/Val loss per epoch
   - Module-specific losses
   
2. **Convergence Indicators**
   - Loss reduction rate
   - Gradient norms
   - Stability scores

3. **Efficiency Metrics**
   - Time per epoch
   - Memory usage
   - Parameter utilization

### ğŸ¨ ì‹œê°í™” ê³„íš

```python
# 1. Convergence Heatmap
# Xì¶•: Stage (0-4)
# Yì¶•: Learning Rate (log scale)
# Color: Performance metric

# 2. Search Path Visualization
# ê° ë‹¨ê³„ì—ì„œ ì„ íƒëœ êµ¬ê°„ í‘œì‹œ
# ìµœì¢… ìˆ˜ë ´ ê²½ë¡œ í•˜ì´ë¼ì´íŠ¸

# 3. Efficiency Comparison
# Grid Search: 500+ evaluations
# Hierarchical: 25 evaluations
# Time saved: ~95%
```

### ğŸ“Š ë…¼ë¬¸ í™œìš© ë°©ì•ˆ

#### Methods Section
```latex
\subsection{Hierarchical Learning Rate Optimization}
We employed a coarse-to-fine adaptive grid search strategy,
progressively narrowing the search space based on empirical 
performance metrics. This approach reduced computational cost 
by approximately 95\% compared to exhaustive grid search 
(25 vs 500+ evaluations).
```

#### Results Section
- Stage-wise performance improvement graphs
- Variance reduction across stages
- Final optimal LR with confidence intervals

### ğŸš€ êµ¬í˜„ ê³„íš

#### Phase 1: Infrastructure (30ë¶„)
```python
class HierarchicalLRSweep:
    def __init__(self, stages=4, points_per_stage=5):
        self.stages = stages
        self.points_per_stage = points_per_stage
        self.stage_results = []
        
    def analyze_stage(self, results):
        """í˜„ì¬ ë‹¨ê³„ ê²°ê³¼ ë¶„ì„ ë° ë‹¤ìŒ ë‹¨ê³„ êµ¬ê°„ ì„ íƒ"""
        
    def generate_next_stage(self, top_intervals):
        """ë‹¤ìŒ ë‹¨ê³„ LR í¬ì¸íŠ¸ ìƒì„±"""
```

#### Phase 2: Execution (1ì‹œê°„)
- Stage 1: 15ë¶„ (5 points Ã— 3 epochs)
- Stage 2: 15ë¶„
- Stage 3: 15ë¶„  
- Stage 4: 15ë¶„

#### Phase 3: Analysis (30ë¶„)
- ê²°ê³¼ ì§‘ê³„ ë° ì‹œê°í™”
- ìµœì  LR ê²°ì •
- ë³´ê³ ì„œ ìƒì„±

### ğŸ¯ ì˜ˆìƒ ê²°ê³¼

1. **ì •ë°€ë„**: Â±5% ì´ë‚´ ìµœì  LR ë°œê²¬
2. **íš¨ìœ¨ì„±**: 95% ê³„ì‚° ë¹„ìš© ì ˆê°
3. **ì¬í˜„ì„±**: ëª…í™•í•œ ë‹¨ê³„ë³„ í”„ë¡œì„¸ìŠ¤
4. **í•™ìˆ ì„±**: ë°©ë²•ë¡  ì„¹ì…˜ ê°•í™”

### âš™ï¸ ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­

1. **ì¤‘ë³µ ë°©ì§€**: ì´ì „ ë‹¨ê³„ì—ì„œ í‰ê°€í•œ LR ì œì™¸
2. **ê²½ê³„ ì²˜ë¦¬**: êµ¬ê°„ ê²½ê³„ëŠ” í•œ ë²ˆë§Œ í‰ê°€
3. **ìˆ˜ë ´ ì¡°ê±´**: ì„±ëŠ¥ ê°œì„  < 1% ì‹œ ì¡°ê¸° ì¢…ë£Œ
4. **ë©”ëª¨ë¦¬ íš¨ìœ¨**: ê° ë‹¨ê³„ ê²°ê³¼ë§Œ ì €ì¥ (ëˆ„ì  X)

### ğŸ“ ì°¸ê³ ë¬¸í—Œ
- Adaptive Grid Search methodologies
- Bayesian Optimization principles  
- Coarse-to-fine optimization strategies

---
*Last Updated: 2025-08-20*
*Status: Planning Phase*