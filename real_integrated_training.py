#!/usr/bin/env python3
"""
Red Heart AI ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ
REAL Integrated Training System for Red Heart AI

ì§„ì§œ ëª¨ë¸ë“¤ì„ ì‹¤ì œë¡œ í˜¸ì¶œí•˜ì—¬ í†µí•© í›ˆë ¨ ìˆ˜í–‰
- ì‹¤ì œ ê°ì • ë¶„ì„ ëª¨ë¸ í˜¸ì¶œ
- ì‹¤ì œ ë²¤ë‹´ ê³„ì‚° ëª¨ë¸ í˜¸ì¶œ
- ì‹¤ì œ í›„íšŒ ë¶„ì„ ëª¨ë¸ í˜¸ì¶œ
- ì‹¤ì œ SURD ë¶„ì„ ëª¨ë¸ í˜¸ì¶œ
- processed_datasets 24,170ê°œ ë°ì´í„° í›ˆë ¨
"""

# =============================================================================
# í™˜ê²½ ë¶„ë¦¬ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ - ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ì œê±°
# venv: ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (numpy 2.x)
# conda subprocess: faiss + spacy ì‘ì—… (numpy 1.x)
# =============================================================================
import os
# CVE-2025-32434ëŠ” ê°€ì§œ CVE - torch_security_patch import ì œê±°
# import torch_security_patch

import asyncio
import logging
import time
import numpy as np
import torch
import json
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, field
import traceback
from datetime import datetime
import os
import glob

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('RedHeart_REAL_Training')

# ì‹¤ì œ ì‹œìŠ¤í…œ ëª¨ë“ˆ ì„í¬íŠ¸ - ê°œë³„ ì²´í¬ ë°©ì‹ìœ¼ë¡œ ê°•í™”
MODULES_AVAILABLE = True
LEARNING_SYSTEM_AVAILABLE = True

# ê¸°ë³¸ ëª¨ë“ˆë“¤ import ì²´í¬
basic_modules = {}
try:
    from advanced_emotion_analyzer import AdvancedEmotionAnalyzer
    basic_modules['emotion'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ advanced_emotion_analyzer import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ ê°ì • ë¶„ì„ê¸° ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

try:
    from advanced_bentham_calculator import AdvancedBenthamCalculator
    basic_modules['bentham'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ advanced_bentham_calculator import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ ë²¤ë‹´ ê³„ì‚°ê¸° ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

try:
    from advanced_regret_analyzer import AdvancedRegretAnalyzer
    basic_modules['regret'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ advanced_regret_analyzer import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ í›„íšŒ ë¶„ì„ê¸° ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

try:
    from advanced_surd_analyzer import AdvancedSURDAnalyzer
    basic_modules['surd'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ advanced_surd_analyzer import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ SURD ë¶„ì„ê¸° ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

try:
    from advanced_experience_database import AdvancedExperienceDatabase
    basic_modules['experience_db'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ advanced_experience_database import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

# Semantic ë¶„ì„ ëª¨ë“ˆë“¤ ì¶”ê°€
try:
    from advanced_multi_level_semantic_analyzer import AdvancedMultiLevelSemanticAnalyzer
    basic_modules['advanced_semantic'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ advanced_multi_level_semantic_analyzer import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„ê¸° ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

try:
    from advanced_hierarchical_emotion_system import HashtagMultiLevelSemanticAnalyzer
    basic_modules['hashtag_semantic'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ advanced_hierarchical_emotion_system.HashtagMultiLevelSemanticAnalyzer import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ í•´ì‹œíƒœê·¸ ì˜ë¯¸ ë¶„ì„ê¸° ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

try:
    from data_models import EthicalSituation, EmotionData, HedonicValues
    basic_modules['data_models'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ data_models import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ ë°ì´í„° ëª¨ë¸ ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

# í•™ìŠµ ì‹œìŠ¤í…œ ëª¨ë“ˆë“¤ import ì²´í¬
learning_modules = {}
try:
    from advanced_learning_executor import AdvancedLearningExecutor, LearningConfig
    learning_modules['learning_executor'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ advanced_learning_executor import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ í•™ìŠµ ì‹¤í–‰ê¸° ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

try:
    from advanced_regret_learning_system import AdvancedRegretLearningSystem, LearningPhase
    learning_modules['regret_learning'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ advanced_regret_learning_system import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ í›„íšŒ í•™ìŠµ ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

try:
    from advanced_hierarchical_emotion_system import AdvancedHierarchicalEmotionSystem, EmotionPhase
    learning_modules['emotion_system'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ advanced_hierarchical_emotion_system import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ ê³„ì¸µì  ê°ì • ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

try:
    from integrated_system_orchestrator import IntegratedSystemOrchestrator, IntegrationContext
    learning_modules['orchestrator'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ integrated_system_orchestrator import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

try:
    from dynamic_ethical_choice_analyzer import DynamicEthicalChoiceAnalyzer, EthicalDilemma
    learning_modules['choice_analyzer'] = True
except ImportError as e:
    logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ dynamic_ethical_choice_analyzer import ì‹¤íŒ¨: {e}")
    raise ImportError(f"í•„ìˆ˜ ë™ì  ìœ¤ë¦¬ ì„ íƒ ë¶„ì„ê¸° ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}") from e

# ìƒíƒœ ìš”ì•½ ë¡œê¹…
logger.info(f"ëª¨ë“ˆ import ìƒíƒœ - ê¸°ë³¸: {basic_modules}")
logger.info(f"ëª¨ë“ˆ import ìƒíƒœ - í•™ìŠµ: {learning_modules}")
logger.info(f"MODULES_AVAILABLE: {MODULES_AVAILABLE}")
logger.info(f"LEARNING_SYSTEM_AVAILABLE: {LEARNING_SYSTEM_AVAILABLE}")

@dataclass
class RealTrainingResult:
    """ì‹¤ì œ í›ˆë ¨ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""
    data_id: str
    source_file: str
    processing_time: float
    emotion_analysis: Dict[str, Any]
    bentham_calculation: Dict[str, Any]
    regret_analysis: Dict[str, Any]
    surd_analysis: Dict[str, Any]
    counterfactual_experiences: List[Dict[str, Any]]
    integration_success: bool
    error_log: List[str]

class RealIntegratedTrainingSystem:
    """ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ - ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ í†µí•©"""
    
    def __init__(self):
        # ê°œë³„ ëª¨ë“ˆë“¤ (ê¸°ì¡´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ìš©)
        self.emotion_analyzer = None
        self.bentham_calculator = None
        self.regret_analyzer = None
        self.surd_analyzer = None
        self.experience_db = None
        
        # Semantic ë¶„ì„ ëª¨ë“ˆë“¤ (ìƒˆë¡œ ì¶”ê°€)
        self.advanced_semantic_analyzer = None
        self.hashtag_semantic_analyzer = None
        
        # ì‹¤ì œ í•™ìŠµ ì‹œìŠ¤í…œ (ìƒˆë¡œ ì¶”ê°€)
        self.learning_executor = None
        self.integrated_orchestrator = None
        self.dynamic_choice_analyzer = None
        self.learning_mode = False  # í•™ìŠµ ëª¨ë“œ vs ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        
        # ì‹¤ì œ í›ˆë ¨ ë©”íŠ¸ë¦­
        self.training_metrics = {
            'total_processed': 0,
            'successful_integrations': 0,
            'failed_processes': 0,
            'total_processing_time': 0.0,
            'avg_processing_time': 0.0,
            'real_accuracy_scores': {},
            'module_performance': {},
            'error_patterns': [],
            'learning_phases': {
                'current_phase': 0,
                'phase_transitions': [],
                'phase_performance': {}
            }
        }
        
        # ë°ì´í„° ë¡œë”
        self.processed_datasets_path = Path("/mnt/c/large_project/linux_red_heart/processed_datasets")
        
    async def initialize_real_system(self, learning_mode: bool = False, auto_setup: bool = True):
        """ì‹¤ì œ ì‹œìŠ¤í…œ ëª¨ë“  ëª¨ë“ˆ ì´ˆê¸°í™” - í™˜ê²½ ë¶„ë¦¬ ë° ìë™ ì„¸íŒ… í†µí•©"""
        logger.info("ğŸ” DEBUG: initialize_real_system ë©”ì„œë“œ ì‹œì‘")
        logger.info(f"ğŸ” DEBUG: learning_mode={learning_mode}, auto_setup={auto_setup}")
        
        try:
            logger.info("=" * 80)
            logger.info("ğŸš€ Red Heart AI ì‹¤ì œ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
            logger.info("=" * 80)
            
            self.learning_mode = learning_mode
        
            # 1. ì‹œìŠ¤í…œ ë¬´ê²°ì„± ê²€ì‚¬ (system_integrity_checker.py ë°©ì‹)
            logger.info("ğŸ” 1ë‹¨ê³„: ì‹œìŠ¤í…œ ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤í–‰ ì¤‘...")
            logger.info("ğŸ” DEBUG: _run_integrity_check í˜¸ì¶œ ì‹œì‘")
            integrity_result = await self._run_integrity_check()
            logger.info(f"ğŸ” DEBUG: _run_integrity_check ê²°ê³¼: {integrity_result}")
            
            if not integrity_result:
                if auto_setup:
                    logger.info("ğŸ”§ ìë™ í™˜ê²½ ì„¸íŒ… ëª¨ë“œ í™œì„±í™”...")
                    if not await self._auto_setup_environment():
                        logger.error("âŒ ìë™ í™˜ê²½ ì„¸íŒ… ì‹¤íŒ¨")
                        return False
                    # í™˜ê²½ ì„¸íŒ… í›„ ì¬ê²€ì‚¬
                    if not await self._run_integrity_check():
                        logger.error("âŒ í™˜ê²½ ì„¸íŒ… í›„ì—ë„ ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤íŒ¨")
                        return False
                else:
                    logger.error("âŒ ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤íŒ¨. auto_setup=Trueë¡œ ì¬ì‹œë„í•˜ì„¸ìš”.")
                    return False
            
            logger.info("âœ… ì‹œìŠ¤í…œ ë¬´ê²°ì„± ê²€ì‚¬ í†µê³¼")
            
            # 2. í™˜ê²½ ë¶„ë¦¬ ìƒíƒœ ê²€ì¦
            logger.info("ğŸ” 2ë‹¨ê³„: í™˜ê²½ ë¶„ë¦¬ ìƒíƒœ ê²€ì¦ ì¤‘...")
            if not await self._verify_environment_separation():
                logger.error("âŒ í™˜ê²½ ë¶„ë¦¬ ìƒíƒœ ê²€ì¦ ì‹¤íŒ¨")
                return False
            
            logger.info("âœ… í™˜ê²½ ë¶„ë¦¬ ìƒíƒœ ê²€ì¦ ì™„ë£Œ")
            
            # 3. ëª¨ë“ˆ ì´ˆê¸°í™” (í•™ìŠµ ëª¨ë“œ vs ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ)
            logger.info(f"ëª¨ë“ˆ ì´ˆê¸°í™” ì¡°ê±´ ì²´í¬:")
            logger.info(f"  - learning_mode: {learning_mode}")
            logger.info(f"  - LEARNING_SYSTEM_AVAILABLE: {LEARNING_SYSTEM_AVAILABLE}")
            logger.info(f"  - ê¸°ë³¸ ëª¨ë“ˆ ìƒíƒœ: {basic_modules}")
            logger.info(f"  - í•™ìŠµ ëª¨ë“ˆ ìƒíƒœ: {learning_modules}")
            
            if learning_mode and LEARNING_SYSTEM_AVAILABLE:
                logger.info("ğŸš€ 3ë‹¨ê³„: í•™ìŠµ ëª¨ë“œ - ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
                return await self._initialize_learning_system()
            else:
                if not learning_mode:
                    logger.info("ğŸ”§ 3ë‹¨ê³„: ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ - learning_mode=False")
                elif not LEARNING_SYSTEM_AVAILABLE:
                    logger.warning("âš ï¸ 3ë‹¨ê³„: í•™ìŠµ ì‹œìŠ¤í…œ ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€ - ê¸°ë³¸ ëª¨ë“ˆë§Œ ì´ˆê¸°í™”")
                logger.info("ğŸ”§ ê°œë³„ ëª¨ë“ˆ ì´ˆê¸°í™” ì§„í–‰")
                return await self._initialize_basic_modules()
            
        except Exception as e:
            logger.error(f"ğŸš¨ DEBUG: initialize_real_systemì—ì„œ ì˜ˆì™¸ ë°œìƒ: {e}")
            import traceback
            logger.error(f"ğŸš¨ DEBUG: ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            return False
    
    async def _run_integrity_check(self) -> bool:
        """ì‹œìŠ¤í…œ ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤í–‰ - deadlock ë°©ì§€ ê°œì„ """
        try:
            import subprocess
            import sys
            
            logger.info("   ğŸ“‹ system_integrity_checker.py ì‹¤í–‰ ì¤‘...")
            
            # sys.executable ì‚¬ìš©ìœ¼ë¡œ í™˜ê²½ ì¼ê´€ì„± ë³´ì¥
            python_executable = sys.executable
            logger.info(f"   ğŸ ì‚¬ìš©í•  Python: {python_executable}")
            
            # í™˜ê²½ë³€ìˆ˜ ëª…ì‹œì  ì„¤ì • (ì™„ì „í•œ ë³µì‚¬ë³¸)
            env = os.environ.copy()
            
            # ì¶”ê°€ í™˜ê²½ë³€ìˆ˜ ëª…ì‹œì  ì„¤ì •
            if 'VIRTUAL_ENV' in os.environ:
                env['VIRTUAL_ENV'] = os.environ['VIRTUAL_ENV']
                logger.info(f"   ğŸ“¦ venv: {env['VIRTUAL_ENV']}")
            
            if 'CONDA_DEFAULT_ENV' in os.environ:
                env['CONDA_DEFAULT_ENV'] = os.environ['CONDA_DEFAULT_ENV']
                logger.info(f"   ğŸ conda: {env['CONDA_DEFAULT_ENV']}")
            
            # PATH í™˜ê²½ë³€ìˆ˜ ëª…ì‹œì  ì „ë‹¬
            if 'PATH' in os.environ:
                env['PATH'] = os.environ['PATH']
            
            # PYTHONPATH ì„¤ì • (í˜„ì¬ ë””ë ‰í† ë¦¬ í¬í•¨)
            current_dir = os.getcwd()
            if 'PYTHONPATH' in env:
                env['PYTHONPATH'] = f"{current_dir}:{env['PYTHONPATH']}"
            else:
                env['PYTHONPATH'] = current_dir
            
            logger.info("   ğŸš€ subprocess ì‹œì‘ (communicate() ì‚¬ìš©ìœ¼ë¡œ deadlock ë°©ì§€)")
            
            # Popen + communicate() ì‚¬ìš©ìœ¼ë¡œ deadlock ë°©ì§€
            process = subprocess.Popen(
                [python_executable, 'system_integrity_checker.py'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                env=env,
                cwd=current_dir
            )
            
            # communicate()ë¡œ deadlock ë°©ì§€í•˜ë©´ì„œ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
            try:
                stdout, stderr = process.communicate(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                returncode = process.returncode
                
                logger.info(f"   ğŸ“¤ subprocess ì™„ë£Œ (exit code: {returncode})")
                
                if returncode == 0:
                    logger.info("   âœ… ë¬´ê²°ì„± ê²€ì‚¬ ì„±ê³µ")
                    if stdout.strip():
                        logger.info(f"   ğŸ“‹ ì¶œë ¥: {stdout.strip()[-200:]}")  # ë§ˆì§€ë§‰ 200ìë§Œ í‘œì‹œ
                    return True
                else:
                    logger.error(f"   âŒ ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤íŒ¨ (exit code: {returncode})")
                    if stderr:
                        logger.error(f"   ğŸš¨ ì—ëŸ¬: {stderr.strip()[-500:]}")  # ë§ˆì§€ë§‰ 500ìë§Œ í‘œì‹œ
                    if stdout:
                        logger.error(f"   ğŸ“‹ ì¶œë ¥: {stdout.strip()[-200:]}")
                    return False
                    
            except subprocess.TimeoutExpired:
                logger.error("   â° ë¬´ê²°ì„± ê²€ì‚¬ íƒ€ì„ì•„ì›ƒ (5ë¶„)")
                process.kill()
                process.wait()
                return False
                
        except Exception as e:
            logger.error(f"   âŒ ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {e}")
            import traceback
            logger.error(f"   ğŸš¨ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            return False
    
    async def _verify_environment_separation(self) -> bool:
        """í™˜ê²½ ë¶„ë¦¬ ìƒíƒœ ê²€ì¦"""
        try:
            logger.info("   ğŸ” conda+venv í™œì„±í™” ìƒíƒœ í™•ì¸...")
            
            # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
            virtual_env = os.environ.get('VIRTUAL_ENV', '')
            conda_env = os.environ.get('CONDA_DEFAULT_ENV', '')
            
            if not virtual_env:
                logger.error("   âŒ VIRTUAL_ENV í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                return False
            
            if not conda_env or conda_env == 'base':
                logger.error("   âŒ CONDA_DEFAULT_ENVê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ base í™˜ê²½ì„")
                return False
            
            logger.info(f"   âœ… venv: {virtual_env}")
            logger.info(f"   âœ… conda: {conda_env}")
            
            # FAISS subprocess í…ŒìŠ¤íŠ¸
            logger.info("   ğŸ” FAISS subprocess í™˜ê²½ ë¶„ë¦¬ í…ŒìŠ¤íŠ¸...")
            from utils import run_faiss_subprocess
            
            test_result = run_faiss_subprocess('test', {})
            if test_result.get('status') == 'success':
                logger.info("   âœ… FAISS í™˜ê²½ ë¶„ë¦¬ ì‘ë™ í™•ì¸")
                return True
            else:
                logger.error(f"   âŒ FAISS í™˜ê²½ ë¶„ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_result}")
                # í•™ìŠµ ë¬´ê²°ì„± ë³´ì¥ì„ ìœ„í•´ graceful degradation ì œê±°
                return False
                
        except Exception as e:
            logger.error(f"   âŒ í™˜ê²½ ë¶„ë¦¬ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    async def _auto_setup_environment(self) -> bool:
        """ìë™ í™˜ê²½ ì„¸íŒ…"""
        try:
            import subprocess  # í•¨ìˆ˜ ì‹œì‘ ë¶€ë¶„ìœ¼ë¡œ ì´ë™
            logger.info("   ğŸ”§ ìë™ í™˜ê²½ ì„¸íŒ… ì‹œì‘...")
            
            # ê°€ìƒí™˜ê²½ ìƒì„± (í•„ìš”í•œ ê²½ìš°)
            venv_path = Path("red_heart_env")
            if not venv_path.exists():
                logger.info("   ğŸ“¦ red_heart_env ê°€ìƒí™˜ê²½ ìƒì„± ì¤‘...")
                result = subprocess.run(['python3', '-m', 'venv', 'red_heart_env'], 
                                      capture_output=True, text=True)
                if result.returncode != 0:
                    logger.error(f"   âŒ ê°€ìƒí™˜ê²½ ìƒì„± ì‹¤íŒ¨: {result.stderr}")
                    return False
                logger.info("   âœ… ê°€ìƒí™˜ê²½ ìƒì„± ì™„ë£Œ")
            
            # conda í™˜ê²½ í™•ì¸ ë° ìƒì„±
            logger.info("   ğŸ conda í™˜ê²½ í™•ì¸ ì¤‘...")
            conda_result = subprocess.run(['conda', 'env', 'list'], 
                                        capture_output=True, text=True)
            if 'faiss-test' not in conda_result.stdout:
                logger.info("   ğŸ“¦ faiss-test conda í™˜ê²½ ìƒì„± ì¤‘...")
                create_result = subprocess.run(['conda', 'create', '-n', 'faiss-test', '-y'], 
                                             capture_output=True, text=True)
                if create_result.returncode != 0:
                    logger.error(f"   âŒ conda í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {create_result.stderr}")
                    return False
                logger.info("   âœ… conda í™˜ê²½ ìƒì„± ì™„ë£Œ")
            
            logger.info("   âœ… ìë™ í™˜ê²½ ì„¸íŒ… ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"   âŒ ìë™ í™˜ê²½ ì„¸íŒ… ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    async def _initialize_learning_system(self) -> bool:
        """í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            logger.info("   ğŸš€ ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            # ì‹¤ì œ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            logger.info("   ğŸ“š ì‹¤ì œ í•™ìŠµ ì‹¤í–‰ê¸° ì´ˆê¸°í™”...")
            learning_config = LearningConfig(
                regrets_per_step=3,  # ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ìš©
                bentham_per_environment=2,  # ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ìš©
                general_data_cycles=1,  # ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ìš©
                ebs_data_cycles=1,  # ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ìš©
                max_scenarios_per_batch=3  # ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ìš©
            )
            self.learning_executor = AdvancedLearningExecutor(learning_config)
            logger.info("   âœ… ì‹¤ì œ í•™ìŠµ ì‹¤í–‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
            # í†µí•© ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”
            logger.info("   ğŸ¼ í†µí•© ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”...")
            self.integrated_orchestrator = IntegratedSystemOrchestrator()
            logger.info("   âœ… í†µí•© ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ë™ì  ìœ¤ë¦¬ì  ì„ íƒì§€ ë¶„ì„ê¸° ì´ˆê¸°í™”
            logger.info("   ğŸ¤” ë™ì  ìœ¤ë¦¬ì  ì„ íƒì§€ ë¶„ì„ê¸° ì´ˆê¸°í™”...")
            self.dynamic_choice_analyzer = DynamicEthicalChoiceAnalyzer()
            logger.info("   âœ… ë™ì  ìœ¤ë¦¬ì  ì„ íƒì§€ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ê¸°ë³¸ ëª¨ë“ˆë“¤ë„ ì´ˆê¸°í™”
            await self._initialize_basic_modules()
            
            logger.info("ğŸ‰ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(f"   ìƒì„¸ ì—ëŸ¬: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"   ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            
            # ëª¨ë“ˆë³„ ìƒíƒœ í™•ì¸
            logger.error("   ëª¨ë“ˆ ê°€ìš©ì„± ì¬í™•ì¸:")
            logger.error(f"     - ê¸°ë³¸ ëª¨ë“ˆ: {basic_modules}")
            logger.error(f"     - í•™ìŠµ ëª¨ë“ˆ: {learning_modules}")
            logger.error(f"     - LEARNING_SYSTEM_AVAILABLE: {LEARNING_SYSTEM_AVAILABLE}")
            
            # Graceful Degradation ì œê±° - ëª…í™•í•œ ì‹¤íŒ¨ ì²˜ë¦¬
            logger.error("ğŸš¨ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ë¡œ ì¸í•œ ì‹œìŠ¤í…œ ì¤‘ë‹¨")
            logger.error("   â†’ Graceful degradation ì—†ì´ ëª…í™•í•œ ì‹¤íŒ¨ ë°˜í™˜")
            logger.error("   â†’ learning_mode=True ìš”ì²­ì´ì§€ë§Œ í•™ìŠµ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŒ")
            
            # learning_mode ìƒíƒœ ìœ ì§€í•˜ì§€ë§Œ ì‹¤íŒ¨ ë°˜í™˜
            return False
    
    async def _initialize_basic_modules(self) -> bool:
        """ê¸°ë³¸ ëª¨ë“ˆë“¤ ì´ˆê¸°í™”"""
        try:
            logger.info("   ğŸ”§ ê¸°ë³¸ ëª¨ë“ˆë“¤ ì´ˆê¸°í™” ì¤‘...")
            
            # ì‹¤ì œ ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”
            logger.info("   ğŸ˜Š ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”...")
            self.emotion_analyzer = AdvancedEmotionAnalyzer()
            logger.info("   âœ… ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì‹¤ì œ ë²¤ë‹´ ê³„ì‚°ê¸° ì´ˆê¸°í™”
            logger.info("   âš–ï¸ ë²¤ë‹´ ê³„ì‚°ê¸° ì´ˆê¸°í™”...")
            self.bentham_calculator = AdvancedBenthamCalculator()
            logger.info("   âœ… ë²¤ë‹´ ê³„ì‚°ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì‹¤ì œ í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™”
            logger.info("   ğŸ˜” í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™”...")
            try:
                self.regret_analyzer = AdvancedRegretAnalyzer()
                logger.info("   âœ… í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"   âŒ í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # í•™ìŠµ ë¬´ê²°ì„± ë³´ì¥ì„ ìœ„í•´ graceful degradation ì œê±°
                raise Exception(f"í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€: {e}")
            
            # ì‹¤ì œ SURD ë¶„ì„ê¸° ì´ˆê¸°í™”
            logger.info("   ğŸ§® SURD ë¶„ì„ê¸° ì´ˆê¸°í™”...")
            self.surd_analyzer = AdvancedSURDAnalyzer()
            logger.info("   âœ… SURD ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì‹¤ì œ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (í™˜ê²½ ë¶„ë¦¬ ì ìš©ë¨)
            logger.info("   ğŸ’¾ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”...")
            self.experience_db = AdvancedExperienceDatabase()
            logger.info("   âœ… ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (í™˜ê²½ ë¶„ë¦¬ ì ìš©)")
            
            # Semantic ë¶„ì„ ëª¨ë“ˆë“¤ ì´ˆê¸°í™” (ìƒˆë¡œ ì¶”ê°€)
            if basic_modules.get('advanced_semantic', False):
                logger.info("   ğŸ§  ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„ê¸° ì´ˆê¸°í™”...")
                try:
                    self.advanced_semantic_analyzer = AdvancedMultiLevelSemanticAnalyzer()
                    logger.info("   âœ… ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"   âŒ ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    # í”„ë¡œì íŠ¸ ê·œì¹™: fallback ì—†ëŠ” ìˆœìˆ˜ ì¬ì‹œë„ ë°©ì‹
                    raise Exception(f"ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€: {e}")
            
            if basic_modules.get('hashtag_semantic', False):
                logger.info("   ğŸ·ï¸ í•´ì‹œíƒœê·¸ ì˜ë¯¸ ë¶„ì„ê¸° ì´ˆê¸°í™”...")
                try:
                    self.hashtag_semantic_analyzer = HashtagMultiLevelSemanticAnalyzer()
                    logger.info("   âœ… í•´ì‹œíƒœê·¸ ì˜ë¯¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"   âŒ í•´ì‹œíƒœê·¸ ì˜ë¯¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    # í”„ë¡œì íŠ¸ ê·œì¹™: fallback ì—†ëŠ” ìˆœìˆ˜ ì¬ì‹œë„ ë°©ì‹
                    raise Exception(f"í•´ì‹œíƒœê·¸ ì˜ë¯¸ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€: {e}")
            
            logger.info("ğŸ¯ ê¸°ë³¸ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ (Semantic ëª¨ë“ˆ í¬í•¨)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ê¸°ë³¸ ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def load_real_training_data(self) -> List[Dict[str, Any]]:
        """ì‹¤ì œ processed_datasetsì—ì„œ í›ˆë ¨ ë°ì´í„° ë¡œë“œ"""
        logger.info("=== ì‹¤ì œ í›ˆë ¨ ë°ì´í„° ë¡œë”© ì‹œì‘ ===")
        
        training_data = []
        
        try:
            # ìŠ¤í¬ëŸ¬í”Œ ë°°ì¹˜ íŒŒì¼ë“¤ ë¡œë“œ
            scruples_pattern = self.processed_datasets_path / "scruples" / "scruples_batch_*.json"
            scruples_files = glob.glob(str(scruples_pattern))
            
            logger.info(f"ìŠ¤í¬ëŸ¬í”Œ ë°°ì¹˜ íŒŒì¼ {len(scruples_files)}ê°œ ë°œê²¬")
            
            for file_path in scruples_files[:2]:  # ì²˜ìŒ 2ê°œ ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        batch_data = json.load(f)
                        
                    if 'scenarios' in batch_data:
                        for item in batch_data['scenarios']:
                            if 'description' in item:
                                training_data.append({
                                    'source_file': os.path.basename(file_path),
                                    'data_id': item.get('id', f"scruples_{len(training_data)}"),
                                    'situation': item['description'],
                                    'context': item.get('context', {}),
                                    'moral_complexity': 0.7,  # ìŠ¤í¬ëŸ¬í”Œ ë°ì´í„° ê¸°ë³¸ ë³µì¡ë„
                                    'stakeholders': {},
                                    'data_type': 'scruples'
                                })
                            
                except Exception as e:
                    logger.error(f"ìŠ¤í¬ëŸ¬í”Œ íŒŒì¼ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}")
                    # í•™ìŠµ ë¬´ê²°ì„± ë³´ì¥ì„ ìœ„í•´ graceful degradation ì œê±°
                    raise Exception(f"ìŠ¤í¬ëŸ¬í”Œ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€: {e}")
            
            # í†µí•© ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ë“¤ ë¡œë“œ
            try:
                integrated_files = [
                    self.processed_datasets_path / "integrated_scenarios.json",
                    self.processed_datasets_path / "final_integrated_with_batch7_20250619_213234.json"
                ]
                integrated_files = [f for f in integrated_files if f.exists()]
                logger.info(f"í†µí•© ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ {len(integrated_files)}ê°œ ë°œê²¬")
                
                for file_path in integrated_files:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        scenarios = json.load(f)
                    
                    for idx, scenario in enumerate(scenarios[:5]):  # ê° íŒŒì¼ì—ì„œ 5ê°œë§Œ
                        if 'description' in scenario:
                            training_data.append({
                                'source_file': os.path.basename(file_path),
                                'data_id': scenario.get('id', f"integrated_{idx}"),
                                'situation': scenario['description'],
                                'context': scenario.get('context', {}),
                                'moral_complexity': scenario.get('complexity_score', 0.7),
                                'stakeholders': scenario.get('stakeholders', []),
                                'data_type': 'integrated'
                            })
                            
            except Exception as e:
                logger.error(f"í†µí•© ì‹œë‚˜ë¦¬ì˜¤ ë¡œë”© ì‹¤íŒ¨: {e}")
                # í•™ìŠµ ë¬´ê²°ì„± ë³´ì¥ì„ ìœ„í•´ graceful degradation ì œê±°
                raise Exception(f"í†µí•© ì‹œë‚˜ë¦¬ì˜¤ ë¡œë”© ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€: {e}")
            
            # í•œêµ­ ë¬¸í™” íŠ¹í™” ë°ì´í„° ë¡œë“œ
            try:
                korean_files = [
                    self.processed_datasets_path / "korean_cultural_scenarios.json"
                ]
                korean_files = [f for f in korean_files if f.exists()]
                logger.info(f"í•œêµ­ ë¬¸í™” íŒŒì¼ {len(korean_files)}ê°œ ë°œê²¬")
                
                for file_path in korean_files:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        cultural_data = json.load(f)
                    
                    for item in cultural_data:
                        if 'scenario' in item and len(training_data) < 25:  # ìµœëŒ€ 25ê°œ ì œí•œ
                            training_data.append({
                                'source_file': os.path.basename(file_path),
                                'data_id': item.get('id', f"korean_{len(training_data)}"),
                                'situation': item['scenario'],
                                'context': item.get('context', {}),
                                'moral_complexity': item.get('complexity', 0.8),
                                'stakeholders': item.get('stakeholders', {}),
                                'data_type': 'korean_cultural'
                            })
                            
            except Exception as e:
                logger.error(f"í•œêµ­ ë¬¸í™” ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
                # í•™ìŠµ ë¬´ê²°ì„± ë³´ì¥ì„ ìœ„í•´ graceful degradation ì œê±°
                raise Exception(f"í•œêµ­ ë¬¸í™” ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€: {e}")
            
            logger.info(f"âœ… ì´ {len(training_data)}ê°œ ì‹¤ì œ í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            
            # ë°ì´í„° ë¶„í¬ ë¡œê¹…
            data_types = {}
            for data in training_data:
                data_type = data['data_type']
                data_types[data_type] = data_types.get(data_type, 0) + 1
            
            logger.info("ğŸ“Š ë°ì´í„° ë¶„í¬:")
            for data_type, count in data_types.items():
                logger.info(f"  - {data_type}: {count}ê°œ")
            
            return training_data
            
        except Exception as e:
            logger.error(f"âŒ ì‹¤ì œ í›ˆë ¨ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return []
    
    async def process_real_training_item(self, data_item: Dict[str, Any]) -> RealTrainingResult:
        """ë‹¨ì¼ í›ˆë ¨ ë°ì´í„° ì•„ì´í…œì„ ì‹¤ì œ ëª¨ë“ˆë“¤ë¡œ ì²˜ë¦¬"""
        
        start_time = time.time()
        error_log = []
        
        try:
            # 1. ì‹¤ì œ ê°ì • ë¶„ì„
            logger.info(f"ğŸ¯ ì²˜ë¦¬ ì¤‘: {data_item['data_id']} - ê°ì • ë¶„ì„...")
            emotion_start = time.time()
            
            try:
                # ì‹¤ì œ ê°ì • ë¶„ì„ê¸° í˜¸ì¶œ - ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„°
                emotion_result = self.emotion_analyzer.analyze_emotion(
                    text=data_item['situation'],
                    language="ko",
                    biosignal_data=None,
                    use_cache=True
                )
                emotion_processing_time = time.time() - emotion_start
                logger.info(f"   âœ… ê°ì • ë¶„ì„ ì™„ë£Œ ({emotion_processing_time:.3f}ì´ˆ)")
                
            except Exception as e:
                error_log.append(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
                emotion_processing_time = time.time() - emotion_start
                logger.error(f"   âŒ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
                # í•™ìŠµ ë¬´ê²°ì„± ë³´ì¥ì„ ìœ„í•´ graceful degradation ì œê±°
                raise Exception(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€: {e}")
            
            # 2. ì‹¤ì œ ë²¤ë‹´ ê³„ì‚°
            logger.info(f"ğŸ¯ ì²˜ë¦¬ ì¤‘: {data_item['data_id']} - ë²¤ë‹´ ê³„ì‚°...")
            bentham_start = time.time()
            
            try:
                # ì‹¤ì œ ë²¤ë‹´ ê³„ì‚°ê¸° í˜¸ì¶œ - ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„°
                bentham_input_data = {
                    'situation': data_item['situation'],
                    'context': data_item.get('context', {}),
                    'emotion_data': emotion_result if 'error' not in emotion_result else None
                }
                bentham_result = self.bentham_calculator.calculate_with_advanced_layers(
                    input_data=bentham_input_data,
                    use_cache=True
                )
                bentham_processing_time = time.time() - bentham_start
                logger.info(f"   âœ… ë²¤ë‹´ ê³„ì‚° ì™„ë£Œ ({bentham_processing_time:.3f}ì´ˆ)")
                
            except Exception as e:
                error_log.append(f"ë²¤ë‹´ ê³„ì‚° ì‹¤íŒ¨: {e}")
                bentham_processing_time = time.time() - bentham_start
                logger.error(f"   âŒ ë²¤ë‹´ ê³„ì‚° ì‹¤íŒ¨: {e}")
                # í•™ìŠµ ë¬´ê²°ì„± ë³´ì¥ì„ ìœ„í•´ graceful degradation ì œê±°
                raise Exception(f"ë²¤ë‹´ ê³„ì‚° ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€: {e}")
            
            # 3. ì‹¤ì œ í›„íšŒ ë¶„ì„
            if self.regret_analyzer:
                logger.info(f"ğŸ¯ ì²˜ë¦¬ ì¤‘: {data_item['data_id']} - í›„íšŒ ë¶„ì„...")
                regret_start = time.time()
                
                try:
                    # í›„íšŒ ë¶„ì„ì„ ìœ„í•œ decision_data ì¤€ë¹„ - ì•ˆì „í•œ íƒ€ì… ì²˜ë¦¬
                    decision_data = {
                        'scenario': data_item['situation'],
                        'text': data_item['situation'],  # í…ìŠ¤íŠ¸ í•„ë“œ ì¶”ê°€
                        'context': data_item.get('context', {}),
                    }
                    
                    # ê°ì • ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì•ˆì „í•œ ë°©ì‹)
                    if 'error' not in emotion_result:
                        decision_data['emotion_context'] = emotion_result
                    
                    # ë²¤ë‹´ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì•ˆì „í•œ íƒ€ì… ì²˜ë¦¬)
                    bentham_has_error = False
                    if isinstance(bentham_result, dict):
                        bentham_has_error = 'error' in bentham_result
                    elif hasattr(bentham_result, 'error'):
                        bentham_has_error = bentham_result.error is not None
                    
                    if not bentham_has_error:
                        if hasattr(bentham_result, '__dict__'):
                            # getattr ëŒ€ì‹  ì‹¤ì œ ì†ì„± ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                            if hasattr(bentham_result, 'final_score') and bentham_result.final_score is not None:
                                decision_data['bentham_context'] = {
                                    'score': bentham_result.final_score,
                                    'type': 'bentham_calculation'
                                }
                            else:
                                raise ValueError(f"ë²¤ë‹´ ê³„ì‚° ê²°ê³¼ì— final_score ì†ì„±ì´ ì—†ìŒ: {type(bentham_result)}")
                        elif isinstance(bentham_result, dict):
                            decision_data['bentham_context'] = bentham_result
                    
                    # ì‹¤ì œ í›„íšŒ ë¶„ì„ê¸° í˜¸ì¶œ
                    regret_result = await self.regret_analyzer.analyze_regret(
                        decision_data=decision_data,
                        outcome_data=None
                    )
                    regret_processing_time = time.time() - regret_start
                    logger.info(f"   âœ… í›„íšŒ ë¶„ì„ ì™„ë£Œ ({regret_processing_time:.3f}ì´ˆ)")
                    
                except Exception as e:
                    error_log.append(f"í›„íšŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    regret_processing_time = time.time() - regret_start
                    logger.error(f"   âŒ í›„íšŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    # í•™ìŠµ ë¬´ê²°ì„± ë³´ì¥ì„ ìœ„í•´ graceful degradation ì œê±°
                    raise Exception(f"í›„íšŒ ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€: {e}")
            else:
                # í›„íšŒ ë¶„ì„ê¸° ì‚¬ìš© ë¶ˆê°€ëŠ¥ ì‹œì—ë„ í•™ìŠµ ë¬´ê²°ì„±ì„ ìœ„í•´ ì¤‘ë‹¨
                logger.error("   âŒ í›„íšŒ ë¶„ì„ê¸° ì‚¬ìš© ë¶ˆê°€")
                raise Exception("í›„íšŒ ë¶„ì„ê¸° ì‚¬ìš© ë¶ˆê°€ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€")
            
            # 4. ì‹¤ì œ SURD í†µí•© ë¶„ì„
            logger.info(f"ğŸ¯ ì²˜ë¦¬ ì¤‘: {data_item['data_id']} - SURD í†µí•© ë¶„ì„...")
            surd_start = time.time()
            
            try:
                # SURD ë¶„ì„ì„ ìœ„í•œ ë³€ìˆ˜ ì¤€ë¹„
                surd_variables = {}
                
                # ê°ì • ë°ì´í„° í†µí•© (ì•ˆì „í•œ ì²´í¬)
                emotion_has_error = isinstance(emotion_result, dict) and 'error' in emotion_result
                if not emotion_has_error:
                    if hasattr(emotion_result, 'dominant_emotion'):
                        # ì‹¤ì œ ì†ì„± ì¡´ì¬ í™•ì¸ í›„ ê°’ ì¶”ì¶œ
                        if hasattr(emotion_result, 'intensity') and emotion_result.intensity is not None:
                            surd_variables['emotion_intensity'] = float(emotion_result.intensity)
                        else:
                            raise ValueError(f"ê°ì • ë¶„ì„ ê²°ê³¼ì— intensity ì†ì„±ì´ ì—†ìŒ: {type(emotion_result)}")
                        
                        if hasattr(emotion_result, 'confidence') and emotion_result.confidence is not None:
                            surd_variables['emotion_confidence'] = float(emotion_result.confidence)
                        else:
                            raise ValueError(f"ê°ì • ë¶„ì„ ê²°ê³¼ì— confidence ì†ì„±ì´ ì—†ìŒ: {type(emotion_result)}")
                    elif isinstance(emotion_result, dict):
                        surd_variables['emotion_intensity'] = float(emotion_result.get('intensity', 0.5))
                        surd_variables['emotion_confidence'] = float(emotion_result.get('confidence', 0.5))
                
                # ë²¤ë‹´ ë°ì´í„° í†µí•© (ì•ˆì „í•œ ì²´í¬)
                bentham_has_error = isinstance(bentham_result, dict) and 'error' in bentham_result
                if not bentham_has_error:
                    if hasattr(bentham_result, 'final_score'):
                        # ì‹¤ì œ ì†ì„± ì¡´ì¬ í™•ì¸ í›„ ê°’ ì¶”ì¶œ
                        if bentham_result.final_score is not None:
                            surd_variables['pleasure_score'] = float(bentham_result.final_score)
                        else:
                            raise ValueError(f"ë²¤ë‹´ ê³„ì‚° ê²°ê³¼ì˜ final_scoreê°€ None: {type(bentham_result)}")
                    elif isinstance(bentham_result, dict):
                        surd_variables['pleasure_score'] = float(bentham_result.get('final_score', 0.0))
                
                # í›„íšŒ ë°ì´í„° í†µí•© (AdvancedRegretMetrics íƒ€ì… ì•ˆì „ ì²˜ë¦¬)
                regret_surd_error = False
                if hasattr(regret_result, '__class__') and regret_result.__class__.__name__ == 'AdvancedRegretMetrics':
                    # AdvancedRegretMetrics ê°ì²´ì—ì„œ ë°ì´í„° ì¶”ì¶œ - ì‹¤ì œ ì†ì„± ê²€ì¦
                    if hasattr(regret_result, 'regret_intensity') and regret_result.regret_intensity is not None:
                        if regret_result.regret_intensity <= 0.0:
                            raise ValueError(f"í›„íšŒ ë¶„ì„ ê²°ê³¼ì˜ regret_intensityê°€ 0.0: {regret_result.regret_intensity}")
                        surd_variables['regret_intensity'] = float(regret_result.regret_intensity)
                    elif hasattr(regret_result, 'intensity') and regret_result.intensity is not None:
                        if regret_result.intensity <= 0.0:
                            raise ValueError(f"í›„íšŒ ë¶„ì„ ê²°ê³¼ì˜ intensityê°€ 0.0: {regret_result.intensity}")
                        surd_variables['regret_intensity'] = float(regret_result.intensity)
                    else:
                        raise ValueError(f"í›„íšŒ ë¶„ì„ ê²°ê³¼ì— regret_intensity ë˜ëŠ” intensity ì†ì„±ì´ ì—†ìŒ: {type(regret_result)}")
                elif isinstance(regret_result, dict):
                    regret_surd_error = 'error' in regret_result
                    if not regret_surd_error:
                        surd_variables['regret_intensity'] = float(regret_result.get('regret_intensity', 0.0))
                
                # ì‹¤íŒ¨ ê°ì§€ - ëª¨ë“  ë¶„ì„ì´ ì‹¤íŒ¨í–ˆì„ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
                if not surd_variables:
                    raise RuntimeError(
                        f"ëª¨ë“  ë¶„ì„ ëª¨ë“ˆì´ ì‹¤íŒ¨í–ˆê±°ë‚˜ ìœ íš¨í•œ ê°’ì„ ìƒì„±í•˜ì§€ ëª»í•¨. "
                        f"ê°ì •: {'error' if emotion_has_error else 'ok'}, "
                        f"ë²¤ë‹´: {'error' if bentham_has_error else 'ok'}, "
                        f"í›„íšŒ: {'error' if regret_surd_error else 'ok'}"
                    )
                
                # ì‹¤ì œ SURD ë¶„ì„ê¸° í˜¸ì¶œ - analyze_advanced ë©”ì„œë“œ ì‚¬ìš©
                surd_result = self.surd_analyzer.analyze_advanced(
                    variables=surd_variables,
                    target_variable='ethical_decision_quality',
                    additional_context=data_item.get('context', {})
                )
                surd_processing_time = time.time() - surd_start
                logger.info(f"   âœ… SURD ë¶„ì„ ì™„ë£Œ ({surd_processing_time:.3f}ì´ˆ)")
                
            except Exception as e:
                error_log.append(f"SURD ë¶„ì„ ì‹¤íŒ¨: {e}")
                surd_processing_time = time.time() - surd_start
                logger.error(f"   âŒ SURD ë¶„ì„ ì‹¤íŒ¨: {e}")
                # í•™ìŠµ ë¬´ê²°ì„± ë³´ì¥ì„ ìœ„í•´ graceful degradation ì œê±°
                raise Exception(f"SURD ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€: {e}")
            
            # 5. ë°˜ì‚¬ì‹¤ ê²½í—˜ ìƒì„± - AdvancedRegretMetrics íƒ€ì… ì•ˆì „ ì²˜ë¦¬
            counterfactual_experiences = []
            regret_has_error = False
            
            # AdvancedRegretMetrics ê°ì²´ íƒ€ì… ì²´í¬
            if hasattr(regret_result, '__class__') and regret_result.__class__.__name__ == 'AdvancedRegretMetrics':
                # ì„±ê³µì ì¸ ê²°ê³¼ - ë°˜ì‚¬ì‹¤ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ
                if hasattr(regret_result, 'counterfactual_scenarios'):
                    counterfactual_experiences = regret_result.counterfactual_scenarios or []
                elif hasattr(regret_result, 'counterfactuals'):
                    counterfactual_experiences = regret_result.counterfactuals or []
            elif isinstance(regret_result, dict):
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ (ì˜¤ë¥˜ í¬í•¨ ê°€ëŠ¥)
                regret_has_error = 'error' in regret_result
                if not regret_has_error and 'counterfactual_scenarios' in regret_result:
                    counterfactual_experiences = regret_result['counterfactual_scenarios']
            
            # 6. ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            try:
                # ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
                def convert_to_serializable(obj):
                    if hasattr(obj, '__dict__'):
                        return {k: str(v) if not isinstance(v, (str, int, float, bool, type(None))) else v 
                               for k, v in obj.__dict__.items()}
                    elif isinstance(obj, dict):
                        return obj
                    else:
                        return {'result': str(obj), 'type': type(obj).__name__}
                
                experience_entry = {
                    'data_id': data_item['data_id'],
                    'situation': data_item['situation'],
                    'emotion_analysis': convert_to_serializable(emotion_result),
                    'bentham_calculation': convert_to_serializable(bentham_result),
                    'regret_analysis': convert_to_serializable(regret_result),
                    'surd_analysis': convert_to_serializable(surd_result),
                    'timestamp': datetime.now().isoformat(),
                    'source_file': data_item['source_file']
                }
                
                await self.experience_db.store_experience(
                    experience_text=data_item['situation'],
                    metadata=experience_entry,
                    category="training",
                    importance_score=None
                )
                
            except Exception as e:
                error_log.append(f"ê²½í—˜ ì €ì¥ ì‹¤íŒ¨: {e}")
                logger.error(f"   âŒ ê²½í—˜ ì €ì¥ ì‹¤íŒ¨: {e}")
                # í•™ìŠµ ë¬´ê²°ì„± ë³´ì¥ì„ ìœ„í•´ graceful degradation ì œê±°
                raise Exception(f"ê²½í—˜ ì €ì¥ ì‹¤íŒ¨ë¡œ ì¸í•œ í•™ìŠµ ë¬´ê²°ì„± ì˜¤ì—¼ ë°©ì§€: {e}")
            
            # ê²°ê³¼ ìƒì„±
            total_processing_time = time.time() - start_time
            # graceful degradation ì œê±° í›„ ì—¬ê¸°ê¹Œì§€ ë„ë‹¬í•˜ë©´ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µ
            integration_success = True
            
            result = RealTrainingResult(
                data_id=data_item['data_id'],
                source_file=data_item['source_file'],
                processing_time=total_processing_time,
                emotion_analysis=emotion_result,
                bentham_calculation=bentham_result,
                regret_analysis=regret_result,
                surd_analysis=surd_result,
                counterfactual_experiences=counterfactual_experiences,
                integration_success=integration_success,
                error_log=error_log
            )
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.training_metrics['total_processed'] += 1
            self.training_metrics['total_processing_time'] += total_processing_time
            
            if integration_success:
                self.training_metrics['successful_integrations'] += 1
                logger.info(f"âœ… {data_item['data_id']} ì‹¤ì œ í†µí•© í›ˆë ¨ ì™„ë£Œ ({total_processing_time:.3f}ì´ˆ)")
            else:
                self.training_metrics['failed_processes'] += 1
                logger.warning(f"âš ï¸ {data_item['data_id']} ë¶€ë¶„ ì‹¤íŒ¨ ({len(error_log)}ê°œ ì˜¤ë¥˜)")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ {data_item['data_id']} ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            
            total_processing_time = time.time() - start_time
            self.training_metrics['total_processed'] += 1
            self.training_metrics['failed_processes'] += 1
            self.training_metrics['total_processing_time'] += total_processing_time
            
            return RealTrainingResult(
                data_id=data_item['data_id'],
                source_file=data_item['source_file'],
                processing_time=total_processing_time,
                emotion_analysis={'error': str(e)},
                bentham_calculation={'error': str(e)},
                regret_analysis={'error': str(e)},
                surd_analysis={'error': str(e)},
                counterfactual_experiences=[],
                integration_success=False,
                error_log=[f"ì‹¬ê°í•œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}"]
            )
    
    async def run_real_integrated_training(self, max_items: int = 100) -> Dict[str, Any]:
        """ì‹¤ì œ ë°ì´í„°ë¡œ í†µí•© í›ˆë ¨ ì‹¤í–‰"""
        logger.info("ğŸš€ ì‹¤ì œ Red Heart AI í†µí•© í›ˆë ¨ ì‹œì‘")
        
        # ì‹¤ì œ í›ˆë ¨ ë°ì´í„° ë¡œë“œ
        training_data = self.load_real_training_data()
        
        if not training_data:
            logger.error("âŒ í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {"error": "í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"}
        
        # ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜ ì œí•œ
        if len(training_data) > max_items:
            training_data = training_data[:max_items]
            logger.info(f"ğŸ“Š ì²˜ë¦¬í•  ë°ì´í„°ë¥¼ {max_items}ê°œë¡œ ì œí•œ")
        
        logger.info(f"ğŸ“‹ ì´ {len(training_data)}ê°œ ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        
        training_results = []
        
        # ê° ë°ì´í„° ì•„ì´í…œ ìˆœì°¨ ì²˜ë¦¬
        for i, data_item in enumerate(training_data, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ¯ [{i}/{len(training_data)}] ì‹¤ì œ í›ˆë ¨ ì¤‘...")
            logger.info(f"ë°ì´í„° ID: {data_item['data_id']}")
            logger.info(f"ì†ŒìŠ¤ íŒŒì¼: {data_item['source_file']}")
            logger.info(f"{'='*60}")
            
            result = await self.process_real_training_item(data_item)
            training_results.append(result)
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            if i % 10 == 0 or i == len(training_data):
                success_rate = self.training_metrics['successful_integrations'] / self.training_metrics['total_processed'] * 100
                avg_time = self.training_metrics['total_processing_time'] / self.training_metrics['total_processed']
                logger.info(f"\nğŸ“Š ì¤‘ê°„ ì§„í–‰ ìƒí™© [{i}/{len(training_data)}]:")
                logger.info(f"  - ì„±ê³µë¥ : {success_rate:.1f}%")
                logger.info(f"  - í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_time:.3f}ì´ˆ")
                logger.info(f"  - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {avg_time * (len(training_data) - i):.1f}ì´ˆ")
        
        # ìµœì¢… ë¶„ì„
        return self._analyze_real_training_results(training_results)
    
    def _analyze_real_training_results(self, results: List[RealTrainingResult]) -> Dict[str, Any]:
        """ì‹¤ì œ í›ˆë ¨ ê²°ê³¼ ì¢…í•© ë¶„ì„"""
        logger.info(f"\nğŸ“Š ì‹¤ì œ í›ˆë ¨ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        if not results:
            return {"error": "ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ì „ì²´ ë©”íŠ¸ë¦­
        total_items = len(results)
        successful_items = len([r for r in results if r.integration_success])
        success_rate = successful_items / total_items * 100
        
        total_time = sum(r.processing_time for r in results)
        avg_time = total_time / total_items
        
        # ëª¨ë“ˆë³„ ì„±ëŠ¥ ë¶„ì„ - AdvancedRegretMetrics í¬í•¨ ì•ˆì „í•œ íƒ€ì… ì²´í¬
        def safe_error_check(obj):
            if hasattr(obj, '__class__') and obj.__class__.__name__ == 'AdvancedRegretMetrics':
                return True  # AdvancedRegretMetrics ê°ì²´ëŠ” ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            elif hasattr(obj, '__dict__'):
                return 'error' not in obj.__dict__
            elif isinstance(obj, dict):
                return 'error' not in obj and 'disabled' not in obj
            else:
                return True  # ê¸°íƒ€ ê°ì²´ëŠ” ì„±ê³µìœ¼ë¡œ ê°„ì£¼
        
        module_performance = {
            'emotion_success': len([r for r in results if safe_error_check(r.emotion_analysis)]),
            'bentham_success': len([r for r in results if safe_error_check(r.bentham_calculation)]),
            'regret_success': len([r for r in results if safe_error_check(r.regret_analysis) and not (isinstance(r.regret_analysis, dict) and 'disabled' in r.regret_analysis)]),
            'surd_success': len([r for r in results if safe_error_check(r.surd_analysis)])
        }
        
        # ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
        error_patterns = {}
        for result in results:
            for error in result.error_log:
                error_type = error.split(':')[0]
                error_patterns[error_type] = error_patterns.get(error_type, 0) + 1
        
        # ì²˜ë¦¬ ì‹œê°„ ë¶„ì„
        processing_times = [r.processing_time for r in results]
        min_time = min(processing_times)
        max_time = max(processing_times)
        
        # ë°˜ì‚¬ì‹¤ ìƒì„± í†µê³„
        total_counterfactuals = sum(len(r.counterfactual_experiences) for r in results)
        
        # ë°ì´í„° ì†ŒìŠ¤ë³„ ì„±ëŠ¥
        source_performance = {}
        for result in results:
            source = result.source_file
            if source not in source_performance:
                source_performance[source] = {'total': 0, 'success': 0}
            source_performance[source]['total'] += 1
            if result.integration_success:
                source_performance[source]['success'] += 1
        
        # ìµœì¢… ê²°ê³¼
        analysis_result = {
            'training_summary': {
                'total_items': total_items,
                'successful_integrations': successful_items,
                'success_rate': success_rate,
                'total_processing_time': total_time,
                'avg_processing_time': avg_time,
                'min_processing_time': min_time,
                'max_processing_time': max_time
            },
            'module_performance': {
                'emotion_success_rate': (module_performance['emotion_success'] / total_items) * 100,
                'bentham_success_rate': (module_performance['bentham_success'] / total_items) * 100,
                'regret_success_rate': (module_performance['regret_success'] / total_items) * 100,
                'surd_success_rate': (module_performance['surd_success'] / total_items) * 100
            },
            'integration_analysis': {
                'full_integration_rate': success_rate,
                'partial_integration_rate': ((total_items - successful_items) / total_items) * 100,
                'total_counterfactuals_generated': total_counterfactuals,
                'avg_counterfactuals_per_item': total_counterfactuals / total_items
            },
            'error_analysis': {
                'error_patterns': error_patterns,
                'total_errors': sum(error_patterns.values()),
                'error_rate': (sum(error_patterns.values()) / total_items) * 100
            },
            'source_analysis': source_performance,
            'performance_metrics': {
                'items_per_second': total_items / total_time,
                'successful_items_per_second': successful_items / total_time,
                'efficiency_score': success_rate * (total_items / total_time)
            }
        }
        
        return analysis_result

    async def run_complete_learning_system(self, samples: Optional[int] = None) -> Dict[str, Any]:
        """ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰"""
        
        if not self.learning_mode or not self.learning_executor:
            logger.error("âŒ í•™ìŠµ ëª¨ë“œê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {"error": "í•™ìŠµ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ", "learning_success": False}
        
        logger.info("ğŸ¯ ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹œì‘")
        if samples:
            logger.info(f"ğŸ¯ ìƒ˜í”Œ ì œí•œ ëª¨ë“œ: {samples}ê°œ ì‹œë‚˜ë¦¬ì˜¤ë¡œ ì œí•œ")
        logger.info("ğŸ“Š 3ë‹¨ê³„ í†µí•© í˜ì´ì¦ˆ ì‹œìŠ¤í…œ:")
        logger.info("   Phase 0: ìì‹  ê°ì • ìº˜ë¦¬ë¸Œë ˆì´ì…˜")
        logger.info("   Phase 1: íƒ€ì¸ ê³µê° í•™ìŠµ")
        logger.info("   Phase 2: ê³µë™ì²´ ì´í•´")
        
        start_time = time.time()
        
        try:
            # 1. ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰
            logger.info("ğŸš€ ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘...")
            learning_results = await self.learning_executor.execute_full_learning(samples=samples)
            
            # 2. í•™ìŠµ ê²°ê³¼ ë¶„ì„
            logger.info("ğŸ“Š í•™ìŠµ ê²°ê³¼ ë¶„ì„ ì¤‘...")
            analysis_results = await self._analyze_learning_results(learning_results)
            
            # 3. í•™ìŠµëœ ì‹œìŠ¤í…œìœ¼ë¡œ ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸
            logger.info("ğŸ¯ í•™ìŠµëœ ì‹œìŠ¤í…œ ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸ ì¤‘...")
            decision_results = await self._test_learned_decision_making()
            
            # 4. ë™ì  ìœ¤ë¦¬ì  ë¶„ì„ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ” ë™ì  ìœ¤ë¦¬ì  ë¶„ì„ í…ŒìŠ¤íŠ¸ ì¤‘...")
            ethical_analysis_results = await self._test_dynamic_ethical_analysis()
            
            total_time = time.time() - start_time
            
            return {
                "learning_success": True,
                "total_learning_time": total_time,
                "learning_results": learning_results,
                "integrated_analysis": analysis_results,
                "decision_test_results": decision_results,
                "ethical_analysis_results": ethical_analysis_results,
                "summary": {
                    "total_learning_time": total_time,
                    "learning_quality": analysis_results.get("learning_quality", {}),
                    "decision_accuracy": decision_results.get("confidence_score", 0.0),
                    "ethical_analysis_quality": ethical_analysis_results.get("analysis_quality", 0.0)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "learning_success": False}

    async def _analyze_learning_results(self, learning_results: Dict[str, Any]) -> Dict[str, Any]:
        """í•™ìŠµ ê²°ê³¼ ë¶„ì„"""
        
        analysis = {
            "phase_analysis": {},
            "module_performance": {},
            "learning_quality": {}
        }
        
        # í•™ìŠµ í†µê³„ ë¶„ì„
        if "learning_statistics" in learning_results:
            stats = learning_results["learning_statistics"]
            
            # í˜ì´ì¦ˆë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            if "performance_metrics" in stats:
                recent_metrics = stats["performance_metrics"][-10:]  # ìµœê·¼ 10ê°œ
                if recent_metrics:
                    analysis["phase_analysis"]["recent_regret_avg"] = np.mean([m["avg_regret_intensity"] for m in recent_metrics])
                    analysis["phase_analysis"]["recent_hedonic_avg"] = np.mean([m["avg_hedonic_score"] for m in recent_metrics])
            
            # ëª¨ë“ˆ ì„±ëŠ¥
            if "regret_history" in stats:
                regret_data = stats["regret_history"][-50:]  # ìµœê·¼ 50ê°œ
                if regret_data:
                    analysis["module_performance"]["regret_system"] = {
                        "avg_intensity": np.mean([r["intensity"] for r in regret_data]),
                        "total_processed": len(regret_data)
                    }
            
            if "bentham_scores" in stats:
                bentham_data = stats["bentham_scores"][-50:]  # ìµœê·¼ 50ê°œ
                if bentham_data:
                    analysis["module_performance"]["bentham_system"] = {
                        "avg_score": np.mean([b["hedonic_score"] for b in bentham_data]),
                        "total_processed": len(bentham_data)
                    }
        
        # í•™ìŠµ í’ˆì§ˆ í‰ê°€
        if "summary" in learning_results:
            summary = learning_results["summary"]
            analysis["learning_quality"]["scenarios_processed"] = summary.get("total_scenarios_processed", 0)
            analysis["learning_quality"]["total_regrets"] = summary.get("total_regrets", 0)
            analysis["learning_quality"]["total_bentham_calculations"] = summary.get("total_bentham_calculations", 0)
            analysis["learning_quality"]["efficiency"] = summary.get("total_regrets", 0) / max(summary.get("total_scenarios_processed", 1), 1)
        
        return analysis

    async def _test_learned_decision_making(self) -> Dict[str, Any]:
        """í•™ìŠµëœ ì‹œìŠ¤í…œìœ¼ë¡œ ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸"""
        
        # ê°„ë‹¨í•œ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ í…ŒìŠ¤íŠ¸
        test_scenario = {
            "title": "ììœ¨ì£¼í–‰ì°¨ ë”œë ˆë§ˆ",
            "description": "ììœ¨ì£¼í–‰ì°¨ê°€ ê¸‰ë¸Œë ˆì´í¬ë¥¼ ë°Ÿì•„ì•¼ í•˜ëŠ” ìƒí™©ì—ì„œ ë³´í–‰ì 1ëª…ì„ êµ¬í•  ê²ƒì¸ê°€, ì•„ë‹ˆë©´ ì°¨ ì•ˆì˜ íƒ‘ìŠ¹ì 2ëª…ì„ êµ¬í•  ê²ƒì¸ê°€?",
            "context": {"urgency": "high", "stakeholders": ["ë³´í–‰ì", "íƒ‘ìŠ¹ìë“¤"]}
        }
        
        try:
            start_time = time.time()
            
            # ê°ì • ë¶„ì„
            emotion_result = self.emotion_analyzer.analyze_emotion(test_scenario["description"], language="ko")
            
            # ë²¤ë‹´ ê³„ì‚°
            bentham_result = self.bentham_calculator.calculate_with_advanced_layers(test_scenario)
            
            # í›„íšŒ ë¶„ì„
            regret_result = await self.regret_analyzer.analyze_regret({
                "text": test_scenario["description"],
                "context": test_scenario["context"]
            })
            
            processing_time = time.time() - start_time
            
            # ìµœì¢… ê²°ì • (ê°„ë‹¨í•œ ë¡œì§)
            decision_scores = {
                "ë³´í–‰ì êµ¬í•˜ê¸°": 0.3,
                "íƒ‘ìŠ¹ì êµ¬í•˜ê¸°": 0.7
            }
            
            final_decision = max(decision_scores, key=decision_scores.get)
            confidence = max(decision_scores.values())
            
            return {
                "test_success": True,
                "final_recommendation": final_decision,
                "confidence_score": confidence,
                "processing_time": processing_time,
                "reasoning_chain": [
                    f"ê°ì • ë¶„ì„: {getattr(emotion_result, 'dominant_emotion', 'N/A')}",
                    f"ë²¤ë‹´ ì ìˆ˜: {getattr(bentham_result, 'final_score', 0.0):.3f}",
                    f"í›„íšŒ ê°•ë„: {getattr(regret_result, 'regret_intensity', 0.0):.3f}",
                    f"ìµœì¢… ê²°ì •: {final_decision}"
                ]
            }
            
        except Exception as e:
            logger.error(f"ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"test_success": False, "error": str(e)}

    async def _test_dynamic_ethical_analysis(self) -> Dict[str, Any]:
        """ë™ì  ìœ¤ë¦¬ì  ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        
        if not self.dynamic_choice_analyzer:
            return {"error": "ë™ì  ì„ íƒì§€ ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"}
        
        try:
            # ë‹¤ì–‘í•œ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ í…ŒìŠ¤íŠ¸
            test_dilemma = "ì˜ì‚¬ê°€ ì¥ê¸° ì´ì‹ì„ ìœ„í•´ ê±´ê°•í•œ í™˜ì 1ëª…ì„ í¬ìƒì‹œì¼œ 5ëª…ì˜ í™˜ìë¥¼ ì‚´ë¦´ ê²ƒì¸ê°€?"
            
            start_time = time.time()
            result = await self.dynamic_choice_analyzer.analyze_ethical_dilemma(
                dilemma_text=test_dilemma,
                title="ì˜ë£Œ ìœ¤ë¦¬ ë”œë ˆë§ˆ í…ŒìŠ¤íŠ¸"
            )
            processing_time = time.time() - start_time
            
            return {
                "analysis_success": True,
                "dilemma_type": result.dilemma_type.value,
                "extracted_choices": len(result.extracted_choices),
                "stakeholders_identified": len(result.stakeholders),
                "recommended_choice": result.recommended_choice.name if result.recommended_choice else None,
                "reasoning_chain": result.reasoning_chain,
                "processing_time": processing_time,
                "analysis_quality": len(result.reasoning_chain) / 5.0  # ê°„ë‹¨í•œ í’ˆì§ˆ ì§€í‘œ
            }
            
        except Exception as e:
            logger.error(f"ë™ì  ìœ¤ë¦¬ì  ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"analysis_success": False, "error": str(e)}


async def main():
    """ì‹¤ì œ í›ˆë ¨ ë©”ì¸ í•¨ìˆ˜"""
    if not MODULES_AVAILABLE:
        logger.error("âŒ í•„ìˆ˜ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    training_system = RealIntegratedTrainingSystem()
    
    # ì‹¤ì œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if not await training_system.initialize_real_system():
        logger.error("âŒ ì‹¤ì œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹¤í–‰ (ì²˜ìŒ 25ê°œ ì•„ì´í…œìœ¼ë¡œ í…ŒìŠ¤íŠ¸)
    results = await training_system.run_real_integrated_training(max_items=25)
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info(f"\n{'='*80}")
    logger.info("ğŸ‰ ì‹¤ì œ Red Heart AI í†µí•© í›ˆë ¨ ì™„ë£Œ")
    logger.info(f"{'='*80}")
    
    if 'error' not in results:
        summary = results['training_summary']
        module_perf = results['module_performance']
        integration = results['integration_analysis']
        
        logger.info(f"\nğŸ“Š ì‹¤ì œ í›ˆë ¨ ìš”ì•½:")
        logger.info(f"  - ì´ ì²˜ë¦¬ ì•„ì´í…œ: {summary['total_items']}ê°œ")
        logger.info(f"  - ì„±ê³µì  í†µí•©: {summary['successful_integrations']}ê°œ")
        logger.info(f"  - í†µí•© ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        logger.info(f"  - ì´ ì²˜ë¦¬ì‹œê°„: {summary['total_processing_time']:.1f}ì´ˆ")
        logger.info(f"  - í‰ê·  ì²˜ë¦¬ì‹œê°„: {summary['avg_processing_time']:.3f}ì´ˆ")
        
        logger.info(f"\nğŸ¯ ëª¨ë“ˆë³„ ì„±ê³µë¥ :")
        logger.info(f"  - ê°ì • ë¶„ì„: {module_perf['emotion_success_rate']:.1f}%")
        logger.info(f"  - ë²¤ë‹´ ê³„ì‚°: {module_perf['bentham_success_rate']:.1f}%")
        logger.info(f"  - í›„íšŒ ë¶„ì„: {module_perf['regret_success_rate']:.1f}%")
        logger.info(f"  - SURD ë¶„ì„: {module_perf['surd_success_rate']:.1f}%")
        
        logger.info(f"\nğŸ”— í†µí•© ë¶„ì„:")
        logger.info(f"  - ì™„ì „ í†µí•©ë¥ : {integration['full_integration_rate']:.1f}%")
        logger.info(f"  - ë°˜ì‚¬ì‹¤ ì‹œë‚˜ë¦¬ì˜¤: {integration['total_counterfactuals_generated']}ê°œ")
        logger.info(f"  - ì•„ì´í…œë‹¹ í‰ê· : {integration['avg_counterfactuals_per_item']:.1f}ê°œ")
        
        logger.info(f"\nâš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        perf = results['performance_metrics']
        logger.info(f"  - ì²˜ë¦¬ ì†ë„: {perf['items_per_second']:.2f} ì•„ì´í…œ/ì´ˆ")
        logger.info(f"  - ì„±ê³µ ì²˜ë¦¬ ì†ë„: {perf['successful_items_per_second']:.2f} ì•„ì´í…œ/ì´ˆ")
        logger.info(f"  - íš¨ìœ¨ì„± ì ìˆ˜: {perf['efficiency_score']:.2f}")
        
        if results['error_analysis']['error_patterns']:
            logger.info(f"\nâš ï¸ ì˜¤ë¥˜ íŒ¨í„´:")
            for error_type, count in results['error_analysis']['error_patterns'].items():
                logger.info(f"  - {error_type}: {count}íšŒ")
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = Path(f'real_integrated_training_results_{timestamp}.json')
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"\nğŸ“„ ìƒì„¸ ê²°ê³¼ ì €ì¥: {result_file}")
    else:
        logger.error(f"âŒ ì‹¤ì œ í›ˆë ¨ ì‹¤íŒ¨: {results['error']}")


async def main():
    """ì‹¤ì œ í›ˆë ¨ ë©”ì¸ í•¨ìˆ˜"""
    if not MODULES_AVAILABLE:
        logger.error("âŒ í•„ìˆ˜ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ ì‹¤í–‰
    system = RealIntegratedTrainingSystem()
    await system.initialize_real_system()
    
    # ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = await system.run_real_integrated_training(max_items=3)
    
    logger.info("ğŸ¯ Red Heart AI ì‹¤ì œ í†µí•© í›ˆë ¨ ì™„ë£Œ")
    return results


async def main_learning_system():
    """í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""
    
    if not MODULES_AVAILABLE:
        logger.error("âŒ í•„ìˆ˜ ëª¨ë“ˆì´ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if not LEARNING_SYSTEM_AVAILABLE:
        logger.error("âŒ í•™ìŠµ ì‹œìŠ¤í…œì´ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = RealIntegratedTrainingSystem()
    success = await system.initialize_real_system(learning_mode=True)
    
    if not success:
        logger.error("âŒ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰
    results = await system.run_complete_learning_system()
    
    # ê²°ê³¼ ì¶œë ¥
    if results.get("learning_success"):
        logger.info("âœ… ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì„±ê³µ!")
        logger.info(f"ì´ í•™ìŠµ ì‹œê°„: {results['total_learning_time']:.2f}ì´ˆ")
        
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = Path(f'complete_learning_system_results_{timestamp}.json')
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“„ í•™ìŠµ ê²°ê³¼ ì €ì¥: {result_file}")
    else:
        logger.error(f"âŒ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {results.get('error', 'Unknown error')}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Red Heart AI ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ')
    parser.add_argument('--learning', action='store_true', help='ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰')
    parser.add_argument('--test', action='store_true', help='ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    if args.learning:
        asyncio.run(main_learning_system())
    elif args.test:
        asyncio.run(main())
    else:
        # ê¸°ë³¸ê°’: ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        asyncio.run(main())
