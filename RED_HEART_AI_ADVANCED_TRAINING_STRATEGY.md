# Red Heart AI ê³ ê¸‰ í•™ìŠµ ì „ëµ ë° ì‹¤í–‰ ê°€ì´ë“œ

## ğŸ“Œ Executive Summary

10,000ê°œ ë°ì´í„°ì…‹ìœ¼ë¡œ 730M íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ë¡œì»¬ GPU(8GB VRAM)ì—ì„œ í•™ìŠµí•˜ëŠ” ê²€ì¦ëœ ì „ëµ.
60 ì—í­ì€ **ê³¼ì í•© ìœ ë„ê°€ ì•„ë‹Œ ì¶©ë¶„í•œ íƒìƒ‰ ê³µê°„ í™•ë³´**ë¥¼ ìœ„í•¨ì´ë©°, 30ê°œ ì²´í¬í¬ì¸íŠ¸ ì¤‘ **ê³¼ì í•© ì „ Sweet Spotë§Œ ì„ íƒ**í•˜ì—¬ ì‚¬ìš©.

**í•µì‹¬ ì „ëµ:**
- **Phase 0**: ë°ì´í„° í’ˆì§ˆ ë³´ì¦ (ì¤‘ë³µ ì œê±°, ì¸µí™” ë¶„í• )
- **Phase 1**: LR Sweep (5ê°œ í•™ìŠµë¥  Ã— 5 epochs, ìƒì„¸ ë©”íŠ¸ë¦­ ê¸°ë¡)
- **Phase 2**: ë³¸ í•™ìŠµ (60 epochs, 2 epochë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ = 30ê°œ)
- **Phase 3**: Sweet Spot ë¶„ì„ (ëª¨ë“ˆë³„ ìµœì  ì—í­ ìë™ íƒì§€)
- **Phase 4**: íŒŒë¼ë¯¸í„° í¬ë¡œìŠ¤ì˜¤ë²„ (ìµœì  ì¡°í•© ìƒì„±)
- **ì˜ˆìƒ ì‹œê°„**: 170-200ì‹œê°„ (7-8ì¼)
- **ì˜ˆìƒ ì„±ëŠ¥**: ë‹¨ì¼ ëª¨ë¸ 75-80% â†’ í¬ë¡œìŠ¤ì˜¤ë²„ 85-90%

---

## 1. í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ í˜¸í™˜ì„± ë¶„ì„

### 1.1 ì´ë¯¸ êµ¬í˜„ëœ ê¸°ëŠ¥ (unified_training_v2.py ê¸°ì¤€)
```python
# í™•ì¸ëœ ê¸°ëŠ¥ë“¤
âœ… AdamW optimizer (line 638)
âœ… CosineAnnealingLR scheduler (line 658) 
âœ… gradient_accumulation_steps ì§€ì› (line 86)
âœ… mixed_precision (GradScaler) (line 93-94)
âœ… checkpoint ì €ì¥ ê¸°ëŠ¥ (line 1117-1138)
âœ… compute_loss ë©”ì†Œë“œ (ê° í—¤ë“œë³„ êµ¬í˜„)
âœ… DSM (Dynamic Swap Manager) í†µí•©
âœ… ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Claude API)
```

### 1.2 ì¶”ê°€ êµ¬í˜„ í•„ìš” ê¸°ëŠ¥
```python
# ì¶”ê°€ í•„ìš” (í•˜ì§€ë§Œ í˜„ì¬ êµ¬ì¡°ì— ì‰½ê²Œ í†µí•© ê°€ëŠ¥)
âš¡ ëª¨ë“ˆë³„ ê°œë³„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
âš¡ ìƒì„¸ ë©”íŠ¸ë¦­ JSON ì €ì¥
âš¡ ë¼ë²¨ ìŠ¤ë¬´ë”©
âš¡ Layer-wise LR Decay (LLRD)
âš¡ R-Drop ì •ê·œí™”
âš¡ EMA (Exponential Moving Average)
```

---

## 2. Phase 0: ë°ì´í„° ì „ì²˜ë¦¬ ë° í’ˆì§ˆ ë³´ì¦

### 2.1 ì¤‘ë³µ ì œê±° êµ¬í˜„ (rapidfuzz í™œìš©)
```python
# data_quality_assurance.py
from rapidfuzz import fuzz
import json
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from collections import defaultdict

class DataQualityAssurance:
    """ë°ì´í„° í’ˆì§ˆ ë³´ì¦ ëª¨ë“ˆ"""
    
    def __init__(self, data_path: str = "claude_api_preprocessing/claude_preprocessed_complete.json"):
        self.data_path = Path(data_path)
        self.backup_dir = Path("C:/large_project/linux_red_heart/docs/data/preprocessing")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
    def remove_near_duplicates(self, data: List[Dict], threshold: float = 0.92) -> Tuple[List[Dict], Dict]:
        """
        ì¤€ì¤‘ë³µ ì œê±° (ìœ ì‚¬ë„ â‰¥ 0.92)
        Returns:
            cleaned_data: ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°
            removal_stats: ì œê±° í†µê³„ (ë…¼ë¬¸ìš©)
        """
        logger.info(f"ğŸ” ì¤‘ë³µ ì œê±° ì‹œì‘ (threshold={threshold})")
        
        unique_indices = []
        duplicate_pairs = []
        processed = set()
        
        for i, sample1 in enumerate(data):
            if i in processed:
                continue
                
            unique_indices.append(i)
            text1 = sample1.get('text', '')
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            for j in range(i + 1, len(data)):
                if j not in processed:
                    text2 = data[j].get('text', '')
                    similarity = fuzz.ratio(text1, text2) / 100.0
                    
                    if similarity >= threshold:
                        processed.add(j)
                        duplicate_pairs.append({
                            'index1': i,
                            'index2': j,
                            'similarity': similarity,
                            'text1_preview': text1[:100],
                            'text2_preview': text2[:100]
                        })
        
        cleaned_data = [data[i] for i in unique_indices]
        
        # í†µê³„ ì €ì¥ (ë…¼ë¬¸ìš©)
        removal_stats = {
            'original_count': len(data),
            'cleaned_count': len(cleaned_data),
            'removed_count': len(data) - len(cleaned_data),
            'removal_rate': (len(data) - len(cleaned_data)) / len(data) * 100,
            'duplicate_pairs_sample': duplicate_pairs[:10],  # ìƒ˜í”Œë§Œ
            'threshold_used': threshold
        }
        
        # ë°±ì—… ì €ì¥
        stats_path = self.backup_dir / f"duplicate_removal_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(removal_stats, f, indent=2, ensure_ascii=False)
            
        logger.info(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(data)} â†’ {len(cleaned_data)} ({removal_stats['removal_rate']:.1f}% ì œê±°)")
        
        return cleaned_data, removal_stats
    
    def stratified_split(self, data: List[Dict], val_ratio: float = 0.1) -> Tuple[List[Dict], List[Dict], Dict]:
        """
        ì¸µí™” ë¶„í•  + ê¸¸ì´ ë¶„í¬ ë§¤ì¹­
        - í´ë˜ìŠ¤ ê· í˜• ìœ ì§€
        - í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ ìœ ì§€
        - Train-Val ëˆ„ìˆ˜ ë°©ì§€
        """
        logger.info(f"ğŸ“Š ì¸µí™” ë¶„í•  ì‹œì‘ (val_ratio={val_ratio})")
        
        # 1. í´ë˜ìŠ¤ë³„ ê·¸ë£¹í™”
        class_groups = defaultdict(list)
        for sample in data:
            label = sample.get('label', 'unknown')
            class_groups[label].append(sample)
        
        # 2. ê° í´ë˜ìŠ¤ ë‚´ ê¸¸ì´ë³„ ì •ë ¬
        for label in class_groups:
            class_groups[label].sort(key=lambda x: len(x.get('text', '')))
        
        train_data, val_data = [], []
        class_distribution = {}
        
        # 3. ê· ë“± ë¶„í• 
        for label, samples in class_groups.items():
            n_val = max(1, int(len(samples) * val_ratio))  # ìµœì†Œ 1ê°œ
            
            # ê¸¸ì´ ë¶„í¬ ìœ ì§€ë¥¼ ìœ„í•œ ê°„ê²© ìƒ˜í”Œë§
            if len(samples) > n_val:
                val_indices = set(np.linspace(0, len(samples)-1, n_val, dtype=int))
            else:
                val_indices = set(range(len(samples)))
            
            class_train, class_val = [], []
            for i, sample in enumerate(samples):
                if i in val_indices:
                    val_data.append(sample)
                    class_val.append(sample)
                else:
                    train_data.append(sample)
                    class_train.append(sample)
            
            class_distribution[label] = {
                'train': len(class_train),
                'val': len(class_val),
                'total': len(samples)
            }
        
        # 4. ëˆ„ìˆ˜ ê²€ì‚¬
        val_texts = set(s.get('text', '') for s in val_data)
        train_texts = set(s.get('text', '') for s in train_data)
        leakage = val_texts & train_texts
        
        if leakage:
            logger.warning(f"âš ï¸ ë°ì´í„° ëˆ„ìˆ˜ ê°ì§€: {len(leakage)}ê°œ ìƒ˜í”Œ")
            # ëˆ„ìˆ˜ ìƒ˜í”Œ ì œê±°
            val_data = [s for s in val_data if s.get('text', '') not in leakage]
        
        # 5. ë¶„í•  í†µê³„ (ë…¼ë¬¸ìš©)
        split_stats = {
            'train_count': len(train_data),
            'val_count': len(val_data),
            'val_ratio_actual': len(val_data) / (len(train_data) + len(val_data)),
            'class_distribution': class_distribution,
            'leakage_detected': len(leakage),
            'train_text_length_stats': {
                'mean': np.mean([len(s.get('text', '')) for s in train_data]),
                'std': np.std([len(s.get('text', '')) for s in train_data]),
                'min': min(len(s.get('text', '')) for s in train_data),
                'max': max(len(s.get('text', '')) for s in train_data)
            },
            'val_text_length_stats': {
                'mean': np.mean([len(s.get('text', '')) for s in val_data]),
                'std': np.std([len(s.get('text', '')) for s in val_data]),
                'min': min(len(s.get('text', '')) for s in val_data),
                'max': max(len(s.get('text', '')) for s in val_data)
            }
        }
        
        # ë°±ì—… ì €ì¥
        stats_path = self.backup_dir / f"split_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(split_stats, f, indent=2)
        
        logger.info(f"âœ… ë¶„í•  ì™„ë£Œ: Train {len(train_data)}, Val {len(val_data)}")
        
        return train_data, val_data, split_stats
```

---

## 3. Phase 1: Learning Rate Sweep (ë…¼ë¬¸ìš© ìƒì„¸ ë©”íŠ¸ë¦­)

### 3.1 LR Sweep ì‹¤í–‰ ì½”ë“œ
```python
# lr_sweep.py
import torch
import json
from pathlib import Path
from datetime import datetime
import numpy as np
from typing import Dict, List

class LRSweepManager:
    """í•™ìŠµë¥  ìŠ¤ìœ• ê´€ë¦¬ì"""
    
    def __init__(self, base_config: Dict):
        self.base_config = base_config
        self.lr_candidates = [1e-5, 5e-5, 1e-4, 2e-4, 5e-4]
        self.sweep_results = {}
        self.backup_dir = Path("C:/large_project/linux_red_heart/docs/data/lr_sweep")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
    def run_lr_experiment(self, lr: float, model, train_loader, val_loader) -> Dict:
        """
        ë‹¨ì¼ LR ì‹¤í—˜ ì‹¤í–‰ (5 epochs)
        ë…¼ë¬¸ìš© ìƒì„¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        """
        logger.info(f"ğŸ”¬ LR {lr} ì‹¤í—˜ ì‹œì‘")
        
        # ì„¤ì • ì—…ë°ì´íŠ¸
        config = self.base_config.copy()
        config['lr'] = lr
        config['epochs'] = 5
        
        # ë©”íŠ¸ë¦­ ì €ì¥ì†Œ
        metrics = {
            'lr': lr,
            'config': config,
            'epochs': [],
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': [],
            'gradient_norms': [],
            'weight_updates': [],
            'learning_efficiency': [],  # loss ê°ì†Œìœ¨
            'convergence_speed': [],    # ìˆ˜ë ´ ì†ë„
            'stability_score': [],      # ì•ˆì •ì„± ì ìˆ˜
            'gpu_memory': [],
            'time_per_epoch': []
        }
        
        # ëª¨ë¸ ë³µì‚¬ (ê° LRë³„ ë…ë¦½ ì‹¤í—˜)
        import copy
        model_copy = copy.deepcopy(model)
        optimizer = torch.optim.AdamW(model_copy.parameters(), lr=lr)
        
        initial_loss = None
        for epoch in range(5):
            start_time = time.time()
            
            # Training
            train_loss, train_acc, grad_norms = self._train_epoch(
                model_copy, train_loader, optimizer, epoch
            )
            
            # Validation
            val_loss, val_acc = self._validate(model_copy, val_loader)
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            if initial_loss is None:
                initial_loss = train_loss
            
            learning_efficiency = (initial_loss - train_loss) / initial_loss if initial_loss > 0 else 0
            convergence_speed = abs(metrics['train_loss'][-1] - train_loss) if metrics['train_loss'] else 0
            stability = 1.0 / (1.0 + np.std(grad_norms) if grad_norms else 1.0)
            
            # ì €ì¥
            metrics['epochs'].append(epoch)
            metrics['train_loss'].append(train_loss)
            metrics['val_loss'].append(val_loss)
            metrics['train_acc'].append(train_acc)
            metrics['val_acc'].append(val_acc)
            metrics['gradient_norms'].append({
                'mean': np.mean(grad_norms),
                'std': np.std(grad_norms),
                'max': np.max(grad_norms),
                'min': np.min(grad_norms)
            })
            metrics['learning_efficiency'].append(learning_efficiency)
            metrics['convergence_speed'].append(convergence_speed)
            metrics['stability_score'].append(stability)
            metrics['gpu_memory'].append(torch.cuda.max_memory_allocated() / 1e9)
            metrics['time_per_epoch'].append(time.time() - start_time)
            
            logger.info(f"  Epoch {epoch}: Loss={train_loss:.4f}, Val={val_loss:.4f}, Eff={learning_efficiency:.2%}")
        
        # ìµœì¢… í‰ê°€ (ë…¼ë¬¸ìš©)
        metrics['final_evaluation'] = {
            'avg_val_loss': np.mean(metrics['val_loss'][-3:]),  # ë§ˆì§€ë§‰ 3 ì—í­ í‰ê· 
            'overfit_gap': metrics['train_loss'][-1] - metrics['val_loss'][-1],
            'total_improvement': metrics['train_loss'][0] - metrics['train_loss'][-1],
            'stability_overall': np.mean(metrics['stability_score']),
            'convergence_rate': np.mean(metrics['convergence_speed'])
        }
        
        # ë°±ì—… ì €ì¥
        self._save_lr_metrics(lr, metrics)
        
        return metrics
    
    def select_best_lr(self) -> Tuple[float, Dict]:
        """
        ìµœì  LR ì„ íƒ (ë‹¤ê°ë„ í‰ê°€)
        ë…¼ë¬¸ìš© ì„ íƒ ê·¼ê±° ìƒì„±
        """
        logger.info("ğŸ¯ ìµœì  LR ì„ íƒ ì¤‘...")
        
        selection_criteria = {
            'val_loss_weight': 0.4,      # ê²€ì¦ ì†ì‹¤ (ê°€ì¥ ì¤‘ìš”)
            'stability_weight': 0.2,      # ì•ˆì •ì„±
            'efficiency_weight': 0.2,     # í•™ìŠµ íš¨ìœ¨
            'overfit_weight': 0.2         # ê³¼ì í•© ë°©ì§€
        }
        
        scores = {}
        detailed_analysis = {}
        
        for lr, metrics in self.sweep_results.items():
            eval_data = metrics['final_evaluation']
            
            # ê° ì§€í‘œ ì •ê·œí™” (0-1)
            val_score = 1.0 / (1.0 + eval_data['avg_val_loss'])
            stability_score = eval_data['stability_overall']
            efficiency_score = min(1.0, eval_data['total_improvement'])
            overfit_score = 1.0 / (1.0 + abs(eval_data['overfit_gap']))
            
            # ì¢…í•© ì ìˆ˜
            total_score = (
                val_score * selection_criteria['val_loss_weight'] +
                stability_score * selection_criteria['stability_weight'] +
                efficiency_score * selection_criteria['efficiency_weight'] +
                overfit_score * selection_criteria['overfit_weight']
            )
            
            scores[lr] = total_score
            detailed_analysis[lr] = {
                'total_score': total_score,
                'val_score': val_score,
                'stability_score': stability_score,
                'efficiency_score': efficiency_score,
                'overfit_score': overfit_score,
                'raw_metrics': eval_data
            }
        
        # ìµœê³  ì ìˆ˜ LR
        best_lr = max(scores, key=scores.get)
        
        # ì„ íƒ ê·¼ê±° ë¬¸ì„œí™” (ë…¼ë¬¸ìš©)
        selection_report = {
            'selected_lr': best_lr,
            'selection_score': scores[best_lr],
            'all_scores': scores,
            'detailed_analysis': detailed_analysis,
            'selection_criteria': selection_criteria,
            'selection_reason': self._generate_selection_reason(best_lr, detailed_analysis),
            'timestamp': datetime.now().isoformat()
        }
        
        # ì €ì¥
        report_path = self.backup_dir / f"lr_selection_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w') as f:
            json.dump(selection_report, f, indent=2)
        
        # ì‹œê°í™”ìš© ë°ì´í„° ìƒì„±
        self._generate_lr_curves()
        
        logger.info(f"âœ… ìµœì  LR ì„ íƒ: {best_lr} (ì ìˆ˜: {scores[best_lr]:.3f})")
        
        return best_lr, selection_report
    
    def _generate_selection_reason(self, best_lr: float, analysis: Dict) -> str:
        """ë…¼ë¬¸ìš© ì„ íƒ ê·¼ê±° í…ìŠ¤íŠ¸ ìƒì„±"""
        best_data = analysis[best_lr]
        
        reasons = []
        if best_data['val_score'] > 0.7:
            reasons.append(f"ë‚®ì€ ê²€ì¦ ì†ì‹¤ (score: {best_data['val_score']:.3f})")
        if best_data['stability_score'] > 0.6:
            reasons.append(f"ë†’ì€ í•™ìŠµ ì•ˆì •ì„± (score: {best_data['stability_score']:.3f})")
        if best_data['efficiency_score'] > 0.5:
            reasons.append(f"íš¨ìœ¨ì ì¸ ìˆ˜ë ´ (score: {best_data['efficiency_score']:.3f})")
        if best_data['overfit_score'] > 0.6:
            reasons.append(f"ê³¼ì í•© ìœ„í—˜ ë‚®ìŒ (score: {best_data['overfit_score']:.3f})")
        
        return f"LR {best_lr}ì´ ì„ íƒëœ ì´ìœ : " + ", ".join(reasons)
    
    def _generate_lr_curves(self):
        """ë…¼ë¬¸ìš© í•™ìŠµ ê³¡ì„  ë°ì´í„° ìƒì„±"""
        curves_data = {
            'learning_rates': list(self.sweep_results.keys()),
            'train_losses': {},
            'val_losses': {},
            'convergence_speeds': {}
        }
        
        for lr, metrics in self.sweep_results.items():
            curves_data['train_losses'][str(lr)] = metrics['train_loss']
            curves_data['val_losses'][str(lr)] = metrics['val_loss']
            curves_data['convergence_speeds'][str(lr)] = metrics['convergence_speed']
        
        # ì €ì¥
        curves_path = self.backup_dir / "lr_curves_data.json"
        with open(curves_path, 'w') as f:
            json.dump(curves_data, f, indent=2)
        
        logger.info(f"ğŸ“ˆ í•™ìŠµ ê³¡ì„  ë°ì´í„° ì €ì¥: {curves_path}")
```

---

## 4. Phase 2: ë³¸ í•™ìŠµ (60 Epochs, 30ê°œ ì²´í¬í¬ì¸íŠ¸)

### 4.1 í–¥ìƒëœ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
```python
# enhanced_checkpoint_manager.py
class EnhancedCheckpointManager:
    """30ê°œ ì²´í¬í¬ì¸íŠ¸ ì™„ë²½ ê´€ë¦¬"""
    
    def __init__(self, base_dir: str = "checkpoints_v2", backup_dir: str = "C:/large_project/linux_red_heart/docs/data/checkpoints"):
        self.base_dir = Path(base_dir)
        self.backup_dir = Path(backup_dir)
        self.base_dir.mkdir(exist_ok=True)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.metrics_history = []
        self.checkpoint_metadata = {}
        
    def save_modular_checkpoint(self, epoch: int, model, optimizer, metrics: Dict):
        """
        ëª¨ë“ˆë³„ ê°œë³„ ì €ì¥ (2 ì—í­ë§ˆë‹¤ = 30ê°œ)
        ë…¼ë¬¸ìš© ìƒì„¸ ë©”íŠ¸ë¦­ í¬í•¨
        """
        checkpoint_name = f"epoch_{epoch:03d}"
        
        # 1. ê¸°ë³¸ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°
        checkpoint = {
            'epoch': epoch,
            'timestamp': datetime.now().isoformat(),
            'optimizer_state': optimizer.state_dict(),
            'metrics': metrics,
            'model_states': {}
        }
        
        # 2. ëª¨ë“ˆë³„ ê°œë³„ ì €ì¥ (ì˜ì¡´ì„± ê·¸ë£¹ë³„)
        # ê·¸ë£¹ A: Backbone + Heads
        if hasattr(model, 'backbone') and model.backbone:
            checkpoint['model_states']['backbone'] = model.backbone.state_dict()
        
        if hasattr(model, 'heads'):
            checkpoint['model_states']['heads'] = {
                name: head.state_dict() 
                for name, head in model.heads.items()
            }
        
        # ê·¸ë£¹ B: Neural Analyzers
        if hasattr(model, 'analyzers'):
            checkpoint['model_states']['neural_analyzers'] = {
                name: analyzer.state_dict()
                for name, analyzer in model.analyzers.items()
                if 'neural_' in name and hasattr(analyzer, 'state_dict')
            }
            
            # ê·¸ë£¹ C: DSP + Kalman
            checkpoint['model_states']['dsp_kalman'] = {}
            if 'dsp' in model.analyzers:
                checkpoint['model_states']['dsp_kalman']['dsp'] = model.analyzers['dsp'].state_dict()
            if 'kalman' in model.analyzers:
                checkpoint['model_states']['dsp_kalman']['kalman'] = model.analyzers['kalman'].state_dict()
            
            # ë…ë¦½ ëª¨ë“ˆ: Advanced Analyzers
            checkpoint['model_states']['advanced_analyzers'] = {
                name: analyzer.state_dict()
                for name, analyzer in model.analyzers.items()
                if 'advanced_' in name and hasattr(analyzer, 'state_dict')
            }
        
        # 3. ìƒì„¸ ë©”íŠ¸ë¦­ (ë…¼ë¬¸ìš©)
        checkpoint['detailed_metrics'] = self._calculate_detailed_metrics(model, metrics)
        
        # 4. ì €ì¥ (ë©”ì¸ + ë°±ì—…)
        # ë©”ì¸ ì €ì¥
        main_path = self.base_dir / f"{checkpoint_name}.pt"
        torch.save(checkpoint, main_path)
        
        # ì••ì¶• ë°±ì—…
        import gzip
        backup_path = self.backup_dir / f"{checkpoint_name}_backup.pt.gz"
        with gzip.open(backup_path, 'wb') as f:
            torch.save(checkpoint, f)
        
        # 5. ë©”íŠ¸ë¦­ë§Œ ë³„ë„ JSON ì €ì¥ (ë¹ ë¥¸ ë¶„ì„ìš©)
        metrics_path = self.backup_dir / f"metrics_{checkpoint_name}.json"
        metrics_json = {
            'epoch': epoch,
            'metrics': metrics,
            'detailed': checkpoint['detailed_metrics']
        }
        with open(metrics_path, 'w') as f:
            json.dump(metrics_json, f, indent=2)
        
        # 6. ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        self.checkpoint_metadata[epoch] = {
            'path': str(main_path),
            'backup_path': str(backup_path),
            'metrics_path': str(metrics_path),
            'size_mb': main_path.stat().st_size / 1e6,
            'timestamp': checkpoint['timestamp']
        }
        
        # 7. íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.metrics_history.append(metrics_json)
        
        logger.info(f"ğŸ’¾ Checkpoint {epoch:03d} ì €ì¥ ì™„ë£Œ (í¬ê¸°: {self.checkpoint_metadata[epoch]['size_mb']:.1f}MB)")
        
        # 8. 2 ì—í­ë§ˆë‹¤ ëˆ„ì  ë³´ê³ ì„œ ìƒì„±
        if epoch % 2 == 0:
            self._generate_progress_report(epoch)
    
    def _calculate_detailed_metrics(self, model, basic_metrics: Dict) -> Dict:
        """ë…¼ë¬¸ìš© ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        detailed = {}
        
        # 1. ëª¨ë“ˆë³„ íŒŒë¼ë¯¸í„° í†µê³„
        detailed['parameter_stats'] = {}
        for name, module in model.named_modules():
            if hasattr(module, 'parameters'):
                params = list(module.parameters())
                if params:
                    param_tensor = torch.cat([p.flatten() for p in params])
                    detailed['parameter_stats'][name] = {
                        'mean': param_tensor.mean().item(),
                        'std': param_tensor.std().item(),
                        'norm': param_tensor.norm().item(),
                        'sparsity': (param_tensor.abs() < 1e-6).float().mean().item(),
                        'num_params': param_tensor.numel()
                    }
        
        # 2. ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„
        detailed['gradient_stats'] = {}
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad = param.grad.data
                detailed['gradient_stats'][name] = {
                    'mean': grad.mean().item(),
                    'std': grad.std().item(),
                    'norm': grad.norm().item(),
                    'max': grad.max().item(),
                    'min': grad.min().item()
                }
        
        # 3. í•™ìŠµ íš¨ìœ¨ì„± ì§€í‘œ
        if len(self.metrics_history) > 0:
            prev_metrics = self.metrics_history[-1]['metrics']
            detailed['learning_efficiency'] = {
                'loss_improvement': prev_metrics.get('train_loss', 0) - basic_metrics.get('train_loss', 0),
                'val_improvement': prev_metrics.get('val_loss', 0) - basic_metrics.get('val_loss', 0),
                'convergence_rate': abs(prev_metrics.get('train_loss', 0) - basic_metrics.get('train_loss', 0))
            }
        
        # 4. ê³¼ì í•© ì§€í‘œ
        detailed['overfitting_metrics'] = {
            'train_val_gap': basic_metrics.get('train_loss', 0) - basic_metrics.get('val_loss', 0),
            'generalization_error': abs(basic_metrics.get('train_acc', 0) - basic_metrics.get('val_acc', 0))
        }
        
        # 5. GPU ë©”ëª¨ë¦¬ ìƒíƒœ
        detailed['gpu_status'] = {
            'allocated_gb': torch.cuda.memory_allocated() / 1e9,
            'reserved_gb': torch.cuda.memory_reserved() / 1e9,
            'max_allocated_gb': torch.cuda.max_memory_allocated() / 1e9
        }
        
        return detailed
    
    def _generate_progress_report(self, epoch: int):
        """ì§„í–‰ ìƒí™© ë³´ê³ ì„œ ìƒì„± (ë…¼ë¬¸ìš©)"""
        report = {
            'epoch': epoch,
            'total_checkpoints': len(self.checkpoint_metadata),
            'training_progress': epoch / 60 * 100,
            'metrics_summary': self._summarize_metrics(),
            'best_performance': self._find_best_checkpoint(),
            'overfitting_analysis': self._analyze_overfitting(),
            'timestamp': datetime.now().isoformat()
        }
        
        report_path = self.backup_dir / f"progress_report_epoch_{epoch:03d}.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"ğŸ“Š ì§„í–‰ ë³´ê³ ì„œ ìƒì„±: {report_path}")
```

### 4.2 ê³ ê¸‰ í•™ìŠµ ê¸°ë²• í†µí•©
```python
# advanced_training_techniques.py
class AdvancedTrainingTechniques:
    """í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ì— í†µí•© ê°€ëŠ¥í•œ ê³ ê¸‰ ê¸°ë²•"""
    
    def __init__(self, model, config):
        self.model = model
        self.config = config
        
        # Label Smoothing
        self.label_smoothing = config.get('label_smoothing', 0.05)
        
        # EMA
        if config.get('use_ema', True):
            self.ema = self._init_ema(model)
        else:
            self.ema = None
            
        # R-Drop
        self.r_drop_lambda = config.get('r_drop_lambda', 1.0)
        
    def apply_label_smoothing(self, targets: torch.Tensor, num_classes: int) -> torch.Tensor:
        """ë¼ë²¨ ìŠ¤ë¬´ë”© ì ìš©"""
        if self.label_smoothing > 0:
            confidence = 1 - self.label_smoothing
            smoothed = torch.full((targets.size(0), num_classes), 
                                 self.label_smoothing / (num_classes - 1))
            smoothed.scatter_(1, targets.unsqueeze(1), confidence)
            return smoothed
        return F.one_hot(targets, num_classes).float()
    
    def compute_r_drop_loss(self, model, inputs, targets):
        """R-Drop ì†ì‹¤ ê³„ì‚° (ì¼ê´€ì„± ì •ê·œí™”)"""
        # ë‘ ë²ˆì˜ forward pass (ë‹¤ë¥¸ dropout)
        outputs1 = model(inputs)
        outputs2 = model(inputs)
        
        # ê¸°ë³¸ ì†ì‹¤
        ce_loss1 = F.cross_entropy(outputs1, targets, label_smoothing=self.label_smoothing)
        ce_loss2 = F.cross_entropy(outputs2, targets, label_smoothing=self.label_smoothing)
        ce_loss = (ce_loss1 + ce_loss2) / 2
        
        # KL divergence
        kl_loss = F.kl_div(
            F.log_softmax(outputs1, dim=-1),
            F.softmax(outputs2, dim=-1),
            reduction='batchmean'
        ) + F.kl_div(
            F.log_softmax(outputs2, dim=-1),
            F.softmax(outputs1, dim=-1),
            reduction='batchmean'
        )
        kl_loss = kl_loss / 2
        
        return ce_loss + self.r_drop_lambda * kl_loss
    
    def update_ema(self):
        """EMA ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        if self.ema is not None:
            with torch.no_grad():
                for ema_param, param in zip(self.ema.parameters(), self.model.parameters()):
                    ema_param.data.mul_(0.999).add_(param.data, alpha=0.001)
    
    def get_llrd_params(self, base_lr: float) -> List[Dict]:
        """Layer-wise Learning Rate Decay íŒŒë¼ë¯¸í„°"""
        params = []
        
        # ë°±ë³¸: ë‚®ì€ LR (ë³´ìˆ˜ì )
        if hasattr(self.model, 'backbone') and self.model.backbone:
            # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë³„ ë‹¤ë¥¸ LR
            if hasattr(self.model.backbone, 'transformer_encoder'):
                layers = self.model.backbone.transformer_encoder.layers
                num_layers = len(layers)
                for i, layer in enumerate(layers):
                    lr = base_lr * (0.9 ** (num_layers - i))  # ê¹Šì„ìˆ˜ë¡ ë‚®ì€ LR
                    params.append({'params': layer.parameters(), 'lr': lr})
            else:
                params.append({'params': self.model.backbone.parameters(), 'lr': base_lr * 0.5})
        
        # í—¤ë“œ: ë†’ì€ LR (ì ê·¹ì )
        if hasattr(self.model, 'heads'):
            for head in self.model.heads.values():
                params.append({'params': head.parameters(), 'lr': base_lr})
        
        # Analyzers: ì¤‘ê°„ LR
        if hasattr(self.model, 'analyzers'):
            for analyzer in self.model.analyzers.values():
                if hasattr(analyzer, 'parameters'):
                    params.append({'params': analyzer.parameters(), 'lr': base_lr * 0.7})
        
        return params
```

---

## 5. Phase 3: Sweet Spot ìë™ ë¶„ì„

### 5.1 Sweet Spot ë¶„ì„ê¸°
```python
# sweet_spot_analyzer.py
class SweetSpotAnalyzer:
    """30ê°œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë“ˆë³„ ìµœì  ì—í­ íƒì§€"""
    
    def __init__(self, checkpoint_dir: str, metrics_dir: str):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.metrics_dir = Path(metrics_dir)
        self.analysis_results = {}
        
    def analyze_all_checkpoints(self) -> Dict:
        """
        30ê°œ ì²´í¬í¬ì¸íŠ¸ ì „ì²´ ë¶„ì„
        ê³¼ì í•© ì „ Sweet Spot ìë™ íƒì§€
        """
        logger.info("ğŸ” Sweet Spot ë¶„ì„ ì‹œì‘ (30ê°œ ì²´í¬í¬ì¸íŠ¸)")
        
        # 1. ëª¨ë“  ë©”íŠ¸ë¦­ ë¡œë“œ
        all_metrics = {}
        for epoch in range(2, 62, 2):  # 2, 4, 6, ..., 60
            metrics_file = self.metrics_dir / f"metrics_epoch_{epoch:03d}.json"
            if metrics_file.exists():
                with open(metrics_file, 'r') as f:
                    all_metrics[epoch] = json.load(f)
        
        # 2. ê³¼ì í•© ì§€ì  íƒì§€
        overfitting_point = self._detect_overfitting_point(all_metrics)
        logger.info(f"ê³¼ì í•© ì‹œì‘ ì§€ì : Epoch {overfitting_point}")
        
        # 3. ëª¨ë“ˆ ê·¸ë£¹ë³„ Sweet Spot ì°¾ê¸°
        sweet_spots = {}
        
        # ê·¸ë£¹ A: Backbone + Heads (ì—°ë™)
        sweet_spots['group_a_backbone_heads'] = self._find_group_sweet_spot(
            all_metrics, 
            ['backbone', 'heads'],
            max_epoch=overfitting_point
        )
        
        # ê·¸ë£¹ B: Neural Analyzers (ì—°ë™)
        sweet_spots['group_b_neural'] = self._find_group_sweet_spot(
            all_metrics,
            ['neural_emotion', 'neural_bentham', 'neural_regret', 'neural_surd'],
            max_epoch=overfitting_point
        )
        
        # ê·¸ë£¹ C: DSP + Kalman (ì—°ë™)
        sweet_spots['group_c_dsp_kalman'] = self._find_group_sweet_spot(
            all_metrics,
            ['dsp', 'kalman'],
            max_epoch=overfitting_point
        )
        
        # ë…ë¦½ ëª¨ë“ˆë“¤ (ê°œë³„ ìµœì í™” ê°€ëŠ¥)
        for module in ['advanced_emotion', 'advanced_regret', 'advanced_bentham']:
            sweet_spots[module] = self._find_module_sweet_spot(
                all_metrics,
                module,
                max_epoch=60  # ë…ë¦½ ëª¨ë“ˆì€ ê³¼ì í•© ì˜í–¥ ì ìŒ
            )
        
        # 4. ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
        report = self._generate_analysis_report(sweet_spots, all_metrics, overfitting_point)
        
        # 5. ì €ì¥
        report_path = Path("C:/large_project/linux_red_heart/docs/data/analysis/sweet_spot_analysis.json")
        report_path.parent.mkdir(parents=True, exist_ok=True)
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"âœ… Sweet Spot ë¶„ì„ ì™„ë£Œ: {report_path}")
        
        return sweet_spots
    
    def _detect_overfitting_point(self, metrics: Dict) -> int:
        """ê³¼ì í•© ì‹œì‘ ì§€ì  íƒì§€"""
        train_losses = []
        val_losses = []
        epochs = sorted(metrics.keys())
        
        for epoch in epochs:
            train_losses.append(metrics[epoch]['metrics'].get('train_loss', 0))
            val_losses.append(metrics[epoch]['metrics'].get('val_loss', 0))
        
        # Train-Val gapì´ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ì§€ì  ì°¾ê¸°
        gaps = [val - train for train, val in zip(train_losses, val_losses)]
        
        # 3 ì—í­ ì—°ì† gap ì¦ê°€ ì‹œ ê³¼ì í•©ìœ¼ë¡œ íŒë‹¨
        for i in range(2, len(gaps)):
            if gaps[i] > gaps[i-1] > gaps[i-2]:
                return epochs[i-2]  # ì¦ê°€ ì‹œì‘ ì „ ì—í­
        
        return epochs[-1]  # ê³¼ì í•© ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ì—í­
    
    def _find_group_sweet_spot(self, metrics: Dict, modules: List[str], max_epoch: int) -> Dict:
        """ì—°ë™ ëª¨ë“ˆ ê·¸ë£¹ì˜ Sweet Spot"""
        best_epoch = None
        best_score = float('inf')
        
        for epoch in metrics:
            if epoch > max_epoch:
                continue
                
            # ê·¸ë£¹ ë‚´ ëª¨ë“  ëª¨ë“ˆì˜ í‰ê·  ì„±ëŠ¥
            group_score = 0
            valid_count = 0
            
            detailed = metrics[epoch].get('detailed', {})
            
            # ê²€ì¦ ì†ì‹¤
            val_loss = metrics[epoch]['metrics'].get('val_loss', float('inf'))
            
            # ê³¼ì í•© í˜ë„í‹°
            overfit_gap = abs(
                metrics[epoch]['metrics'].get('train_loss', 0) - 
                metrics[epoch]['metrics'].get('val_loss', 0)
            )
            
            # ì•ˆì •ì„± (ê·¸ë˜ë””ì–¸íŠ¸ norm ë³€ë™)
            grad_stats = detailed.get('gradient_stats', {})
            stability = 1.0 / (1.0 + np.mean([
                stat.get('std', 1.0) for stat in grad_stats.values()
            ]))
            
            # ì¢…í•© ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            score = val_loss + 0.2 * overfit_gap - 0.1 * stability
            
            if score < best_score:
                best_score = score
                best_epoch = epoch
        
        return {
            'best_epoch': best_epoch,
            'score': best_score,
            'modules': modules
        }
    
    def _find_module_sweet_spot(self, metrics: Dict, module: str, max_epoch: int) -> Dict:
        """ë…ë¦½ ëª¨ë“ˆì˜ Sweet Spot"""
        best_epoch = None
        best_loss = float('inf')
        
        for epoch in metrics:
            if epoch > max_epoch:
                continue
            
            # ëª¨ë“ˆë³„ ì†ì‹¤ í™•ì¸
            detailed = metrics[epoch].get('detailed', {})
            param_stats = detailed.get('parameter_stats', {})
            
            # í•´ë‹¹ ëª¨ë“ˆì˜ ì†ì‹¤ì´ë‚˜ ì•ˆì •ì„± ì§€í‘œ
            if module in param_stats:
                module_score = 1.0 / (1.0 + param_stats[module].get('sparsity', 0))
                
                if module_score < best_loss:
                    best_loss = module_score
                    best_epoch = epoch
        
        return {
            'best_epoch': best_epoch if best_epoch else max_epoch // 2,
            'score': best_loss,
            'module': module
        }
    
    def _generate_analysis_report(self, sweet_spots: Dict, metrics: Dict, overfit_point: int) -> Dict:
        """ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ (ë…¼ë¬¸ìš©)"""
        report = {
            'analysis_timestamp': datetime.now().isoformat(),
            'total_checkpoints': len(metrics),
            'overfitting_detected_at': overfit_point,
            'sweet_spots': sweet_spots,
            'detailed_reasoning': {},
            'visualization_data': {}
        }
        
        # ê° Sweet Spot ì„ íƒ ì´ìœ 
        for group_name, spot_info in sweet_spots.items():
            epoch = spot_info['best_epoch']
            if epoch and epoch in metrics:
                report['detailed_reasoning'][group_name] = {
                    'selected_epoch': epoch,
                    'metrics_at_selection': metrics[epoch]['metrics'],
                    'selection_score': spot_info['score'],
                    'reason': self._generate_selection_reasoning(epoch, metrics[epoch])
                }
        
        # ì‹œê°í™”ìš© ë°ì´í„°
        epochs = sorted(metrics.keys())
        report['visualization_data'] = {
            'epochs': epochs,
            'train_losses': [metrics[e]['metrics'].get('train_loss', 0) for e in epochs],
            'val_losses': [metrics[e]['metrics'].get('val_loss', 0) for e in epochs],
            'sweet_spot_markers': {
                name: info['best_epoch'] 
                for name, info in sweet_spots.items()
            }
        }
        
        return report
    
    def _generate_selection_reasoning(self, epoch: int, epoch_metrics: Dict) -> str:
        """Sweet Spot ì„ íƒ ì´ìœ  ìƒì„±"""
        reasons = []
        
        metrics = epoch_metrics['metrics']
        detailed = epoch_metrics.get('detailed', {})
        
        # ê²€ì¦ ì†ì‹¤
        val_loss = metrics.get('val_loss', 0)
        if val_loss < 1.0:
            reasons.append(f"ë‚®ì€ ê²€ì¦ ì†ì‹¤ ({val_loss:.4f})")
        
        # ê³¼ì í•© ìƒíƒœ
        overfit = detailed.get('overfitting_metrics', {})
        gap = overfit.get('train_val_gap', 0)
        if abs(gap) < 0.1:
            reasons.append(f"ê³¼ì í•© ë¯¸ë°œìƒ (gap: {gap:.4f})")
        
        # ìˆ˜ë ´ ìƒíƒœ
        efficiency = detailed.get('learning_efficiency', {})
        if efficiency.get('convergence_rate', 1) < 0.01:
            reasons.append("ì•ˆì •ì  ìˆ˜ë ´ ìƒíƒœ")
        
        return " / ".join(reasons) if reasons else "ê¸°ë³¸ ì„ íƒ ê¸°ì¤€ ì¶©ì¡±"
```

---

## 6. Phase 4: íŒŒë¼ë¯¸í„° í¬ë¡œìŠ¤ì˜¤ë²„

### 6.1 í¬ë¡œìŠ¤ì˜¤ë²„ ì‹¤í–‰
```python
# parameter_crossover.py
class ParameterCrossoverManager:
    """30ê°œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìµœì  ì¡°í•© ìƒì„±"""
    
    def __init__(self, checkpoint_dir: str, sweet_spots: Dict):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.sweet_spots = sweet_spots
        self.backup_dir = Path("C:/large_project/linux_red_heart/docs/data/final")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
    def create_optimal_model(self, model_template) -> torch.nn.Module:
        """Sweet Spot ì¡°í•©ìœ¼ë¡œ ìµœì  ëª¨ë¸ ìƒì„±"""
        logger.info("ğŸ”§ ìµœì  ëª¨ë¸ í¬ë¡œìŠ¤ì˜¤ë²„ ì‹œì‘")
        
        # ë¹ˆ ëª¨ë¸ ì´ˆê¸°í™”
        optimal_model = copy.deepcopy(model_template)
        
        # 1. ì—°ë™ ê·¸ë£¹ A (Backbone + Heads)
        if 'group_a_backbone_heads' in self.sweet_spots:
            epoch = self.sweet_spots['group_a_backbone_heads']['best_epoch']
            self._load_group_a(optimal_model, epoch)
            logger.info(f"  ê·¸ë£¹ A ë¡œë“œ: Epoch {epoch}")
        
        # 2. ì—°ë™ ê·¸ë£¹ B (Neural Analyzers)
        if 'group_b_neural' in self.sweet_spots:
            epoch = self.sweet_spots['group_b_neural']['best_epoch']
            self._load_group_b(optimal_model, epoch)
            logger.info(f"  ê·¸ë£¹ B ë¡œë“œ: Epoch {epoch}")
        
        # 3. ì—°ë™ ê·¸ë£¹ C (DSP + Kalman)
        if 'group_c_dsp_kalman' in self.sweet_spots:
            epoch = self.sweet_spots['group_c_dsp_kalman']['best_epoch']
            self._load_group_c(optimal_model, epoch)
            logger.info(f"  ê·¸ë£¹ C ë¡œë“œ: Epoch {epoch}")
        
        # 4. ë…ë¦½ ëª¨ë“ˆë“¤
        for module_name in ['advanced_emotion', 'advanced_regret', 'advanced_bentham']:
            if module_name in self.sweet_spots:
                epoch = self.sweet_spots[module_name]['best_epoch']
                self._load_independent_module(optimal_model, module_name, epoch)
                logger.info(f"  {module_name} ë¡œë“œ: Epoch {epoch}")
        
        # 5. ê²€ì¦
        self._validate_crossover(optimal_model)
        
        # 6. ì €ì¥
        save_path = self.backup_dir / "optimal_crossover_model.pt"
        torch.save(optimal_model.state_dict(), save_path)
        logger.info(f"âœ… ìµœì  ëª¨ë¸ ì €ì¥: {save_path}")
        
        return optimal_model
    
    def create_ensemble_variants(self, model_template, delta: int = 2) -> List:
        """Â±2 ì—í­ ë³€í˜•ìœ¼ë¡œ ì•™ìƒë¸” ìƒì„±"""
        logger.info(f"ğŸ­ ì•™ìƒë¸” ë³€í˜• ìƒì„± (Â±{delta} epochs)")
        
        variants = []
        
        for offset in [-2*delta, -delta, 0, delta, 2*delta]:  # -4, -2, 0, 2, 4
            # Sweet Spot ì¡°ì •
            adjusted_spots = {}
            for key, info in self.sweet_spots.items():
                epoch = info['best_epoch']
                if epoch:
                    # ë²”ìœ„ ì œí•œ (2-60)
                    adjusted_epoch = max(2, min(60, epoch + offset))
                    adjusted_spots[key] = {
                        'best_epoch': adjusted_epoch,
                        'score': info['score']
                    }
            
            # ëª¨ë¸ ìƒì„±
            variant_model = self._create_variant(model_template, adjusted_spots)
            
            variants.append({
                'offset': offset,
                'model': variant_model,
                'sweet_spots': adjusted_spots
            })
            
            logger.info(f"  ë³€í˜• {offset:+d} ìƒì„± ì™„ë£Œ")
        
        return variants
    
    def _create_variant(self, model_template, adjusted_spots: Dict):
        """ì¡°ì •ëœ Sweet Spotìœ¼ë¡œ ë³€í˜• ìƒì„±"""
        self.sweet_spots = adjusted_spots  # ì„ì‹œ êµì²´
        variant = self.create_optimal_model(model_template)
        return variant
```

---

## 7. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ë° ëª¨ë‹ˆí„°ë§

### 7.1 í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```python
# main_training_pipeline.py
import argparse
import torch
from pathlib import Path
from datetime import datetime
import json

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data-path', default='claude_api_preprocessing/claude_preprocessed_complete.json')
    parser.add_argument('--max-samples', type=int, default=10000)
    parser.add_argument('--batch-size', type=int, default=4)
    parser.add_argument('--gradient-accumulation', type=int, default=16)
    parser.add_argument('--use-dsm', action='store_true', default=True)
    parser.add_argument('--backup-dir', default='C:/large_project/linux_red_heart/docs/data')
    args = parser.parse_args()
    
    # ì„¸ì…˜ ì´ˆê¸°í™”
    session_id = datetime.now().strftime('%Y%m%d_%H%M%S')
    session_dir = Path(args.backup_dir) / f'session_{session_id}'
    session_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"ğŸš€ Red Heart AI ê³ ê¸‰ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    logger.info(f"ğŸ“ ì„¸ì…˜ ë””ë ‰í† ë¦¬: {session_dir}")
    
    # Phase 0: ë°ì´í„° ì¤€ë¹„
    logger.info("\n" + "="*60)
    logger.info("Phase 0: ë°ì´í„° í’ˆì§ˆ ë³´ì¦")
    logger.info("="*60)
    
    qa = DataQualityAssurance(args.data_path)
    with open(args.data_path, 'r') as f:
        raw_data = json.load(f)
    
    # ì¤‘ë³µ ì œê±°
    clean_data, dup_stats = qa.remove_near_duplicates(raw_data, threshold=0.92)
    
    # ì¸µí™” ë¶„í• 
    train_data, val_data, split_stats = qa.stratified_split(clean_data, val_ratio=0.1)
    
    # Phase 1: LR Sweep
    logger.info("\n" + "="*60)
    logger.info("Phase 1: Learning Rate Sweep")
    logger.info("="*60)
    
    lr_manager = LRSweepManager(base_config={
        'batch_size': args.batch_size,
        'gradient_accumulation': args.gradient_accumulation,
        'mixed_precision': True,
        'label_smoothing': 0.05
    })
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    from unified_training_v2 import UnifiedTrainingSystemV2
    model = UnifiedTrainingSystemV2(args)
    model.prepare_data()
    model.initialize_models()
    
    # LR ì‹¤í—˜
    for lr in lr_manager.lr_candidates:
        metrics = lr_manager.run_lr_experiment(lr, model, train_data, val_data)
        lr_manager.sweep_results[lr] = metrics
    
    # ìµœì  LR ì„ íƒ
    best_lr, selection_report = lr_manager.select_best_lr()
    
    # Phase 2: ë³¸ í•™ìŠµ
    logger.info("\n" + "="*60)
    logger.info(f"Phase 2: ë³¸ í•™ìŠµ (LR={best_lr})")
    logger.info("="*60)
    
    # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì
    ckpt_manager = EnhancedCheckpointManager(
        base_dir=f"checkpoints_{session_id}",
        backup_dir=str(session_dir / 'checkpoints')
    )
    
    # ê³ ê¸‰ ê¸°ë²• ì ìš©
    techniques = AdvancedTrainingTechniques(model, {
        'label_smoothing': 0.05,
        'use_ema': True,
        'r_drop_lambda': 1.0
    })
    
    # LLRD ì ìš©
    optimizer_params = techniques.get_llrd_params(best_lr)
    model.optimizer = torch.optim.AdamW(optimizer_params, weight_decay=0.01)
    
    # 60 ì—í­ í•™ìŠµ
    for epoch in range(60):
        # í•™ìŠµ
        metrics = train_epoch_with_techniques(model, train_data, techniques, epoch)
        
        # ê²€ì¦
        val_metrics = validate(model, val_data)
        
        # ë©”íŠ¸ë¦­ ë³‘í•©
        metrics.update({
            'val_loss': val_metrics['loss'],
            'val_acc': val_metrics['accuracy']
        })
        
        # 2 ì—í­ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if epoch % 2 == 0:
            ckpt_manager.save_modular_checkpoint(epoch, model, model.optimizer, metrics)
        
        # EMA ì—…ë°ì´íŠ¸
        if techniques.ema:
            techniques.update_ema()
        
        logger.info(f"Epoch {epoch}: Train={metrics['train_loss']:.4f}, Val={metrics['val_loss']:.4f}")
    
    # Phase 3: Sweet Spot ë¶„ì„
    logger.info("\n" + "="*60)
    logger.info("Phase 3: Sweet Spot ë¶„ì„")
    logger.info("="*60)
    
    analyzer = SweetSpotAnalyzer(
        checkpoint_dir=ckpt_manager.base_dir,
        metrics_dir=ckpt_manager.backup_dir
    )
    
    sweet_spots = analyzer.analyze_all_checkpoints()
    
    # Phase 4: í¬ë¡œìŠ¤ì˜¤ë²„
    logger.info("\n" + "="*60)
    logger.info("Phase 4: íŒŒë¼ë¯¸í„° í¬ë¡œìŠ¤ì˜¤ë²„")
    logger.info("="*60)
    
    crossover_manager = ParameterCrossoverManager(
        checkpoint_dir=ckpt_manager.base_dir,
        sweet_spots=sweet_spots
    )
    
    # ìµœì  ëª¨ë¸ ìƒì„±
    optimal_model = crossover_manager.create_optimal_model(model)
    
    # ì•™ìƒë¸” ë³€í˜• ìƒì„±
    variants = crossover_manager.create_ensemble_variants(model)
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    final_results = {
        'session_id': session_id,
        'best_lr': best_lr,
        'sweet_spots': sweet_spots,
        'data_stats': {
            'duplicate_removal': dup_stats,
            'split': split_stats
        },
        'lr_sweep': selection_report,
        'training_complete': True,
        'timestamp': datetime.now().isoformat()
    }
    
    with open(session_dir / 'final_results.json', 'w') as f:
        json.dump(final_results, f, indent=2)
    
    logger.info(f"âœ… í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {session_dir / 'final_results.json'}")

if __name__ == '__main__':
    main()
```

### 7.2 ì‹¤í–‰ ëª…ë ¹
```bash
#!/bin/bash
# run_advanced_training.sh

# GPU ì„¤ì •
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# ë¡œê·¸ ë””ë ‰í† ë¦¬
LOG_DIR="logs/training_$(date +%Y%m%d_%H%M%S)"
mkdir -p $LOG_DIR

# ì‹¤í–‰
nohup python main_training_pipeline.py \
    --data-path claude_api_preprocessing/claude_preprocessed_complete.json \
    --max-samples 10000 \
    --batch-size 4 \
    --gradient-accumulation 16 \
    --use-dsm \
    --backup-dir "C:/large_project/linux_red_heart/docs/data" \
    > $LOG_DIR/training.log 2>&1 &

echo "PID: $!"
echo "ë¡œê·¸: tail -f $LOG_DIR/training.log"
```

---

## 8. ê²°ë¡ 

### 8.1 ê²€ì¦ëœ ì ‘ê·¼ë²•
- **60 ì—í­ì€ ê³¼ì í•© ìœ ë„ê°€ ì•„ë‹Œ íƒìƒ‰ ê³µê°„ í™•ë³´**: 30ê°œ ì²´í¬í¬ì¸íŠ¸ ì¤‘ ìµœì ì  ì„ íƒ
- **ëª¨ë“  ê¸°ë²•ì´ í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ì™€ í˜¸í™˜**: unified_training_v2.py êµ¬ì¡° í™œìš©
- **ë…¼ë¬¸ìš© ë©”íŠ¸ë¦­ ì™„ë²½ ìˆ˜ì§‘**: LR ì„ íƒ ê·¼ê±°, í•™ìŠµ ê³¡ì„ , Sweet Spot ë¶„ì„

### 8.2 ì˜ˆìƒ ê²°ê³¼
| ì²´í¬í¬ì¸íŠ¸ | ì—í­ | ìƒíƒœ | í™œìš© |
|-----------|------|------|------|
| 1-10 | 2-20 | ì´ˆê¸° í•™ìŠµ | ë¶ˆì•ˆì • |
| 11-20 | 22-40 | ì•ˆì • ìˆ˜ë ´ | **Sweet Spot í›„ë³´** |
| 21-25 | 42-50 | ì„±ìˆ™ | **ìµœì  êµ¬ê°„** |
| 26-30 | 52-60 | ê³¼ì í•© ì‹œì‘ | ì œì™¸ |

### 8.3 ë°±ì—… êµ¬ì¡°
```
C:/large_project/linux_red_heart/docs/data/
â””â”€â”€ session_YYYYMMDD_HHMMSS/
    â”œâ”€â”€ preprocessing/          # ë°ì´í„° í’ˆì§ˆ í†µê³„
    â”œâ”€â”€ lr_sweep/              # 5ê°œ LR ìƒì„¸ ë¶„ì„
    â”œâ”€â”€ checkpoints/           # 30ê°œ ì²´í¬í¬ì¸íŠ¸
    â”œâ”€â”€ analysis/              # Sweet Spot ë¶„ì„
    â””â”€â”€ final/                 # í¬ë¡œìŠ¤ì˜¤ë²„ ëª¨ë¸
```

ì´ ì „ëµì„ í†µí•´ ì œí•œëœ ë°ì´í„°(10K)ì™€ ë¦¬ì†ŒìŠ¤(8GB GPU)ë¡œë„ ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.