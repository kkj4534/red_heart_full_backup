# ğŸš¨ RedHeart AI Critical Issues Deep Analysis Report

## ğŸ”¥ Executive Summary
ì‹œìŠ¤í…œì´ GPU OOM(Out of Memory)ë¡œ ì¸í•œ ì¹˜ëª…ì  ì‹¤íŒ¨ë¥¼ ê²½í—˜í•˜ê³  ìˆìŒ. ê·¼ë³¸ ì›ì¸ì€ ì˜ëª»ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ ë¹„íš¨ìœ¨ì ì¸ ëª¨ë¸ ë¡œë”© ì „ëµ.

## ğŸ“Š í•µì‹¬ ë¬¸ì œ ë¶„ì„

### 1. SentenceTransformer Subprocess GPU ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ (800MB+ ë‚­ë¹„)
**ë¬¸ì œì :**
- `sentence_transformer_singleton.py`ê°€ ê° ëª¨ë¸ë§ˆë‹¤ ë³„ë„ subprocess ìƒì„±
- ê° subprocessê°€ GPUì— ë…ë¦½ì ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ (400MB Ã— 2 = 800MB)
- MEDIUM ëª¨ë“œì—ì„œë„ GPU ì‚¬ìš© (FORCE_CPU_INIT ë¬´ì‹œ)

**ì½”ë“œ ë¶„ì„:**
```python
# advanced_emotion_analyzer.py:338
self.device = get_device()  # MEDIUM ëª¨ë“œì—ì„œë„ 'cuda' ë°˜í™˜

# advanced_emotion_analyzer.py:872-881
self.embedders['multilingual'] = get_sentence_transformer(
    multilingual_model,
    device=str(self.device)  # 'cuda' ì „ë‹¬
)
self.embedders['korean'] = get_sentence_transformer(
    korean_embedding_model,
    device=str(self.device)  # 'cuda' ì „ë‹¬
)
```

**ê²°ê³¼:**
- ë‘ ê°œì˜ ë…ë¦½ subprocessê°€ GPUì— ëª¨ë¸ ë¡œë“œ
- DSMì´ ì´ subprocess ë©”ëª¨ë¦¬ë¥¼ ê´€ë¦¬í•  ìˆ˜ ì—†ìŒ
- GPU ì‚¬ìš©ë¥  82.3% â†’ 99.4% ê¸‰ì¦

### 2. ëª¨ë“ˆ ë™ì‹œ ë¡œë”© ë¬¸ì œ
**ë¬¸ì œì :**
- `main_unified.py:initialize()`ì—ì„œ ëª¨ë“  ëª¨ë“ˆ ë™ì‹œ ì´ˆê¸°í™”
- Workflow phase ë¬´ê´€í•˜ê²Œ ì „ì²´ ë¡œë“œ
- ë©”ëª¨ë¦¬ ì••ë°• ë°œìƒ

**ì½”ë“œ ë¶„ì„:**
```python
# main_unified.py:380-400
await self._load_unified_model()       # 250M
await self._load_translator()          # CPU (OK)
await self._load_neural_analyzers()    # 368M (GPU)
await self._load_advanced_wrappers()   # 112M (ì¼ë¶€ GPU)
await self._load_dsp_components()      # ì¶”ê°€ ë©”ëª¨ë¦¬
await self._load_phase_networks()      # ì¶”ê°€ ë©”ëª¨ë¦¬
```

### 3. DSM ì–¸ë¡œë“œ ì‹¤íŒ¨
**ë¬¸ì œì :**
- CRITICAL priority ëª¨ë¸ (backbone, heads) ì–¸ë¡œë“œ ë¶ˆê°€
- SentenceTransformer subprocess ëª¨ë¸ ê´€ë¦¬ ë¶ˆê°€
- ì‹¤ì œ í•´ì œ ê°€ëŠ¥í•œ ëª¨ë¸ ì—†ìŒ (0.0MB freed)

**ì½”ë“œ ë¶„ì„:**
```python
# dynamic_swap_manager.py:1873-1875
if model_info.priority == SwapPriority.CRITICAL:
    logger.debug(f"[CRITICAL ë³´í˜¸] {name}ì€ ì–¸ë¡œë“œ ë¶ˆê°€")
    continue
```

### 4. FORCE_CPU_INIT ë¶€ë¶„ ì ìš©
**ë¬¸ì œì :**
- AdvancedBenthamCalculatorëŠ” FORCE_CPU_INIT ì¤€ìˆ˜ âœ…
- AdvancedEmotionAnalyzerëŠ” ë¬´ì‹œ âŒ
- ì¼ê´€ì„± ì—†ëŠ” CPU/GPU ì •ì±…

### 5. Perplexity API JSON íŒŒì‹± ì‹¤íŒ¨
**ë¬¸ì œì :**
- ì‘ë‹µ í˜•ì‹ ë¶ˆì¼ì¹˜ë¡œ JSON íŒŒì‹± ì—ëŸ¬ ë°˜ë³µ
- ì—ëŸ¬ í•¸ë“¤ë§ ë¶€ì¬ë¡œ ì‹œìŠ¤í…œ ì¤‘ë‹¨

## ğŸ› ï¸ ì¢…í•© í•´ê²° ë°©ì•ˆ

### Phase 1: ì¦‰ì‹œ ìˆ˜ì • (Critical)

#### 1.1 SentenceTransformer MEDIUM ëª¨ë“œ CPU ê°•ì œ
```python
# advanced_emotion_analyzer.py ìˆ˜ì •
def __init__(self):
    # ... ê¸°ì¡´ ì½”ë“œ ...
    
    # MEDIUM ëª¨ë“œ ì²´í¬
    import os
    if os.environ.get('FORCE_CPU_INIT', '0') == '1':
        self.device = torch.device('cpu')
        logger.info("ğŸ“Œ FORCE_CPU_INIT: AdvancedEmotionAnalyzer CPU ëª¨ë“œ")
    else:
        from config import get_device
        self.device = get_device()
    
    # ... ë‚˜ë¨¸ì§€ ì½”ë“œ ...
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œ device ê°•ì œ
    device_str = 'cpu' if os.environ.get('FORCE_CPU_INIT', '0') == '1' else str(self.device)
    
    self.embedders['multilingual'] = get_sentence_transformer(
        multilingual_model,
        device=device_str  # CPU/GPU ë¶„ê¸°
    )
```

#### 1.2 SentenceTransformer Subprocess ì œê±°
```python
# sentence_transformer_singleton.py ëŒ€ì²´ êµ¬í˜„
class SentenceTransformerManager:
    def get_model(self, model_name: str, device: str = None):
        # subprocess ëŒ€ì‹  ì§ì ‘ ë¡œë“œ
        if device == 'cpu' or device == 'cpu:0':
            # CPUì—ì„œ ì§ì ‘ ë¡œë“œ
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer(model_name, device='cpu')
        else:
            # GPU ë¡œë“œëŠ” DSM í†µí•´ ê´€ë¦¬
            model = self._load_with_dsm(model_name, device)
        return model
```

### Phase 2: Workflow ê¸°ë°˜ ìˆœì°¨ ë¡œë”©

#### 2.1 Lazy Loading êµ¬í˜„
```python
# main_unified.py ìˆ˜ì •
async def initialize(self):
    # í•„ìˆ˜ ëª¨ë“ˆë§Œ ì´ˆê¸°í™”
    await self._load_unified_model()  # backbone/heads í•„ìˆ˜
    
    # ë‚˜ë¨¸ì§€ëŠ” lazy loading
    self.pending_modules = {
        'neural_analyzers': self._load_neural_analyzers,
        'advanced_wrappers': self._load_advanced_wrappers,
        # ...
    }
    
async def _ensure_module_loaded(self, module_name: str):
    """í•„ìš” ì‹œì ì— ëª¨ë“ˆ ë¡œë“œ"""
    if module_name in self.pending_modules:
        await self.pending_modules[module_name]()
        del self.pending_modules[module_name]
```

#### 2.2 Workflow Phase ì •ì˜
```python
# workflow_phases.py
class WorkflowPhase(Enum):
    INIT = "init"           # backboneë§Œ
    EMBED = "embed"         # +embedding models
    ANALYZE = "analyze"     # +analyzers
    INTEGRATE = "integrate" # +integration
    GENERATE = "generate"   # +LLM
```

### Phase 3: DSM ê°œì„ 

#### 3.1 Priority ì¬ì¡°ì •
```python
# CRITICALì„ ì„¸ë¶„í™”
class SwapPriority(Enum):
    BACKBONE = 5     # ì ˆëŒ€ ì–¸ë¡œë“œ ë¶ˆê°€
    PRIMARY = 4      # ì›Œí¬í”Œë¡œìš° í•µì‹¬ (ì¡°ê±´ë¶€ ì–¸ë¡œë“œ)
    SECONDARY = 3    # ë³´ì¡° ëª¨ë“ˆ (ì–¸ë¡œë“œ ê°€ëŠ¥)
    AUXILIARY = 2    # ë¶€ê°€ ê¸°ëŠ¥ (ìš°ì„  ì–¸ë¡œë“œ)
    TEMPORARY = 1    # ì„ì‹œ (ì¦‰ì‹œ ì–¸ë¡œë“œ)
```

#### 3.2 Subprocess ëª¨ë¸ ê´€ë¦¬
```python
# DSMì— subprocess ëª¨ë¸ ì¶”ê°€
def register_subprocess_model(self, process_id: int, model_name: str, size_mb: float):
    """subprocess ëª¨ë¸ ë“±ë¡ ë° ì¶”ì """
    self.subprocess_models[process_id] = {
        'name': model_name,
        'size_mb': size_mb,
        'can_terminate': True
    }
```

### Phase 4: ë©”ëª¨ë¦¬ ëª©í‘œ ì„¤ì •

#### 4.1 MEDIUM ëª¨ë“œ ë©”ëª¨ë¦¬ ë°°ë¶„
```yaml
Total GPU: 8192 MB
Reserved: 1500 MB (OS/Driver)
Available: 6692 MB

Allocation:
- Backbone: 250 MB (BACKBONE priority)
- Heads: 250 MB (BACKBONE priority)
- Current Phase Models: 1500 MB (PRIMARY)
- Buffer: 500 MB
- Dynamic: 4192 MB (SECONDARY/AUXILIARY)
```

## ğŸ“ Action Items

### ì¦‰ì‹œ ì‹¤í–‰ (ì˜¤ëŠ˜)
1. [ ] AdvancedEmotionAnalyzer FORCE_CPU_INIT ìˆ˜ì •
2. [ ] SentenceTransformer subprocess â†’ ì§ì ‘ ë¡œë“œ ë³€ê²½
3. [ ] Perplexity API ì—ëŸ¬ í•¸ë“¤ë§ ì¶”ê°€

### ë‹¨ê¸° (1-2ì¼)
1. [ ] Lazy loading êµ¬í˜„
2. [ ] Workflow phase ì‹œìŠ¤í…œ êµ¬í˜„
3. [ ] DSM priority ì„¸ë¶„í™”

### ì¤‘ê¸° (3-5ì¼)
1. [ ] WAUP ì •ì±… ì™„ì „ êµ¬í˜„
2. [ ] ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ë„êµ¬ ì¶”ê°€
3. [ ] ìë™ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ

## ğŸ¯ ì˜ˆìƒ ê²°ê³¼
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: 99.4% â†’ 85% ëª©í‘œ
- ëª¨ë¸ ë¡œë“œ ì‹œê°„: 50% ë‹¨ì¶•
- OOM ì—ëŸ¬: ì™„ì „ ì œê±°
- ì¶”ë¡  ì†ë„: 30% ê°œì„ 

## ğŸ“Š ì¸¡ì • ì§€í‘œ
```python
# ìˆ˜ì • ì „
GPU Usage: 99.4% (7.7GB/8.0GB)
Models Loaded: 15ê°œ ë™ì‹œ
Subprocess: 2ê°œ (800MB)
DSM Freed: 0.0MB

# ëª©í‘œ
GPU Usage: 85% (6.8GB/8.0GB)
Models Loaded: 5-7ê°œ (phaseë³„)
Subprocess: 0ê°œ
DSM Freed: 500MB+
```

## ğŸ” ê²€ì¦ ë°©ë²•
```bash
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
python3 main_unified.py \
  --memory-mode medium \
  --mode inference \
  --text "í…ŒìŠ¤íŠ¸" \
  --debug \
  --monitor-memory
```

---
*ì‘ì„±ì¼: 2025-09-04*
*ì‘ì„±ì: Claude 4 Extended Thinking*
*ê²€í†  í•„ìš”: GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ë¬¸ê°€*