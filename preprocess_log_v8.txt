INFO:RedHeart.DataPreprocessingV3:디바이스: cuda
INFO:RedHeart.DataPreprocessingV3:Loading sentence transformer: sentence-transformers/all-MiniLM-L6-v2
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
INFO:RedHeart.DataPreprocessingV3:🌐 번역 파이프라인 초기화 중...
Device set to use cuda:0
INFO:RedHeart.DataPreprocessingV3:✅ 번역 파이프라인 로드 완료
INFO:RedHeart.DataPreprocessingV3:✅ HelpingAI 전처리 파이프라인 v3 초기화 완료
INFO:RedHeart.DataPreprocessingV3:📚 데이터셋 처리 시작: parsed_raw_datasets.json
INFO:RedHeart.DataPreprocessingV3:총 3개 샘플 처리 예정
INFO:RedHeart.DataPreprocessingV3:🔄 HelpingAI 엔진 초기화 중...
INFO:RedHeart.AdvancedLLM:모델 설정 준비 완료: ['helpingai']
INFO:RedHeart.AdvancedLLM:동적 GPU-RAM 스왑 시스템 초기화 완료
INFO:RedHeart.DataPreprocessingV3:✅ LLM 초기화 완료
INFO:RedHeart.DataPreprocessingV3:진행률: 0/3 (0.0%)
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.80it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.80it/s]
INFO:RedHeart.DataPreprocessingV3:번역 완료: 일제강점기 이후의 혼란스러운 시대에 적응하지 못하고 몰락한 세대인 '안 초시', '서 참의... → Anchosi, "Anchocy," Park Hee-Wan, who has collapse...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'emotion_analysis' (복잡도: simple)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:🚀 모델 helpingai (llama_cpp) GPU 로딩 시작...
INFO:RedHeart.AdvancedLLM:📁 모델 경로: /mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 2070 SUPER, compute capability 7.5, VMM: yes
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 2070 SUPER) - 6667 MiB free
llama_model_loader: loaded meta data with 33 key-value pairs and 327 tensors from /mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = HelpingAI2 9B
llama_model_loader: - kv   3:                       general.organization str              = Abhaykoul
llama_model_loader: - kv   4:                           general.basename str              = HelpingAI2
llama_model_loader: - kv   5:                         general.size_label str              = 9B
llama_model_loader: - kv   6:                            general.license str              = other
llama_model_loader: - kv   7:                       general.license.name str              = helpingai
llama_model_loader: - kv   8:                       general.license.link str              = LICENSE.md
llama_model_loader: - kv   9:                               general.tags arr[str,4]       = ["HelpingAI", "Emotionally Intelligen...
llama_model_loader: - kv  10:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  11:                           general.datasets arr[str,6]       = ["OEvortex/SentimentSynth", "JeanKadd...
llama_model_loader: - kv  12:                          llama.block_count u32              = 36
llama_model_loader: - kv  13:                       llama.context_length u32              = 131072
llama_model_loader: - kv  14:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  15:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  16:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  17:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  19:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  20:                          general.file_type u32              = 15
llama_model_loader: - kv  21:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  22:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 128004
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   73 tensors
llama_model_loader: - type q4_K:  217 tensors
llama_model_loader: - type q6_K:   37 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 5.06 GiB (4.88 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG
load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG
load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG
load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG
load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG
load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG
load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG
load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG
load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG
load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG
load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG
load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG
load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG
load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG
load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG
load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG
load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG
load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG
load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG
load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG
load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG
load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG
load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG
load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG
load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG
load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG
load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG
load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG
load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG
load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG
load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG
load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG
load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG
load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG
load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG
load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG
load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG
load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG
load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG
load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG
load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG
load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG
load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG
load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG
load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG
load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG
load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG
load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG
load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG
load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG
load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG
load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG
load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG
load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG
load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG
load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG
load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG
load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG
load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG
load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG
load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG
load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG
load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG
load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG
load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG
load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG
load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG
load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG
load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG
load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG
load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG
load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG
load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG
load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG
load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG
load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG
load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG
load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG
load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG
load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG
load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG
load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG
load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG
load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG
load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG
load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG
load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG
load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG
load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG
load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG
load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG
load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG
load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG
load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG
load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG
load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG
load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG
load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG
load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG
load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG
load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG
load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG
load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG
load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG
load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG
load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG
load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG
load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG
load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG
load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG
load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG
load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG
load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG
load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG
load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG
load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG
load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG
load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG
load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG
load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG
load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG
load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG
load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG
load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG
load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG
load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG
load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG
load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG
load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG
load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG
load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG
load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG
load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG
load: control token: 128007 '<|end_header_id|>' is not marked as EOG
load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG
load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG
load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG
load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG
load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG
load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG
load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG
load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG
load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG
load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG
load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG
load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG
load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG
load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG
load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG
load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG
load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG
load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG
load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG
load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG
load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG
load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG
load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG
load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG
load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG
load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG
load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG
load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG
load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG
load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG
load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG
load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG
load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG
load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG
load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG
load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG
load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG
load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG
load: control token: 128006 '<|start_header_id|>' is not marked as EOG
load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG
load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG
load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG
load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG
load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG
load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG
load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG
load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG
load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG
load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG
load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG
load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG
load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG
load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG
load: control token: 128000 '<|begin_of_text|>' is not marked as EOG
load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG
load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG
load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG
load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG
load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG
load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG
load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG
load: control token: 128010 '<|python_tag|>' is not marked as EOG
load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG
load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG
load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG
load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG
load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG
load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG
load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG
load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG
load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG
load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG
load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG
load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG
load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG
load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG
load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG
load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG
load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG
load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG
load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG
load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG
load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG
load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG
load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG
load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG
load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG
load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG
load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG
load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG
load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG
load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG
load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG
load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG
load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG
load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG
load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG
load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG
load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG
load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG
load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG
load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG
load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG
load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG
load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG
load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG
load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG
load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG
load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG
load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG
load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG
load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG
load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG
load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG
load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG
load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG
load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG
load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG
load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.90 B
print_info: general.name     = HelpingAI2 9B
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: PAD token        = 128004 '<|finetune_right_pad_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CPU, is_swa = 0
load_tensors: layer   1 assigned to device CUDA0, is_swa = 0
load_tensors: layer   2 assigned to device CUDA0, is_swa = 0
load_tensors: layer   3 assigned to device CUDA0, is_swa = 0
load_tensors: layer   4 assigned to device CUDA0, is_swa = 0
load_tensors: layer   5 assigned to device CUDA0, is_swa = 0
load_tensors: layer   6 assigned to device CUDA0, is_swa = 0
load_tensors: layer   7 assigned to device CUDA0, is_swa = 0
load_tensors: layer   8 assigned to device CUDA0, is_swa = 0
load_tensors: layer   9 assigned to device CUDA0, is_swa = 0
load_tensors: layer  10 assigned to device CUDA0, is_swa = 0
load_tensors: layer  11 assigned to device CUDA0, is_swa = 0
load_tensors: layer  12 assigned to device CUDA0, is_swa = 0
load_tensors: layer  13 assigned to device CUDA0, is_swa = 0
load_tensors: layer  14 assigned to device CUDA0, is_swa = 0
load_tensors: layer  15 assigned to device CUDA0, is_swa = 0
load_tensors: layer  16 assigned to device CUDA0, is_swa = 0
load_tensors: layer  17 assigned to device CUDA0, is_swa = 0
load_tensors: layer  18 assigned to device CUDA0, is_swa = 0
load_tensors: layer  19 assigned to device CUDA0, is_swa = 0
load_tensors: layer  20 assigned to device CUDA0, is_swa = 0
load_tensors: layer  21 assigned to device CUDA0, is_swa = 0
load_tensors: layer  22 assigned to device CUDA0, is_swa = 0
load_tensors: layer  23 assigned to device CUDA0, is_swa = 0
load_tensors: layer  24 assigned to device CUDA0, is_swa = 0
load_tensors: layer  25 assigned to device CUDA0, is_swa = 0
load_tensors: layer  26 assigned to device CUDA0, is_swa = 0
load_tensors: layer  27 assigned to device CUDA0, is_swa = 0
load_tensors: layer  28 assigned to device CUDA0, is_swa = 0
load_tensors: layer  29 assigned to device CUDA0, is_swa = 0
load_tensors: layer  30 assigned to device CUDA0, is_swa = 0
load_tensors: layer  31 assigned to device CUDA0, is_swa = 0
load_tensors: layer  32 assigned to device CUDA0, is_swa = 0
load_tensors: layer  33 assigned to device CUDA0, is_swa = 0
load_tensors: layer  34 assigned to device CUDA0, is_swa = 0
load_tensors: layer  35 assigned to device CUDA0, is_swa = 0
load_tensors: layer  36 assigned to device CPU, is_swa = 0
load_tensors: tensor 'token_embd.weight' (q4_K) (and 11 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead
load_tensors: offloading 35 repeating layers to GPU
load_tensors: offloaded 35/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4359.06 MiB
load_tensors:   CPU_Mapped model buffer size =  5184.37 MiB
.........................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_per_seq = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
set_abort_callback: call
llama_context:        CPU  output buffer size =     0.49 MiB
create_memory: n_ctx = 131072 (padded)
llama_kv_cache_unified: layer   0: dev = CPU
llama_kv_cache_unified: layer   1: dev = CUDA0
llama_kv_cache_unified: layer   2: dev = CUDA0
llama_kv_cache_unified: layer   3: dev = CUDA0
llama_kv_cache_unified: layer   4: dev = CUDA0
llama_kv_cache_unified: layer   5: dev = CUDA0
llama_kv_cache_unified: layer   6: dev = CUDA0
llama_kv_cache_unified: layer   7: dev = CUDA0
llama_kv_cache_unified: layer   8: dev = CUDA0
llama_kv_cache_unified: layer   9: dev = CUDA0
llama_kv_cache_unified: layer  10: dev = CUDA0
llama_kv_cache_unified: layer  11: dev = CUDA0
llama_kv_cache_unified: layer  12: dev = CUDA0
llama_kv_cache_unified: layer  13: dev = CUDA0
llama_kv_cache_unified: layer  14: dev = CUDA0
llama_kv_cache_unified: layer  15: dev = CUDA0
llama_kv_cache_unified: layer  16: dev = CUDA0
llama_kv_cache_unified: layer  17: dev = CUDA0
llama_kv_cache_unified: layer  18: dev = CUDA0
llama_kv_cache_unified: layer  19: dev = CUDA0
llama_kv_cache_unified: layer  20: dev = CUDA0
llama_kv_cache_unified: layer  21: dev = CUDA0
llama_kv_cache_unified: layer  22: dev = CUDA0
llama_kv_cache_unified: layer  23: dev = CUDA0
llama_kv_cache_unified: layer  24: dev = CUDA0
llama_kv_cache_unified: layer  25: dev = CUDA0
llama_kv_cache_unified: layer  26: dev = CUDA0
llama_kv_cache_unified: layer  27: dev = CUDA0
llama_kv_cache_unified: layer  28: dev = CUDA0
llama_kv_cache_unified: layer  29: dev = CUDA0
llama_kv_cache_unified: layer  30: dev = CUDA0
llama_kv_cache_unified: layer  31: dev = CUDA0
llama_kv_cache_unified: layer  32: dev = CUDA0
llama_kv_cache_unified: layer  33: dev = CUDA0
llama_kv_cache_unified: layer  34: dev = CUDA0
llama_kv_cache_unified: layer  35: dev = CUDA0
llama_kv_cache_unified:      CUDA0 KV buffer size = 17920.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB
llama_kv_cache_unified: size = 18432.00 MiB (131072 cells,  36 layers,  1 seqs), K (f16): 9216.00 MiB, V (f16): 9216.00 MiB
llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 2
llama_context: max_nodes = 65536
llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512
llama_context:      CUDA0 compute buffer size =  8985.00 MiB
llama_context:  CUDA_Host compute buffer size =   264.01 MiB
llama_context: graph nodes  = 1302
llama_context: graph splits = 15 (with bs=512), 3 (with bs=1)
CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
Model metadata: {'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '8', 'llama.block_count': '36', 'tokenizer.ggml.padding_token_id': '128004', 'general.basename': 'HelpingAI2', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'HelpingAI2 9B', 'general.organization': 'Abhaykoul', 'general.type': 'model', 'general.size_label': '9B', 'general.license.name': 'helpingai', 'tokenizer.chat_template': '{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = "26 Jul 2024" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][\'role\'] == \'system\' %}\n    {%- set system_message = messages[0][\'content\']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = "" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- "<|start_header_id|>system<|end_header_id|>\\n\\n" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- "Environment: ipython\\n" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- "Tools: " + builtin_tools | reject(\'equalto\', \'code_interpreter\') | join(", ") + "\\n\\n"}}\n{%- endif %}\n{{- "Cutting Knowledge Date: December 2023\\n" }}\n{{- "Today Date: " + date_string + "\\n\\n" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- "<|eot_id|>" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][\'content\']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception("Cannot put tools in the first user message when there\'s no first user message!") }}\n{%- endif %}\n    {{- \'<|start_header_id|>user<|end_header_id|>\\n\\n\' -}}\n    {{- "Given the following functions, please respond with a JSON for a function call " }}\n    {{- "with its proper arguments that best answers the given prompt.\\n\\n" }}\n    {{- \'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.\' }}\n    {{- "Do not use variables.\\n\\n" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- "\\n\\n" }}\n    {%- endfor %}\n    {{- first_user_message + "<|eot_id|>"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == \'ipython\' or message.role == \'tool\' or \'tool_calls\' in message) %}\n        {{- \'<|start_header_id|>\' + message[\'role\'] + \'<|end_header_id|>\\n\\n\'+ message[\'content\'] | trim + \'<|eot_id|>\' }}\n    {%- elif \'tool_calls\' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception("This model only supports single tool-calls at once!") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- "<|python_tag|>" + tool_call.name + ".call(" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + \'="\' + arg_val + \'"\' }}\n                {%- if not loop.last %}\n                    {{- ", " }}\n                {%- endif %}\n                {%- endfor %}\n            {{- ")" }}\n        {%- else  %}\n            {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' -}}\n            {{- \'{"name": "\' + tool_call.name + \'", \' }}\n            {{- \'"parameters": \' }}\n            {{- tool_call.arguments | tojson }}\n            {{- "}" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we\'re in ipython mode #}\n            {{- "<|eom_id|>" }}\n        {%- else %}\n            {{- "<|eot_id|>" }}\n        {%- endif %}\n    {%- elif message.role == "tool" or message.role == "ipython" %}\n        {{- "<|start_header_id|>ipython<|end_header_id|>\\n\\n" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- "<|eot_id|>" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- \'<|start_header_id|>assistant<|end_header_id|>\\n\\n\' }}\n{%- endif %}\n', 'general.license.link': 'LICENSE.md', 'general.license': 'other', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096'}
Available chat formats from metadata: chat_template.default
Using gguf chat template: {{- bos_token }}
{%- if custom_tools is defined %}
    {%- set tools = custom_tools %}
{%- endif %}
{%- if not tools_in_user_message is defined %}
    {%- set tools_in_user_message = true %}
{%- endif %}
{%- if not date_string is defined %}
    {%- set date_string = "26 Jul 2024" %}
{%- endif %}
{%- if not tools is defined %}
    {%- set tools = none %}
{%- endif %}

{#- This block extracts the system message, so we can slot it into the right place. #}
{%- if messages[0]['role'] == 'system' %}
    {%- set system_message = messages[0]['content']|trim %}
    {%- set messages = messages[1:] %}
{%- else %}
    {%- set system_message = "" %}
{%- endif %}

{#- System message + builtin tools #}
{{- "<|start_header_id|>system<|end_header_id|>\n\n" }}
{%- if builtin_tools is defined or tools is not none %}
    {{- "Environment: ipython\n" }}
{%- endif %}
{%- if builtin_tools is defined %}
    {{- "Tools: " + builtin_tools | reject('equalto', 'code_interpreter') | join(", ") + "\n\n"}}
{%- endif %}
{{- "Cutting Knowledge Date: December 2023\n" }}
{{- "Today Date: " + date_string + "\n\n" }}
{%- if tools is not none and not tools_in_user_message %}
    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}
    {{- 'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.' }}
    {{- "Do not use variables.\n\n" }}
    {%- for t in tools %}
        {{- t | tojson(indent=4) }}
        {{- "\n\n" }}
    {%- endfor %}
{%- endif %}
{{- system_message }}
{{- "<|eot_id|>" }}

{#- Custom tools are passed in a user message with some extra guidance #}
{%- if tools_in_user_message and not tools is none %}
    {#- Extract the first user message so we can plug it in here #}
    {%- if messages | length != 0 %}
        {%- set first_user_message = messages[0]['content']|trim %}
        {%- set messages = messages[1:] %}
    {%- else %}
        {{- raise_exception("Cannot put tools in the first user message when there's no first user message!") }}
{%- endif %}
    {{- '<|start_header_id|>user<|end_header_id|>\n\n' -}}
    {{- "Given the following functions, please respond with a JSON for a function call " }}
    {{- "with its proper arguments that best answers the given prompt.\n\n" }}
    {{- 'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.' }}
    {{- "Do not use variables.\n\n" }}
    {%- for t in tools %}
        {{- t | tojson(indent=4) }}
        {{- "\n\n" }}
    {%- endfor %}
    {{- first_user_message + "<|eot_id|>"}}
{%- endif %}

{%- for message in messages %}
    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}
        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' }}
    {%- elif 'tool_calls' in message %}
        {%- if not message.tool_calls|length == 1 %}
            {{- raise_exception("This model only supports single tool-calls at once!") }}
        {%- endif %}
        {%- set tool_call = message.tool_calls[0].function %}
        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}
            {{- '<|start_header_id|>assistant<|end_header_id|>\n\n' -}}
            {{- "<|python_tag|>" + tool_call.name + ".call(" }}
            {%- for arg_name, arg_val in tool_call.arguments | items %}
                {{- arg_name + '="' + arg_val + '"' }}
                {%- if not loop.last %}
                    {{- ", " }}
                {%- endif %}
                {%- endfor %}
            {{- ")" }}
        {%- else  %}
            {{- '<|start_header_id|>assistant<|end_header_id|>\n\n' -}}
            {{- '{"name": "' + tool_call.name + '", ' }}
            {{- '"parameters": ' }}
            {{- tool_call.arguments | tojson }}
            {{- "}" }}
        {%- endif %}
        {%- if builtin_tools is defined %}
            {#- This means we're in ipython mode #}
            {{- "<|eom_id|>" }}
        {%- else %}
            {{- "<|eot_id|>" }}
        {%- endif %}
    {%- elif message.role == "tool" or message.role == "ipython" %}
        {{- "<|start_header_id|>ipython<|end_header_id|>\n\n" }}
        {%- if message.content is mapping or message.content is iterable %}
            {{- message.content | tojson }}
        {%- else %}
            {{- message.content }}
        {%- endif %}
        {{- "<|eot_id|>" }}
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
    {{- '<|start_header_id|>assistant<|end_header_id|>\n\n' }}
{%- endif %}

Using chat eos_token: <|eot_id|>
Using chat bos_token: <|begin_of_text|>
INFO:RedHeart.AdvancedLLM:Llama.cpp 모델 로드 완료: /mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf
INFO:RedHeart.AdvancedLLM:🦙 Llama.cpp 모델 helpingai 로드 완료
INFO:RedHeart.AdvancedLLM:✅ 모델 helpingai GPU 로드 완료 및 활성화
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=100, temp=0.5, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 346 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 100
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =    1369.21 ms /   101 tokens (   13.56 ms per token,    73.77 tokens per second)
llama_perf_context_print:        eval time =    5208.26 ms /    99 runs   (   52.61 ms per token,    19.01 tokens per second)
llama_perf_context_print:       total time =    6796.67 ms /   200 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-5383d17f-0ae2-4917-882d-faf8731e22f8', 'object': 'text_completion', 'created': 1755136159, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': "0-10\n\nJoy: 0\nSadness: 8\nAnger: 0\nFear: 0\nSurprise: 0\nDisgust: 0\nTrust: 0\n\nI'm not sure about the context of this text, but based on the information provided, it seems that Park Hee-Wan has been dealing with some personal struggles since the first-term period. \n\nIs there any additional information you can provide to help me better understand the situation?", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 101, 'completion_tokens': 100, 'total_tokens': 201}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 357 문자
INFO:RedHeart.DataPreprocessingV3:감정 분석 응답: 
INFO:RedHeart.DataPreprocessingV3:번역 완료: 일제강점기 이후의 혼란스러운 시대에 적응하지 못하고 몰락한 세대인 '안 초시', '서 참의... → Anchosi, "Anchocy," Park Hee-Wan, who has collapse...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'regret_analysis' (복잡도: simple)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:📋 모델 helpingai 이미 로드됨 (재사용)
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=20, temp=0.3, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 311 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 20
Llama.generate: 1 prefix-match hit, remaining 89 prompt tokens to eval
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =     390.58 ms /    89 tokens (    4.39 ms per token,   227.87 tokens per second)
llama_perf_context_print:        eval time =     942.97 ms /    19 runs   (   49.63 ms per token,    20.15 tokens per second)
llama_perf_context_print:       total time =    1373.35 ms /   108 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-d9867213-7d27-4faf-806d-92a5b7b4fe61', 'object': 'text_completion', 'created': 1755136166, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': '8\n\nThis text describes someone who has failed in their plans to make a lot of money on real', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 90, 'completion_tokens': 20, 'total_tokens': 110}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 91 문자
INFO:RedHeart.DataPreprocessingV3:번역 완료: 일제강점기 이후의 혼란스러운 시대에 적응하지 못하고 몰락한 세대인 '안 초시', '서 참의... → Anchosi, "Anchocy," Park Hee-Wan, who has collapse...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'bentham_analysis' (복잡도: moderate)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:📋 모델 helpingai 이미 로드됨 (재사용)
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=80, temp=0.4, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 470 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 80
Llama.generate: 1 prefix-match hit, remaining 118 prompt tokens to eval
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =     488.19 ms /   118 tokens (    4.14 ms per token,   241.71 tokens per second)
llama_perf_context_print:        eval time =    4151.36 ms /    79 runs   (   52.55 ms per token,    19.03 tokens per second)
llama_perf_context_print:       total time =    4814.67 ms /   197 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-992803de-0e21-48fc-b497-231306e243ae', 'object': 'text_completion', 'created': 1755136168, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': ' 0\n\nintensity ( strength of happiness):\nduration (how long it lasts):\ncertainty ( likelihood):\npropinquity (closeness in time): 0\n\nintensity ( strength of happiness):\nduration (how long it lasts):\ncertainty ( likelihood):\npropinquity (closness in time): 0\n\nintensity ( strength of happiness):\n duration ( how long it lasts', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 119, 'completion_tokens': 80, 'total_tokens': 199}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 321 문자
INFO:RedHeart.DataPreprocessingV3:벤담 분석 응답: 
INFO:RedHeart.DataPreprocessingV3:번역 완료: 일제강점기 이후의 혼란스러운 시대에 적응하지 못하고 몰락한 세대인 '안 초시', '서 참의... → Anchosi, "Anchocy," Park Hee-Wan, who has collapse...
INFO:RedHeart.DataPreprocessingV3:번역 완료: 급변하는 시대에 적응하지 못한 구세대의 몰락과 그들의 비참한 죽음 앞에서, 친구로서의 의리... → The conflict that arises between the breakdown of ...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'surd_analysis' (복잡도: moderate)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:📋 모델 helpingai 이미 로드됨 (재사용)
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=80, temp=0.4, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 716 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 80
Llama.generate: 1 prefix-match hit, remaining 168 prompt tokens to eval
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =     672.86 ms /   168 tokens (    4.01 ms per token,   249.68 tokens per second)
llama_perf_context_print:        eval time =    4350.22 ms /    79 runs   (   55.07 ms per token,    18.16 tokens per second)
llama_perf_context_print:       total time =    5194.06 ms /   247 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-2e33ff7d-62fa-45cb-bb66-b6265bea5654', 'object': 'text_completion', 'created': 1755136174, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': ' 0\nethics (moral principles):\nempathetic consideration:\nconcern for others:\nfairness:\ncompassion:\nconscience:\nmorally sound decision making:\nneutrality:\nobjectivity:\nimpersonal consideration:\ninclusiveness:\nopen-mindedness:\ntolerance:\nenlightened perspective:\nmellowing effect:\nemotional intelligence:\nempathy:\naltruism', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 169, 'completion_tokens': 80, 'total_tokens': 249}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 318 문자
INFO:RedHeart.DataPreprocessingV3:SURD 분석 응답: 
INFO:RedHeart.DataPreprocessingV3:✅ 샘플 0 처리 완료
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00, 169.96it/s]
INFO:RedHeart.DataPreprocessingV3:번역 완료: 역마살을 타고난 '성기'가 정착을 바라는 어머니 '옥화'와 할머니의 기대에 부응하여 중(僧... → Although the genitals, innately, are either active...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'emotion_analysis' (복잡도: simple)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:📋 모델 helpingai 이미 로드됨 (재사용)
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=100, temp=0.5, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 346 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 100
Llama.generate: 3 prefix-match hit, remaining 88 prompt tokens to eval
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =     392.24 ms /    88 tokens (    4.46 ms per token,   224.35 tokens per second)
llama_perf_context_print:        eval time =    4957.99 ms /    96 runs   (   51.65 ms per token,    19.36 tokens per second)
llama_perf_context_print:       total time =    5555.76 ms /   184 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-b3fddb76-ad89-43f9-a1de-5e4e047dd609', 'object': 'text_completion', 'created': 1755136179, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': "0-10\n\nJoy: 0\nSadness: 8\nAnger: 4\nFear: 2\nSurprise: 0\nDisgust: 0\nTrust: 0\n\nI'm not sure if my rating is correct because the text seems to be talking about something abstract and it's hard for me to rate the emotions based on that. I'm basing my ratings off of the overall tone which seems sad. Am I correct?", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 91, 'completion_tokens': 96, 'total_tokens': 187}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 306 문자
INFO:RedHeart.DataPreprocessingV3:감정 분석 응답: 
INFO:RedHeart.DataPreprocessingV3:번역 완료: 역마살을 타고난 '성기'가 정착을 바라는 어머니 '옥화'와 할머니의 기대에 부응하여 중(僧... → Although the genitals, innately, are either active...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'regret_analysis' (복잡도: simple)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:📋 모델 helpingai 이미 로드됨 (재사용)
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=20, temp=0.3, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 311 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 20
Llama.generate: 1 prefix-match hit, remaining 79 prompt tokens to eval
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =     401.24 ms /    79 tokens (    5.08 ms per token,   196.89 tokens per second)
llama_perf_context_print:        eval time =     927.74 ms /    19 runs   (   48.83 ms per token,    20.48 tokens per second)
llama_perf_context_print:       total time =    1368.89 ms /    98 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-7454145b-ef5c-471d-bfcc-e1d459b3237b', 'object': 'text_completion', 'created': 1755136185, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': "8\n\nThis text is about the internal conflict that can arise when someone's desire for love is frustrated", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 20, 'total_tokens': 100}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 103 문자
INFO:RedHeart.DataPreprocessingV3:번역 완료: 역마살을 타고난 '성기'가 정착을 바라는 어머니 '옥화'와 할머니의 기대에 부응하여 중(僧... → Although the genitals, innately, are either active...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'bentham_analysis' (복잡도: moderate)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:📋 모델 helpingai 이미 로드됨 (재사용)
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=80, temp=0.4, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 474 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 80
Llama.generate: 1 prefix-match hit, remaining 110 prompt tokens to eval
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =     471.20 ms /   110 tokens (    4.28 ms per token,   233.44 tokens per second)
llama_perf_context_print:        eval time =    4126.00 ms /    79 runs   (   52.23 ms per token,    19.15 tokens per second)
llama_perf_context_print:       total time =    4765.80 ms /   189 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-6b884b48-51f2-4878-9fa2-33851b027a17', 'object': 'text_completion', 'created': 1755136187, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': ' 0: Not at all 10: Very much\n\nText: "Although the genitals, innately, are either active in their mother\'s \'cure\' and her grandmother\'s expectation of settling down, they end up in internal conflict when love is frustrated by the tragic bloodline of the story, and they accept their own backselves as fate."\n\nintensity ( strength of happiness): 0', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 111, 'completion_tokens': 80, 'total_tokens': 191}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 344 문자
INFO:RedHeart.DataPreprocessingV3:벤담 분석 응답: 
INFO:RedHeart.DataPreprocessingV3:번역 완료: 역마살을 타고난 '성기'가 정착을 바라는 어머니 '옥화'와 할머니의 기대에 부응하여 중(僧... → Although the genitals, innately, are either active...
INFO:RedHeart.DataPreprocessingV3:번역 완료: 타고난 운명을 거부하고 정착을 바라는 가족의 기대에 부응하려 노력하지만, 혈연적 인연에 얽... → While refusing natural destiny and trying to live ...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'surd_analysis' (복잡도: moderate)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:📋 모델 helpingai 이미 로드됨 (재사용)
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=80, temp=0.4, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 720 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 80
Llama.generate: 1 prefix-match hit, remaining 159 prompt tokens to eval
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =     666.76 ms /   159 tokens (    4.19 ms per token,   238.47 tokens per second)
llama_perf_context_print:        eval time =    2147.45 ms /    40 runs   (   53.69 ms per token,    18.63 tokens per second)
llama_perf_context_print:       total time =    2898.98 ms /   199 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-3b0af9eb-ff38-4d5f-91a8-e9c16cf72539', 'object': 'text_completion', 'created': 1755136191, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': "0.5\nethics (moral principles):\nempathy (consideration of others' feelings):0.5\ncreativity (novelty and originality):0.5\n\n", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 160, 'completion_tokens': 41, 'total_tokens': 201}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 119 문자
INFO:RedHeart.DataPreprocessingV3:SURD 분석 응답: 
INFO:RedHeart.DataPreprocessingV3:✅ 샘플 1 처리 완료
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00, 199.78it/s]
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
INFO:RedHeart.DataPreprocessingV3:번역 완료: 해방 후 혼란스러운 사회에서, 기회주의적이고 세속적인 삶의 태도를 지닌 '옥임'과 전통적인... → In a mixed society after liberation, the ‘Oms ’ of...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'emotion_analysis' (복잡도: simple)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:📋 모델 helpingai 이미 로드됨 (재사용)
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=100, temp=0.5, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 346 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 100
Llama.generate: 3 prefix-match hit, remaining 91 prompt tokens to eval
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =     390.51 ms /    91 tokens (    4.29 ms per token,   233.03 tokens per second)
llama_perf_context_print:        eval time =    5132.84 ms /    99 runs   (   51.85 ms per token,    19.29 tokens per second)
llama_perf_context_print:       total time =    5735.25 ms /   190 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-bcefb98e-2f81-4e7e-80ea-6eb512520302', 'object': 'text_completion', 'created': 1755136194, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': '0-10, where 0 is no emotion at all and 10 is the strongest emotion possible\n\nJoy: 0\nSadness: 0\nAnger: 0\nFear: 0\nSurprise: 0\nDisgust: 0\nTrust: 0\n\nAnalyze the emotions in this text. What do you think the author is trying to say?\n\nThe author seems to be talking about a society after liberation where there are people who lead secular and opportun', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 94, 'completion_tokens': 100, 'total_tokens': 194}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 344 문자
INFO:RedHeart.DataPreprocessingV3:감정 분석 응답: 
INFO:RedHeart.DataPreprocessingV3:번역 완료: 해방 후 혼란스러운 사회에서, 기회주의적이고 세속적인 삶의 태도를 지닌 '옥임'과 전통적인... → In a mixed society after liberation, the ‘Oms ’ of...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'regret_analysis' (복잡도: simple)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:📋 모델 helpingai 이미 로드됨 (재사용)
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=20, temp=0.3, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 311 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 20
Llama.generate: 1 prefix-match hit, remaining 82 prompt tokens to eval
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =     371.48 ms /    82 tokens (    4.53 ms per token,   220.74 tokens per second)
llama_perf_context_print:        eval time =     935.85 ms /    19 runs   (   49.26 ms per token,    20.30 tokens per second)
llama_perf_context_print:       total time =    1347.30 ms /   101 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-168ad4b1-3bd0-4cbe-8d19-98378ea11344', 'object': 'text_completion', 'created': 1755136200, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': '8\n\nThis text describes a situation where someone is torn between two different lifestyles and values. The presence', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 83, 'completion_tokens': 20, 'total_tokens': 103}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 114 문자
INFO:RedHeart.DataPreprocessingV3:번역 완료: 해방 후 혼란스러운 사회에서, 기회주의적이고 세속적인 삶의 태도를 지닌 '옥임'과 전통적인... → In a mixed society after liberation, the ‘Oms ’ of...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'bentham_analysis' (복잡도: moderate)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:📋 모델 helpingai 이미 로드됨 (재사용)
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=80, temp=0.4, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 478 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 80
Llama.generate: 1 prefix-match hit, remaining 110 prompt tokens to eval
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =     457.94 ms /   110 tokens (    4.16 ms per token,   240.21 tokens per second)
llama_perf_context_print:        eval time =    4119.37 ms /    79 runs   (   52.14 ms per token,    19.18 tokens per second)
llama_perf_context_print:       total time =    4745.59 ms /   189 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-3b932d18-2a32-490d-ad76-d0aea6c85c91', 'object': 'text_completion', 'created': 1755136202, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': " \n\n0-10 intensity: 3\n0-10 duration: 2\n0 description: It's a moderate strength of happiness but it doesn't last long and there is no certainty about its likelihood. It happened recently so it's relatively close in time.\n\n0-10 intensity: 8\n0-10 duration: 9\n0 description: It's very strong and lasts for a", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 111, 'completion_tokens': 80, 'total_tokens': 191}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 299 문자
INFO:RedHeart.DataPreprocessingV3:벤담 분석 응답: 
INFO:RedHeart.DataPreprocessingV3:번역 완료: 해방 후 혼란스러운 사회에서, 기회주의적이고 세속적인 삶의 태도를 지닌 '옥임'과 전통적인... → In a mixed society after liberation, the ‘Oms ’ of...
INFO:RedHeart.DataPreprocessingV3:번역 완료: 급변하는 사회에서 자신의 이익만을 좇는 기회주의적 태도(물질주의)와 전통적인 가치를 지키려... → A conflict between an opportunistic attitude in a ...
INFO:RedHeart.AdvancedLLM:🎯 작업 유형 'surd_analysis' (복잡도: moderate)에 대해 HelpingAI 모델 사용
INFO:RedHeart.AdvancedLLM:💡 HelpingAI 모델 로딩 시도...
INFO:RedHeart.AdvancedLLM:📋 모델 helpingai 이미 로드됨 (재사용)
INFO:RedHeart.AdvancedLLM:✅ HelpingAI 모델 사용 확정
INFO:RedHeart.AdvancedLLM:🔧 LlamaCpp 생성 매개변수: max_tokens=80, temp=0.4, top_p=0.9
INFO:RedHeart.AdvancedLLM:📝 입력 프롬프트 길이: 706 문자
INFO:RedHeart.AdvancedLLM:🚀 실제 생성 시작 - max_tokens: 80
Llama.generate: 1 prefix-match hit, remaining 150 prompt tokens to eval
llama_perf_context_print:        load time =    1370.06 ms
llama_perf_context_print: prompt eval time =     654.82 ms /   150 tokens (    4.37 ms per token,   229.07 tokens per second)
llama_perf_context_print:        eval time =    1804.49 ms /    34 runs   (   53.07 ms per token,    18.84 tokens per second)
llama_perf_context_print:       total time =    2530.68 ms /   184 tokens
INFO:RedHeart.AdvancedLLM:🔍 LlamaCpp 원시 응답: {'id': 'cmpl-69b2ad14-ed18-4ed4-8582-16d8aeb1aa47', 'object': 'text_completion', 'created': 1755136207, 'model': '/mnt/c/large_project/linux_red_heart/llm_module/HelpingAI2-9B.Q4_K_M.gguf', 'choices': [{'text': ' 0\nethics (moral principles): 8\nempathy (considering others): 7\ncreativity (novel solutions): 2\n\n', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 151, 'completion_tokens': 35, 'total_tokens': 186}}
INFO:RedHeart.AdvancedLLM:✅ 생성된 텍스트 길이: 94 문자
INFO:RedHeart.DataPreprocessingV3:SURD 분석 응답: 
INFO:RedHeart.DataPreprocessingV3:✅ 샘플 2 처리 완료
INFO:RedHeart.DataPreprocessingV3:✅ 처리 완료: 3개 샘플 → preprocessed_dataset_v3.json

📊 데이터셋 통계:
  - 총 샘플 수: 3

  소스별 분포:
    - ebs_literature: 3개

  평균 후회 지수: 0.500

  평균 감정 분포:
    - joy: 0.143
    - sadness: 0.143
    - anger: 0.143
    - fear: 0.143
    - surprise: 0.143
    - disgust: 0.143
    - trust: 0.143

  임베딩 차원: 384

  평균 벤담 점수:
    - intensity: 0.500
    - duration: 0.500
    - certainty: 0.500
    - propinquity: 0.500
    - purity: 0.285
    - extent: 0.500
    - fecundity: 0.228
    - remoteness: 0.500
    - succession: 0.350
    - utility: 0.429

  평균 SURD 메트릭:
    - sufficiency: 0.500
    - understandability: 0.500
    - resilience: 0.500
    - decisiveness: 0.500
