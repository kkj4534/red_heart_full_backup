"""
Red Heart Linux Advanced - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ
GPU/CPU ì‚¬ìš©ëŸ‰, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±, ì²˜ë¦¬ ì†ë„ ìµœì í™” ë¶„ì„

ì´ ëª¨ë“ˆì€ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
1. ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
2. ê° êµ¬ì„± ìš”ì†Œë³„ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
3. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ ë° ìµœì í™” ì œì•ˆ
4. GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ë™ì  í• ë‹¹ ìµœì í™”
5. ë³‘ëª© ì§€ì  ë¶„ì„ ë° ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­
"""

import asyncio
import time
import psutil
import threading
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import json
import gc
import numpy as np
from collections import defaultdict, deque
import tracemalloc

try:
    import torch
    import torch.profiler
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import GPUtil
    GPUTIL_AVAILABLE = True
except ImportError:
    GPUTIL_AVAILABLE = False

from config import (
    SYSTEM_CONFIG, 
    get_smart_device, 
    get_gpu_memory_info,
    gpu_model_context,
    setup_logging
)

# ë¡œê±° ì„¤ì •
logger = setup_logging()

@dataclass
class ResourceSnapshot:
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìŠ¤ëƒ…ìƒ·"""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    memory_mb: float
    gpu_memory_mb: Optional[float] = None
    gpu_utilization: Optional[float] = None
    disk_io_read: float = 0
    disk_io_write: float = 0
    network_io_sent: float = 0
    network_io_recv: float = 0

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„°"""
    component_name: str
    execution_time: float
    memory_peak_mb: float
    memory_growth_mb: float
    cpu_usage_avg: float
    gpu_memory_peak_mb: Optional[float] = None
    gpu_utilization_avg: Optional[float] = None
    throughput_ops_per_sec: Optional[float] = None
    error_count: int = 0
    cache_hit_rate: Optional[float] = None

@dataclass
class OptimizationRecommendation:
    """ìµœì í™” ê¶Œì¥ì‚¬í•­"""
    category: str  # memory, cpu, gpu, io, algorithm
    priority: str  # high, medium, low
    description: str
    estimated_improvement: str
    implementation_effort: str  # low, medium, high
    code_location: Optional[str] = None

class SystemResourceMonitor:
    """ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, sampling_interval: float = 0.5):
        self.sampling_interval = sampling_interval
        self.is_monitoring = False
        self.resource_history: deque = deque(maxlen=1000)  # ìµœê·¼ 1000ê°œ ìƒ˜í”Œ ìœ ì§€
        self.monitor_thread = None
        self.start_time = None
        
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_monitoring:
            return
            
        self.is_monitoring = True
        self.start_time = datetime.now()
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        logger.info("ğŸ” ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2.0)
        logger.info("â¹ï¸ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        last_disk_io = psutil.disk_io_counters()
        last_net_io = psutil.net_io_counters()
        
        while self.is_monitoring:
            try:
                # ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
                cpu_percent = psutil.cpu_percent(interval=None)
                memory = psutil.virtual_memory()
                
                # ë””ìŠ¤í¬ I/O ë³€í™”ëŸ‰
                current_disk_io = psutil.disk_io_counters()
                disk_read_delta = current_disk_io.read_bytes - last_disk_io.read_bytes
                disk_write_delta = current_disk_io.write_bytes - last_disk_io.write_bytes
                last_disk_io = current_disk_io
                
                # ë„¤íŠ¸ì›Œí¬ I/O ë³€í™”ëŸ‰
                current_net_io = psutil.net_io_counters()
                net_sent_delta = current_net_io.bytes_sent - last_net_io.bytes_sent
                net_recv_delta = current_net_io.bytes_recv - last_net_io.bytes_recv
                last_net_io = current_net_io
                
                # GPU ë©”íŠ¸ë¦­ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
                gpu_memory_mb = None
                gpu_utilization = None
                
                if TORCH_AVAILABLE and torch.cuda.is_available():
                    gpu_info = get_gpu_memory_info()
                    if gpu_info:
                        gpu_memory_mb = gpu_info['allocated_mb']
                        gpu_utilization = gpu_info['usage_percent']
                
                # ìŠ¤ëƒ…ìƒ· ìƒì„±
                snapshot = ResourceSnapshot(
                    timestamp=datetime.now(),
                    cpu_percent=cpu_percent,
                    memory_percent=memory.percent,
                    memory_mb=memory.used / (1024 * 1024),
                    gpu_memory_mb=gpu_memory_mb,
                    gpu_utilization=gpu_utilization,
                    disk_io_read=disk_read_delta / (1024 * 1024),  # MB/s
                    disk_io_write=disk_write_delta / (1024 * 1024),  # MB/s
                    network_io_sent=net_sent_delta / (1024 * 1024),  # MB/s
                    network_io_recv=net_recv_delta / (1024 * 1024)  # MB/s
                )
                
                self.resource_history.append(snapshot)
                
                time.sleep(self.sampling_interval)
                
            except Exception as e:
                logger.error(f"ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                time.sleep(self.sampling_interval)
    
    def get_current_stats(self) -> Dict[str, Any]:
        """í˜„ì¬ ë¦¬ì†ŒìŠ¤ í†µê³„"""
        if not self.resource_history:
            return {}
        
        recent_snapshots = list(self.resource_history)[-20:]  # ìµœê·¼ 20ê°œ
        
        return {
            "current": asdict(recent_snapshots[-1]) if recent_snapshots else {},
            "avg_last_20": {
                "cpu_percent": np.mean([s.cpu_percent for s in recent_snapshots]),
                "memory_percent": np.mean([s.memory_percent for s in recent_snapshots]),
                "memory_mb": np.mean([s.memory_mb for s in recent_snapshots]),
                "gpu_memory_mb": np.mean([s.gpu_memory_mb for s in recent_snapshots if s.gpu_memory_mb]),
                "gpu_utilization": np.mean([s.gpu_utilization for s in recent_snapshots if s.gpu_utilization])
            },
            "peak": {
                "cpu_percent": max(s.cpu_percent for s in recent_snapshots),
                "memory_mb": max(s.memory_mb for s in recent_snapshots),
                "gpu_memory_mb": max(s.gpu_memory_mb for s in recent_snapshots if s.gpu_memory_mb) if any(s.gpu_memory_mb for s in recent_snapshots) else None
            }
        }

class ComponentProfiler:
    """ê°œë³„ êµ¬ì„± ìš”ì†Œ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§"""
    
    def __init__(self, component_name: str, monitor: SystemResourceMonitor):
        self.component_name = component_name
        self.monitor = monitor
        self.start_time = None
        self.start_memory = None
        self.start_gpu_memory = None
        self.start_snapshot = None
        self.operation_count = 0
        self.error_count = 0
        self.cache_hits = 0
        self.cache_misses = 0
        
        # ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
        tracemalloc.start()
        
    def __enter__(self):
        """í”„ë¡œíŒŒì¼ë§ ì‹œì‘"""
        self.start_time = time.time()
        self.operation_count = 0
        self.error_count = 0
        
        # ì´ˆê¸° ë¦¬ì†ŒìŠ¤ ìƒíƒœ ìº¡ì²˜
        if self.monitor.resource_history:
            self.start_snapshot = self.monitor.resource_history[-1]
        
        # ë©”ëª¨ë¦¬ ê¸°ì¤€ì  ì„¤ì •
        self.start_memory = tracemalloc.get_traced_memory()[0]
        
        # GPU ë©”ëª¨ë¦¬ ê¸°ì¤€ì 
        if TORCH_AVAILABLE and torch.cuda.is_available():
            gpu_info = get_gpu_memory_info()
            self.start_gpu_memory = gpu_info['allocated_mb'] if gpu_info else 0
        
        logger.info(f"ğŸ” {self.component_name} í”„ë¡œíŒŒì¼ë§ ì‹œì‘")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """í”„ë¡œíŒŒì¼ë§ ì¢…ë£Œ ë° ë©”íŠ¸ë¦­ ê³„ì‚°"""
        execution_time = time.time() - self.start_time
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
        current_memory, peak_memory = tracemalloc.get_traced_memory()
        memory_growth = (current_memory - self.start_memory) / (1024 * 1024)  # MB
        memory_peak = peak_memory / (1024 * 1024)  # MB
        
        # GPU ë©”ëª¨ë¦¬ ê³„ì‚°
        gpu_memory_peak = None
        if TORCH_AVAILABLE and torch.cuda.is_available() and self.start_gpu_memory is not None:
            gpu_info = get_gpu_memory_info()
            current_gpu_memory = gpu_info['allocated_mb'] if gpu_info else 0
            gpu_memory_peak = max(current_gpu_memory, self.start_gpu_memory)
        
        # CPU ì‚¬ìš©ë¥  í‰ê·  ê³„ì‚°
        cpu_usage_avg = 0
        gpu_utilization_avg = None
        
        if self.monitor.resource_history and self.start_snapshot:
            relevant_snapshots = [
                s for s in self.monitor.resource_history 
                if s.timestamp >= self.start_snapshot.timestamp
            ]
            if relevant_snapshots:
                cpu_usage_avg = np.mean([s.cpu_percent for s in relevant_snapshots])
                gpu_utils = [s.gpu_utilization for s in relevant_snapshots if s.gpu_utilization]
                if gpu_utils:
                    gpu_utilization_avg = np.mean(gpu_utils)
        
        # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
        throughput = self.operation_count / execution_time if execution_time > 0 else 0
        
        # ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°
        cache_hit_rate = None
        if self.cache_hits + self.cache_misses > 0:
            cache_hit_rate = self.cache_hits / (self.cache_hits + self.cache_misses)
        
        self.metrics = PerformanceMetrics(
            component_name=self.component_name,
            execution_time=execution_time,
            memory_peak_mb=memory_peak,
            memory_growth_mb=memory_growth,
            cpu_usage_avg=cpu_usage_avg,
            gpu_memory_peak_mb=gpu_memory_peak,
            gpu_utilization_avg=gpu_utilization_avg,
            throughput_ops_per_sec=throughput,
            error_count=self.error_count,
            cache_hit_rate=cache_hit_rate
        )
        
        logger.info(f"âœ… {self.component_name} í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ: "
                   f"{execution_time:.3f}s, {memory_peak:.1f}MB peak, "
                   f"{throughput:.1f} ops/s")
    
    def record_operation(self):
        """ì‘ì—… í•˜ë‚˜ ì™„ë£Œ ê¸°ë¡"""
        self.operation_count += 1
    
    def record_error(self):
        """ì—ëŸ¬ ë°œìƒ ê¸°ë¡"""
        self.error_count += 1
    
    def record_cache_hit(self):
        """ìºì‹œ íˆíŠ¸ ê¸°ë¡"""
        self.cache_hits += 1
    
    def record_cache_miss(self):
        """ìºì‹œ ë¯¸ìŠ¤ ê¸°ë¡"""
        self.cache_misses += 1

class PerformanceBenchmarkSuite:
    """ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™” ë¶„ì„"""
    
    def __init__(self):
        self.monitor = SystemResourceMonitor(sampling_interval=0.2)
        self.component_metrics: List[PerformanceMetrics] = []
        self.optimization_recommendations: List[OptimizationRecommendation] = []
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹œë‚˜ë¦¬ì˜¤
        self.benchmark_scenarios = [
            {
                "name": "small_empathy_analysis",
                "text": "ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢‹ì§€ ì•Šì•„ìš”.",
                "context": {"complexity": "low"},
                "expected_time": 2.0,
                "iterations": 10
            },
            {
                "name": "medium_semantic_analysis", 
                "text": "íšŒì‚¬ì—ì„œ ìŠ¹ì§„ ê¸°íšŒê°€ ìƒê²¼ì§€ë§Œ ì´ì‚¬ë¥¼ ê°€ì•¼ í•˜ëŠ” ìƒí™©ì´ë¼ ê°€ì¡±ê³¼ ìƒì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "context": {"complexity": "medium", "domains": ["career", "family"]},
                "expected_time": 5.0,
                "iterations": 5
            },
            {
                "name": "complex_moral_dilemma",
                "text": "í™˜ê²½ ë³´í˜¸ë¥¼ ìœ„í•´ í”Œë¼ìŠ¤í‹± ì‚¬ìš©ì„ ì¤„ì´ê³  ì‹¶ì§€ë§Œ, ê²½ì œì  ì—¬ê±´ìƒ ì¹œí™˜ê²½ ì œí’ˆì„ êµ¬ë§¤í•˜ê¸° ì–´ë ¤ìš´ ìƒí™©ì…ë‹ˆë‹¤. ë˜í•œ ì§ì¥ì—ì„œëŠ” í™˜ê²½ì— í•´ë¡œìš´ ì‚¬ì—…ì„ í•˜ê³  ìˆì–´ì„œ ìœ¤ë¦¬ì  ê°ˆë“±ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤.",
                "context": {"complexity": "high", "moral_weight": 0.9, "domains": ["environment", "economics", "ethics"]},
                "expected_time": 8.0,
                "iterations": 3
            },
            {
                "name": "batch_processing_test",
                "texts": [
                    "ì¹œêµ¬ì™€ ì•½ì†ì„ ì¡ì•˜ì–´ìš”.",
                    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.", 
                    "ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.",
                    "ê°€ì¡±ê³¼ í•¨ê»˜ ì‹œê°„ì„ ë³´ëƒˆì–´ìš”.",
                    "ê±´ê°•í•œ ì‹ì‚¬ë¥¼ í–ˆìŠµë‹ˆë‹¤."
                ],
                "context": {"batch_mode": True},
                "expected_time": 3.0,
                "iterations": 5
            }
        ]
    
    async def benchmark_empathy_learning_system(self) -> PerformanceMetrics:
        """ê³µê° í•™ìŠµ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬"""
        from advanced_hierarchical_emotion_system import EnhancedEmpathyLearner
        
        with ComponentProfiler("EnhancedEmpathyLearner", self.monitor) as profiler:
            empathy_learner = EnhancedEmpathyLearner()
            await empathy_learner.initialize()
            
            # ê° ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
            for scenario in self.benchmark_scenarios[:3]:  # ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì œì™¸
                for i in range(scenario["iterations"]):
                    try:
                        start_time = time.time()
                        result = await empathy_learner.process_empathy_learning(
                            scenario["text"], scenario["context"]
                        )
                        
                        # ì„±ëŠ¥ ê¸°ëŒ€ì¹˜ ê²€ì¦
                        elapsed = time.time() - start_time
                        if elapsed > scenario["expected_time"]:
                            logger.warning(f"âš ï¸ {scenario['name']} ì˜ˆìƒë³´ë‹¤ ëŠë¦¼: {elapsed:.2f}s > {scenario['expected_time']}s")
                        
                        profiler.record_operation()
                        
                        # ê²°ê³¼ ìœ íš¨ì„± ê²€ì¦
                        assert result is not None
                        assert 'empathy_score' in result
                        assert 0 <= result['empathy_score'] <= 1
                        
                    except Exception as e:
                        profiler.record_error()
                        logger.error(f"ê³µê° í•™ìŠµ ë²¤ì¹˜ë§ˆí¬ ì˜¤ë¥˜: {str(e)}")
        
        return profiler.metrics
    
    async def benchmark_bentham_calculator(self) -> PerformanceMetrics:
        """ë²¤ë‹´ ê³„ì‚°ê¸° ë²¤ì¹˜ë§ˆí¬"""
        from advanced_bentham_calculator import FrommEnhancedBenthamCalculator
        
        with ComponentProfiler("FrommEnhancedBenthamCalculator", self.monitor) as profiler:
            calculator = FrommEnhancedBenthamCalculator()
            
            for scenario in self.benchmark_scenarios[:3]:
                for i in range(scenario["iterations"]):
                    try:
                        start_time = time.time()
                        result = await calculator.calculate_enhanced_utility(
                            scenario["text"], scenario["context"]
                        )
                        
                        elapsed = time.time() - start_time
                        profiler.record_operation()
                        
                        # ê²°ê³¼ ê²€ì¦
                        assert result is not None
                        assert 'total_utility' in result
                        assert result['total_utility'] >= 0
                        
                    except Exception as e:
                        profiler.record_error()
                        logger.error(f"ë²¤ë‹´ ê³„ì‚°ê¸° ë²¤ì¹˜ë§ˆí¬ ì˜¤ë¥˜: {str(e)}")
        
        return profiler.metrics
    
    async def benchmark_memory_efficiency(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë²¤ì¹˜ë§ˆí¬"""
        logger.info("ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        gc.collect()
        initial_memory = psutil.virtual_memory().used / (1024 * 1024)
        
        # ì´ˆê¸° GPU ë©”ëª¨ë¦¬
        initial_gpu_memory = 0
        if TORCH_AVAILABLE and torch.cuda.is_available():
            torch.cuda.empty_cache()
            gpu_info = get_gpu_memory_info()
            initial_gpu_memory = gpu_info['allocated_mb'] if gpu_info else 0
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        memory_samples = []
        gpu_memory_samples = []
        
        # ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        from advanced_hierarchical_emotion_system import EnhancedEmpathyLearner
        empathy_learner = EnhancedEmpathyLearner()
        await empathy_learner.initialize()
        
        for i in range(50):  # 50íšŒ ì—°ì† ì²˜ë¦¬
            # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
            current_memory = psutil.virtual_memory().used / (1024 * 1024)
            memory_samples.append(current_memory - initial_memory)
            
            if TORCH_AVAILABLE and torch.cuda.is_available():
                gpu_info = get_gpu_memory_info()
                current_gpu = gpu_info['allocated_mb'] if gpu_info else 0
                gpu_memory_samples.append(current_gpu - initial_gpu_memory)
            
            # ì²˜ë¦¬ ì‹¤í–‰
            await empathy_learner.process_empathy_learning(
                f"í…ŒìŠ¤íŠ¸ ë¬¸ì¥ {i}: ë‹¤ì–‘í•œ ê°ì •ê³¼ ìƒí™©ì„ í¬í•¨í•œ ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤",
                {"iteration": i}
            )
            
            # ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            if i % 10 == 0:
                gc.collect()
                if TORCH_AVAILABLE and torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        # ìµœì¢… ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        if TORCH_AVAILABLE and torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        final_memory = psutil.virtual_memory().used / (1024 * 1024)
        memory_leak = final_memory - initial_memory
        
        final_gpu_memory = 0
        gpu_memory_leak = 0
        if TORCH_AVAILABLE and torch.cuda.is_available():
            gpu_info = get_gpu_memory_info()
            final_gpu_memory = gpu_info['allocated_mb'] if gpu_info else 0
            gpu_memory_leak = final_gpu_memory - initial_gpu_memory
        
        return {
            "initial_memory_mb": initial_memory,
            "final_memory_mb": final_memory,
            "memory_leak_mb": memory_leak,
            "peak_memory_usage_mb": max(memory_samples) if memory_samples else 0,
            "avg_memory_usage_mb": np.mean(memory_samples) if memory_samples else 0,
            "initial_gpu_memory_mb": initial_gpu_memory,
            "final_gpu_memory_mb": final_gpu_memory, 
            "gpu_memory_leak_mb": gpu_memory_leak,
            "peak_gpu_memory_mb": max(gpu_memory_samples) if gpu_memory_samples else 0,
            "memory_stability": memory_leak < 50,  # 50MB ì´í•˜ë©´ ì•ˆì •ì 
            "gpu_memory_stability": gpu_memory_leak < 100  # 100MB ì´í•˜ë©´ ì•ˆì •ì 
        }
    
    def analyze_bottlenecks(self) -> List[OptimizationRecommendation]:
        """ë³‘ëª© ì§€ì  ë¶„ì„ ë° ìµœì í™” ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        if not self.component_metrics:
            return recommendations
        
        # ì‹¤í–‰ ì‹œê°„ ë¶„ì„
        slow_components = [m for m in self.component_metrics if m.execution_time > 5.0]
        for component in slow_components:
            recommendations.append(OptimizationRecommendation(
                category="performance",
                priority="high",
                description=f"{component.component_name}ì˜ ì‹¤í–‰ ì‹œê°„ì´ {component.execution_time:.2f}ì´ˆë¡œ ëŠë¦½ë‹ˆë‹¤.",
                estimated_improvement="30-50% ì†ë„ í–¥ìƒ ê°€ëŠ¥",
                implementation_effort="medium",
                code_location=f"{component.component_name} í´ë˜ìŠ¤"
            ))
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        memory_heavy_components = [m for m in self.component_metrics if m.memory_peak_mb > 500]
        for component in memory_heavy_components:
            recommendations.append(OptimizationRecommendation(
                category="memory",
                priority="high",
                description=f"{component.component_name}ì´ {component.memory_peak_mb:.1f}MBì˜ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                estimated_improvement="20-40% ë©”ëª¨ë¦¬ ì ˆì•½ ê°€ëŠ¥",
                implementation_effort="medium",
                code_location=f"{component.component_name} ë©”ëª¨ë¦¬ ê´€ë¦¬"
            ))
        
        # GPU ë©”ëª¨ë¦¬ ë¶„ì„
        gpu_heavy_components = [m for m in self.component_metrics 
                               if m.gpu_memory_peak_mb and m.gpu_memory_peak_mb > 1000]
        for component in gpu_heavy_components:
            recommendations.append(OptimizationRecommendation(
                category="gpu",
                priority="medium",
                description=f"{component.component_name}ì´ {component.gpu_memory_peak_mb:.1f}MBì˜ GPU ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                estimated_improvement="GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„  ê°€ëŠ¥",
                implementation_effort="low",
                code_location="GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •"
            ))
        
        # CPU ì‚¬ìš©ë¥  ë¶„ì„
        cpu_intensive_components = [m for m in self.component_metrics if m.cpu_usage_avg > 80]
        for component in cpu_intensive_components:
            recommendations.append(OptimizationRecommendation(
                category="cpu",
                priority="medium", 
                description=f"{component.component_name}ì´ í‰ê·  {component.cpu_usage_avg:.1f}%ì˜ CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                estimated_improvement="ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë¶€í•˜ ë¶„ì‚° ê°€ëŠ¥",
                implementation_effort="high",
                code_location="ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„"
            ))
        
        # ì—ëŸ¬ìœ¨ ë¶„ì„
        error_prone_components = [m for m in self.component_metrics if m.error_count > 0]
        for component in error_prone_components:
            recommendations.append(OptimizationRecommendation(
                category="reliability",
                priority="high",
                description=f"{component.component_name}ì—ì„œ {component.error_count}íšŒì˜ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                estimated_improvement="ì‹œìŠ¤í…œ ì•ˆì •ì„± í–¥ìƒ",
                implementation_effort="medium",
                code_location=f"{component.component_name} ì—ëŸ¬ ì²˜ë¦¬"
            ))
        
        # ìºì‹œ íˆíŠ¸ìœ¨ ë¶„ì„
        low_cache_components = [m for m in self.component_metrics 
                               if m.cache_hit_rate and m.cache_hit_rate < 0.7]
        for component in low_cache_components:
            recommendations.append(OptimizationRecommendation(
                category="algorithm",
                priority="medium",
                description=f"{component.component_name}ì˜ ìºì‹œ íˆíŠ¸ìœ¨ì´ {component.cache_hit_rate:.1%}ë¡œ ë‚®ìŠµë‹ˆë‹¤.",
                estimated_improvement="ìºì‹œ íš¨ìœ¨ì„± ê°œì„ ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ",
                implementation_effort="low",
                code_location="ìºì‹œ ì „ëµ ê°œì„ "
            ))
        
        return recommendations
    
    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        logger.info("ğŸš€ Red Heart ì‹œìŠ¤í…œ ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.monitor.start_monitoring()
        
        try:
            benchmark_start = time.time()
            
            # 1. ê³µê° í•™ìŠµ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬
            logger.info("ğŸ§  ê³µê° í•™ìŠµ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬...")
            empathy_metrics = await self.benchmark_empathy_learning_system()
            self.component_metrics.append(empathy_metrics)
            
            # 2. ë²¤ë‹´ ê³„ì‚°ê¸° ë²¤ì¹˜ë§ˆí¬
            logger.info("âš–ï¸ ë²¤ë‹´ ê³„ì‚°ê¸° ë²¤ì¹˜ë§ˆí¬...")
            bentham_metrics = await self.benchmark_bentham_calculator()
            self.component_metrics.append(bentham_metrics)
            
            # 3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë²¤ì¹˜ë§ˆí¬
            logger.info("ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë²¤ì¹˜ë§ˆí¬...")
            memory_analysis = await self.benchmark_memory_efficiency()
            
            # 4. ë³‘ëª© ì§€ì  ë¶„ì„
            logger.info("ğŸ” ë³‘ëª© ì§€ì  ë¶„ì„...")
            self.optimization_recommendations = self.analyze_bottlenecks()
            
            total_benchmark_time = time.time() - benchmark_start
            
            # ìµœì¢… ì‹œìŠ¤í…œ ìƒíƒœ
            final_stats = self.monitor.get_current_stats()
            
            return {
                "success": True,
                "total_benchmark_time": total_benchmark_time,
                "component_metrics": [asdict(m) for m in self.component_metrics],
                "memory_analysis": memory_analysis,
                "optimization_recommendations": [asdict(r) for r in self.optimization_recommendations],
                "system_stats": final_stats,
                "summary": {
                    "total_components_tested": len(self.component_metrics),
                    "avg_execution_time": np.mean([m.execution_time for m in self.component_metrics]),
                    "total_memory_peak": sum(m.memory_peak_mb for m in self.component_metrics),
                    "total_errors": sum(m.error_count for m in self.component_metrics),
                    "optimization_opportunities": len(self.optimization_recommendations)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "error": str(e)}
            
        finally:
            self.monitor.stop_monitoring()
    
    def save_benchmark_report(self, results: Dict[str, Any], filename: str = None) -> str:
        """ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"red_heart_performance_benchmark_{timestamp}.json"
        
        report_path = f"/mnt/c/large_project/linux_red_heart/logs/{filename}"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ì €ì¥ë¨: {report_path}")
        return report_path

async def main():
    """ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸš€ Red Heart Linux Advanced - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸")
    print("=" * 80)
    
    benchmark_suite = PerformanceBenchmarkSuite()
    
    try:
        # ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        results = await benchmark_suite.run_comprehensive_benchmark()
        
        if results["success"]:
            print("\n" + "=" * 80)
            print("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
            print("=" * 80)
            
            summary = results["summary"]
            print(f"í…ŒìŠ¤íŠ¸ëœ êµ¬ì„± ìš”ì†Œ: {summary['total_components_tested']}")
            print(f"í‰ê·  ì‹¤í–‰ì‹œê°„: {summary['avg_execution_time']:.3f}ì´ˆ")
            print(f"ì´ ë©”ëª¨ë¦¬ í”¼í¬: {summary['total_memory_peak']:.1f}MB")
            print(f"ì´ ì—ëŸ¬ ìˆ˜: {summary['total_errors']}")
            print(f"ìµœì í™” ê¸°íšŒ: {summary['optimization_opportunities']}ê°œ")
            
            # ë©”ëª¨ë¦¬ ë¶„ì„ ê²°ê³¼
            memory = results["memory_analysis"]
            print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ë¶„ì„:")
            print(f"  ë©”ëª¨ë¦¬ ëˆ„ìˆ˜: {memory['memory_leak_mb']:.1f}MB")
            print(f"  GPU ë©”ëª¨ë¦¬ ëˆ„ìˆ˜: {memory['gpu_memory_leak_mb']:.1f}MB")
            print(f"  ë©”ëª¨ë¦¬ ì•ˆì •ì„±: {'âœ…' if memory['memory_stability'] else 'âŒ'}")
            print(f"  GPU ì•ˆì •ì„±: {'âœ…' if memory['gpu_memory_stability'] else 'âŒ'}")
            
            # ìµœì í™” ê¶Œì¥ì‚¬í•­
            recommendations = results["optimization_recommendations"]
            if recommendations:
                print(f"\nğŸ”§ ì£¼ìš” ìµœì í™” ê¶Œì¥ì‚¬í•­:")
                for rec in recommendations[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                    priority_emoji = "ğŸ”´" if rec["priority"] == "high" else "ğŸŸ¡" if rec["priority"] == "medium" else "ğŸŸ¢"
                    print(f"  {priority_emoji} [{rec['category']}] {rec['description']}")
            
            # ë³´ê³ ì„œ ì €ì¥
            report_path = benchmark_suite.save_benchmark_report(results)
            print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {report_path}")
            
            print("\nğŸ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        else:
            print(f"\nâŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨: {results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return 1
            
    except Exception as e:
        print(f"\nğŸ’¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {str(e)}")
        print(traceback.format_exc())
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)