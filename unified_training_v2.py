#!/usr/bin/env python3
"""
Red Heart AI í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ v2
320M íŒŒë¼ë¯¸í„° + LLM ì „ì²˜ë¦¬ + ëª¨ë“ˆ ì„ íƒ
Gate 9 í†µê³¼ë¥¼ ìœ„í•œ ìµœì í™” ë²„ì „
"""

import os
import sys
import json
import torch
import torch.nn.functional as F
import logging
import time
import gc
from typing import Dict, List, Any, Optional
from pathlib import Path
import argparse
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from config import ADVANCED_CONFIG, get_device, get_gpu_memory_info, register_system_module, get_system_module
from module_selector import ModuleSelector, ExecutionMode, get_module_selector
from data_preprocessing_pipeline_v3 import HelpingAIPreprocessor as DataPreprocessingPipeline
from target_mapping_utils import TargetMapper
from data_loader import PreprocessedDataLoader
from dynamic_swap_manager import get_swap_manager
from workflow_aware_memory_manager import WorkflowAwareMemoryManager
from analyzer_neural_modules import create_neural_analyzers
from advanced_analyzer_wrappers import create_advanced_analyzer_wrappers
from phase_neural_networks import Phase0ProjectionNet, Phase2CommunityNet, HierarchicalEmotionIntegrator
try:
    from emotion_dsp_simulator import EmotionDSPSimulator, DynamicKalmanFilter
except ImportError:
    EmotionDSPSimulator = None
    DynamicKalmanFilter = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('RedHeart.UnifiedTrainingV2')

class UnifiedTrainingSystemV2:
    """
    í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ v2
    - 320M íŒŒë¼ë¯¸í„° ëª¨ë¸
    - LLM ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    - ë™ì  ëª¨ë“ˆ ì„ íƒ
    - 3ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°
    """
    
    def __init__(self, args: argparse.Namespace):
        """ì´ˆê¸°í™”"""
        self.args = args
        self.device = get_device()
        self.verbose = args.verbose if hasattr(args, 'verbose') else False
        
        # ëª¨ë“ˆ ì„ íƒê¸°
        self.module_selector = get_module_selector()
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì
        self.memory_manager = WorkflowAwareMemoryManager()
        self.swap_manager = get_swap_manager()
        
        # ëª¨ë¸ë“¤
        self.backbone = None
        self.heads = {}
        self.analyzers = {}
        
        # ë°ì´í„°
        self.train_data = None
        self.val_data = None
        
        # ìƒíƒœ
        self.current_epoch = 0
        self.global_step = 0
        self.best_loss = float('inf')
        
        # ì˜µí‹°ë§ˆì´ì € (ë‚˜ì¤‘ì— ì´ˆê¸°í™”)
        self.optimizer = None
        self.scheduler = None
        
        # Gradient Accumulation ì„¤ì •
        self.gradient_accumulation_steps = max(1, getattr(args, 'gradient_accumulation', 1))
        
        # Mixed Precision ì„¤ì •
        self.use_mixed_precision = bool(
            getattr(args, 'use_mixed_precision', False) or 
            getattr(args, 'mixed_precision', False)
        )
        self.scaler = (
            torch.amp.GradScaler('cuda', enabled=self.use_mixed_precision)
            if str(self.device).startswith('cuda') else None
        )
        
        logger.info("=" * 60)
        logger.info("Red Heart AI í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ v2 ì´ˆê¸°í™”")
        logger.info(f"  - ëª¨ë¸ í¬ê¸°: 320M íŒŒë¼ë¯¸í„°")
        logger.info(f"  - ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"  - ëª¨ë“œ: {args.mode}")
        logger.info(f"  - í•™ìŠµë¥ : {args.learning_rate}")
        logger.info("=" * 60)
    
    def prepare_data(self):
        """ë°ì´í„° ì¤€ë¹„ (LLM ì „ì²˜ë¦¬ í¬í•¨)"""
        logger.info("\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì‹œì‘")
        
        # 1. ì›ë³¸ ë°ì´í„° í™•ì¸ - processed_datasets ë””ë ‰í† ë¦¬ ì‚¬ìš©
        raw_data_path = Path("processed_datasets/integrated_scenarios.json")
        # Claude APIë¡œ ì „ì²˜ë¦¬ëœ ì™„ì „í•œ ë°ì´í„° ì‚¬ìš©
        preprocessed_path = Path("claude_api_preprocessing/claude_preprocessed_complete.json")
        
        # 2. ì „ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ í™•ì¸
        if not preprocessed_path.exists() or self.args.force_preprocess:
            logger.info("ğŸ”„ LLM ì „ì²˜ë¦¬ ì‹œì‘...")
            
            # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            pipeline = DataPreprocessingPipeline()
            
            try:
                # LLM ë¡œë“œ (CPU)
                pipeline.initialize_llm(force_cpu=True)
                
                # ì›ë³¸ ë°ì´í„° ë¡œë“œ
                with open(raw_data_path, 'r', encoding='utf-8') as f:
                    raw_data = json.load(f)
                
                # ìƒ˜í”Œ ì œí•œ
                if self.args.max_samples:
                    raw_data = raw_data[:self.args.max_samples]
                
                logger.info(f"ì „ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜: {len(raw_data)}")
                
                # ë°°ì¹˜ ì „ì²˜ë¦¬
                texts = [item.get('text', '') for item in raw_data]
                labels = [item.get('label', 'unknown') for item in raw_data]
                
                enriched_data = pipeline.preprocess_batch(
                    texts, labels, 
                    batch_size=self.args.preprocess_batch_size
                )
                
                # ì €ì¥
                pipeline.save_preprocessed_dataset(enriched_data, str(preprocessed_path))
                
            finally:
                # LLM ì •ë¦¬
                pipeline.cleanup_llm()
                gc.collect()
            
            logger.info("âœ… LLM ì „ì²˜ë¦¬ ì™„ë£Œ")
        
        # 3. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
        logger.info(f"ğŸ“‚ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ: {preprocessed_path}")
        with open(preprocessed_path, 'r', encoding='utf-8') as f:
            processed_data = json.load(f)
        
        # ë°ì´í„° ê²€ì¦
        logger.info(f"ë¡œë“œëœ ë°ì´í„°: {len(processed_data)} ìƒ˜í”Œ")
        if len(processed_data) < 10:
            logger.error(f"âš ï¸ ë°ì´í„° ë¶€ì¡±: {len(processed_data)}ê°œë§Œ ë¡œë“œë¨")
            logger.error(f"   íŒŒì¼ ê²½ë¡œ: {preprocessed_path}")
            logger.error(f"   ìµœì†Œ 10ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            raise ValueError(f"ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (í˜„ì¬: {len(processed_data)}ê°œ)")
        
        # max_samples ì ìš©
        if self.args.max_samples and len(processed_data) > self.args.max_samples:
            logger.info(f"ìƒ˜í”Œ ì œí•œ ì ìš©: {len(processed_data)} â†’ {self.args.max_samples}")
            processed_data = processed_data[:self.args.max_samples]
        
        # 4. í•™ìŠµ/ê²€ì¦ ë¶„í• 
        split_idx = int(len(processed_data) * 0.8)
        self.train_data = processed_data[:split_idx]
        self.val_data = processed_data[split_idx:]
        
        logger.info(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
        logger.info(f"  - í•™ìŠµ: {len(self.train_data)} ìƒ˜í”Œ")
        logger.info(f"  - ê²€ì¦: {len(self.val_data)} ìƒ˜í”Œ")
    
    def initialize_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ëª¨ë“œë³„ ì„ íƒ)"""
        logger.info("\nğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘")
        
        # ì‹¤í–‰ ëª¨ë“œ ì„¤ì •
        if self.args.mode in ['train', 'training', 'train-test']:
            self.module_selector.set_mode(ExecutionMode.TRAINING)
        elif self.args.mode in ['eval', 'test']:
            self.module_selector.set_mode(ExecutionMode.EVALUATION)
        else:
            self.module_selector.set_mode(ExecutionMode.INFERENCE)
        
        # ëª¨ë“ˆ ì„ íƒ ìš”ì•½ (ì‹ ê²½ë§ ë¶„ì„ê¸° ë“±ë¡ ì „)
        self.module_selector.print_summary()
        
        # ë©”ëª¨ë¦¬ ì²´í¬
        memory_info = self.module_selector.calculate_memory_usage()
        gpu_info = get_gpu_memory_info()
        
        logger.info(f"ğŸ“Š ë©”ëª¨ë¦¬ ìƒíƒœ:")
        logger.info(f"  - í•„ìš”: {memory_info['gpu_memory_mb']:.1f} MB")
        logger.info(f"  - ê°€ìš©: {gpu_info['free_mb']:.1f} MB")
        
        if memory_info['gpu_memory_mb'] > gpu_info['free_mb']:
            logger.warning("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - ìŠ¤ì™‘ ëª¨ë“œ í™œì„±í™”")
        
        # ëª¨ë“ˆ ë¡œë“œ ìˆœì„œ
        load_order = self.module_selector.get_load_order()
        
        for module_name in load_order:
            if self.module_selector.should_use_module(module_name):
                self._load_module(module_name)
        
        # í•™ìŠµ ëª¨ë“œì—ì„œ í•„ìˆ˜ ëª¨ë“ˆ ê°•ì œ ë¡œë“œ
        if self.args.mode in ['train', 'training', 'train-test']:
            logger.info("ğŸ¯ í•™ìŠµ í•„ìˆ˜ ëª¨ë“ˆ ê°•ì œ ë¡œë“œ ì¤‘...")
            
            # ì‹ ê²½ë§ ë¶„ì„ê¸° ë¡œë“œ (232M íŒŒë¼ë¯¸í„°)
            logger.info("ğŸ¤– ì‹ ê²½ë§ ë¶„ì„ê¸° ë¡œë“œ ì¤‘ (232M íŒŒë¼ë¯¸í„°)...")
            neural_analyzers = create_neural_analyzers()
            for name, analyzer in neural_analyzers.items():
                self.analyzers[f'neural_{name}'] = analyzer
                self.analyzers[f'neural_{name}'].to(self.device)
                # module_selectorì— í™œì„± ëª¨ë“ˆë¡œ ë“±ë¡
                self.module_selector.active_modules.add(f'neural_{name}')
                logger.info(f"  âœ… {name} ì‹ ê²½ë§ ë¶„ì„ê¸° ë¡œë“œ ì„±ê³µ")
            
            # ğŸ”´ ì¤‘ìš”: translator ëª¨ë“ˆ ì‚¬ì „ ì´ˆê¸°í™” (Advanced ë¶„ì„ê¸° ì˜ì¡´ì„±)
            logger.info("ğŸ”„ Translator ëª¨ë“ˆ ì´ˆê¸°í™” ì¤‘...")
            try:
                # translatorê°€ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                existing_translator = get_system_module('translator')
                if existing_translator is None:
                    # LocalTranslator ì´ˆê¸°í™” ë° ì „ì—­ ë“±ë¡
                    from local_translator import LocalTranslator
                    translator = LocalTranslator()
                    register_system_module('translator', translator)
                    logger.info("  âœ… LocalTranslator ì´ˆê¸°í™” ë° ì „ì—­ ë“±ë¡ ì™„ë£Œ")
                else:
                    logger.info("  â„¹ï¸ Translatorê°€ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            except Exception as e:
                logger.error(f"  âŒ Translator ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                logger.error("     Advanced ë¶„ì„ê¸°ëŠ” translator ëª¨ë“ˆì— ì˜ì¡´í•©ë‹ˆë‹¤")
                raise RuntimeError(f"Translator ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # Advanced ë¶„ì„ê¸° í•„ìˆ˜ í†µí•© (nn.Module Wrapper ì‚¬ìš©)
            logger.info("ğŸš€ Advanced ë¶„ì„ê¸° í†µí•© ì¤‘ (nn.Module Wrapper)...")
            
            # Advanced Analyzer Wrappers ìƒì„± ë° ë“±ë¡
            try:
                advanced_wrappers = create_advanced_analyzer_wrappers()
                for name, wrapper in advanced_wrappers.items():
                    self.analyzers[name] = wrapper.to(self.device)
                    # âœ… í™œì„± ëª¨ë“ˆë¡œ ë“±ë¡ (ì¤‘ìš”!)
                    self.module_selector.active_modules.add(name)
                    param_count = sum(p.numel() for p in wrapper.parameters())
                    logger.info(f"  âœ… {name} Wrapper ë¡œë“œ ì™„ë£Œ ({param_count:,} íŒŒë¼ë¯¸í„°)")
            except Exception as e:
                logger.error(f"  âŒ Advanced Analyzer Wrappers ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"í•„ìˆ˜ í•™ìŠµ ëª¨ë“ˆì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            
            # Phase 0/1/2 ê³„ì¸µì  ê°ì • ì‹œìŠ¤í…œ í†µí•©
            logger.info("ğŸŒ€ 3-Phase Hierarchical Emotion System í†µí•© ì¤‘...")
            try:
                # Phase 0: íƒ€ìâ†’ìì‹  íˆ¬ì˜ (2M)
                self.phase0_net = Phase0ProjectionNet().to(self.device)
                logger.info("  âœ… Phase0 ProjectionNet ë¡œë“œ (2M íŒŒë¼ë¯¸í„°)")
                
                # Phase 2: ê°œì¸â†’ê³µë™ì²´ (2.5M)
                self.phase2_net = Phase2CommunityNet().to(self.device)
                logger.info("  âœ… Phase2 CommunityNet ë¡œë“œ (2.5M íŒŒë¼ë¯¸í„°)")
                
                # í†µí•© ëª¨ë“ˆ
                self.hierarchical_integrator = HierarchicalEmotionIntegrator().to(self.device)
                logger.info("  âœ… Hierarchical Emotion Integrator ë¡œë“œ")
            except Exception as e:
                logger.error(f"  âŒ Phase ë„¤íŠ¸ì›Œí¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"Phase ë„¤íŠ¸ì›Œí¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # DSP ì‹œë®¬ë ˆì´í„° ë° ì¹¼ë§Œ í•„í„° í†µí•©
            logger.info("ğŸµ DSP Simulator & Kalman Filter í†µí•© ì¤‘...")
            try:
                if EmotionDSPSimulator is not None:
                    # DSP ì‹œë®¬ë ˆì´í„° (14M) - 384ì°¨ì› hidden_dim ëª…ì‹œ
                    self.dsp_simulator = EmotionDSPSimulator({'hidden_dim': 384}).to(self.device)
                    # analyzers dictì— ë“±ë¡
                    self.analyzers['dsp'] = self.dsp_simulator
                    # í™œì„± ëª¨ë“ˆë¡œ ë“±ë¡
                    self.module_selector.active_modules.add('dsp')
                    logger.info("  âœ… Emotion DSP Simulator ë¡œë“œ (14M íŒŒë¼ë¯¸í„°, 384ì°¨ì›)")
                    
                    # ë‹¤ì´ë‚˜ë¯¹ ì¹¼ë§Œ í•„í„° (0.7K)
                    self.kalman_filter = DynamicKalmanFilter(state_dim=7).to(self.device)
                    # analyzers dictì— ë“±ë¡
                    self.analyzers['kalman'] = self.kalman_filter
                    # í™œì„± ëª¨ë“ˆë¡œ ë“±ë¡
                    self.module_selector.active_modules.add('kalman')
                    logger.info("  âœ… Dynamic Kalman Filter ë¡œë“œ (700 íŒŒë¼ë¯¸í„°, 7ì°¨ì› state)")
                else:
                    logger.warning("  âš ï¸ DSP/Kalman ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€")
                    self.dsp_simulator = None
                    self.kalman_filter = None
            except Exception as e:
                logger.error(f"  âŒ DSP/Kalman ë¡œë“œ ì‹¤íŒ¨: {e}")
                # DSP/Kalmanì€ ì„ íƒì ì´ë¯€ë¡œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                self.dsp_simulator = None
                self.kalman_filter = None
            
            # í—¤ë“œ ëª¨ë“ˆ
            essential_heads = ['emotion_head', 'bentham_head', 'regret_head', 'surd_head']
            for head in essential_heads:
                logger.info(f"  - {head} ë¡œë“œ")
                self._load_module(head)
        
        logger.info("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ìµœì¢… ëª¨ë“ˆ ì„ íƒ ìš”ì•½ (ì‹ ê²½ë§ ë¶„ì„ê¸° í¬í•¨)
        logger.info("\nğŸ“Š ìµœì¢… ë¡œë“œëœ ëª¨ë“ˆ ìš”ì•½:")
        self.module_selector.print_summary()
        
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œ ìƒì„¸ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥
        if self.args.mode in ['test', 'eval'] or self.args.debug:
            self._print_detailed_parameters()
    
    def _print_detailed_parameters(self):
        """ìƒì„¸ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ” ìƒì„¸ íŒŒë¼ë¯¸í„° ë¶„ì„")
        logger.info("=" * 70)
        
        total_params = 0
        trainable_params = 0
        
        # ë°±ë³¸
        if self.backbone:
            backbone_params = sum(p.numel() for p in self.backbone.parameters())
            backbone_trainable = sum(p.numel() for p in self.backbone.parameters() if p.requires_grad)
            total_params += backbone_params
            trainable_params += backbone_trainable
            logger.info(f"\nğŸ“Œ ë°±ë³¸:")
            logger.info(f"  - ì´ íŒŒë¼ë¯¸í„°: {backbone_params:,} ({backbone_params/1e6:.2f}M)")
            logger.info(f"  - í•™ìŠµê°€ëŠ¥: {backbone_trainable:,} ({backbone_trainable/1e6:.2f}M)")
        
        # í—¤ë“œ
        if self.heads:
            logger.info(f"\nğŸ“Œ í—¤ë“œ ëª¨ë“ˆ ({len(self.heads)}ê°œ):")
            for name, head in self.heads.items():
                head_params = sum(p.numel() for p in head.parameters())
                head_trainable = sum(p.numel() for p in head.parameters() if p.requires_grad)
                total_params += head_params
                trainable_params += head_trainable
                logger.info(f"  [{name}]")
                logger.info(f"    - ì´: {head_params:,} ({head_params/1e6:.2f}M)")
                logger.info(f"    - í•™ìŠµê°€ëŠ¥: {head_trainable:,}")
        
        # ë¶„ì„ê¸°
        if self.analyzers:
            logger.info(f"\nğŸ“Œ ë¶„ì„ê¸° ëª¨ë“ˆ ({len(self.analyzers)}ê°œ):")
            
            # Neural Analyzer ë¶„ë¦¬
            neural_analyzers = {k: v for k, v in self.analyzers.items() if 'neural_' in k}
            other_analyzers = {k: v for k, v in self.analyzers.items() if 'neural_' not in k}
            
            if neural_analyzers:
                logger.info("  ğŸ¤– Neural Analyzers:")
                neural_total = 0
                for name, analyzer in neural_analyzers.items():
                    if hasattr(analyzer, 'parameters'):
                        analyzer_params = sum(p.numel() for p in analyzer.parameters())
                        analyzer_trainable = sum(p.numel() for p in analyzer.parameters() if p.requires_grad)
                        neural_total += analyzer_params
                        total_params += analyzer_params
                        trainable_params += analyzer_trainable
                        logger.info(f"    [{name}]")
                        logger.info(f"      - ì´: {analyzer_params:,} ({analyzer_params/1e6:.2f}M)")
                        logger.info(f"      - í•™ìŠµê°€ëŠ¥: {analyzer_trainable:,}")
                logger.info(f"    ğŸ“Š Neural Analyzer í•©ê³„: {neural_total:,} ({neural_total/1e6:.2f}M)")
            
            if other_analyzers:
                logger.info("  ğŸ“ˆ ê¸°íƒ€ ë¶„ì„ê¸°:")
                for name, analyzer in other_analyzers.items():
                    if hasattr(analyzer, 'parameters'):
                        try:
                            analyzer_params = sum(p.numel() for p in analyzer.parameters())
                            if analyzer_params > 0:
                                analyzer_trainable = sum(p.numel() for p in analyzer.parameters() if p.requires_grad)
                                total_params += analyzer_params
                                trainable_params += analyzer_trainable
                                logger.info(f"    [{name}]: {analyzer_params:,} params")
                        except:
                            logger.info(f"    [{name}]: íŒŒë¼ë¯¸í„° ì—†ìŒ")
        
        # ì´ê³„
        logger.info("\n" + "=" * 70)
        logger.info(f"ğŸ“Š ì „ì²´ í†µê³„:")
        logger.info(f"  - ì´ íŒŒë¼ë¯¸í„°: {total_params:,} ({total_params/1e6:.2f}M)")
        logger.info(f"  - í•™ìŠµê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({trainable_params/1e6:.2f}M)")
        logger.info(f"  - ëª©í‘œ: 450M, ì‹¤ì œ: {total_params/1e6:.2f}M")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        gpu_info = get_gpu_memory_info()
        logger.info(f"\nğŸ’¾ GPU ë©”ëª¨ë¦¬:")
        logger.info(f"  - í• ë‹¹ë¨: {gpu_info['allocated_mb']:.1f} MB")
        logger.info(f"  - ì—¬ìœ : {gpu_info['free_mb']:.1f} MB")
        logger.info(f"  - ì „ì²´: {gpu_info['total_mb']:.1f} MB")
        logger.info("=" * 70 + "\n")
    
    def _load_module(self, module_name: str):
        """ê°œë³„ ëª¨ë“ˆ ë¡œë“œ"""
        module_info = self.module_selector.get_module_info(module_name)
        if not module_info:
            return
        
        logger.debug(f"ëª¨ë“ˆ ë¡œë“œ: {module_name}")
        
        # ëª¨ë“ˆë³„ ë¡œë“œ ë¡œì§
        if module_name == 'unified_backbone':
            from unified_backbone import RedHeartUnifiedBackbone
            self.backbone = RedHeartUnifiedBackbone(ADVANCED_CONFIG['unified_backbone'])
            self.backbone.to(self.device)
            
        elif 'head' in module_name:
            # í—¤ë“œ ë¡œë“œ (80M íŒŒë¼ë¯¸í„°)
            try:
                from unified_heads import EmotionHead, BenthamHead, RegretHead, SURDHead
                
                if 'emotion' in module_name:
                    self.heads['emotion'] = EmotionHead(input_dim=896)
                    self.heads['emotion'].to(self.device)
                    logger.info("âœ… ê°ì • í—¤ë“œ ë¡œë“œ ì„±ê³µ (30M)")
                elif 'bentham' in module_name:
                    self.heads['bentham'] = BenthamHead(input_dim=896)
                    self.heads['bentham'].to(self.device)
                    logger.info("âœ… ë²¤ë‹´ í—¤ë“œ ë¡œë“œ ì„±ê³µ (27M)")
                elif 'regret' in module_name:
                    self.heads['regret'] = RegretHead(input_dim=896)
                    self.heads['regret'].to(self.device)
                    logger.info("âœ… í›„íšŒ í—¤ë“œ ë¡œë“œ ì„±ê³µ (30M)")
                elif 'surd' in module_name:
                    self.heads['surd'] = SURDHead(input_dim=896)
                    self.heads['surd'].to(self.device)
                    logger.info("âœ… SURD í—¤ë“œ ë¡œë“œ ì„±ê³µ (22M)")
            except Exception as e:
                logger.error(f"âŒ í—¤ë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"NO FALLBACK: í•„ìˆ˜ í—¤ë“œ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨ - {module_name}: {e}")
            
        elif module_name == 'emotion_dsp_simulator':
            from emotion_dsp_simulator import EmotionDSPSimulator
            self.analyzers['dsp'] = EmotionDSPSimulator({'hidden_dim': 384})
            self.analyzers['dsp'].to(self.device)
            
        elif module_name == 'kalman_filter':
            from emotion_dsp_simulator import DynamicKalmanFilter
            self.analyzers['kalman'] = DynamicKalmanFilter(state_dim=7)
            self.analyzers['kalman'].to(self.device)
            
    
    def _initialize_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” - NO FALLBACK ì›ì¹™"""
        logger.info("ğŸ”§ ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì¤‘...")
        
        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ì§‘
        params = []
        
        if self.backbone and hasattr(self.backbone, 'parameters'):
            params.extend(list(self.backbone.parameters()))
            logger.info(f"  ë°±ë³¸ íŒŒë¼ë¯¸í„° ì¶”ê°€ë¨")
        
        for name, head in self.heads.items():
            if hasattr(head, 'parameters'):
                params.extend(list(head.parameters()))
                logger.info(f"  {name} í—¤ë“œ íŒŒë¼ë¯¸í„° ì¶”ê°€ë¨")
        
        # ëª¨ë“  ë¶„ì„ê¸°ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ì§‘ (Neural + Advanced)
        for name, analyzer in self.analyzers.items():
            param_count = 0
            
            # Neural Analyzers (nn.Module ìƒì†)
            if hasattr(analyzer, 'parameters'):
                try:
                    analyzer_params = list(analyzer.parameters())
                    if analyzer_params:
                        params.extend(analyzer_params)
                        param_count = sum(p.numel() for p in analyzer_params)
                        logger.info(f"  {name} ë¶„ì„ê¸° íŒŒë¼ë¯¸í„° ì¶”ê°€ë¨: {param_count:,}")
                except Exception as e:
                    logger.warning(f"  {name} ë¶„ì„ê¸° íŒŒë¼ë¯¸í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            
            # Advanced Analyzers (ë‚´ë¶€ nn.Module ìˆ˜ì§‘)
            elif 'advanced_' in name:
                logger.info(f"  {name} Advanced ë¶„ì„ê¸° ë‚´ë¶€ ëª¨ë“ˆ ìˆ˜ì§‘ ì¤‘...")
                
                # Advanced Emotion Analyzer ë‚´ë¶€ ëª¨ë“ˆë“¤
                if name == 'advanced_emotion' and hasattr(analyzer, 'biometric_processor'):
                    try:
                        biometric_params = list(analyzer.biometric_processor.parameters())
                        params.extend(biometric_params)
                        count = sum(p.numel() for p in biometric_params)
                        param_count += count
                        logger.info(f"    - biometric_processor: {count:,}")
                    except: pass
                    
                    if hasattr(analyzer, 'multimodal_fusion'):
                        try:
                            fusion_params = list(analyzer.multimodal_fusion.parameters())
                            params.extend(fusion_params)
                            count = sum(p.numel() for p in fusion_params)
                            param_count += count
                            logger.info(f"    - multimodal_fusion: {count:,}")
                        except: pass
                    
                    if hasattr(analyzer, 'temporal_emotion'):
                        try:
                            temporal_params = list(analyzer.temporal_emotion.parameters())
                            params.extend(temporal_params)
                            count = sum(p.numel() for p in temporal_params)
                            param_count += count
                            logger.info(f"    - temporal_emotion: {count:,}")
                        except: pass
                    
                    if hasattr(analyzer, 'cultural_nuance'):
                        try:
                            cultural_params = list(analyzer.cultural_nuance.parameters())
                            params.extend(cultural_params)
                            count = sum(p.numel() for p in cultural_params)
                            param_count += count
                            logger.info(f"    - cultural_nuance: {count:,}")
                        except: pass
                    
                    if hasattr(analyzer, 'advanced_moe'):
                        try:
                            moe_params = list(analyzer.advanced_moe.parameters())
                            params.extend(moe_params)
                            count = sum(p.numel() for p in moe_params)
                            param_count += count
                            logger.info(f"    - advanced_moe: {count:,}")
                        except: pass
                
                # Advanced Regret Analyzer ë‚´ë¶€ ëª¨ë“ˆë“¤
                elif name == 'advanced_regret':
                    if hasattr(analyzer, 'regret_network'):
                        try:
                            regret_params = list(analyzer.regret_network.parameters())
                            params.extend(regret_params)
                            count = sum(p.numel() for p in regret_params)
                            param_count += count
                            logger.info(f"    - regret_network: {count:,}")
                        except: pass
                    
                    if hasattr(analyzer, 'counterfactual_sim'):
                        try:
                            cf_params = list(analyzer.counterfactual_sim.parameters())
                            params.extend(cf_params)
                            count = sum(p.numel() for p in cf_params)
                            param_count += count
                            logger.info(f"    - counterfactual_sim: {count:,}")
                        except: pass
                    
                    if hasattr(analyzer, 'temporal_propagation'):
                        try:
                            tp_params = list(analyzer.temporal_propagation.parameters())
                            params.extend(tp_params)
                            count = sum(p.numel() for p in tp_params)
                            param_count += count
                            logger.info(f"    - temporal_propagation: {count:,}")
                        except: pass
                    
                    if hasattr(analyzer, 'decision_tree'):
                        try:
                            dt_params = list(analyzer.decision_tree.parameters())
                            params.extend(dt_params)
                            count = sum(p.numel() for p in dt_params)
                            param_count += count
                            logger.info(f"    - decision_tree: {count:,}")
                        except: pass
                
                # Advanced SURD Analyzer ë‚´ë¶€ ëª¨ë“ˆë“¤
                elif name == 'advanced_surd':
                    if hasattr(analyzer, 'deep_causal'):
                        try:
                            causal_params = list(analyzer.deep_causal.parameters())
                            params.extend(causal_params)
                            count = sum(p.numel() for p in causal_params)
                            param_count += count
                            logger.info(f"    - deep_causal: {count:,}")
                        except: pass
                    
                    if hasattr(analyzer, 'info_decomposition'):
                        try:
                            info_params = list(analyzer.info_decomposition.parameters())
                            params.extend(info_params)
                            count = sum(p.numel() for p in info_params)
                            param_count += count
                            logger.info(f"    - info_decomposition: {count:,}")
                        except: pass
                
                # Advanced Bentham Calculator ë‚´ë¶€ ëª¨ë“ˆë“¤
                elif name == 'advanced_bentham':
                    # ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ ì°¾ê¸° (ì†ì„± ë™ì  íƒìƒ‰)
                    for attr_name in dir(analyzer):
                        if not attr_name.startswith('_'):
                            attr = getattr(analyzer, attr_name, None)
                            if attr is not None and isinstance(attr, torch.nn.Module):
                                try:
                                    module_params = list(attr.parameters())
                                    if module_params:
                                        params.extend(module_params)
                                        count = sum(p.numel() for p in module_params)
                                        param_count += count
                                        logger.info(f"    - {attr_name}: {count:,}")
                                except: pass
                
                if param_count > 0:
                    logger.info(f"  {name} ì´ íŒŒë¼ë¯¸í„°: {param_count:,}")
                else:
                    logger.warning(f"  âš ï¸ {name} ë¶„ì„ê¸°ì—ì„œ í•™ìŠµ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        # Phase ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„° ì¶”ê°€
        if hasattr(self, 'phase0_net'):
            params.extend(list(self.phase0_net.parameters()))
            logger.info(f"  Phase0 ProjectionNet íŒŒë¼ë¯¸í„° ì¶”ê°€ë¨: {sum(p.numel() for p in self.phase0_net.parameters()):,}")
        
        if hasattr(self, 'phase2_net'):
            params.extend(list(self.phase2_net.parameters()))
            logger.info(f"  Phase2 CommunityNet íŒŒë¼ë¯¸í„° ì¶”ê°€ë¨: {sum(p.numel() for p in self.phase2_net.parameters()):,}")
        
        if hasattr(self, 'hierarchical_integrator'):
            params.extend(list(self.hierarchical_integrator.parameters()))
            logger.info(f"  Hierarchical Integrator íŒŒë¼ë¯¸í„° ì¶”ê°€ë¨: {sum(p.numel() for p in self.hierarchical_integrator.parameters()):,}")
        
        # DSP/Kalmanì€ ì´ë¯¸ self.analyzersì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        
        if not params:
            raise RuntimeError("í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ì´ˆê¸°í™”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # AdamW ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©
        self.optimizer = torch.optim.AdamW(
            params, 
            lr=self.args.learning_rate,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (Cosine Annealing)
        # ë°°ì¹˜ ìˆ˜ê°€ 0ì´ ë˜ì§€ ì•Šë„ë¡ ìµœì†Œ 1ë¡œ ë³´ì¥
        if self.train_data:
            num_batches = max(1, len(self.train_data) // self.args.batch_size)
            if len(self.train_data) % self.args.batch_size != 0:
                num_batches += 1  # ë‚˜ë¨¸ì§€ê°€ ìˆìœ¼ë©´ ë°°ì¹˜ í•˜ë‚˜ ì¶”ê°€
            total_steps = self.args.epochs * num_batches
        else:
            total_steps = 1000
            
        # T_maxê°€ 0ì´ ë˜ì§€ ì•Šë„ë¡ ìµœì†Œê°’ ë³´ì¥
        total_steps = max(1, total_steps)
        
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=total_steps,
            eta_min=1e-6
        )
        
        logger.info(f"  - ìŠ¤ì¼€ì¤„ëŸ¬ T_max: {total_steps} steps")
        
        logger.info(f"âœ… ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - ì˜µí‹°ë§ˆì´ì €: AdamW")
        logger.info(f"  - í•™ìŠµë¥ : {self.args.learning_rate}")
        logger.info(f"  - ìŠ¤ì¼€ì¤„ëŸ¬: CosineAnnealingLR")
        logger.info(f"  - ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in params):,}")
        logger.info(f"  - í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in params if p.requires_grad):,}")
    
    def train_epoch(self, epoch: int):
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        logger.info(f"\nğŸ“š Epoch {epoch+1}/{self.args.epochs} ì‹œì‘")
        
        # í•™ìŠµ ëª¨ë“œ - ëª¨ë“  ëª¨ë“ˆì„ train ëª¨ë“œë¡œ ì„¤ì •
        if self.backbone:
            self.backbone.train()
        
        for head in self.heads.values():
            head.train()
        
        for analyzer in self.analyzers.values():
            if hasattr(analyzer, 'train'):
                analyzer.train()
        
        total_loss = 0.0
        num_batches = max(1, len(self.train_data) // self.args.batch_size)
        
        if self.verbose:
            logger.info(f"ğŸ“Š ì—í¬í¬ ìƒì„¸ ì •ë³´:")
            logger.info(f"  - ì´ ìƒ˜í”Œ ìˆ˜: {len(self.train_data)}")
            logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {self.args.batch_size}")
            logger.info(f"  - ë°°ì¹˜ ìˆ˜: {num_batches}")
        
        for batch_idx in range(num_batches):
            # ë°°ì¹˜ ë°ì´í„°
            batch_start = batch_idx * self.args.batch_size
            batch_end = batch_start + self.args.batch_size
            batch_data = self.train_data[batch_start:batch_end]
            
            if self.verbose and batch_idx < 3:  # ì²˜ìŒ 3ê°œ ë°°ì¹˜ë§Œ ìƒì„¸ ì¶œë ¥
                logger.info(f"\nğŸ” ë°°ì¹˜ {batch_idx+1} ì²˜ë¦¬ ì¤‘:")
                logger.info(f"  - ìƒ˜í”Œ ë²”ìœ„: {batch_start} ~ {batch_end}")
                logger.info(f"  - ìƒ˜í”Œ ìˆ˜: {len(batch_data)}")
            
            # 3ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°
            loss = self._train_step(batch_data, batch_idx)
            total_loss += loss
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (batch_idx + 1) % 10 == 0 or (self.verbose and batch_idx < 3):
                avg_loss = total_loss / (batch_idx + 1)
                logger.info(f"  Batch {batch_idx+1}/{num_batches} - Loss: {avg_loss:.4f}")
            
            self.global_step += 1
        
        # ì—í¬í¬ ì™„ë£Œ
        avg_epoch_loss = total_loss / num_batches
        logger.info(f"âœ… Epoch {epoch+1} ì™„ë£Œ - í‰ê·  ì†ì‹¤: {avg_epoch_loss:.4f}")
        
        return avg_epoch_loss
    
    def _train_step(self, batch_data: List[Dict], batch_idx: int) -> float:
        """
        í•™ìŠµ ìŠ¤í… - 3ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°
        1. FORWARD: ë°ì´í„° â†’ ë°±ë³¸ â†’ í—¤ë“œ
        2. COMPUTE: ì†ì‹¤ ê³„ì‚° + ì‹œë„ˆì§€
        3. UPDATE: ì—­ì „íŒŒ + ìµœì í™”
        """
        
        # ========== STAGE 1: FORWARD ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 1] Forward Pass ì‹œì‘")
        
        # ì‹¤ì œ ë°ì´í„° ì‚¬ìš© (ë”ë¯¸ ë°ì´í„° ëŒ€ì²´)
        batch_size = len(batch_data)
        
        # batch_dataì—ì„œ ì‹¤ì œ ì…ë ¥ ì¶”ì¶œ (context_embedding ì‚¬ìš©)
        if not batch_data or not isinstance(batch_data[0], dict):
            raise ValueError("ë°°ì¹˜ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë¨")
        
        # TargetMapperë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ
        try:
            # context_embedding ì¶”ì¶œ (text_embedding ëŒ€ì‹ ) - ë°±ë³¸ ëª¨ë¸ ì „ë‹¬
            input_embeddings = TargetMapper.extract_context_embedding(batch_data, backbone_model=self.backbone)
            dummy_input = input_embeddings.to(self.device).requires_grad_(True)
        except KeyError as e:
            # NO FALLBACK ì›ì¹™ - ì‹¤íŒ¨ì‹œ ì˜ˆì™¸ ë°œìƒ
            raise RuntimeError(f"ì…ë ¥ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        if self.backbone:
            # ë°±ë³¸ forward - Dict[str, Tensor] ë°˜í™˜
            backbone_outputs = self.backbone(dummy_input, return_all_tasks=True)
            # ê°ì • íƒœìŠ¤í¬ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš© (7ì°¨ì› ì¶œë ¥ í•„ìš”)
            features = backbone_outputs.get('emotion', dummy_input)
            if self.verbose and batch_idx < 3:
                logger.info(f"      - ë°±ë³¸ ì¶œë ¥ shape: {features.shape}")
                logger.info(f"      - ë°±ë³¸ ì¶œë ¥ í‚¤: {list(backbone_outputs.keys())}")
        else:
            features = dummy_input
        
        # ========== STAGE 2: COMPUTE ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 2] Compute Loss")
        
        # í—¤ë“œ í†µê³¼ ë° ì†ì‹¤ ê³„ì‚°
        losses = []
        
        # ê°ì • í—¤ë“œ ì˜ˆì‹œ
        if 'emotion' in self.heads and features is not None:
            emotion_output = self.heads['emotion'](features)
            # í—¤ë“œëŠ” Dict[str, Tensor] ë°˜í™˜ - 'emotions' í‚¤ë¡œ ì‹¤ì œ ì¶œë ¥ ì¶”ì¶œ
            emotion_pred = emotion_output['emotions'] if isinstance(emotion_output, dict) else emotion_output
            # ì‹¤ì œ ê°ì • íƒ€ê¹ƒ ì¶”ì¶œ (TargetMapper ì‚¬ìš©)
            target = TargetMapper.extract_emotion_target(batch_data).to(self.device)
            # head.compute_loss ì‚¬ìš©
            emotion_loss = self.heads['emotion'].compute_loss(emotion_pred, target)
            losses.append(emotion_loss)
            
            if self.verbose and batch_idx < 3:
                logger.info(f"      - ê°ì • ì˜ˆì¸¡ shape: {emotion_pred.shape}")
        
        # ë²¤ë‹´ í—¤ë“œ (ì„ íƒì )
        if 'bentham' in self.heads and features is not None:
            bentham_output = self.heads['bentham'](features)
            bentham_pred = bentham_output['bentham_scores'] if isinstance(bentham_output, dict) else bentham_output
            # ì‹¤ì œ ë²¤ë‹´ íƒ€ê¹ƒ ì¶”ì¶œ (10ì°¨ì›) - TargetMapper ì‚¬ìš©
            target = TargetMapper.extract_bentham_target(batch_data).to(self.device)
            # head.compute_loss ì‚¬ìš©
            bentham_loss = self.heads['bentham'].compute_loss(bentham_pred, target)
            losses.append(bentham_loss)
        
        # ========== STAGE 2: NEURAL ANALYZERS ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 2] Neural Analyzer Processing")
        
        # Neural/Advanced Analyzer ì²˜ë¦¬ (378M+ íŒŒë¼ë¯¸í„° í™œìš©)
        analyzer_losses = []
        dsp_output = None  # DSP ì¶œë ¥ ì €ì¥ (Kalman ì…ë ¥ìš©)
        neural_emotion_output = None  # neural_emotion ì¶œë ¥ ì €ì¥ (Kalman traditional emotionsìš©)
        
        # neural_emotionì„ ë¨¼ì € ì²˜ë¦¬ (Kalmanì´ traditional emotionsë¡œ ì‚¬ìš©)
        if 'neural_emotion' in self.analyzers:
            try:
                emotion_analyzer = self.analyzers['neural_emotion']
                neural_emotion_output = emotion_analyzer(features)
                
                # ì†ì‹¤ ê³„ì‚°
                if 'emotion_logits' in neural_emotion_output:
                    target = TargetMapper.extract_emotion_target(batch_data).to(self.device)
                    if target.dim() == 1:
                        target = F.one_hot(target, num_classes=7).float()
                    emotion_loss = F.cross_entropy(neural_emotion_output['emotion_logits'], target)
                    analyzer_losses.append(emotion_loss)
                    if self.verbose and batch_idx < 3:
                        logger.info(f"      - neural_emotion ì†ì‹¤: {emotion_loss.item():.6f}")
            except Exception as e:
                logger.error(f"    âŒ neural_emotion ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise  # í•„ìˆ˜ ëª¨ë“ˆì´ë¯€ë¡œ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
        
        # DSP ì²˜ë¦¬ (ë‘ ë²ˆì§¸)
        if 'dsp' in self.analyzers:
            try:
                dsp_analyzer = self.analyzers['dsp']
                # DSPëŠ” 384ì°¨ì› ì…ë ¥ì„ ê¸°ëŒ€ - ë°±ë³¸ ì¶œë ¥(896ì°¨ì›)ì„ íˆ¬ì˜
                if not hasattr(self, 'dsp_projection'):
                    self.dsp_projection = torch.nn.Linear(features.shape[-1], 384).to(self.device)
                
                dsp_input = self.dsp_projection(features)
                dsp_output = dsp_analyzer(dsp_input)
                
                if self.verbose and batch_idx < 3:
                    logger.info(f"      - DSP ì¶œë ¥: {dsp_output.get('final_emotions', torch.zeros(1)).shape if isinstance(dsp_output, dict) else 'scalar'}")
            except Exception as e:
                logger.warning(f"    âš ï¸ DSP ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                dsp_output = None
        
        # Kalman ì²˜ë¦¬ (neural_emotionê³¼ DSP ì¶œë ¥ ëª¨ë‘ í•„ìš”)
        if 'kalman' in self.analyzers and dsp_output is not None and neural_emotion_output is not None:
            try:
                kalman_analyzer = self.analyzers['kalman']
                # neural_emotionì˜ logitsë¥¼ traditional emotionsë¡œ ì‚¬ìš©
                traditional_emotions = neural_emotion_output.get('emotion_logits', None)
                dsp_emotions = dsp_output.get('final_emotions', None) if isinstance(dsp_output, dict) else dsp_output
                
                if traditional_emotions is not None and dsp_emotions is not None:
                    # ë‘˜ ë‹¤ 7ì°¨ì›ì´ì–´ì•¼ í•¨
                    kalman_output = kalman_analyzer(
                        traditional_emotions=traditional_emotions,
                        dsp_emotions=dsp_emotions
                    )
                    if self.verbose and batch_idx < 3:
                        logger.info(f"      - Kalman í•„í„° ì¶œë ¥ ì²˜ë¦¬ë¨")
            except Exception as e:
                logger.warning(f"    âš ï¸ Kalman ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ë‚˜ë¨¸ì§€ analyzer ì²˜ë¦¬
        for name, analyzer in self.analyzers.items():
            # ì´ë¯¸ ì²˜ë¦¬í•œ ëª¨ë“ˆë“¤ì€ ê±´ë„ˆëœ€
            if name in ['dsp', 'kalman', 'neural_emotion']:
                continue
                
            # ëª¨ë“  nn.Module ê¸°ë°˜ analyzer ì²˜ë¦¬ (neural_, advanced_ ë“± ëª¨ë‘ í¬í•¨)
            if isinstance(analyzer, torch.nn.Module) and hasattr(analyzer, 'forward'):
                try:
                    # Neural analyzer forward pass - features ì‚¬ìš© (backbone ì¶œë ¥)
                    analyzer_output = analyzer(features)
                    
                    # ê° analyzer íƒ€ì…ë³„ ì†ì‹¤ ê³„ì‚°
                    if 'emotion' in name and 'emotion_logits' in analyzer_output:
                        # ì‹¤ì œ ê°ì • íƒ€ê¹ƒ ì¶”ì¶œ (ë”ë¯¸ ë°ì´í„° ì œê±°)
                        target = TargetMapper.extract_emotion_target(batch_data).to(self.device)
                        # ë¶„ë¥˜ê°€ ì•„ë‹Œ íšŒê·€ íƒœìŠ¤í¬ë¡œ ì²˜ë¦¬
                        if target.dim() == 1:
                            target = F.one_hot(target, num_classes=7).float()
                        analyzer_loss = F.cross_entropy(analyzer_output['emotion_logits'], target)
                        analyzer_losses.append(analyzer_loss)
                        if self.verbose and batch_idx < 3:
                            logger.info(f"      - {name} ì†ì‹¤: {analyzer_loss.item():.6f}")
                    
                    elif 'bentham' in name and 'bentham_scores' in analyzer_output:
                        # ì‹¤ì œ ë²¤ë‹´ íƒ€ê¹ƒ ì¶”ì¶œ
                        target = TargetMapper.extract_bentham_target(batch_data).to(self.device)
                        analyzer_loss = F.mse_loss(analyzer_output['bentham_scores'], target)
                        analyzer_losses.append(analyzer_loss)
                        if self.verbose and batch_idx < 3:
                            logger.info(f"      - {name} ì†ì‹¤: {analyzer_loss.item():.6f}")
                    
                    elif 'regret' in name and 'regret_score' in analyzer_output:
                        # ì‹¤ì œ í›„íšŒ íƒ€ê¹ƒ ì¶”ì¶œ
                        target = TargetMapper.extract_regret_target(batch_data).to(self.device)
                        analyzer_loss = F.smooth_l1_loss(analyzer_output['regret_score'], target)
                        analyzer_losses.append(analyzer_loss)
                        if self.verbose and batch_idx < 3:
                            logger.info(f"      - {name} ì†ì‹¤: {analyzer_loss.item():.6f}")
                    
                    elif 'surd' in name and 'surd_scores' in analyzer_output:
                        # ì‹¤ì œ SURD íƒ€ê¹ƒ ì¶”ì¶œ (ì •ê·œí™”ëœ 4ì°¨ì›)
                        target = TargetMapper.extract_surd_target(batch_data, normalize=True).to(self.device)
                        analyzer_loss = F.mse_loss(analyzer_output['surd_scores'], target)
                        analyzer_losses.append(analyzer_loss)
                        if self.verbose and batch_idx < 3:
                            logger.info(f"      - {name} ì†ì‹¤: {analyzer_loss.item():.6f}")
                    
                except Exception as e:
                    if self.args.debug:
                        logger.error(f"    {name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì „ì²´ ì†ì‹¤ í†µí•© (í—¤ë“œ + Neural Analyzer)
        all_losses = losses + analyzer_losses
        
        if all_losses:
            # ê°€ì¤‘ í‰ê· : í—¤ë“œ 70%, Neural Analyzer 30%
            if losses and analyzer_losses:
                head_loss = sum(losses) / len(losses)
                analyzer_loss = sum(analyzer_losses) / len(analyzer_losses)
                loss = 0.7 * head_loss + 0.3 * analyzer_loss
                if self.verbose and batch_idx < 3:
                    logger.info(f"      - í—¤ë“œ ì†ì‹¤: {head_loss.item():.6f}")
                    logger.info(f"      - ë¶„ì„ê¸° ì†ì‹¤: {analyzer_loss.item():.6f}")
            else:
                loss = sum(all_losses) / len(all_losses)
        else:
            # NO FALLBACK ì›ì¹™ - ì†ì‹¤ì´ ì—†ìœ¼ë©´ ì˜ˆì™¸
            raise RuntimeError("ì†ì‹¤ ê³„ì‚° ì‹¤íŒ¨: í—¤ë“œì™€ ë¶„ì„ê¸° ëª¨ë‘ ì†ì‹¤ì„ ìƒì„±í•˜ì§€ ëª»í•¨")
        
        if self.verbose and batch_idx < 3:
            logger.info(f"      - ì†ì‹¤ê°’: {loss.item():.6f}")
        
        # ========== STAGE 3: UPDATE ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 3] Parameter Update")
        
        if self.optimizer is not None:
            # NO FALLBACK ì›ì¹™ - ì˜µí‹°ë§ˆì´ì €ê°€ ì—†ìœ¼ë©´ í•™ìŠµ ë¶ˆê°€
            
            # Gradient Accumulation: ê²½ê³„ì—ì„œë§Œ zero_grad í˜¸ì¶œ
            if batch_idx % self.gradient_accumulation_steps == 0:
                self.optimizer.zero_grad()
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬ (NaN, Inf ê²€ì¦)
            total_norm = 0.0
            for p in self.backbone.parameters() if self.backbone else []:
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
                    
                    # NaN/Inf ì²´í¬
                    if torch.isnan(p.grad).any() or torch.isinf(p.grad).any():
                        logger.error(f"âš ï¸ ê·¸ë˜ë””ì–¸íŠ¸ ì´ìƒ ê°ì§€: NaN ë˜ëŠ” Inf")
                        if self.args.debug:
                            logger.error(f"   íŒŒë¼ë¯¸í„° shape: {p.shape}")
            
            total_norm = total_norm ** 0.5
            if self.verbose and batch_idx < 3:
                logger.info(f"      - ê·¸ë˜ë””ì–¸íŠ¸ norm: {total_norm:.4f}")
            elif self.args.debug and batch_idx % 10 == 0:
                logger.debug(f"   ê·¸ë˜ë””ì–¸íŠ¸ norm: {total_norm:.4f}")
            
            # Gradient Accumulationì„ ìœ„í•œ loss ìŠ¤ì¼€ì¼ë§
            if self.gradient_accumulation_steps > 1:
                loss = loss / self.gradient_accumulation_steps
            
            # Gradient Accumulation ì²´í¬
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì•ˆì •ì ì¸ í•™ìŠµ)
                all_params = []
                if self.backbone:
                    all_params.extend([p for p in self.backbone.parameters() if p.requires_grad])
                for head in self.heads.values():
                    all_params.extend([p for p in head.parameters() if p.requires_grad])
                # Neural Analyzer íŒŒë¼ë¯¸í„°ë„ í¬í•¨
                for name, analyzer in self.analyzers.items():
                    if 'neural_' in name and hasattr(analyzer, 'parameters'):
                        all_params.extend([p for p in analyzer.parameters() if p.requires_grad])
                
                if all_params:
                    if self.use_mixed_precision and self.scaler is not None:
                        self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(all_params, max_norm=1.0)
                
                # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (--no-param-update ì˜µì…˜ ì²´í¬)
                if not self.args.no_param_update:
                    # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                    if self.use_mixed_precision and self.scaler is not None:
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        self.optimizer.step()
                    
                    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                    if self.scheduler:
                        self.scheduler.step()
                
                if self.verbose and batch_idx < 3:
                    logger.info(f"      - íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                if self.verbose and batch_idx < 3:
                    logger.info(f"      - íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ìŠ¤í‚µ (--no-param-update)")
                elif self.args.debug:
                    logger.debug("   íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ìƒëµ (ë² ì´ìŠ¤ë¼ì¸ íšŒê·€)")
        elif self.args.mode in ['train', 'training', 'train-test']:
            # í•™ìŠµ ëª¨ë“œì¸ë° ì˜µí‹°ë§ˆì´ì €ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜
            raise RuntimeError("í•™ìŠµ ëª¨ë“œì´ì§€ë§Œ ì˜µí‹°ë§ˆì´ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        
        return loss.item()
    
    def _eval_step(self, batch_data: List[Dict], batch_idx: int) -> float:
        """
        í‰ê°€ ìŠ¤í… - backward ì—†ì´ ì†ì‹¤ë§Œ ê³„ì‚°
        """
        # ========== STAGE 1: FORWARD ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 1] Forward Pass ì‹œì‘")
        
        # ì‹¤ì œ ë°ì´í„° ì‚¬ìš© (NO FALLBACK - ë”ë¯¸ ë°ì´í„° ì œê±°)
        batch_size = len(batch_data)
        
        # batch_dataì—ì„œ ì‹¤ì œ ì…ë ¥ ì¶”ì¶œ
        if not batch_data or not isinstance(batch_data[0], dict):
            raise ValueError("ë°°ì¹˜ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë¨")
        
        # TargetMapperë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ
        try:
            # context_embedding ì¶”ì¶œ
            input_embeddings = TargetMapper.extract_context_embedding(batch_data)
            dummy_input = input_embeddings.to(self.device).requires_grad_(False)  # í‰ê°€ ëª¨ë“œì´ë¯€ë¡œ gradient ë¶ˆí•„ìš”
        except KeyError as e:
            # NO FALLBACK ì›ì¹™ - ì‹¤íŒ¨ì‹œ ì˜ˆì™¸ ë°œìƒ
            raise RuntimeError(f"ì…ë ¥ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        if self.backbone:
            # ë°±ë³¸ forward - Dict[str, Tensor] ë°˜í™˜
            backbone_outputs = self.backbone(dummy_input, return_all_tasks=True)
            features = backbone_outputs.get('emotion', dummy_input)
            if self.verbose and batch_idx < 3:
                logger.info(f"      - ë°±ë³¸ ì¶œë ¥ shape: {features.shape}")
                logger.info(f"      - ë°±ë³¸ ì¶œë ¥ í‚¤: {list(backbone_outputs.keys())}")
        else:
            features = dummy_input
        
        # ========== STAGE 2: COMPUTE LOSS (í‰ê°€ìš©) ==========
        if self.verbose and batch_idx < 3:
            logger.info("    [STAGE 2] Compute Loss (í‰ê°€)")
        
        # í—¤ë“œ í†µê³¼ ë° ì†ì‹¤ ê³„ì‚°
        losses = []
        
        # ê°ì • í—¤ë“œ ì˜ˆì‹œ
        if 'emotion' in self.heads and features is not None:
            emotion_output = self.heads['emotion'](features)
            emotion_pred = emotion_output['emotions'] if isinstance(emotion_output, dict) else emotion_output
            # ì‹¤ì œ ê°ì • íƒ€ê¹ƒ ì¶”ì¶œ (TargetMapper ì‚¬ìš© - List[Dict] ì²˜ë¦¬ìš©)
            target = TargetMapper.extract_emotion_target(batch_data).to(self.device)
            # head.compute_loss ì‚¬ìš©
            emotion_loss = self.heads['emotion'].compute_loss(emotion_pred, target)
            losses.append(emotion_loss)
            
            if self.verbose and batch_idx < 3:
                logger.info(f"      - ê°ì • ì˜ˆì¸¡ shape: {emotion_pred.shape}")
        
        # ë²¤ë‹´ í—¤ë“œ (ì„ íƒì )
        if 'bentham' in self.heads and features is not None:
            bentham_output = self.heads['bentham'](features)
            bentham_pred = bentham_output['bentham_scores'] if isinstance(bentham_output, dict) else bentham_output
            # ì‹¤ì œ ë²¤ë‹´ íƒ€ê¹ƒ ì¶”ì¶œ (TargetMapper ì‚¬ìš©)
            target = TargetMapper.extract_bentham_target(batch_data).to(self.device)
            # head.compute_loss ì‚¬ìš© (ì¼ê´€ì„±)
            bentham_loss = self.heads['bentham'].compute_loss(bentham_pred, target)
            losses.append(bentham_loss)
        
        # ì „ì²´ ì†ì‹¤
        if losses:
            loss = sum(losses) / len(losses)
        else:
            # NO FALLBACK ì›ì¹™ - ì†ì‹¤ì´ ì—†ìœ¼ë©´ ì˜ˆì™¸
            raise RuntimeError("ê²€ì¦ ì†ì‹¤ ê³„ì‚° ì‹¤íŒ¨: í—¤ë“œê°€ ì†ì‹¤ì„ ìƒì„±í•˜ì§€ ëª»í•¨")
        
        if self.verbose and batch_idx < 3:
            logger.info(f"      - í‰ê°€ ì†ì‹¤ê°’: {loss.item():.6f}")
        
        return loss.item()
    
    def evaluate(self):
        """í‰ê°€"""
        logger.info("\nğŸ§ª í‰ê°€ ì‹œì‘")
        
        if self.backbone:
            self.backbone.eval()
        
        for head in self.heads.values():
            head.eval()
        
        total_loss = 0.0
        num_batches = max(1, len(self.val_data) // self.args.batch_size)
        
        with torch.no_grad():
            for batch_idx in range(num_batches):
                batch_start = batch_idx * self.args.batch_size
                batch_end = batch_start + self.args.batch_size
                batch_data = self.val_data[batch_start:batch_end]
                
                # í‰ê°€ ìŠ¤í… (backward ì—†ìŒ)
                loss = self._eval_step(batch_data, batch_idx)
                total_loss += loss
        
        avg_val_loss = total_loss / max(num_batches, 1)
        logger.info(f"âœ… í‰ê°€ ì™„ë£Œ - í‰ê·  ì†ì‹¤: {avg_val_loss:.4f}")
        
        return avg_val_loss
    
    def save_checkpoint(self, epoch: int, loss: float):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_dir = Path(self.args.checkpoint_dir)
        checkpoint_dir.mkdir(exist_ok=True)
        
        checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch+1}.pt"
        
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'loss': loss,
            'config': ADVANCED_CONFIG,
            'args': vars(self.args),
            'timestamp': datetime.now().isoformat()
        }
        
        # ëª¨ë¸ ìƒíƒœ ì €ì¥
        if self.backbone:
            checkpoint['backbone_state'] = self.backbone.state_dict()
        
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if loss < self.best_loss:
            self.best_loss = loss
            best_path = checkpoint_dir / "best_model.pt"
            torch.save(checkpoint, best_path)
            logger.info(f"ğŸ† ìµœê³  ëª¨ë¸ ê°±ì‹ : {best_path}")
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸš€ Red Heart AI í†µí•© í•™ìŠµ ì‹œì‘")
        logger.info("=" * 60)
        
        try:
            # 1. ë°ì´í„° ì¤€ë¹„
            self.prepare_data()
            
            # 2. ëª¨ë¸ ì´ˆê¸°í™”
            self.initialize_models()
            
            # 2-1. í•™ìŠµ ëª¨ë“œì¼ ë•Œ ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
            if self.args.mode in ['train', 'training', 'train-test']:
                self._initialize_optimizer()
            
            # 3. í•™ìŠµ ë£¨í”„
            if self.args.mode in ['train', 'training', 'train-test']:
                for epoch in range(self.args.epochs):
                    # í•™ìŠµ
                    train_loss = self.train_epoch(epoch)
                    
                    # í‰ê°€
                    val_loss = self.evaluate()
                    
                    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                    self.save_checkpoint(epoch, val_loss)
                    
                    # ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
                    if (epoch + 1) % 5 == 0:
                        gpu_info = get_gpu_memory_info()
                        logger.info(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {gpu_info['usage_percent']:.1f}% ì‚¬ìš©")
            
            elif self.args.mode in ['eval', 'test']:
                # í‰ê°€ë§Œ
                val_loss = self.evaluate()
                logger.info(f"ìµœì¢… í‰ê°€ ì†ì‹¤: {val_loss:.4f}")
            
            logger.info("\n" + "=" * 60)
            logger.info("âœ… í•™ìŠµ/í‰ê°€ ì™„ë£Œ!")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        finally:
            # ì •ë¦¬
            self.cleanup()
    
    def cleanup(self):
        """ì •ë¦¬"""
        logger.info("\nğŸ§¹ ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘...")
        
        # ëª¨ë¸ ì •ë¦¬
        if self.backbone:
            del self.backbone
        self.heads.clear()
        self.analyzers.clear()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("âœ… ì •ë¦¬ ì™„ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Red Heart AI í†µí•© í•™ìŠµ v2")
    
    # ê¸°ë³¸ ì˜µì…˜
    parser.add_argument('--mode', choices=['train', 'eval', 'test', 'training', 'train-test'], 
                       default='train', help='ì‹¤í–‰ ëª¨ë“œ')
    parser.add_argument('--no-param-update', action='store_true',
                       help='íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì—†ì´ ê·¸ë˜ë””ì–¸íŠ¸ë§Œ ì²´í¬ (í•™ìŠµ í…ŒìŠ¤íŠ¸ìš©)')
    parser.add_argument('--epochs', type=int, default=10, 
                       help='í•™ìŠµ ì—í¬í¬')
    parser.add_argument('--batch-size', type=int, default=4, 
                       help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--learning-rate', type=float, default=1e-4, 
                       help='í•™ìŠµë¥ ')
    
    # ë°ì´í„° ì˜µì…˜
    parser.add_argument('--max-samples', type=int, default=1000,
                       help='ìµœëŒ€ ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--samples', type=int, dest='max_samples',
                       help='ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (--max-samplesì™€ ë™ì¼)')
    parser.add_argument('--force-preprocess', action='store_true',
                       help='ê°•ì œ ì „ì²˜ë¦¬')
    parser.add_argument('--preprocess-batch-size', type=int, default=5,
                       help='ì „ì²˜ë¦¬ ë°°ì¹˜ í¬ê¸°')
    
    # ì‹œìŠ¤í…œ ì˜µì…˜
    parser.add_argument('--checkpoint-dir', default='./checkpoints_v2',
                       help='ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬')
    parser.add_argument('--debug', action='store_true',
                       help='ë””ë²„ê·¸ ëª¨ë“œ')
    parser.add_argument('--verbose', action='store_true',
                       help='ìƒì„¸ ì¶œë ¥ ëª¨ë“œ')
    parser.add_argument('--use-advanced', action='store_true',
                       help='Advanced ë¶„ì„ê¸° í†µí•© (FocalLoss, MoE, GPUê°€ì† ë“±)')
    
    args = parser.parse_args()
    
    # ë””ë²„ê·¸ ëª¨ë“œ
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # ì‹œìŠ¤í…œ ì‹¤í–‰
    system = UnifiedTrainingSystemV2(args)
    system.run()


if __name__ == "__main__":
    main()