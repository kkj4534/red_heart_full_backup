"""
í˜ì´ì¦ˆ ì»¨íŠ¸ë¡¤ëŸ¬ (Phase Controller)
Phase Controller Module

í•™ìŠµ(Learning), ì‹¤í–‰(Execution), ë°˜ì„±(Reflection) í˜ì´ì¦ˆë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë¶„ê¸° ì²˜ë¦¬í•˜ì—¬
ê° í˜ì´ì¦ˆë³„ ìµœì í™”ëœ ì •ì±…ê³¼ íŒŒë¼ë¯¸í„°ë¥¼ ì ìš©í•˜ëŠ” ì ì‘ì  ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
1. í˜ì´ì¦ˆë³„ ì°¨ë“± ì •ì±… ì ìš©
2. ë™ì  í˜ì´ì¦ˆ ì „í™˜ ì²´í¬í¬ì¸íŠ¸
3. í˜ì´ì¦ˆë³„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
4. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í˜ì´ì¦ˆ ì„ íƒ
"""

import numpy as np
import torch
import logging
import json
import time
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import threading

try:
    from config import ADVANCED_CONFIG, DEVICE
except ImportError:
    # config.pyì— ë¬¸ì œê°€ ìˆì„ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
    ADVANCED_CONFIG = {'enable_gpu': False}
    DEVICE = 'cpu'
    print("âš ï¸  config.py ì„í¬íŠ¸ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
from data_models import EmotionData

logger = logging.getLogger('PhaseController')

class Phase(Enum):
    """ì‹œìŠ¤í…œ í˜ì´ì¦ˆ ì •ì˜"""
    LEARNING = "learning"         # í•™ìŠµ í˜ì´ì¦ˆ - íƒìƒ‰ì , ì‹¤í—˜ì 
    EXECUTION = "execution"       # ì‹¤í–‰ í˜ì´ì¦ˆ - ë³´ìˆ˜ì , ì•ˆì •ì 
    REFLECTION = "reflection"     # ë°˜ì„± í˜ì´ì¦ˆ - ë¶„ì„ì , ê°œì„ ì 
    TRANSITION = "transition"     # ì „í™˜ í˜ì´ì¦ˆ - í˜ì´ì¦ˆ ê°„ ì „í™˜

@dataclass
class PhaseConfig:
    """í˜ì´ì¦ˆë³„ ì„¤ì •"""
    phase: Phase
    
    # íƒìƒ‰/í™œìš© ê· í˜•
    exploration_rate: float = 0.5
    exploitation_rate: float = 0.5
    
    # ì•ˆì „ì„± ì„ê³„ê°’
    safety_threshold: float = 0.5
    risk_tolerance: float = 0.5
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    learning_rate: float = 0.01
    memory_weight: float = 0.5
    
    # ì˜ì‚¬ê²°ì • íŒŒë¼ë¯¸í„°
    confidence_threshold: float = 0.7
    consensus_requirement: float = 0.6
    
    # ì‹œê°„ ì œì•½
    max_processing_time: float = 5.0
    urgency_factor: float = 1.0
    
    # í”¼ë“œë°± ê°ë„
    regret_sensitivity: float = 0.5
    reward_sensitivity: float = 0.5
    
    # ìœ¤ë¦¬ì  ì—„ê²©ì„±
    ethical_strictness: float = 0.7
    moral_flexibility: float = 0.3

@dataclass
class PhaseTransitionCriteria:
    """í˜ì´ì¦ˆ ì „í™˜ ê¸°ì¤€"""
    
    # ì„±ëŠ¥ ê¸°ë°˜ ì „í™˜
    performance_threshold: float = 0.8
    consistency_requirement: float = 0.7
    improvement_rate: float = 0.05
    
    # ì‹œê°„ ê¸°ë°˜ ì „í™˜
    min_phase_duration: float = 300.0  # 5ë¶„
    max_phase_duration: float = 3600.0  # 1ì‹œê°„
    
    # ê²½í—˜ ê¸°ë°˜ ì „í™˜
    min_experiences: int = 10
    experience_quality_threshold: float = 0.7
    
    # í™˜ê²½ ê¸°ë°˜ ì „í™˜
    stability_indicator: float = 0.8
    novelty_threshold: float = 0.3
    
    # ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜
    user_satisfaction_threshold: float = 0.8
    intervention_count_limit: int = 3

@dataclass
class PhaseState:
    """í˜„ì¬ í˜ì´ì¦ˆ ìƒíƒœ"""
    current_phase: Phase
    phase_start_time: float
    phase_duration: float = 0.0
    
    # í˜ì´ì¦ˆ ì„±ëŠ¥ ì§€í‘œ
    success_rate: float = 0.0
    average_confidence: float = 0.5
    error_rate: float = 0.0
    user_satisfaction: float = 0.5
    
    # ì „í™˜ ì¤€ë¹„ë„
    transition_readiness: float = 0.0
    next_recommended_phase: Optional[Phase] = None
    
    # ë©”íƒ€ë°ì´í„°
    decision_count: int = 0
    last_update_time: float = field(default_factory=time.time)
    phase_history: List[Tuple[Phase, float]] = field(default_factory=list)

@dataclass
class PhaseDecisionContext:
    """í˜ì´ì¦ˆë³„ ì˜ì‚¬ê²°ì • ë§¥ë½"""
    scenario_complexity: float = 0.5
    uncertainty_level: float = 0.5
    time_pressure: float = 0.5
    stakeholder_count: int = 1
    ethical_weight: float = 0.7
    
    # ê³¼ê±° ê²½í—˜
    similar_scenarios: int = 0
    past_success_rate: float = 0.5
    
    # í˜„ì¬ ìƒí™©
    available_information: float = 0.8
    user_confidence: float = 0.7
    external_pressure: float = 0.3

class PhaseController:
    """í˜ì´ì¦ˆ ì»¨íŠ¸ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.logger = logger
        
        # í˜„ì¬ ìƒíƒœ
        self.current_state = PhaseState(
            current_phase=Phase.LEARNING,
            phase_start_time=time.time()
        )
        
        # í˜ì´ì¦ˆë³„ ì„¤ì •
        self.phase_configs = self._initialize_phase_configs()
        
        # ì „í™˜ ê¸°ì¤€
        self.transition_criteria = PhaseTransitionCriteria()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_history = {
            Phase.LEARNING: [],
            Phase.EXECUTION: [],
            Phase.REFLECTION: []
        }
        
        # í˜ì´ì¦ˆë³„ í†µê³„
        self.phase_statistics = {
            phase: {
                'total_time': 0.0,
                'total_decisions': 0,
                'success_count': 0,
                'average_performance': 0.0
            } for phase in Phase
        }
        
        # ë™ì  ì¡°ì • íŒŒë¼ë¯¸í„°
        self.adaptation_memory = {
            'successful_transitions': [],
            'failed_transitions': [],
            'optimal_phase_durations': {}
        }
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self.state_lock = threading.Lock()
        
        self.logger.info("í˜ì´ì¦ˆ ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_phase_configs(self) -> Dict[Phase, PhaseConfig]:
        """í˜ì´ì¦ˆë³„ ì„¤ì • ì´ˆê¸°í™”"""
        return {
            Phase.LEARNING: PhaseConfig(
                phase=Phase.LEARNING,
                exploration_rate=0.8,      # ë†’ì€ íƒìƒ‰
                exploitation_rate=0.2,     # ë‚®ì€ í™œìš©
                safety_threshold=0.3,      # ë‚®ì€ ì•ˆì „ ì„ê³„ê°’
                risk_tolerance=0.8,        # ë†’ì€ ìœ„í—˜ í—ˆìš©
                learning_rate=0.05,        # ë†’ì€ í•™ìŠµë¥ 
                memory_weight=0.3,         # ë‚®ì€ ê¸°ì–µ ê°€ì¤‘ì¹˜
                confidence_threshold=0.5,  # ë‚®ì€ ì‹ ë¢°ë„ ìš”êµ¬
                regret_sensitivity=0.9,    # ë†’ì€ í›„íšŒ ê°ë„
                ethical_strictness=0.6     # ì¤‘ê°„ ìœ¤ë¦¬ ì—„ê²©ì„±
            ),
            
            Phase.EXECUTION: PhaseConfig(
                phase=Phase.EXECUTION,
                exploration_rate=0.1,      # ë‚®ì€ íƒìƒ‰
                exploitation_rate=0.9,     # ë†’ì€ í™œìš©
                safety_threshold=0.8,      # ë†’ì€ ì•ˆì „ ì„ê³„ê°’
                risk_tolerance=0.2,        # ë‚®ì€ ìœ„í—˜ í—ˆìš©
                learning_rate=0.01,        # ë‚®ì€ í•™ìŠµë¥ 
                memory_weight=0.8,         # ë†’ì€ ê¸°ì–µ ê°€ì¤‘ì¹˜
                confidence_threshold=0.8,  # ë†’ì€ ì‹ ë¢°ë„ ìš”êµ¬
                regret_sensitivity=0.3,    # ë‚®ì€ í›„íšŒ ê°ë„
                ethical_strictness=0.9     # ë†’ì€ ìœ¤ë¦¬ ì—„ê²©ì„±
            ),
            
            Phase.REFLECTION: PhaseConfig(
                phase=Phase.REFLECTION,
                exploration_rate=0.5,      # ì¤‘ê°„ íƒìƒ‰
                exploitation_rate=0.5,     # ì¤‘ê°„ í™œìš©
                safety_threshold=0.6,      # ì¤‘ê°„ ì•ˆì „ ì„ê³„ê°’
                risk_tolerance=0.4,        # ì¤‘ê°„ ìœ„í—˜ í—ˆìš©
                learning_rate=0.03,        # ì¤‘ê°„ í•™ìŠµë¥ 
                memory_weight=0.9,         # ë†’ì€ ê¸°ì–µ ê°€ì¤‘ì¹˜
                confidence_threshold=0.6,  # ì¤‘ê°„ ì‹ ë¢°ë„ ìš”êµ¬
                regret_sensitivity=0.7,    # ë†’ì€ í›„íšŒ ê°ë„
                ethical_strictness=0.8     # ë†’ì€ ìœ¤ë¦¬ ì—„ê²©ì„±
            )
        }
    
    def get_current_phase_config(self) -> PhaseConfig:
        """í˜„ì¬ í˜ì´ì¦ˆ ì„¤ì • ë°˜í™˜"""
        with self.state_lock:
            return self.phase_configs[self.current_state.current_phase]
    
    def determine_optimal_phase(self, context: PhaseDecisionContext) -> Phase:
        """ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì  í˜ì´ì¦ˆ ê²°ì •"""
        
        # í˜ì´ì¦ˆë³„ ì í•©ë„ ì ìˆ˜ ê³„ì‚°
        phase_scores = {}
        
        # í•™ìŠµ í˜ì´ì¦ˆ ì í•©ë„
        learning_score = self._calculate_learning_phase_score(context)
        phase_scores[Phase.LEARNING] = learning_score
        
        # ì‹¤í–‰ í˜ì´ì¦ˆ ì í•©ë„
        execution_score = self._calculate_execution_phase_score(context)
        phase_scores[Phase.EXECUTION] = execution_score
        
        # ë°˜ì„± í˜ì´ì¦ˆ ì í•©ë„
        reflection_score = self._calculate_reflection_phase_score(context)
        phase_scores[Phase.REFLECTION] = reflection_score
        
        # ìµœê³  ì ìˆ˜ í˜ì´ì¦ˆ ì„ íƒ
        optimal_phase = max(phase_scores.keys(), key=lambda p: phase_scores[p])
        
        self.logger.debug(
            f"í˜ì´ì¦ˆ ì í•©ë„ ì ìˆ˜: "
            f"í•™ìŠµ={learning_score:.3f}, "
            f"ì‹¤í–‰={execution_score:.3f}, "
            f"ë°˜ì„±={reflection_score:.3f} "
            f"-> ì„ íƒ: {optimal_phase.value}"
        )
        
        return optimal_phase
    
    def _calculate_learning_phase_score(self, context: PhaseDecisionContext) -> float:
        """í•™ìŠµ í˜ì´ì¦ˆ ì í•©ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ë†’ì€ ë¶ˆí™•ì‹¤ì„± -> í•™ìŠµ í˜ì´ì¦ˆ ì„ í˜¸
        score += context.uncertainty_level * 0.3
        
        # ë‚®ì€ ê³¼ê±° ê²½í—˜ -> í•™ìŠµ í˜ì´ì¦ˆ ì„ í˜¸
        if context.similar_scenarios < 5:
            score += 0.3
        else:
            score += max(0.0, 0.3 - (context.similar_scenarios - 5) * 0.05)
        
        # ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ -> í•™ìŠµ í˜ì´ì¦ˆ ì„ í˜¸
        score += context.scenario_complexity * 0.2
        
        # ë‚®ì€ ì‹œê°„ ì••ë°• -> í•™ìŠµ í˜ì´ì¦ˆ ì„ í˜¸
        score += (1.0 - context.time_pressure) * 0.2
        
        return np.clip(score, 0.0, 1.0)
    
    def _calculate_execution_phase_score(self, context: PhaseDecisionContext) -> float:
        """ì‹¤í–‰ í˜ì´ì¦ˆ ì í•©ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ë†’ì€ í™•ì‹¤ì„± -> ì‹¤í–‰ í˜ì´ì¦ˆ ì„ í˜¸
        score += (1.0 - context.uncertainty_level) * 0.3
        
        # ë§ì€ ê³¼ê±° ê²½í—˜ -> ì‹¤í–‰ í˜ì´ì¦ˆ ì„ í˜¸
        if context.similar_scenarios >= 10:
            score += 0.3
        elif context.similar_scenarios >= 5:
            score += 0.2
        
        # ë†’ì€ ê³¼ê±° ì„±ê³µë¥  -> ì‹¤í–‰ í˜ì´ì¦ˆ ì„ í˜¸
        score += context.past_success_rate * 0.2
        
        # ë†’ì€ ì‹œê°„ ì••ë°• -> ì‹¤í–‰ í˜ì´ì¦ˆ ì„ í˜¸
        score += context.time_pressure * 0.2
        
        return np.clip(score, 0.0, 1.0)
    
    def _calculate_reflection_phase_score(self, context: PhaseDecisionContext) -> float:
        """ë°˜ì„± í˜ì´ì¦ˆ ì í•©ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ë†’ì€ ìœ¤ë¦¬ì  ê°€ì¤‘ì¹˜ -> ë°˜ì„± í˜ì´ì¦ˆ ì„ í˜¸
        score += context.ethical_weight * 0.3
        
        # ì¤‘ê°„ ì •ë„ì˜ ê²½í—˜ê³¼ ë¶ˆí™•ì‹¤ì„± -> ë°˜ì„± í˜ì´ì¦ˆ ì„ í˜¸
        if 3 <= context.similar_scenarios <= 8:
            score += 0.2
        
        if 0.3 <= context.uncertainty_level <= 0.7:
            score += 0.2
        
        # ë§ì€ ì´í•´ê´€ê³„ì -> ë°˜ì„± í˜ì´ì¦ˆ ì„ í˜¸
        stakeholder_factor = min(context.stakeholder_count / 10.0, 1.0)
        score += stakeholder_factor * 0.3
        
        return np.clip(score, 0.0, 1.0)
    
    def check_phase_transition_needed(self) -> Tuple[bool, Optional[Phase]]:
        """í˜ì´ì¦ˆ ì „í™˜ í•„ìš”ì„± ì²´í¬"""
        
        with self.state_lock:
            current_time = time.time()
            phase_duration = current_time - self.current_state.phase_start_time
            
            # ì‹œê°„ ê¸°ë°˜ ì „í™˜ ì²´í¬
            if phase_duration > self.transition_criteria.max_phase_duration:
                return True, self._suggest_next_phase()
            
            # ì„±ëŠ¥ ê¸°ë°˜ ì „í™˜ ì²´í¬
            if self.current_state.success_rate < (1.0 - self.transition_criteria.performance_threshold):
                if phase_duration > self.transition_criteria.min_phase_duration:
                    return True, self._suggest_next_phase()
            
            # ì‚¬ìš©ì ë§Œì¡±ë„ ê¸°ë°˜ ì „í™˜ ì²´í¬
            if self.current_state.user_satisfaction < self.transition_criteria.user_satisfaction_threshold:
                if phase_duration > self.transition_criteria.min_phase_duration:
                    return True, self._suggest_next_phase()
            
            # ê²½í—˜ ì¶©ë¶„ì„± ì²´í¬
            if (self.current_state.current_phase == Phase.LEARNING and 
                self.current_state.decision_count >= self.transition_criteria.min_experiences):
                if self.current_state.success_rate > self.transition_criteria.performance_threshold:
                    return True, Phase.EXECUTION
            
            return False, None
    
    def _suggest_next_phase(self) -> Phase:
        """ë‹¤ìŒ í˜ì´ì¦ˆ ì œì•ˆ"""
        current = self.current_state.current_phase
        
        # ìˆœí™˜ì  í˜ì´ì¦ˆ ì „í™˜ ë¡œì§
        if current == Phase.LEARNING:
            # í•™ìŠµì—ì„œ ì‹¤í–‰ìœ¼ë¡œ (ì¶©ë¶„í•œ í•™ìŠµ í›„)
            if self.current_state.success_rate > 0.7:
                return Phase.EXECUTION
            else:
                return Phase.REFLECTION  # ì„±ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ë°˜ì„±
        
        elif current == Phase.EXECUTION:
            # ì‹¤í–‰ì—ì„œ ë°˜ì„±ìœ¼ë¡œ (ì„±ê³¼ ê²€í† )
            if self.current_state.success_rate < 0.6:
                return Phase.REFLECTION
            else:
                return Phase.LEARNING  # ìƒˆë¡œìš´ í•™ìŠµ ê¸°íšŒ íƒìƒ‰
        
        elif current == Phase.REFLECTION:
            # ë°˜ì„±ì—ì„œ í•™ìŠµìœ¼ë¡œ (ê°œì„  ë°©í–¥ ë„ì¶œ í›„)
            return Phase.LEARNING
        
        return Phase.LEARNING  # ê¸°ë³¸ê°’
    
    def transition_to_phase(self, target_phase: Phase, reason: str = "manual") -> bool:
        """í˜ì´ì¦ˆ ì „í™˜ ì‹¤í–‰"""
        
        with self.state_lock:
            old_phase = self.current_state.current_phase
            
            if old_phase == target_phase:
                self.logger.debug(f"ì´ë¯¸ {target_phase.value} í˜ì´ì¦ˆì…ë‹ˆë‹¤.")
                return False
            
            # í˜„ì¬ í˜ì´ì¦ˆ í†µê³„ ì—…ë°ì´íŠ¸
            self._update_phase_statistics(old_phase)
            
            # í˜ì´ì¦ˆ ì „í™˜ ì‹¤í–‰
            self.current_state.current_phase = target_phase
            self.current_state.phase_start_time = time.time()
            self.current_state.phase_duration = 0.0
            
            # í˜ì´ì¦ˆ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.current_state.phase_history.append((old_phase, time.time()))
            
            # ì„±ëŠ¥ ì§€í‘œ ì´ˆê¸°í™”
            self.current_state.success_rate = 0.0
            self.current_state.average_confidence = 0.5
            self.current_state.error_rate = 0.0
            self.current_state.decision_count = 0
            
            self.logger.info(
                f"í˜ì´ì¦ˆ ì „í™˜: {old_phase.value} -> {target_phase.value} (ì´ìœ : {reason})"
            )
            
            return True
    
    def apply_phase_policy(
        self, 
        base_decision_params: Dict[str, Any],
        context: PhaseDecisionContext
    ) -> Dict[str, Any]:
        """í˜ì´ì¦ˆë³„ ì •ì±…ì„ ê¸°ë³¸ ì˜ì‚¬ê²°ì • íŒŒë¼ë¯¸í„°ì— ì ìš©"""
        
        current_config = self.get_current_phase_config()
        modified_params = base_decision_params.copy()
        
        # íƒìƒ‰/í™œìš© ê· í˜• ì¡°ì •
        if 'exploration_weight' in modified_params:
            modified_params['exploration_weight'] *= current_config.exploration_rate
        
        if 'exploitation_weight' in modified_params:
            modified_params['exploitation_weight'] *= current_config.exploitation_rate
        
        # ì•ˆì „ì„± ì„ê³„ê°’ ì¡°ì •
        if 'safety_threshold' in modified_params:
            modified_params['safety_threshold'] = max(
                modified_params['safety_threshold'],
                current_config.safety_threshold
            )
        
        # í•™ìŠµë¥  ì¡°ì •
        if 'learning_rate' in modified_params:
            modified_params['learning_rate'] *= current_config.learning_rate / 0.01  # ì •ê·œí™”
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì •
        if 'confidence_threshold' in modified_params:
            modified_params['confidence_threshold'] = max(
                modified_params['confidence_threshold'],
                current_config.confidence_threshold
            )
        
        # ìœ¤ë¦¬ì  ì—„ê²©ì„± ì¡°ì •
        if 'ethical_strictness' in modified_params:
            modified_params['ethical_strictness'] *= current_config.ethical_strictness
        
        # í˜ì´ì¦ˆë³„ íŠ¹ë³„ ì¡°ì •
        self._apply_phase_specific_adjustments(modified_params, current_config, context)
        
        return modified_params
    
    def _apply_phase_specific_adjustments(
        self,
        params: Dict[str, Any],
        config: PhaseConfig,
        context: PhaseDecisionContext
    ):
        """í˜ì´ì¦ˆë³„ íŠ¹ë³„ ì¡°ì •"""
        
        if config.phase == Phase.LEARNING:
            # í•™ìŠµ í˜ì´ì¦ˆ: ì‹¤í—˜ì , íƒìƒ‰ì  ì¡°ì •
            if 'risk_tolerance' in params:
                params['risk_tolerance'] *= 1.5  # ìœ„í—˜ í—ˆìš©ë„ ì¦ê°€
            
            if 'novelty_bonus' in params:
                params['novelty_bonus'] *= 2.0  # ìƒˆë¡œìš´ ì‹œë„ ë³´ìƒ ì¦ê°€
        
        elif config.phase == Phase.EXECUTION:
            # ì‹¤í–‰ í˜ì´ì¦ˆ: ì•ˆì •ì , ë³´ìˆ˜ì  ì¡°ì •
            if 'uncertainty_penalty' in params:
                params['uncertainty_penalty'] *= 1.5  # ë¶ˆí™•ì‹¤ì„± í˜ë„í‹° ì¦ê°€
            
            if 'consistency_weight' in params:
                params['consistency_weight'] *= 1.3  # ì¼ê´€ì„± ê°€ì¤‘ì¹˜ ì¦ê°€
        
        elif config.phase == Phase.REFLECTION:
            # ë°˜ì„± í˜ì´ì¦ˆ: ì‹ ì¤‘í•˜ê³  ë¶„ì„ì  ì¡°ì •
            if 'analysis_depth' in params:
                params['analysis_depth'] *= 1.4  # ë¶„ì„ ê¹Šì´ ì¦ê°€
            
            if 'stakeholder_consideration' in params:
                params['stakeholder_consideration'] *= 1.2  # ì´í•´ê´€ê³„ì ê³ ë ¤ ì¦ê°€
    
    def record_decision_outcome(
        self,
        decision_success: bool,
        confidence: float,
        user_satisfaction: float,
        processing_time: float
    ):
        """ì˜ì‚¬ê²°ì • ê²°ê³¼ ê¸°ë¡"""
        
        with self.state_lock:
            self.current_state.decision_count += 1
            
            # ì„±ê³µë¥  ì—…ë°ì´íŠ¸
            current_success = self.current_state.success_rate
            decision_weight = 1.0 / self.current_state.decision_count
            
            if decision_success:
                new_success_rate = current_success + (1.0 - current_success) * decision_weight
            else:
                new_success_rate = current_success * (1.0 - decision_weight)
            
            self.current_state.success_rate = new_success_rate
            
            # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
            current_conf = self.current_state.average_confidence
            new_conf = current_conf + (confidence - current_conf) * decision_weight
            self.current_state.average_confidence = new_conf
            
            # ì‚¬ìš©ì ë§Œì¡±ë„ ì—…ë°ì´íŠ¸
            current_sat = self.current_state.user_satisfaction
            new_sat = current_sat + (user_satisfaction - current_sat) * decision_weight
            self.current_state.user_satisfaction = new_sat
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹œê°„ ê¸°ë¡
            self.current_state.last_update_time = time.time()
            
            # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            performance_record = {
                'timestamp': time.time(),
                'success': decision_success,
                'confidence': confidence,
                'user_satisfaction': user_satisfaction,
                'processing_time': processing_time
            }
            
            current_phase = self.current_state.current_phase
            self.performance_history[current_phase].append(performance_record)
            
            # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
            if len(self.performance_history[current_phase]) > 100:
                self.performance_history[current_phase] = self.performance_history[current_phase][-100:]
    
    def _update_phase_statistics(self, phase: Phase):
        """í˜ì´ì¦ˆ í†µê³„ ì—…ë°ì´íŠ¸"""
        
        if phase in self.performance_history and self.performance_history[phase]:
            recent_records = self.performance_history[phase][-50:]  # ìµœê·¼ 50ê°œ
            
            # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
            if recent_records:
                avg_performance = np.mean([r['success'] for r in recent_records])
                self.phase_statistics[phase]['average_performance'] = avg_performance
                
                total_decisions = self.phase_statistics[phase]['total_decisions']
                success_count = sum(r['success'] for r in recent_records)
                
                self.phase_statistics[phase]['total_decisions'] = total_decisions + len(recent_records)
                self.phase_statistics[phase]['success_count'] += success_count
        
        # í˜ì´ì¦ˆ ì‹œê°„ ëˆ„ì 
        phase_duration = time.time() - self.current_state.phase_start_time
        self.phase_statistics[phase]['total_time'] += phase_duration
    
    def get_analytics(self) -> Dict[str, Any]:
        """ë¶„ì„ ì •ë³´ ë°˜í™˜ (í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­)"""
        return self.get_phase_analytics()
    
    def get_phase_analytics(self) -> Dict[str, Any]:
        """í˜ì´ì¦ˆ ë¶„ì„ ì •ë³´ ë°˜í™˜"""
        
        with self.state_lock:
            current_time = time.time()
            current_duration = current_time - self.current_state.phase_start_time
            
            return {
                'current_state': {
                    'phase': self.current_state.current_phase.value,
                    'duration': current_duration,
                    'success_rate': self.current_state.success_rate,
                    'average_confidence': self.current_state.average_confidence,
                    'user_satisfaction': self.current_state.user_satisfaction,
                    'decision_count': self.current_state.decision_count
                },
                'phase_statistics': {
                    phase.value: stats for phase, stats in self.phase_statistics.items()
                },
                'transition_readiness': self.current_state.transition_readiness,
                'recent_transitions': self.current_state.phase_history[-5:],
                'performance_trends': self._calculate_performance_trends()
            }
    
    def _calculate_performance_trends(self) -> Dict[str, float]:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ê³„ì‚°"""
        trends = {}
        
        for phase, records in self.performance_history.items():
            if len(records) >= 10:
                recent_10 = records[-10:]
                previous_10 = records[-20:-10] if len(records) >= 20 else records[:-10]
                
                recent_avg = np.mean([r['success'] for r in recent_10])
                previous_avg = np.mean([r['success'] for r in previous_10]) if previous_10 else recent_avg
                
                trend = recent_avg - previous_avg
                trends[phase.value] = trend
            else:
                trends[phase.value] = 0.0
        
        return trends
    
    def save_phase_state(self, filepath: str):
        """í˜ì´ì¦ˆ ìƒíƒœ ì €ì¥"""
        
        state_data = {
            'current_state': {
                'current_phase': self.current_state.current_phase.value,
                'phase_start_time': self.current_state.phase_start_time,
                'success_rate': self.current_state.success_rate,
                'average_confidence': self.current_state.average_confidence,
                'user_satisfaction': self.current_state.user_satisfaction,
                'decision_count': self.current_state.decision_count,
                'phase_history': [(p.value, t) for p, t in self.current_state.phase_history]
            },
            'phase_statistics': {
                phase.value: stats for phase, stats in self.phase_statistics.items()
            },
            'performance_history_summary': {
                phase.value: len(records) for phase, records in self.performance_history.items()
            },
            'analytics': self.get_phase_analytics()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(state_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"í˜ì´ì¦ˆ ìƒíƒœë¥¼ {filepath}ì— ì €ì¥ ì™„ë£Œ")


# ìë™ í˜ì´ì¦ˆ ê´€ë¦¬ì
class AutoPhaseManager:
    """ìë™ í˜ì´ì¦ˆ ê´€ë¦¬ì - ì£¼ê¸°ì ìœ¼ë¡œ í˜ì´ì¦ˆ ìµœì í™”"""
    
    def __init__(self, phase_controller: PhaseController, check_interval: float = 60.0):
        self.phase_controller = phase_controller
        self.check_interval = check_interval
        self.running = False
        self.thread = None
        self.logger = logging.getLogger('AutoPhaseManager')
    
    def start(self):
        """ìë™ í˜ì´ì¦ˆ ê´€ë¦¬ ì‹œì‘"""
        if self.running:
            return
        
        self.running = True
        self.thread = threading.Thread(target=self._management_loop, daemon=True)
        self.thread.start()
        self.logger.info("ìë™ í˜ì´ì¦ˆ ê´€ë¦¬ ì‹œì‘")
    
    def stop(self):
        """ìë™ í˜ì´ì¦ˆ ê´€ë¦¬ ì¤‘ì§€"""
        self.running = False
        if self.thread:
            self.thread.join()
        self.logger.info("ìë™ í˜ì´ì¦ˆ ê´€ë¦¬ ì¤‘ì§€")
    
    def _management_loop(self):
        """ê´€ë¦¬ ë£¨í”„"""
        while self.running:
            try:
                # í˜ì´ì¦ˆ ì „í™˜ í•„ìš”ì„± ì²´í¬
                needs_transition, suggested_phase = self.phase_controller.check_phase_transition_needed()
                
                if needs_transition and suggested_phase:
                    success = self.phase_controller.transition_to_phase(
                        suggested_phase, 
                        reason="auto_optimization"
                    )
                    
                    if success:
                        self.logger.info(f"ìë™ í˜ì´ì¦ˆ ì „í™˜ ì™„ë£Œ: {suggested_phase.value}")
                
                time.sleep(self.check_interval)
                
            except Exception as e:
                self.logger.error(f"ìë™ í˜ì´ì¦ˆ ê´€ë¦¬ ì˜¤ë¥˜: {e}")
                time.sleep(self.check_interval)


# í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ í•¨ìˆ˜
def test_phase_controller():
    """í˜ì´ì¦ˆ ì»¨íŠ¸ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    print("ğŸ›ï¸ í˜ì´ì¦ˆ ì»¨íŠ¸ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™”
    controller = PhaseController()
    
    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
    test_contexts = [
        PhaseDecisionContext(
            scenario_complexity=0.8,
            uncertainty_level=0.9,
            time_pressure=0.2,
            stakeholder_count=2,
            similar_scenarios=1,
            past_success_rate=0.3
        ),
        PhaseDecisionContext(
            scenario_complexity=0.4,
            uncertainty_level=0.2,
            time_pressure=0.8,
            stakeholder_count=1,
            similar_scenarios=15,
            past_success_rate=0.9
        ),
        PhaseDecisionContext(
            scenario_complexity=0.6,
            uncertainty_level=0.5,
            time_pressure=0.4,
            stakeholder_count=8,
            ethical_weight=0.9,
            similar_scenarios=5,
            past_success_rate=0.6
        )
    ]
    
    # ê° ë§¥ë½ì— ëŒ€í•œ ìµœì  í˜ì´ì¦ˆ ê²°ì •
    for i, context in enumerate(test_contexts, 1):
        print(f"\n--- ì‹œë‚˜ë¦¬ì˜¤ {i} ---")
        print(f"ë³µì¡ë„: {context.scenario_complexity:.2f}, "
              f"ë¶ˆí™•ì‹¤ì„±: {context.uncertainty_level:.2f}, "
              f"ì‹œê°„ì••ë°•: {context.time_pressure:.2f}")
        
        # ìµœì  í˜ì´ì¦ˆ ê²°ì •
        optimal_phase = controller.determine_optimal_phase(context)
        print(f"ğŸ¯ ìµœì  í˜ì´ì¦ˆ: {optimal_phase.value}")
        
        # í˜ì´ì¦ˆ ì „í™˜
        controller.transition_to_phase(optimal_phase, f"scenario_{i}")
        
        # í˜ì´ì¦ˆë³„ ì •ì±… ì ìš©
        base_params = {
            'exploration_weight': 0.5,
            'safety_threshold': 0.6,
            'learning_rate': 0.01,
            'confidence_threshold': 0.7,
            'ethical_strictness': 0.8
        }
        
        modified_params = controller.apply_phase_policy(base_params, context)
        
        print(f"ğŸ“Š ì¡°ì •ëœ íŒŒë¼ë¯¸í„°:")
        for key, value in modified_params.items():
            original = base_params.get(key, 0)
            change = value - original
            print(f"  {key}: {original:.3f} â†’ {value:.3f} ({change:+.3f})")
        
        # ê°€ìƒì˜ ì˜ì‚¬ê²°ì • ê²°ê³¼ ê¸°ë¡
        import random
        success = random.random() > 0.3
        confidence = random.uniform(0.5, 0.9)
        satisfaction = random.uniform(0.4, 0.9)
        
        controller.record_decision_outcome(success, confidence, satisfaction, 2.5)
        
        print(f"ğŸ”„ ê²°ê³¼ ê¸°ë¡: ì„±ê³µ={success}, ì‹ ë¢°ë„={confidence:.3f}, ë§Œì¡±ë„={satisfaction:.3f}")
    
    # í˜ì´ì¦ˆ ë¶„ì„ ì •ë³´
    analytics = controller.get_phase_analytics()
    print(f"\nğŸ“ˆ í˜ì´ì¦ˆ ë¶„ì„:")
    print(f"í˜„ì¬ í˜ì´ì¦ˆ: {analytics['current_state']['phase']}")
    print(f"ì„±ê³µë¥ : {analytics['current_state']['success_rate']:.3f}")
    print(f"í‰ê·  ì‹ ë¢°ë„: {analytics['current_state']['average_confidence']:.3f}")
    print(f"ì‚¬ìš©ì ë§Œì¡±ë„: {analytics['current_state']['user_satisfaction']:.3f}")
    
    # ìë™ í˜ì´ì¦ˆ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸
    print(f"\nğŸ¤– ìë™ í˜ì´ì¦ˆ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸")
    auto_manager = AutoPhaseManager(controller, check_interval=1.0)
    auto_manager.start()
    
    # ì ì‹œ ëŒ€ê¸° í›„ ì¤‘ì§€
    time.sleep(3)
    auto_manager.stop()
    
    print("âœ… í˜ì´ì¦ˆ ì»¨íŠ¸ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    return controller


if __name__ == "__main__":
    test_phase_controller()
