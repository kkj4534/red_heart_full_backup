"""
ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„° - Red Heart AI System
Optimized Dimension Adapter for Red Heart AI System

í†µì¼ëœ ì°¨ì› ë³€í™˜ ì „ëµìœ¼ë¡œ ì •ë³´ ì†ì‹¤ ìµœì†Œí™” ë° ê³„ì‚° íš¨ìœ¨ì„± í–¥ìƒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)

class OptimizedDimensionAdapter(nn.Module):
    """
    ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„°
    
    íŠ¹ì§•:
    - í‘œì¤€ internal dimension (1024) ì‚¬ìš©
    - Residual connectionìœ¼ë¡œ ì •ë³´ ì†ì‹¤ ìµœì†Œí™”
    - ì ì§„ì  ì°¨ì› ë³€í™˜ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ
    - ê° í—¤ë“œë³„ ë§ì¶¤í˜• ì°¨ì› ì§€ì›
    """
    
    def __init__(self, 
                 backbone_dim: int = 1280,
                 target_dim: Optional[int] = None,
                 standard_dim: int = 1024,
                 use_residual: bool = True,
                 dropout_rate: float = 0.1):
        """
        Args:
            backbone_dim: ë°±ë³¸ ì°¨ì› (ê¸°ë³¸ 1280)
            target_dim: íƒ€ê²Ÿ í—¤ë“œì˜ ìš”êµ¬ ì°¨ì› (Noneì´ë©´ standard_dim ì‚¬ìš©)
            standard_dim: í‘œì¤€ internal ì°¨ì› (ê¸°ë³¸ 1024)
            use_residual: residual connection ì‚¬ìš© ì—¬ë¶€
            dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        """
        super().__init__()
        
        self.backbone_dim = backbone_dim
        self.target_dim = target_dim or standard_dim
        self.standard_dim = standard_dim
        self.use_residual = use_residual
        
        # 1. Backbone â†’ Standard ë³€í™˜ (1280 â†’ 1024)
        self.backbone_to_standard = nn.Sequential(
            nn.Linear(backbone_dim, standard_dim),
            nn.LayerNorm(standard_dim),
            nn.GELU(),
            nn.Dropout(dropout_rate)
        )
        
        # 2. Standard â†’ Target ë³€í™˜ (í•„ìš”ì‹œ)
        if self.target_dim != standard_dim:
            self.standard_to_target = nn.Sequential(
                nn.Linear(standard_dim, self.target_dim),
                nn.LayerNorm(self.target_dim),
                nn.GELU(),
                nn.Dropout(dropout_rate)
            )
            
            # 3. Target â†’ Standard ì—­ë³€í™˜ (í•„ìš”ì‹œ)
            self.target_to_standard = nn.Sequential(
                nn.Linear(self.target_dim, standard_dim),
                nn.LayerNorm(standard_dim),
                nn.GELU(),
                nn.Dropout(dropout_rate)
            )
        else:
            self.standard_to_target = None
            self.target_to_standard = None
        
        # 4. Standard â†’ Backbone ì—­ë³€í™˜ (1024 â†’ 1280)
        self.standard_to_backbone = nn.Sequential(
            nn.Linear(standard_dim, backbone_dim),
            nn.LayerNorm(backbone_dim)
        )
        
        # 5. Residual connectionìš© projection (ì°¨ì›ì´ ë‹¤ë¥¼ ë•Œ)
        if use_residual and backbone_dim != backbone_dim:  # ì‹¤ì œë¡œëŠ” ê°™ì§€ë§Œ ì¼ë°˜í™”ìš©
            self.residual_proj = nn.Linear(backbone_dim, backbone_dim)
        else:
            self.residual_proj = None
            
        logger.info(f"OptimizedDimensionAdapter ì´ˆê¸°í™”: {backbone_dim}â†’{standard_dim}â†’{self.target_dim}â†’{standard_dim}â†’{backbone_dim}")
    
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """
        ì…ë ¥ ì¸ì½”ë”©: backbone_dim â†’ target_dim
        
        Args:
            x: ë°±ë³¸ ì¶œë ¥ í…ì„œ [batch_size, seq_len, backbone_dim]
            
        Returns:
            íƒ€ê²Ÿ ì°¨ì›ìœ¼ë¡œ ë³€í™˜ëœ í…ì„œ [batch_size, seq_len, target_dim]
        """
        # 1. Backbone â†’ Standard
        standard = self.backbone_to_standard(x)
        
        # 2. Standard â†’ Target (í•„ìš”ì‹œ)  
        if self.standard_to_target is not None:
            target = self.standard_to_target(standard)
            return target
        else:
            return standard
    
    def decode(self, x: torch.Tensor, original_input: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        ì¶œë ¥ ë””ì½”ë”©: target_dim â†’ backbone_dim
        
        Args:
            x: í—¤ë“œ ì¶œë ¥ í…ì„œ [batch_size, seq_len, target_dim]
            original_input: residual connectionìš© ì›ë³¸ ì…ë ¥ (ì„ íƒì‚¬í•­)
            
        Returns:
            ë°±ë³¸ ì°¨ì›ìœ¼ë¡œ ë³µì›ëœ í…ì„œ [batch_size, seq_len, backbone_dim]
        """
        # 1. Target â†’ Standard (í•„ìš”ì‹œ)
        if self.target_to_standard is not None:
            standard = self.target_to_standard(x)
        else:
            standard = x
        
        # 2. Standard â†’ Backbone
        output = self.standard_to_backbone(standard)
        
        # 3. Residual connection (ì„ íƒì‚¬í•­)
        if self.use_residual and original_input is not None:
            if self.residual_proj is not None:
                residual = self.residual_proj(original_input)
            else:
                residual = original_input
            output = output + residual
            
        return output
    
    def forward(self, x: torch.Tensor, head_function: Optional[callable] = None) -> Dict[str, torch.Tensor]:
        """
        ì „ì²´ forward pass: encode â†’ head processing â†’ decode
        
        Args:
            x: ì…ë ¥ í…ì„œ [batch_size, seq_len, backbone_dim]
            head_function: í—¤ë“œ ì²˜ë¦¬ í•¨ìˆ˜ (ì„ íƒì‚¬í•­)
            
        Returns:
            ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ {'encoded', 'processed', 'decoded'}
        """
        # ì¸ì½”ë”©
        encoded = self.encode(x)
        
        # í—¤ë“œ ì²˜ë¦¬ (ì œê³µëœ ê²½ìš°)
        if head_function is not None:
            processed = head_function(encoded)
        else:
            processed = encoded
            
        # ë””ì½”ë”©
        decoded = self.decode(processed, original_input=x)
        
        return {
            'encoded': encoded,
            'processed': processed, 
            'decoded': decoded
        }
    
    def get_dimension_info(self) -> Dict[str, int]:
        """ì°¨ì› ì •ë³´ ë°˜í™˜"""
        return {
            'backbone_dim': self.backbone_dim,
            'standard_dim': self.standard_dim,
            'target_dim': self.target_dim,
            'parameter_count': sum(p.numel() for p in self.parameters())
        }


class HeadSpecificAdapters:
    """ê° í—¤ë“œë³„ ìµœì í™”ëœ ì–´ëŒ‘í„° íŒ©í† ë¦¬"""
    
    @staticmethod
    def create_emotion_adapter() -> OptimizedDimensionAdapter:
        """ê°ì • í—¤ë“œìš© ì–´ëŒ‘í„° (1280â†’1024â†’1280)"""
        return OptimizedDimensionAdapter(
            backbone_dim=1280,
            target_dim=1024,
            use_residual=True,
            dropout_rate=0.1
        )
    
    @staticmethod  
    def create_bentham_adapter() -> OptimizedDimensionAdapter:
        """ë²¤ë‹´ í—¤ë“œìš© ì–´ëŒ‘í„° (1280â†’768â†’1280)"""
        return OptimizedDimensionAdapter(
            backbone_dim=1280,
            target_dim=768,
            use_residual=True,
            dropout_rate=0.1
        )
    
    @staticmethod
    def create_semantic_adapter() -> OptimizedDimensionAdapter:
        """ì˜ë¯¸ í—¤ë“œìš© ì–´ëŒ‘í„° (1280â†’512â†’1280)"""
        return OptimizedDimensionAdapter(
            backbone_dim=1280,
            target_dim=512,
            use_residual=True,
            dropout_rate=0.1
        )
    
    @staticmethod
    def create_regret_adapter() -> OptimizedDimensionAdapter:
        """í›„íšŒ í—¤ë“œìš© ì–´ëŒ‘í„° (1280â†’768â†’1280)"""
        return OptimizedDimensionAdapter(
            backbone_dim=1280,
            target_dim=768,
            use_residual=True,
            dropout_rate=0.15  # í›„íšŒ í—¤ë“œëŠ” ë” ê°•í•œ ì •ê·œí™”
        )
    
    @staticmethod
    def create_meta_adapter() -> OptimizedDimensionAdapter:
        """ë©”íƒ€ í†µí•© í—¤ë“œìš© ì–´ëŒ‘í„° (1280â†’256â†’1280)"""
        return OptimizedDimensionAdapter(
            backbone_dim=1280,
            target_dim=256,
            use_residual=True,
            dropout_rate=0.05  # ë©”íƒ€ í—¤ë“œëŠ” ì •ë³´ ë³´ì¡´ ìš°ì„ 
        )


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    batch_size = 4
    seq_len = 128
    backbone_dim = 1280
    
    # ë”ë¯¸ ì…ë ¥ ìƒì„±
    dummy_input = torch.randn(batch_size, seq_len, backbone_dim)
    
    # ê° í—¤ë“œë³„ ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸
    adapters = {
        'emotion': HeadSpecificAdapters.create_emotion_adapter(),
        'bentham': HeadSpecificAdapters.create_bentham_adapter(),
        'semantic': HeadSpecificAdapters.create_semantic_adapter(),
        'regret': HeadSpecificAdapters.create_regret_adapter(),
        'meta': HeadSpecificAdapters.create_meta_adapter()
    }
    
    print("=== ìµœì í™”ëœ ì°¨ì› ë³€í™˜ ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸ ===")
    
    for name, adapter in adapters.items():
        print(f"\n[{name.upper()} ADAPTER]")
        
        # ì°¨ì› ì •ë³´ ì¶œë ¥
        dim_info = adapter.get_dimension_info()
        print(f"ì°¨ì› ë³€í™˜: {dim_info['backbone_dim']} â†’ {dim_info['target_dim']} â†’ {dim_info['backbone_dim']}")
        print(f"íŒŒë¼ë¯¸í„° ìˆ˜: {dim_info['parameter_count']:,}")
        
        # Forward pass í…ŒìŠ¤íŠ¸
        with torch.no_grad():
            result = adapter.forward(dummy_input)
            
            print(f"ì…ë ¥ shape: {dummy_input.shape}")
            print(f"ì¸ì½”ë”© shape: {result['encoded'].shape}")
            print(f"ë””ì½”ë”© shape: {result['decoded'].shape}")
            
            # ì°¨ì› ì¼ê´€ì„± ê²€ì¦
            assert result['decoded'].shape == dummy_input.shape, f"{name} ì–´ëŒ‘í„° ì°¨ì› ë¶ˆì¼ì¹˜!"
            print("âœ… ì°¨ì› ì¼ê´€ì„± ê²€ì¦ í†µê³¼")
    
    print(f"\nğŸ‰ ëª¨ë“  ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")