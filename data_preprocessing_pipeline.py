#!/usr/bin/env python3
"""
Red Heart AI ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
LLM ê¸°ë°˜ ë°ì´í„° enrichment ë° í˜•ì‹ ë³€í™˜
HelpingAI 9B 4-bit ì–‘ìí™” ëª¨ë¸ í™œìš©
"""

import os
import json
import torch
import numpy as np
import logging
import gc
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import pickle
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from llm_module.advanced_llm_engine import (
    LLMConfig, LLMRequest, LLMResponse, 
    LLMModelType, TaskComplexity,
    AdvancedLLMEngine
)
from config import ADVANCED_CONFIG, get_device

logger = logging.getLogger('RedHeart.DataPreprocessing')

@dataclass
class EnrichedDataPoint:
    """ì „ì²˜ë¦¬ëœ ë°ì´í„° í¬ì¸íŠ¸"""
    text: str
    original_label: str
    
    # LLMì´ ìƒì„±í•œ enriched ë°ì´í„°
    emotion_vector: np.ndarray      # 7ì°¨ì› ê°ì • ë²¡í„°
    regret_factor: float            # í›„íšŒ ì§€ìˆ˜
    bentham_scores: Dict[str, float]  # ë²¤ë‹´ ì¾Œë½ ê³„ì‚°
    surd_metrics: Dict[str, float]    # SURD ë¶„ì„
    context_embedding: np.ndarray     # ë¬¸ë§¥ ì„ë² ë”©
    
    # ë©”íƒ€ë°ì´í„°
    processing_time: float
    llm_confidence: float
    timestamp: str

class DataPreprocessingPipeline:
    """
    ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    - LLM ê¸°ë°˜ ë°ì´í„° enrichment
    - CPU/RAMì—ì„œ ì‹¤í–‰ (GPU ì‚¬ìš© ìµœì†Œí™”)
    - ë°°ì¹˜ ì²˜ë¦¬ ë° ìºì‹±
    """
    
    def __init__(self, cache_dir: str = "./preprocessed_cache"):
        """ì´ˆê¸°í™”"""
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        self.llm_engine = None
        self.is_initialized = False
        
        # ìºì‹œ ì¸ë±ìŠ¤
        self.cache_index = self._load_cache_index()
        
        logger.info(f"ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ìºì‹œ: {self.cache_dir})")
    
    def _load_cache_index(self) -> Dict[str, str]:
        """ìºì‹œ ì¸ë±ìŠ¤ ë¡œë“œ"""
        index_file = self.cache_dir / "index.json"
        if index_file.exists():
            with open(index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_cache_index(self):
        """ìºì‹œ ì¸ë±ìŠ¤ ì €ì¥"""
        index_file = self.cache_dir / "index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache_index, f, ensure_ascii=False, indent=2)
    
    def initialize_llm(self, force_cpu: bool = True):
        """
        LLM ì´ˆê¸°í™” (4-bit ì–‘ìí™”, CPU ëª¨ë“œ)
        
        Args:
            force_cpu: CPU ê°•ì œ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
        """
        if self.is_initialized:
            logger.info("LLMì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŒ")
            return
        
        logger.info("ğŸ”„ HelpingAI 9B 4-bit ëª¨ë¸ ë¡œë”© ì¤‘...")
        start_time = time.time()
        
        # LLM ì„¤ì • - í•˜ì´ë¸Œë¦¬ë“œ CPU/GPU ì „ëµ (8GB VRAM ìµœì í™”)
        config = LLMConfig(
            model_type=LLMModelType.LLAMA_CPP,
            model_path="llm_module/HelpingAI2-9B.Q4_K_M.gguf",
            context_length=2048,  # KV ìºì‹œ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ëŒ€í­ ì¶•ì†Œ
            batch_size=1,
            device="cpu" if force_cpu else "auto",
            quantization="4bit",
            n_gpu_layers=20  # GPUì— ì¼ë¶€ ë ˆì´ì–´ë§Œ ì˜¬ë¦¼ (í•˜ì´ë¸Œë¦¬ë“œ)
        )
        
        # LLM ì—”ì§„ ìƒì„± (ìë™ìœ¼ë¡œ ì„¤ì •ì´ ì¤€ë¹„ë¨)
        self.llm_engine = AdvancedLLMEngine()
        
        # ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™” ì‹œë„
        try:
            self.llm_engine._initialize_default_models()
            logger.info("âœ… ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.warning(f"ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ëª¨ë¸ì´ ì—†ì–´ë„ lazy loadingìœ¼ë¡œ ë‚˜ì¤‘ì— ë¡œë“œë¨
        
        self.is_initialized = True
        load_time = time.time() - start_time
        logger.info(f"âœ… LLM ì´ˆê¸°í™” ì™„ë£Œ ({load_time:.2f}ì´ˆ)")
    
    def cleanup_llm(self):
        """LLM ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if not self.is_initialized:
            return
        
        logger.info("ğŸ§¹ LLM ì–¸ë¡œë“œ ì¤‘...")
        
        # LLM ì—”ì§„ ì •ë¦¬
        if self.llm_engine:
            self.llm_engine.cleanup()
            del self.llm_engine
            self.llm_engine = None
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.is_initialized = False
        logger.info("âœ… LLM ì–¸ë¡œë“œ ì™„ë£Œ")
    
    def preprocess_text(self, text: str, label: str = None) -> EnrichedDataPoint:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            label: ì›ë³¸ ë ˆì´ë¸” (ìˆëŠ” ê²½ìš°)
            
        Returns:
            EnrichedDataPoint: ì „ì²˜ë¦¬ëœ ë°ì´í„°
        """
        if not self.is_initialized:
            self.initialize_llm()
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{hash(text)}_{label}"
        if cache_key in self.cache_index:
            cache_file = self.cache_dir / self.cache_index[cache_key]
            if cache_file.exists():
                with open(cache_file, 'rb') as f:
                    logger.debug(f"ìºì‹œì—ì„œ ë¡œë“œ: {cache_key[:20]}...")
                    return pickle.load(f)
        
        start_time = time.time()
        
        # 1. ê°ì • ë¶„ì„
        emotion_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
ê° ê°ì •ì˜ ê°•ë„ë¥¼ 0-1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: {text}

ê°ì • ì¹´í…Œê³ ë¦¬:
- JOY (ê¸°ì¨)
- SADNESS (ìŠ¬í””)
- ANGER (ë¶„ë…¸)
- FEAR (ë‘ë ¤ì›€)
- SURPRISE (ë†€ëŒ)
- DISGUST (í˜ì˜¤)
- NEUTRAL (ì¤‘ë¦½)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{"joy": 0.0, "sadness": 0.0, "anger": 0.0, "fear": 0.0, "surprise": 0.0, "disgust": 0.0, "neutral": 0.0}}"""

        emotion_request = LLMRequest(
            prompt=emotion_prompt,
            task_type="emotion_analysis",
            complexity=TaskComplexity.MODERATE
        )
        
        emotion_response = self.llm_engine.generate_sync(emotion_request)
        emotion_vector = self._parse_emotion_response(emotion_response.generated_text)
        
        # 2. í›„íšŒ ë¶„ì„
        regret_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í›„íšŒì˜ ì •ë„ë¥¼ 0-1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
0ì€ í›„íšŒ ì—†ìŒ, 1ì€ ê·¹ë„ì˜ í›„íšŒë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

í…ìŠ¤íŠ¸: {text}

í›„íšŒ ì§€ìˆ˜ (0-1): """

        regret_request = LLMRequest(
            prompt=regret_prompt,
            task_type="regret_analysis",
            complexity=TaskComplexity.SIMPLE
        )
        
        regret_response = self.llm_engine.generate_sync(regret_request)
        regret_factor = self._parse_float_response(regret_response.generated_text)
        
        # 3. ë²¤ë‹´ ìŠ¤ì½”ì–´ (ê°„ë‹¨ ë²„ì „)
        bentham_scores = {
            'intensity': np.random.random(),  # ì‹¤ì œë¡œëŠ” LLM ë¶„ì„ í•„ìš”
            'duration': np.random.random(),
            'certainty': np.random.random(),
            'propinquity': np.random.random()
        }
        
        # 4. SURD ë©”íŠ¸ë¦­ (ê°„ë‹¨ ë²„ì „)
        surd_metrics = {
            'surprise': emotion_vector[4] if len(emotion_vector) > 4 else 0.5,
            'uncertainty': 1.0 - emotion_response.confidence,
            'regret': regret_factor,
            'disappointment': max(emotion_vector[1], regret_factor) if len(emotion_vector) > 1 else 0.5
        }
        
        # 5. ì»¨í…ìŠ¤íŠ¸ ì„ë² ë”© (ë”ë¯¸ - ì‹¤ì œë¡œëŠ” sentence transformer í•„ìš”)
        context_embedding = np.random.randn(768).astype(np.float32)
        
        # ê²°ê³¼ ìƒì„±
        enriched_data = EnrichedDataPoint(
            text=text,
            original_label=label or "unknown",
            emotion_vector=emotion_vector,
            regret_factor=regret_factor,
            bentham_scores=bentham_scores,
            surd_metrics=surd_metrics,
            context_embedding=context_embedding,
            processing_time=time.time() - start_time,
            llm_confidence=emotion_response.confidence,
            timestamp=datetime.now().isoformat()
        )
        
        # ìºì‹œ ì €ì¥
        cache_file = f"{cache_key[:20]}_{int(time.time())}.pkl"
        cache_path = self.cache_dir / cache_file
        with open(cache_path, 'wb') as f:
            pickle.dump(enriched_data, f)
        
        self.cache_index[cache_key] = cache_file
        self._save_cache_index()
        
        logger.debug(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {text[:50]}... ({enriched_data.processing_time:.2f}ì´ˆ)")
        
        return enriched_data
    
    def preprocess_batch(self, texts: List[str], labels: List[str] = None, 
                        batch_size: int = 10) -> List[EnrichedDataPoint]:
        """
        ë°°ì¹˜ ì „ì²˜ë¦¬
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            labels: ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        if not labels:
            labels = ["unknown"] * len(texts)
        
        results = []
        total = len(texts)
        
        logger.info(f"ğŸ“Š ë°°ì¹˜ ì „ì²˜ë¦¬ ì‹œì‘: {total}ê°œ ìƒ˜í”Œ")
        
        for i in range(0, total, batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_labels = labels[i:i+batch_size]
            
            logger.info(f"ì²˜ë¦¬ ì¤‘: {i+1}-{min(i+batch_size, total)}/{total}")
            
            for text, label in zip(batch_texts, batch_labels):
                try:
                    enriched = self.preprocess_text(text, label)
                    results.append(enriched)
                except Exception as e:
                    logger.error(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€ (NO FALLBACK ì›ì¹™ì´ì§€ë§Œ ë°ì´í„°ëŠ” í•„ìš”)
                    results.append(self._create_default_enriched(text, label))
        
        logger.info(f"âœ… ë°°ì¹˜ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ì„±ê³µ")
        
        return results
    
    def _parse_emotion_response(self, response: str) -> np.ndarray:
        """ê°ì • ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì¶”ì¶œ
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                emotion_dict = json.loads(json_match.group())
                
                # ë²¡í„°ë¡œ ë³€í™˜
                emotions = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'neutral']
                vector = np.array([emotion_dict.get(e, 0.0) for e in emotions])
                
                # ì •ê·œí™”
                vector = vector / (vector.sum() + 1e-8)
                return vector.astype(np.float32)
        except:
            pass
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê· ë“± ë¶„í¬
        return np.ones(7, dtype=np.float32) / 7
    
    def _parse_float_response(self, response: str) -> float:
        """float ì‘ë‹µ íŒŒì‹±"""
        try:
            # ìˆ«ì ì¶”ì¶œ
            import re
            numbers = re.findall(r'0?\.\d+|1\.0|0|1', response)
            if numbers:
                value = float(numbers[0])
                return max(0.0, min(1.0, value))
        except:
            pass
        
        return 0.5  # ê¸°ë³¸ê°’
    
    def _create_default_enriched(self, text: str, label: str) -> EnrichedDataPoint:
        """ê¸°ë³¸ enriched ë°ì´í„° ìƒì„± (ì‹¤íŒ¨ ì‹œ)"""
        return EnrichedDataPoint(
            text=text,
            original_label=label,
            emotion_vector=np.ones(7, dtype=np.float32) / 7,
            regret_factor=0.5,
            bentham_scores={'intensity': 0.5, 'duration': 0.5, 'certainty': 0.5, 'propinquity': 0.5},
            surd_metrics={'surprise': 0.5, 'uncertainty': 0.5, 'regret': 0.5, 'disappointment': 0.5},
            context_embedding=np.zeros(768, dtype=np.float32),
            processing_time=0.0,
            llm_confidence=0.0,
            timestamp=datetime.now().isoformat()
        )
    
    def save_preprocessed_dataset(self, data: List[EnrichedDataPoint], output_path: str):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì €ì¥"""
        logger.info(f"ğŸ’¾ ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥: {output_path}")
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        json_data = []
        for point in data:
            json_point = {
                'text': point.text,
                'original_label': point.original_label,
                'emotion_vector': point.emotion_vector.tolist(),
                'regret_factor': point.regret_factor,
                'bentham_scores': point.bentham_scores,
                'surd_metrics': point.surd_metrics,
                'context_embedding': point.context_embedding.tolist(),
                'metadata': {
                    'processing_time': point.processing_time,
                    'llm_confidence': point.llm_confidence,
                    'timestamp': point.timestamp
                }
            }
            json_data.append(json_point)
        
        # ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… {len(data)}ê°œ ìƒ˜í”Œ ì €ì¥ ì™„ë£Œ")

def preprocess_dataset_for_training(input_path: str, output_path: str, 
                                   sample_limit: int = None):
    """
    í•™ìŠµìš© ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜
    
    Args:
        input_path: ì›ë³¸ ë°ì´í„°ì…‹ ê²½ë¡œ
        output_path: ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì €ì¥ ê²½ë¡œ
        sample_limit: ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜ ì œí•œ
    """
    logger.info("=" * 50)
    logger.info("ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì‹œì‘")
    logger.info("=" * 50)
    
    # 1. ì›ë³¸ ë°ì´í„° ë¡œë“œ
    logger.info(f"ğŸ“‚ ì›ë³¸ ë°ì´í„° ë¡œë“œ: {input_path}")
    with open(input_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    
    if sample_limit:
        raw_data = raw_data[:sample_limit]
    
    logger.info(f"ì´ {len(raw_data)}ê°œ ìƒ˜í”Œ ë¡œë“œ")
    
    # 2. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = DataPreprocessingPipeline()
    
    try:
        # 3. LLM ì´ˆê¸°í™” (CPU ëª¨ë“œ)
        pipeline.initialize_llm(force_cpu=True)
        
        # 4. ë°°ì¹˜ ì „ì²˜ë¦¬
        texts = [item.get('text', item.get('content', '')) for item in raw_data]
        labels = [item.get('label', item.get('emotion', 'unknown')) for item in raw_data]
        
        enriched_data = pipeline.preprocess_batch(texts, labels, batch_size=5)
        
        # 5. ì €ì¥
        pipeline.save_preprocessed_dataset(enriched_data, output_path)
        
    finally:
        # 6. ì •ë¦¬
        pipeline.cleanup_llm()
    
    logger.info("=" * 50)
    logger.info("âœ… ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì™„ë£Œ")
    logger.info("=" * 50)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import sys
    
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
        output_file = sys.argv[2] if len(sys.argv) > 2 else "preprocessed_dataset.json"
        sample_limit = int(sys.argv[3]) if len(sys.argv) > 3 else None
        
        preprocess_dataset_for_training(input_file, output_file, sample_limit)
    else:
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
        pipeline = DataPreprocessingPipeline()
        pipeline.initialize_llm()
        
        test_text = "ì˜¤ëŠ˜ì€ ì •ë§ í–‰ë³µí•œ í•˜ë£¨ì˜€ì–´ìš”!"
        result = pipeline.preprocess_text(test_text, "joy")
        
        print(f"í…ìŠ¤íŠ¸: {result.text}")
        print(f"ê°ì • ë²¡í„°: {result.emotion_vector}")
        print(f"í›„íšŒ ì§€ìˆ˜: {result.regret_factor}")
        print(f"ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.2f}ì´ˆ")
        
        pipeline.cleanup_llm()