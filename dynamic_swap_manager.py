"""
Red Heart ë™ì  ìŠ¤ì™‘ ë§¤ë‹ˆì € - LLM ìŠ¤íƒ€ì¼ RAM ìŠ¤ì™‘ ì‹œìŠ¤í…œ
Dynamic Swap Manager for Red Heart - LLM-style RAM Swap System

800M íŒŒë¼ë¯¸í„°ë¥¼ 8GB GPUì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬:
- 300M ë°±ë³¸: í•­ìƒ GPU ìƒì£¼
- 500M í—¤ë“œë“¤: í•„ìš”ì‹œì—ë§Œ GPU ìŠ¤ì™‘
- ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”©ìœ¼ë¡œ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
- ì••ì¶• ìŠ¤ì™‘ìœ¼ë¡œ ì†ë„ í–¥ìƒ
"""

import asyncio
import logging
import time
import torch
import torch.nn as nn
import pickle
import gzip
import threading
from typing import Dict, List, Any, Optional, Tuple, Union, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
import psutil
import gc
from collections import deque, defaultdict
from enum import Enum
import io
import queue
import weakref

from config import ADVANCED_CONFIG, get_gpu_memory_info, get_smart_device
from workflow_aware_memory_manager import (
    WorkflowAwareMemoryManager, WorkflowStage, WorkflowTracker
)

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

class SwapLocation(Enum):
    """ìŠ¤ì™‘ ìœ„ì¹˜"""
    GPU = "gpu"
    RAM = "ram"
    DISK = "disk"

class SwapPriority(Enum):
    """ìŠ¤ì™‘ ìš°ì„ ìˆœìœ„"""
    CRITICAL = "critical"    # ë°±ë³¸ (í•­ìƒ GPU)
    HIGH = "high"           # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í—¤ë“œ
    MEDIUM = "medium"       # ê³§ í•„ìš”í•  í—¤ë“œ
    LOW = "low"             # ë‹¹ë¶„ê°„ ë¶ˆí•„ìš”í•œ í—¤ë“œ
    WORKFLOW = "workflow"   # ì›Œí¬í”Œë¡œìš° ë³´í˜¸ ëª¨ë¸
    DYNAMIC = "dynamic"     # ë™ì  ì ìˆ˜ ê¸°ë°˜ ìš°ì„ ìˆœìœ„

@dataclass
class SwapableModel:
    """ìŠ¤ì™‘ ê°€ëŠ¥í•œ ëª¨ë¸ ì •ë³´"""
    name: str
    model: Optional[nn.Module]  # ì‹¤ì œ nn.Moduleë§Œ í—ˆìš© (NO FALLBACK)
    location: SwapLocation
    priority: SwapPriority
    last_access: datetime
    size_mb: float
    compressed_data: Optional[bytes] = None
    access_count: int = 0
    swap_count: int = 0
    status: str = 'ready'  # í•­ìƒ 'ready' (NO FALLBACK)
    last_bind_attempt: Optional[datetime] = None
    priority_score: float = 50.0  # ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ë™ì  ì ìˆ˜ (0-100)
    workflow_group: Optional[str] = None  # ì—°ê³„ëœ ëª¨ë“ˆ ê·¸ë£¹
    avoid_unload: bool = False  # ì ìˆ˜ê°€ ë§¤ìš° ë†’ì„ ë•Œ ì–¸ë¡œë“œ íšŒí”¼ í”Œë˜ê·¸
    
    def __post_init__(self):
        self.last_access = datetime.now()

class TaskSequencePredictor:
    """íƒœìŠ¤í¬ ì‹œí€€ìŠ¤ ì˜ˆì¸¡ê¸°"""
    
    def __init__(self, history_size: int = 1000):
        self.history = deque(maxlen=history_size)
        self.patterns = defaultdict(lambda: defaultdict(int))
        self.window_size = 3
        
    def record_task(self, task: str):
        """íƒœìŠ¤í¬ ê¸°ë¡"""
        self.history.append((task, datetime.now()))
        self._update_patterns()
        
    def _update_patterns(self):
        """íŒ¨í„´ ì—…ë°ì´íŠ¸"""
        if len(self.history) < self.window_size + 1:
            return
            
        # ìµœê·¼ window_sizeê°œ íƒœìŠ¤í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ íƒœìŠ¤í¬ ì˜ˆì¸¡ íŒ¨í„´ í•™ìŠµ
        recent_tasks = [item[0] for item in list(self.history)[-self.window_size-1:]]
        context = tuple(recent_tasks[:-1])
        next_task = recent_tasks[-1]
        
        self.patterns[context][next_task] += 1
        
    def predict_next_tasks(self, current_task: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """ë‹¤ìŒ íƒœìŠ¤í¬ë“¤ ì˜ˆì¸¡"""
        if len(self.history) < self.window_size:
            return []
            
        # ìµœê·¼ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        recent_tasks = [item[0] for item in list(self.history)[-self.window_size+1:]]
        recent_tasks.append(current_task)
        context = tuple(recent_tasks)
        
        # ì˜ˆì¸¡
        predictions = self.patterns.get(context, {})
        if not predictions:
            return []
            
        # í™•ë¥  ê³„ì‚° ë° ì •ë ¬
        total_count = sum(predictions.values())
        sorted_predictions = sorted(
            [(task, count/total_count) for task, count in predictions.items()],
            key=lambda x: x[1], reverse=True
        )
        
        return sorted_predictions[:top_k]

class ModelCompressor:
    """ëª¨ë¸ ì••ì¶•ê¸°"""
    
    @staticmethod
    def compress_model(model: nn.Module) -> bytes:
        """ëª¨ë¸ì„ ì••ì¶•ëœ ë°”ì´íŠ¸ë¡œ ë³€í™˜"""
        start_time = time.time()
        
        # ëª¨ë¸ ìƒíƒœë¥¼ ë°”ì´íŠ¸ë¡œ ì§ë ¬í™”
        buffer = io.BytesIO()
        torch.save(model.state_dict(), buffer)
        buffer.seek(0)
        
        # gzip ì••ì¶•
        compressed_data = gzip.compress(buffer.getvalue())
        
        compression_time = time.time() - start_time
        original_size = len(buffer.getvalue())
        compressed_size = len(compressed_data)
        compression_ratio = original_size / compressed_size
        
        logger.debug(f"ëª¨ë¸ ì••ì¶• ì™„ë£Œ: {original_size/1024/1024:.1f}MB -> "
                    f"{compressed_size/1024/1024:.1f}MB "
                    f"(ë¹„ìœ¨: {compression_ratio:.1f}x, ì‹œê°„: {compression_time:.3f}s)")
        
        return compressed_data
    
    @staticmethod
    def decompress_model(compressed_data: bytes, model_template: nn.Module) -> nn.Module:
        """ì••ì¶•ëœ ë°ì´í„°ì—ì„œ ëª¨ë¸ ë³µì›"""
        start_time = time.time()
        
        # ì••ì¶• í•´ì œ
        decompressed_data = gzip.decompress(compressed_data)
        
        # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
        buffer = io.BytesIO(decompressed_data)
        state_dict = torch.load(buffer, map_location='cpu')
        
        # ìƒˆ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ìƒíƒœ ë¡œë“œ
        model_template.load_state_dict(state_dict)
        
        decompression_time = time.time() - start_time
        logger.debug(f"ëª¨ë¸ ì••ì¶• í•´ì œ ì™„ë£Œ: ì‹œê°„ {decompression_time:.3f}s")
        
        return model_template

class AsyncModelSwapper:
    """ë¹„ë™ê¸° ëª¨ë¸ ìŠ¤ì™‘í¼"""
    
    def __init__(self, max_workers: int = 2):
        self.max_workers = max_workers
        self.swap_queue = asyncio.Queue()
        self.workers = []
        self.running = False
        
    async def start(self):
        """ìŠ¤ì™‘í¼ ì‹œì‘"""
        self.running = True
        for i in range(self.max_workers):
            worker = asyncio.create_task(self._swap_worker(f"worker_{i}"))
            self.workers.append(worker)
        logger.info(f"ë¹„ë™ê¸° ìŠ¤ì™‘í¼ ì‹œì‘: {self.max_workers}ê°œ ì›Œì»¤")
        
    async def stop(self):
        """ìŠ¤ì™‘í¼ ì¤‘ì§€"""
        self.running = False
        
        # ëª¨ë“  ì›Œì»¤ì— ì¤‘ì§€ ì‹ í˜¸
        for _ in self.workers:
            await self.swap_queue.put(None)
            
        # ì›Œì»¤ë“¤ ì™„ë£Œ ëŒ€ê¸°
        await asyncio.gather(*self.workers, return_exceptions=True)
        self.workers.clear()
        logger.info("ë¹„ë™ê¸° ìŠ¤ì™‘í¼ ì¤‘ì§€")
        
    async def _swap_worker(self, worker_name: str):
        """ìŠ¤ì™‘ ì›Œì»¤"""
        logger.debug(f"ìŠ¤ì™‘ ì›Œì»¤ {worker_name} ì‹œì‘")
        
        while self.running:
            try:
                # ìŠ¤ì™‘ ì‘ì—… ëŒ€ê¸°
                swap_task = await asyncio.wait_for(self.swap_queue.get(), timeout=1.0)
                
                if swap_task is None:  # ì¤‘ì§€ ì‹ í˜¸
                    break
                    
                # ìŠ¤ì™‘ ì‹¤í–‰
                await swap_task()
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"ìŠ¤ì™‘ ì›Œì»¤ {worker_name} ì˜¤ë¥˜: {str(e)}")
                
        logger.debug(f"ìŠ¤ì™‘ ì›Œì»¤ {worker_name} ì¢…ë£Œ")
        
    async def schedule_swap(self, swap_coroutine):
        """ìŠ¤ì™‘ ì‘ì—… ì˜ˆì•½"""
        await self.swap_queue.put(swap_coroutine)

class RedHeartDynamicSwapManager:
    """
    Red Heart ë™ì  ìŠ¤ì™‘ ë§¤ë‹ˆì €
    
    LLM ìŠ¤íƒ€ì¼ ì ê·¹ì  RAM ìŠ¤ì™‘ìœ¼ë¡œ 800M íŒŒë¼ë¯¸í„°ë¥¼ 8GB GPUì—ì„œ ì²˜ë¦¬
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or ADVANCED_CONFIG.get('dynamic_swap_config', {})
        
        # ê¸°ë³¸ ìš°ì„ ìˆœìœ„ í…Œì´ë¸” - ë™ì  ìš°ì„ ìˆœìœ„ ì§€ì›
        # ì´ˆê¸°ê°’ë§Œ ì œê³µ, ì‹¤ì œë¡œëŠ” ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ë³„ priority_scoreë¡œ ë™ì  ì¡°ì •ë¨
        self.DEFAULT_PRIORITIES = {
            "unified_backbone": SwapPriority.HIGH,  # ê¸°ë³¸ HIGH, ì›Œí¬í”Œë¡œìš°ì—ì„œ ì ìˆ˜ 95ë¡œ ìƒí–¥
            "translator": SwapPriority.HIGH,
            "emotion_analyzer": SwapPriority.HIGH,
            "emotion_empathy_head": SwapPriority.HIGH,  # emotion_analyzerì™€ ì—°ë™
            "semantic_analyzer": SwapPriority.HIGH,
            "bentham_calculator": SwapPriority.HIGH,
            "regret_analyzer": SwapPriority.MEDIUM,
            "neural_components": SwapPriority.LOW,
            "meta_integration": SwapPriority.MEDIUM,
            "surd_analyzer": SwapPriority.MEDIUM,
            "bayesian_engine": SwapPriority.LOW,
            "llm_engine": SwapPriority.LOW,
            "experience_database": SwapPriority.LOW,
            "hierarchical_emotion": SwapPriority.MEDIUM,
            "usage_pattern_analyzer": SwapPriority.LOW,
            # í—¤ë“œë“¤
            "emotion_empathy_head": SwapPriority.HIGH,
            "bentham_fromm_head": SwapPriority.HIGH,
            "semantic_surd_head": SwapPriority.HIGH,
            "regret_learning_head": SwapPriority.MEDIUM,
            "meta_integration_head": SwapPriority.MEDIUM,
        }
        
        # ìŠ¤ì™‘ ê´€ë¦¬
        self.models: Dict[str, SwapableModel] = {}
        self.gpu_resident_models: Dict[str, nn.Module] = {}
        self.ram_models: Dict[str, SwapableModel] = {}
        
        # ì˜ˆì¸¡ ë° ìµœì í™” ì»´í¬ë„ŒíŠ¸
        self.task_predictor = TaskSequencePredictor()
        self.model_compressor = ModelCompressor()
        self.async_swapper = AsyncModelSwapper()
        
        # ì›Œí¬í”Œë¡œìš° ì¸ì‹ ë©”ëª¨ë¦¬ ê´€ë¦¬ì í†µí•©
        self.workflow_memory_manager = WorkflowAwareMemoryManager(
            memory_threshold_mb=self.config.get('memory_threshold_mb', 6500.0)
        )
        
        # ì„¤ì •
        self.memory_threshold = self.config.get('memory_threshold', 0.65)
        self.swap_timeout = self.config.get('swap_timeout', 2.0)
        self.compression_enabled = self.config.get('compression_enabled', True)
        self.async_swap = self.config.get('async_swap', True)
        self.preload_prediction = self.config.get('preload_prediction', True)
        self.workflow_aware = self.config.get('workflow_aware', True)  # ì›Œí¬í”Œë¡œìš° ì¸ì‹ ëª¨ë“œ
        
        # í†µê³„
        self.stats = {
            'total_swaps': 0,
            'successful_swaps': 0,
            'failed_swaps': 0,
            'total_swap_time': 0.0,
            'cache_hits': 0,
            'cache_misses': 0,
            'preload_hits': 0,
            'preload_misses': 0
        }
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
        self._background_tasks: List[asyncio.Task] = []
        self._shutdown_event = asyncio.Event()
        
        logger.info("RedHeartDynamicSwapManager ì´ˆê¸°í™” ì™„ë£Œ")
        
    async def initialize(self):
        """ìŠ¤ì™‘ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        if self.async_swap:
            await self.async_swapper.start()
            
        # ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‘ì—… ì‹œì‘
        cleanup_task = asyncio.create_task(self._background_cleanup())
        self._background_tasks.append(cleanup_task)
        
        logger.info("ë™ì  ìŠ¤ì™‘ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        
    async def shutdown(self):
        """ìŠ¤ì™‘ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self._shutdown_event.set()
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ë“¤ ì •ë¦¬
        for task in self._background_tasks:
            task.cancel()
        await asyncio.gather(*self._background_tasks, return_exceptions=True)
        
        if self.async_swap:
            await self.async_swapper.stop()
            
        logger.info("ë™ì  ìŠ¤ì™‘ ë§¤ë‹ˆì € ì¢…ë£Œ ì™„ë£Œ")
        
    def register_model(self, name: str, model: nn.Module, priority: SwapPriority = None):
        """ëª¨ë¸ ë“±ë¡ - NO FALLBACK ì •ì±… (nn.Module í•„ìˆ˜)
        
        Args:
            name: ëª¨ë¸ ì´ë¦„
            model: ì‹¤ì œ nn.Module ì¸ìŠ¤í„´ìŠ¤ (í•„ìˆ˜)
            priority: ìš°ì„ ìˆœìœ„ (Noneì´ë©´ DEFAULT_PRIORITIESì—ì„œ ì¡°íšŒ)
        """
        # NO FALLBACK - modelì´ Noneì´ê±°ë‚˜ nn.Moduleì´ ì•„ë‹ˆë©´ ì¦‰ì‹œ ì˜ˆì™¸
        if model is None:
            raise RuntimeError(f"[DSM] {name} ë“±ë¡ ì‹¤íŒ¨: model is None (NO FALLBACK)")
        if not isinstance(model, nn.Module):
            raise RuntimeError(f"[DSM] {name} ë“±ë¡ ì‹¤íŒ¨: nn.Moduleì´ ì•„ë‹˜ ({type(model).__name__}) - NO FALLBACK")
            
        # ìš°ì„ ìˆœìœ„ ìë™ ì„¤ì •
        if priority is None:
            priority = self.DEFAULT_PRIORITIES.get(name, SwapPriority.MEDIUM)
            logger.debug(f"DEFAULT_PRIORITIESì—ì„œ {name}ì˜ ìš°ì„ ìˆœìœ„ ì„¤ì •: {priority.value}")
            
        # ì‹¤ì œ ëª¨ë¸ í¬ê¸° ê³„ì‚° (NO ì¶”ì •ì¹˜, NO ë©”íƒ€ ë“±ë¡)
        size_mb = self._calculate_model_size(model)
        
        # size_mb ê²€ì¦ (0 ì´í•˜ë©´ ë“±ë¡ ë¶ˆê°€)
        if size_mb <= 0:
            logger.error(f"[DSM] {name} íŒŒë¼ë¯¸í„° ì—†ìŒ: size_mb={size_mb:.1f}MB")
            raise RuntimeError(f"[DSM] {name} ë“±ë¡ ì‹¤íŒ¨: íŒŒë¼ë¯¸í„° ì—†ìŒ (size={size_mb:.1f}MB)")
        
        logger.info(f"[DSM] {name} í¬ê¸°: {size_mb:.1f}MB (mgr_id={id(self)})")
        
        # ëª¨ë¸ì˜ í˜„ì¬ deviceë¥¼ í™•ì¸í•˜ì—¬ location ê²°ì •
        is_cuda = any(p.is_cuda for p in model.parameters()) if hasattr(model, 'parameters') else False
        location = SwapLocation.GPU if is_cuda else SwapLocation.RAM
        
        swapable_model = SwapableModel(
            name=name,
            model=model,  # í•­ìƒ nn.Module
            location=location,  # ì‹¤ì œ deviceì— ë”°ë¼ ì„¤ì •
            priority=priority,
            last_access=datetime.now(),
            size_mb=size_mb,
            status='ready'  # í•­ìƒ ready (NO deferred)
        )
            
        # ê¸°ì¡´ í‚¤ê°€ ìˆìœ¼ë©´ êµì²´ ë¡œê¹…
        if name in self.models:
            old_model = self.models[name]
            old_size = old_model.size_mb
            old_priority = old_model.priority.name if old_model.priority else "NONE"
            new_priority = priority.name if priority else "NONE"
            logger.info(f"[DSM] êµì²´ ë“±ë¡: {name} ({old_size:.1f}MBâ†’{size_mb:.1f}MB, {old_priority}â†’{new_priority})")
        
        self.models[name] = swapable_model
        
        # locationê³¼ gpu_resident_modelsë¥¼ ì¼ê´€ë˜ê²Œ ê´€ë¦¬
        if is_cuda:
            self.gpu_resident_models[name] = model
            logger.info(f"   âœ… {name}ì„ GPUì— ë“±ë¡ (location=GPU, size={size_mb:.1f}MB)")
        else:
            self.ram_models[name] = swapable_model
            # GPUì—ì„œ ì œê±° (í˜¹ì‹œ ìˆì„ ìˆ˜ ìˆìŒ)
            self.gpu_resident_models.pop(name, None)
            logger.info(f"   ğŸ’¾ {name}ì„ RAMì— ë“±ë¡ (location=RAM, size={size_mb:.1f}MB)")
        
        # master_model_registryì—ë„ ë“±ë¡ (ì–¸ë¡œë“œ í›„ë³´ë¡œ ì‚¬ìš©)
        from config import get_master_orchestrator, ModelPriority
        orchestrator = get_master_orchestrator()
        if orchestrator:
            # SwapPriorityë¥¼ ModelPriorityë¡œ ë§¤í•‘
            priority_map = {
                SwapPriority.CRITICAL: ModelPriority.CRITICAL,
                SwapPriority.HIGH: ModelPriority.HIGH,
                SwapPriority.MEDIUM: ModelPriority.MEDIUM,
                SwapPriority.LOW: ModelPriority.LOW
            }
            model_priority = priority_map.get(priority, ModelPriority.MEDIUM)
            
            orchestrator.master_model_registry[name] = {
                'device': 'cuda' if is_cuda else 'cpu',
                'memory_mb': size_mb,
                'priority': model_priority,
                'access_count': 0,
                'last_access': datetime.now()
            }
            logger.debug(f"   master_model_registryì— {name} ë“±ë¡ (priority={model_priority})")
        
        # ì••ì¶• ë°ì´í„° ìƒì„± (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
        if self.compression_enabled:
            asyncio.create_task(self._compress_model_background(name))
            
        logger.info(f"[REGISTER HEAD] {name} ({size_mb:.1f}MB, ìš°ì„ ìˆœìœ„: {priority.value})")
    
    def update_workflow_priorities(self, workflow_stage: WorkflowStage, required_models: List[str], 
                                 related_groups: Dict[str, List[str]] = None):
        """ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ë™ì  ìš°ì„ ìˆœìœ„ ì—…ë°ì´íŠ¸
        
        Args:
            workflow_stage: í˜„ì¬ ì›Œí¬í”Œë¡œìš° ë‹¨ê³„
            required_models: í˜„ ë‹¨ê³„ì—ì„œ í•„ìš”í•œ ëª¨ë¸ë“¤
            related_groups: ì—°ê³„ëœ ëª¨ë“ˆ ê·¸ë£¹ {ê·¸ë£¹ëª…: [ëª¨ë¸ë“¤]}
        """
        related_groups = related_groups or {}
        
        # ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ë³„ ê¸°ë³¸ ì ìˆ˜
        stage_base_scores = {
            WorkflowStage.INITIALIZATION: 30.0,
            WorkflowStage.DATA_LOADING: 40.0,
            WorkflowStage.BACKBONE_FORWARD: 85.0,  # ë°±ë³¸ ë‹¨ê³„ì—ì„œëŠ” ë†’ì€ ì ìˆ˜
            WorkflowStage.HEAD_PROCESSING: 80.0,
            WorkflowStage.SYNERGY_COMPUTATION: 75.0,
            WorkflowStage.LOSS_COMPUTATION: 70.0,
            WorkflowStage.BACKWARD_PASS: 85.0,
            WorkflowStage.OPTIMIZATION: 60.0,
            WorkflowStage.EVALUATION: 50.0,
            WorkflowStage.FINALIZATION: 30.0
        }
        
        base_score = stage_base_scores.get(workflow_stage, 50.0)
        
        # ëª¨ë“  ëª¨ë¸ì˜ ì ìˆ˜ë¥¼ ì´ˆê¸°í™” (ê¸°ë³¸ ë‚®ì€ ì ìˆ˜)
        for name, model in self.models.items():
            model.priority_score = 20.0  # ì‚¬ìš© ì•ˆ í•˜ëŠ” ëª¨ë¸ì€ ë‚®ì€ ì ìˆ˜
        
        # í•„ìš”í•œ ëª¨ë¸ë“¤ì— ë†’ì€ ì ìˆ˜ ë¶€ì—¬
        for model_name in required_models:
            if model_name in self.models:
                self.models[model_name].priority_score = base_score
                self.models[model_name].workflow_group = f"{workflow_stage.value}_primary"
                
                # ë°±ë³¸ì€ í•­ìƒ ë” ë†’ì€ ì ìˆ˜
                if model_name == "unified_backbone" and workflow_stage in [
                    WorkflowStage.BACKBONE_FORWARD, WorkflowStage.BACKWARD_PASS
                ]:
                    self.models[model_name].priority_score = 95.0
        
        # ì—°ê³„ëœ ê·¸ë£¹ì— ê°™ì€ ì ìˆ˜ ë¶€ì—¬
        for group_name, group_models in related_groups.items():
            group_score = base_score - 5.0  # ì£¼ ëª¨ë¸ë³´ë‹¤ ì•½ê°„ ë‚®ì€ ì ìˆ˜
            for model_name in group_models:
                if model_name in self.models:
                    self.models[model_name].priority_score = group_score
                    self.models[model_name].workflow_group = group_name
        
        # ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ ì¬ì„¤ì • (ìˆœìˆ˜ ì ìˆ˜ì œ)
        for name, model in self.models.items():
            # ì ìˆ˜ë§Œìœ¼ë¡œ ìš°ì„ ìˆœìœ„ ê²°ì • (CRITICAL ì œê±°, ë™ì  ì‹œìŠ¤í…œ ì¼ê´€í™”)
            if model.priority_score >= 90.0:
                # 90ì  ì´ìƒì€ ê·¹íˆ ì¤‘ìš” (ë°±ë³¸ ë“±) - í•˜ì§€ë§Œ ì ˆëŒ€ ì–¸ë¡œë“œ ë¶ˆê°€ëŠ” ì•„ë‹˜
                model.priority = SwapPriority.HIGH
                # ì ìˆ˜ê°€ ë§¤ìš° ë†’ìœ¼ë©´ ì–¸ë¡œë“œ íšŒí”¼ í”Œë˜ê·¸ ì„¤ì •
                model.avoid_unload = True
            elif model.priority_score >= 70.0:
                model.priority = SwapPriority.HIGH
                model.avoid_unload = False
            elif model.priority_score >= 50.0:
                model.priority = SwapPriority.MEDIUM
                model.avoid_unload = False
            elif model.priority_score >= 30.0:
                model.priority = SwapPriority.LOW
                model.avoid_unload = False
            else:
                model.priority = SwapPriority.LOW
                model.avoid_unload = False
        
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] {workflow_stage.value} ë‹¨ê³„ ìš°ì„ ìˆœìœ„ ì—…ë°ì´íŠ¸")
        logger.debug(f"  í•„ìˆ˜ ëª¨ë¸: {required_models[:5]} (ì ìˆ˜: {base_score})")
        logger.debug(f"  ì—°ê³„ ê·¸ë£¹: {list(related_groups.keys())[:3]}")
    
    def get_model_priority_score(self, model_name: str) -> float:
        """ëª¨ë¸ì˜ í˜„ì¬ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ë°˜í™˜"""
        if model_name in self.models:
            return self.models[model_name].priority_score
        return 0.0
    
    def get_workflow_aware_unload_candidates(self, required_mb: float, 
                                            exclude_models: Set[str] = None) -> List[str]:
        """ì›Œí¬í”Œë¡œìš° ì¸ì‹ ì–¸ë¡œë“œ í›„ë³´ ì„ ì •
        
        ì ìˆ˜ê°€ ë‚®ì€ ëª¨ë¸ë¶€í„° ì–¸ë¡œë“œ í›„ë³´ë¡œ ì„ ì •
        """
        exclude_models = exclude_models or set()
        candidates = []
        
        for name, model in self.gpu_resident_models.items():
            if name not in exclude_models:
                model_info = self.models[name]
                score = model_info.priority_score
                size_mb = model_info.size_mb
                
                # avoid_unload í”Œë˜ê·¸ê°€ ì„¤ì •ëœ ëª¨ë¸ì€ ìµœëŒ€í•œ íšŒí”¼ (ì ìˆ˜ 90+ ëª¨ë¸)
                # í•˜ì§€ë§Œ ì ˆëŒ€ ê¸ˆì§€ëŠ” ì•„ë‹˜ (ë™ì  ì‹œìŠ¤í…œ)
                if not model_info.avoid_unload:
                    candidates.append((name, score, size_mb))
                else:
                    # avoid_unloadëŠ” ìµœí›„ì˜ ì„ íƒì§€ë¡œ ë‚¨ê²¨ë‘ 
                    logger.debug(f"  {name} ì–¸ë¡œë“œ íšŒí”¼ (ì ìˆ˜: {score:.1f}, avoid_unload=True)")
        
        # ì ìˆ˜ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        candidates.sort(key=lambda x: x[1])
        
        # í•„ìš”í•œ ë©”ëª¨ë¦¬ë§Œí¼ë§Œ ì„ íƒ
        selected = []
        freed_mb = 0.0
        
        for name, score, size_mb in candidates:
            if freed_mb >= required_mb:
                break
            selected.append(name)
            freed_mb += size_mb
            logger.debug(f"  ì–¸ë¡œë“œ í›„ë³´: {name} (ì ìˆ˜: {score:.1f}, í¬ê¸°: {size_mb:.1f}MB)")
        
        return selected
        
    async def load_model_to_gpu(self, name: str, timeout: float = None) -> nn.Module:
        """ëª¨ë¸ì„ GPUë¡œ ë¡œë“œ"""
        if name not in self.models:
            raise ValueError(f"ë“±ë¡ë˜ì§€ ì•Šì€ ëª¨ë¸: {name}")
            
        timeout = timeout or self.swap_timeout
        start_time = time.time()
        
        # ì´ë¯¸ GPUì— ìˆëŠ” ê²½ìš°
        if name in self.gpu_resident_models:
            self._update_access_stats(name, hit=True)
            return self.gpu_resident_models[name]
            
        self._update_access_stats(name, hit=False)
        
        # GPU ë©”ëª¨ë¦¬ í™•ì¸ ë° ì •ë¦¬
        # GPU ì‚¬ìš©ë¥ ì´ ë†’ìœ¼ë©´ HIGH ìš°ì„ ìˆœìœ„ë„ ì–¸ë¡œë“œ í—ˆìš©
        gpu_info = get_gpu_memory_info()
        allow_high_unload = gpu_info and gpu_info['usage_percent'] >= 85.0
        await self._ensure_gpu_memory(self.models[name].size_mb, allow_high_priority_unload=allow_high_unload)
        
        try:
            # ëª¨ë¸ì„ GPUë¡œ ì´ë™
            model = self.models[name].model
            device = get_smart_device(memory_required_mb=self.models[name].size_mb * 1.2)
            
            if device.type == 'cuda':
                model = model.to(device)
                model.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
                
                # GPU ìƒì£¼ ëª¨ë¸ë¡œ ë“±ë¡
                self.gpu_resident_models[name] = model
                self.models[name].location = SwapLocation.GPU
                
                # RAMì—ì„œ ì œê±°
                if name in self.ram_models:
                    del self.ram_models[name]
                    
                swap_time = time.time() - start_time
                self._update_swap_stats(True, swap_time)
                
                logger.debug(f"ëª¨ë¸ GPU ë¡œë“œ ì™„ë£Œ: {name} ({swap_time:.3f}s)")
                
                # íƒœìŠ¤í¬ ì˜ˆì¸¡ê¸°ì— ê¸°ë¡
                self.task_predictor.record_task(name)
                
                # ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”©
                if self.preload_prediction:
                    asyncio.create_task(self._predictive_preload(name))
                    
                return model
            else:
                # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - CPUì—ì„œ ì‹¤í–‰
                logger.warning(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ CPUì—ì„œ ì‹¤í–‰: {name}")
                return model.to('cpu')
                
        except Exception as e:
            self._update_swap_stats(False, time.time() - start_time)
            logger.error(f"ëª¨ë¸ GPU ë¡œë“œ ì‹¤íŒ¨: {name}, ì˜¤ë¥˜: {str(e)}")
            raise
            
    async def ensure_on_gpu(self, name: str, required_mb: float = None) -> bool:
        """í•´ë‹¹ ëª¨ë¸ì´ DSMì—ë§Œ ìˆìœ¼ë©´ GPUë¡œ ìŠ¹ê²©. ì´ë¯¸ GPUë©´ no-op.
        
        Args:
            name: ëª¨ë¸ ì´ë¦„
            required_mb: í•„ìš”í•œ ë©”ëª¨ë¦¬ (MB), Noneì´ë©´ ëª¨ë¸ í¬ê¸° ì‚¬ìš©
            
        Returns:
            bool: ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False
        """
        if name not in self.models:
            logger.warning(f"ensure_on_gpu: ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ì—†ìŒ: {name}")
            return False
        
        # ì´ë¯¸ GPUì— ìˆìœ¼ë©´ ì„±ê³µ
        if name in self.gpu_resident_models:
            logger.debug(f"ensure_on_gpu: {name}ëŠ” ì´ë¯¸ GPUì— ìˆìŒ")
            return True
        
        # í•„ìš”ì‹œ ë©”ëª¨ë¦¬ í™•ë³´
        if required_mb is None:
            required_mb = self.models[name].size_mb
        
        try:
            # GPUë¡œ ë¡œë“œ
            await self.load_model_to_gpu(name)
            logger.info(f"ensure_on_gpu: {name} GPU ìŠ¹ê²© ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"ensure_on_gpu: {name} GPU ìŠ¹ê²© ì‹¤íŒ¨: {e}")
            return False
    
    async def unload_model_from_gpu(self, name: str):
        """ëª¨ë¸ì„ GPUì—ì„œ ì–¸ë¡œë“œ"""
        if name not in self.gpu_resident_models:
            logger.warning(f"ì–¸ë¡œë“œ ìš”ì²­ëœ ëª¨ë¸ {name}ì´ gpu_resident_modelsì— ì—†ìŒ")
            # modelsì—ì„œ ì°¾ì•„ë³´ê¸°
            if name in self.models and self.models[name].model is not None:
                model = self.models[name].model
                if hasattr(model, 'device') and str(model.device).startswith('cuda'):
                    logger.info(f"modelsì—ì„œ {name} ë°œê²¬, GPUì—ì„œ ì–¸ë¡œë“œ ì‹œë„")
                else:
                    return
            else:
                return
        else:
            model = self.gpu_resident_models[name]
            
        start_time = time.time()
        
        try:
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ì–¸ë¡œë“œ ì „)
            from config import get_gpu_memory_info
            before_info = get_gpu_memory_info()
            before_mb = before_info['allocated_mb'] if before_info else 0
            
            # GPUì—ì„œ ëª¨ë¸ ì œê±°
            if hasattr(model, 'to'):
                model = model.to('cpu')
            elif hasattr(model, 'cpu'):
                model = model.cpu()
            else:
                logger.warning(f"{name}: to() ë˜ëŠ” cpu() ë©”ì„œë“œ ì—†ìŒ")
            
            # ëª¨ë“  í•˜ìœ„ ëª¨ë“ˆë„ CPUë¡œ ì´ë™
            if hasattr(model, 'modules'):
                for submodule in model.modules():
                    if hasattr(submodule, 'to'):
                        submodule.to('cpu')
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            if name in self.gpu_resident_models:
                del self.gpu_resident_models[name]
            
            if name in self.models:
                self.models[name].model = model
                self.models[name].location = SwapLocation.RAM
                self.ram_models[name] = self.models[name]
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë” ì ê·¹ì ìœ¼ë¡œ)
            if torch.cuda.is_available():
                # ì°¸ì¡° ì œê±°
                del model
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                import gc
                gc.collect()
                # GPU ìºì‹œ ì •ë¦¬
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()  # CUDA 11+: OSì— ë©”ëª¨ë¦¬ ë°˜í™˜
                
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ì–¸ë¡œë“œ í›„)
            after_info = get_gpu_memory_info()
            after_mb = after_info['allocated_mb'] if after_info else 0
            freed_mb = before_mb - after_mb
                
            unload_time = time.time() - start_time
            logger.info(f"âœ… {name} GPU ì–¸ë¡œë“œ ì™„ë£Œ ({unload_time:.3f}s, {freed_mb:.1f}MB í•´ì œ)")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ GPU ì–¸ë¡œë“œ ì‹¤íŒ¨: {name}, ì˜¤ë¥˜: {str(e)}")
            raise
            
    async def get_model(self, name: str) -> nn.Module:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (í•„ìš”ì‹œ ìë™ ë¡œë“œ)"""
        self.models[name].last_access = datetime.now()
        self.models[name].access_count += 1
        
        if name in self.gpu_resident_models:
            return self.gpu_resident_models[name]
        else:
            return await self.load_model_to_gpu(name)
            
    async def load_head_to_gpu(self, head_name: str, timeout: float = None) -> nn.Module:
        """í—¤ë“œ íŠ¹í™” GPU ë¡œë”© ë©”ì†Œë“œ - í—¤ë“œë³„ ìµœì í™” ë° ì‹œë„ˆì§€ ê³ ë ¤"""
        logger.info(f"ğŸ” load_head_to_gpu('{head_name}')")
        logger.info(f"ğŸ” DSM keys(pre): {sorted(list(self.models.keys()))[:50]}")
        
        # í—¤ë“œê°€ ë“±ë¡ë˜ì§€ ì•Šì€ ê²½ìš° ì¦‰ì‹œ ì—ëŸ¬
        if head_name not in self.models:
            logger.error(f"[DSM] ë“±ë¡ë˜ì§€ ì•Šì€ í—¤ë“œ ìš”ì²­: {head_name} (mgr_id={id(self)})")
            logger.error(f"[DSM] í˜„ì¬ keys(models): {sorted(list(self.models.keys()))[:50]}")
            raise ValueError(f"ë“±ë¡ë˜ì§€ ì•Šì€ í—¤ë“œ: {head_name}")
        
        # ëª¨ë¸ì´ Noneì¸ ê²½ìš°ë„ ì—ëŸ¬ (NO FALLBACK)
        if self.models[head_name].model is None:
            raise RuntimeError(f"[NO-FALLBACK] {head_name}: model is None")
            
        timeout = timeout or self.swap_timeout
        start_time = time.time()
        
        logger.debug(f"í—¤ë“œ GPU ë¡œë”© ì‹œì‘: {head_name}")
        
        # ì´ë¯¸ GPUì— ìˆëŠ” ê²½ìš°
        if head_name in self.gpu_resident_models:
            self._update_access_stats(head_name, hit=True)
            logger.debug(f"í—¤ë“œ ì´ë¯¸ GPUì— ìƒì£¼: {head_name}")
            return self.gpu_resident_models[head_name]
            
        self._update_access_stats(head_name, hit=False)
        
        # NO FALLBACK - lazy ë°”ì¸ë”© ì œê±°
        # ëª¨ë¸ì´ ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆì–´ì•¼ í•¨
        if self.models[head_name].model is None:
            raise RuntimeError(f"í—¤ë“œ {head_name}ì˜ ëª¨ë¸ì´ Noneì…ë‹ˆë‹¤ (NO FALLBACK ì •ì±…)")
        
        # í—¤ë“œ íŠ¹í™” ë©”ëª¨ë¦¬ í™•ë³´ (ì¼ë°˜ ëª¨ë¸ë³´ë‹¤ ë³´ìˆ˜ì ìœ¼ë¡œ)
        head_size_mb = self.models[head_name].size_mb
        # í˜„ì¬ ë¡œë“œ ì¤‘ì¸ í—¤ë“œëŠ” ì–¸ë¡œë“œì—ì„œ ì œì™¸
        # GPU ì‚¬ìš©ë¥ ì´ ë†’ìœ¼ë©´ HIGH ìš°ì„ ìˆœìœ„ë„ ì–¸ë¡œë“œ í—ˆìš©
        gpu_info = get_gpu_memory_info()
        allow_high_unload = gpu_info and gpu_info['usage_percent'] >= 85.0
        await self._ensure_gpu_memory(head_size_mb * 1.1, exclude_models={head_name}, 
                                     allow_high_priority_unload=allow_high_unload)  # 10% ì—¬ìœ 
        
        # í›„ë³´/ê²°ê³¼ ìš”ì•½
        try:
            mem = get_gpu_memory_info()
            candidates = [name for name, model in self.models.items() 
                         if model.location == SwapLocation.GPU and model.priority != SwapPriority.CRITICAL]
            logger.info(f"[G9] after-ensure free_mb={mem.get('free_mb', 'NA') if mem else 'NA'} "
                       f"alloc={mem.get('allocated_mb', 'NA') if mem else 'NA'}MB "
                       f"candidates={candidates[:10]}")
        except Exception:
            pass
        
        try:
            # í—¤ë“œë¥¼ GPUë¡œ ì´ë™
            head_model = self.models[head_name].model
            if head_model is None:
                raise ValueError(f"í—¤ë“œ ëª¨ë¸ì´ Noneì…ë‹ˆë‹¤: {head_name}")
            device = get_smart_device(memory_required_mb=head_size_mb * 1.2)
            
            if device.type == 'cuda':
                # í—¤ë“œ íŠ¹í™” ìµœì í™”
                head_model = head_model.to(device)
                head_model.eval()  # í—¤ë“œëŠ” í•­ìƒ ì¶”ë¡  ëª¨ë“œ
                
                # í—¤ë“œ íŠ¹í™” ë©”ëª¨ë¦¬ ìµœì í™”
                if hasattr(head_model, 'half') and ADVANCED_CONFIG.get('use_fp16', True):
                    head_model = head_model.half()  # FP16 ì‚¬ìš©
                
                # GPU ìƒì£¼ í—¤ë“œë¡œ ë“±ë¡
                self.gpu_resident_models[head_name] = head_model
                self.models[head_name].location = SwapLocation.GPU
                
                # RAMì—ì„œ ì œê±°
                if head_name in self.ram_models:
                    del self.ram_models[head_name]
                    
                swap_time = time.time() - start_time
                self._update_swap_stats(True, swap_time)
                
                logger.info(f"í—¤ë“œ GPU ë¡œë”© ì™„ë£Œ: {head_name} ({swap_time:.3f}s, {head_size_mb:.1f}MB)")
                
                # í—¤ë“œ ì‚¬ìš© íŒ¨í„´ ê¸°ë¡
                self.task_predictor.record_task(f"head_{head_name}")
                
                # í—¤ë“œ íŠ¹í™” ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”© (ê´€ë ¨ í—¤ë“œë“¤)
                if self.preload_prediction:
                    asyncio.create_task(self._predictive_head_preload(head_name))
                    
                return head_model
            else:
                # NO FALLBACK - GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì¦‰ì‹œ ì‹¤íŒ¨
                logger.error(f"[DSM] {head_name} GPU ë¡œë”© ì‹¤íŒ¨: device={device.type} (NO FALLBACK)")
                raise RuntimeError(f"[DSM] {head_name} GPU ë¡œë”© ì‹¤íŒ¨: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (NO FALLBACK)")
                
        except Exception as e:
            self._update_swap_stats(False, time.time() - start_time)
            logger.error(f"í—¤ë“œ GPU ë¡œë”© ì‹¤íŒ¨: {head_name}, ì˜¤ë¥˜: {str(e)}")
            
            # ìˆœìˆ˜ ì¬ì‹œë„ ë°©ì‹ (fallback ì—†ìŒ)
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                retry_count += 1
                logger.info(f"í—¤ë“œ GPU ë¡œë”© ì¬ì‹œë„ {retry_count}/{max_retries}: {head_name}")
                
                try:
                    # ë©”ëª¨ë¦¬ ê°•ì œ ì •ë¦¬ í›„ ì¬ì‹œë„
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    gc.collect()
                    await asyncio.sleep(0.5)  # ì§§ì€ ëŒ€ê¸°
                    
                    # ë” ë³´ìˆ˜ì ì¸ ë©”ëª¨ë¦¬ í™•ë³´ (í˜„ì¬ í—¤ë“œ ì œì™¸)
                    # ì¬ì‹œë„ ì‹œì—ëŠ” ë” ì ê·¹ì ìœ¼ë¡œ ì–¸ë¡œë“œ
                    gpu_info = get_gpu_memory_info()
                    allow_high_unload = gpu_info and gpu_info['usage_percent'] >= 80.0  # ì¬ì‹œë„ ì‹œ ë” ë‚®ì€ ì„ê³„ì¹˜
                    await self._ensure_gpu_memory(head_size_mb * 1.5, exclude_models={head_name},
                                                 allow_high_priority_unload=allow_high_unload)
                    
                    head_model = self.models[head_name].model
                    device = get_smart_device(memory_required_mb=head_size_mb * 1.3)
                    
                    if device.type == 'cuda':
                        head_model = head_model.to(device)
                        head_model.eval()
                        
                        self.gpu_resident_models[head_name] = head_model
                        self.models[head_name].location = SwapLocation.GPU
                        
                        if head_name in self.ram_models:
                            del self.ram_models[head_name]
                            
                        total_time = time.time() - start_time
                        self._update_swap_stats(True, total_time)
                        
                        logger.info(f"í—¤ë“œ GPU ë¡œë”© ì¬ì‹œë„ ì„±ê³µ: {head_name} ({total_time:.3f}s)")
                        return head_model
                        
                except Exception as retry_error:
                    logger.warning(f"í—¤ë“œ GPU ë¡œë”© ì¬ì‹œë„ {retry_count} ì‹¤íŒ¨: {head_name}, ì˜¤ë¥˜: {str(retry_error)}")
                    
                    if retry_count == max_retries:
                        logger.error(f"í—¤ë“œ GPU ë¡œë”© ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {head_name}")
                        raise retry_error
            
            raise e
            
    def log_memory_reconciliation(self, tag: str = ""):
        """DSM ì¶”ì  ë©”ëª¨ë¦¬ vs ì‹¤ì œ GPU ë©”ëª¨ë¦¬ ë¹„êµ"""
        from config import get_gpu_memory_info
        
        info = get_gpu_memory_info() or {}
        allocated_mb = info.get('allocated_mb', 0.0)
        
        # DSMì´ ì¶”ì í•˜ëŠ” GPU ë©”ëª¨ë¦¬ ì´í•©
        dsm_tracked = sum(
            self.models[name].size_mb 
            for name in self.gpu_resident_models.keys()
            if name in self.models
        )
        
        # ì¶”ì ë˜ì§€ ì•ŠëŠ” ë©”ëª¨ë¦¬ (ë°±ë³¸, ìºì‹œ ë“±)
        untracked_mb = max(0.0, allocated_mb - dsm_tracked)
        untracked_pct = (untracked_mb / allocated_mb * 100) if allocated_mb > 0 else 0
        
        logger.critical(f"[MEM RECON {tag}] "
                       f"torch_alloc={allocated_mb:.1f}MB, "
                       f"dsm_tracked={dsm_tracked:.1f}MB, "
                       f"untracked={untracked_mb:.1f}MB ({untracked_pct:.1f}% ë¯¸ì¶”ì )")
        
        # í° ë¯¸ì¶”ì  ë©”ëª¨ë¦¬ê°€ ìˆìœ¼ë©´ ê²½ê³ 
        if untracked_pct > 50:
            logger.warning(f"âš ï¸ DSM ë¯¸ì¶”ì  ë©”ëª¨ë¦¬ê°€ 50% ì´ìƒ: {untracked_mb:.1f}MB")
            logger.warning(f"   DSM GPU ëª¨ë¸: {list(self.gpu_resident_models.keys())[:10]}")
    
    async def _ensure_gpu_memory(self, required_mb: float, max_attempts: int = 3, 
                                exclude_models: set = None, allow_high_priority_unload: bool = False):
        """GPU ë©”ëª¨ë¦¬ í™•ë³´ - fail-fast êµ¬í˜„
        
        Args:
            required_mb: í•„ìš”í•œ ë©”ëª¨ë¦¬ (MB)
            max_attempts: ìµœëŒ€ ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ 3íšŒ)
            exclude_models: ì–¸ë¡œë“œì—ì„œ ì œì™¸í•  ëª¨ë¸ ì´ë¦„ ì§‘í•©
            allow_high_priority_unload: HIGH ìš°ì„ ìˆœìœ„ ëª¨ë¸ë„ ì–¸ë¡œë“œ í—ˆìš© ì—¬ë¶€
        """
        exclude_models = exclude_models or set()
        
        # ìŠ¤ì™‘ ì‹œì‘ ì „ DSM ìƒíƒœ ë¡œê¹…
        logger.info(f"[DSM ìŠ¤ì™‘ ì‹œì‘] í•„ìš”: {required_mb:.1f}MB, ì œì™¸: {list(exclude_models)[:5]}")
        logger.info(f"[DSM í˜„í™©] models.keys: {list(self.models.keys())[:20]}")
        logger.info(f"[DSM í˜„í™©] gpu_resident: {list(self.gpu_resident_models.keys())[:20]}")
        
        for attempt in range(max_attempts):
            gpu_info = get_gpu_memory_info()
            if gpu_info is None:
                raise RuntimeError("[DSM] GPU ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
                
            available_mb = gpu_info['free_mb']
            
            # ì—¬ìœ  ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œ ê²½ìš°
            if available_mb >= required_mb * 1.2:  # 20% ë§ˆì§„
                logger.info(f"âœ… [DSM] need={required_mb:.1f}MB / free={available_mb:.1f}MB â†’ ì¶©ë¶„í•¨")
                return
                
            logger.info(f"[ì‹œë„ {attempt+1}/{max_attempts}] need={required_mb:.1f}MB / free={available_mb:.1f}MB â†’ ìŠ¤ì™‘ í•„ìš”")
            
            # LRU ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ë“¤ ì–¸ë¡œë“œ
            models_to_unload = []
            
            # âœ… í›„ë³´ ì‚°ì • (GPU ìƒì£¼ + CRITICAL ì œì™¸ + exclude ì œì™¸)
            logger.info(f"[DSM í›„ë³´ ì‚°ì •] GPU ìƒì£¼ ëª¨ë¸ ìˆ˜: {len(self.gpu_resident_models)}, HIGH ì–¸ë¡œë“œ í—ˆìš©: {allow_high_priority_unload}")
            for name, model in list(self.gpu_resident_models.items()):
                model_info = self.models[name]
                
                if name in exclude_models:
                    logger.debug(f"   - {name}: ì œì™¸ (exclude_modelsì— í¬í•¨)")
                elif model_info.avoid_unload and not allow_high_priority_unload:
                    # ë™ì  ì‹œìŠ¤í…œ: avoid_unload í”Œë˜ê·¸ë¡œ íŒë‹¨ (ì ìˆ˜ 90+ ëª¨ë¸)
                    logger.debug(f"   - {name}: ì œì™¸ (avoid_unload=True, ì ìˆ˜={model_info.priority_score:.1f})")
                elif model_info.priority == SwapPriority.HIGH and not allow_high_priority_unload:
                    # HIGH ìš°ì„ ìˆœìœ„ë„ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨
                    logger.debug(f"   - {name}: ì œì™¸ (HIGH, ì ìˆ˜={model_info.priority_score:.1f}, allow_high=False)")
                else:
                    # ì›Œí¬í”Œë¡œìš° ì ìˆ˜ ê³ ë ¤
                    priority_score = model_info.priority_score
                    models_to_unload.append((
                        name, 
                        model_info.last_access, 
                        model_info.priority.value,
                        model_info.size_mb,
                        priority_score  # ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ë™ì  ì ìˆ˜ ì¶”ê°€
                    ))
                    logger.debug(f"   - {name}: í›„ë³´ ì¶”ê°€ ({model_info.size_mb:.1f}MB, ì ìˆ˜={priority_score:.1f})")
                    
            # ë©”íƒ€ ë“±ë¡ëœ ëª¨ë¸ë„ í›„ë³´ì— í¬í•¨ (deferred ìƒíƒœ)
            for name, swapable in self.models.items():
                if (name not in exclude_models and 
                    name not in self.gpu_resident_models and 
                    swapable.status == 'deferred' and
                    not swapable.avoid_unload):  # ë™ì  ì‹œìŠ¤í…œ: avoid_unload í”Œë˜ê·¸ë¡œ íŒë‹¨
                    # ë©”íƒ€ ë“±ë¡ ëª¨ë¸ë„ ì–¸ë¡œë“œ í›„ë³´ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ (ì ì¬ì  ê³µê°„ í™•ë³´ìš©)
                    logger.debug(f"   ë©”íƒ€ ë“±ë¡ ëª¨ë¸ë„ í›„ë³´ì— ì¶”ê°€: {name} ({swapable.size_mb:.1f}MB, ì ìˆ˜={swapable.priority_score:.1f})")
                    
            # ì–¸ë¡œë“œ í›„ë³´ê°€ ì—†ìœ¼ë©´ ì—¬ìœ  ë©”ëª¨ë¦¬ í™•ì¸
            if not models_to_unload:
                if available_mb >= required_mb:
                    logger.info(f"[DSM PASS] í›„ë³´ 0ê°œì§€ë§Œ ì—¬ìœ  {available_mb:.1f}MB â‰¥ í•„ìš” {required_mb:.1f}MB â†’ ì–¸ë¡œë“œ ìƒëµ")
                    return
                    
                logger.warning(f"[G9] ensure_gpu_memory: í›„ë³´ 0ê°œ (exclude={list(exclude_models)}) "
                               f"free_mb={available_mb:.1f}")
                logger.error(f"[DSM FAIL] ì–¸ë¡œë“œ í›„ë³´ 0ê°œ")
                logger.error(f"   - GPU ìƒì£¼ ëª¨ë¸: {list(self.gpu_resident_models.keys())[:10]}")
                logger.error(f"   - ì œì™¸ ëª¨ë¸: {exclude_models}")
                
                # ìƒì„¸ ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
                logger.warning(f"[G9] DSM keys: {sorted(list(self.models.keys()))[:50]}")
                logger.warning(f"[G9] GPU-resident keys: {sorted(list(self.gpu_resident_models.keys()))[:50]}")
                
                # ìš°ì„ ìˆœìœ„ ì •ë³´ ì¶”ê°€
                priorities_info = {}
                for k in list(self.models.keys())[:20]:
                    priorities_info[k] = self.models[k].priority.name if self.models[k].priority else "NONE"
                logger.warning(f"[G9] priorities: {priorities_info}")
                logger.error(f"   - ë“±ë¡ëœ ëª¨ë¸ ìˆ˜: {len(self.models)}")
                logger.error(f"   - avoid_unload ëª¨ë¸ ìˆ˜: {sum(1 for m in self.models.values() if m.avoid_unload)}")
                logger.error(f"   - ì ìˆ˜ 90+ ëª¨ë¸ ìˆ˜: {sum(1 for m in self.models.values() if m.priority_score >= 90.0)}")
                logger.error(f"   - í˜„ì¬ free_mb: {available_mb:.1f}, í•„ìš”: {required_mb:.1f}MB")
                raise RuntimeError(f"GPU ë©”ëª¨ë¦¬ í™•ë³´ ë¶ˆê°€: ì–¸ë¡œë“œ ê°€ëŠ¥í•œ ëª¨ë¸ ì—†ìŒ (í•„ìš”: {required_mb:.1f}MB)")
                
            # â­ ê°œì„ ëœ ì •ë ¬: í° ëª¨ë¸ ìš°ì„ , ë‚®ì€ ì ìˆ˜ ìš°ì„ , ì˜¤ë˜ëœ ì ‘ê·¼ ìš°ì„ 
            # ê¸°ì¡´: (ìš°ì„ ìˆœìœ„ê°’, ë§ˆì§€ë§‰ì ‘ê·¼ì‹œê°„) -> ì‘ì€ ê°’ ìš°ì„ 
            # ì‹ ê·œ: (í¬ê¸° ë‚´ë¦¼ì°¨ìˆœ, ë™ì ì ìˆ˜ ì˜¤ë¦„ì°¨ìˆœ, ìš°ì„ ìˆœìœ„ê°’, ë§ˆì§€ë§‰ì ‘ê·¼)
            models_to_unload.sort(key=lambda x: (-x[3], x[4] if len(x) > 4 else 50.0, x[2], x[1]))
            logger.info(f"[G9] unload candidates: {[c[0] for c in models_to_unload[:20]]} ... "
                        f"need~{required_mb:.1f}MB")
            
            # í•„ìš”í•œ ë§Œí¼ ì–¸ë¡œë“œ
            freed_memory = 0
            unloaded_count = 0
            
            # íŠœí”Œ ì–¸íŒ¨í‚¹ (5ê°œ ìš”ì†Œ: name, last_access, priority, size_mb, priority_score)
            for model_info in models_to_unload:
                name = model_info[0]
                size_mb = model_info[3]
                if freed_memory >= required_mb:
                    break
                    
                try:
                    await self.unload_model_from_gpu(name)
                    freed_memory += size_mb
                    unloaded_count += 1
                    logger.info(f"   ì–¸ë¡œë“œ: {name} ({size_mb:.1f}MB) - ëˆ„ì  í•´ì œ: {freed_memory:.1f}MB")
                except Exception as e:
                    logger.warning(f"   ì–¸ë¡œë“œ ì‹¤íŒ¨: {name} - {e}")
                    
            # ì–¸ë¡œë“œ ê²°ê³¼ í™•ì¸
            if unloaded_count == 0:
                logger.error(f"[DSM] ëª¨ë¸ì„ ì–¸ë¡œë“œí–ˆì§€ë§Œ ë©”ëª¨ë¦¬ê°€ í•´ì œë˜ì§€ ì•ŠìŒ")
                raise RuntimeError(f"GPU ë©”ëª¨ë¦¬ í™•ë³´ ì‹¤íŒ¨: ì–¸ë¡œë“œ íš¨ê³¼ ì—†ìŒ")
                
            # ì–¸ë¡œë“œ ê²°ê³¼ í•œ ì¤„ ìš”ì•½
            unloaded_names = [m[0] for m in models_to_unload[:unloaded_count]]
            logger.info(f"[DSM ìŠ¤ì™‘] candidates={len(models_to_unload)} / unloaded={unloaded_count} / freed={freed_memory:.1f}MB / models={unloaded_names[:3]}")
            
            # ë©”ëª¨ë¦¬ ì¶”ì  ë¡œê¹…
            self.log_memory_reconciliation(f"after_unload_try{attempt+1}")
            
            # ê°•í™”ëœ ë™ê¸°í™”: ì–¸ë¡œë“œ í›„ ì‹¤ì œ ë©”ëª¨ë¦¬ í•´ì œ ë³´ì¥
            if torch.cuda.is_available():
                # 1. ìºì‹œ ë¹„ìš°ê¸°
                torch.cuda.empty_cache()
                # 2. GPU ë™ê¸°í™” (ëª¨ë“  CUDA ì‘ì—… ì™„ë£Œ ëŒ€ê¸°)
                torch.cuda.synchronize()
                # 3. Python GC ì‹¤í–‰
                gc.collect()
                # 4. ì§§ì€ ëŒ€ê¸° (allocator ë°˜ì˜ ì‹œê°„)
                await asyncio.sleep(0.2)
                
                # 5. ì–¸ë¡œë“œ í›„ ì‹¤ì œ ë©”ëª¨ë¦¬ ë³€í™” ì¸¡ì •
                gpu_info_after = get_gpu_memory_info()
                if gpu_info_after:
                    logger.info(f"[DSM ë™ê¸°í™”] ì–¸ë¡œë“œ í›„ GPU ì‚¬ìš©ë¥ : {gpu_info_after['usage_percent']:.1f}% "
                              f"(ì—¬ìœ : {gpu_info_after['free_mb']:.1f}MB)")
                
            # ë‹¤ìŒ ì‹œë„ ì „ ì¶”ê°€ ëŒ€ê¸°
            if attempt < max_attempts - 1:
                await asyncio.sleep(0.3)
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        logger.error(f"[DSM ìŠ¤ì™‘ ì‹¤íŒ¨] ìµœì¢… ìƒíƒœ:")
        logger.error(f"   - models.keys: {list(self.models.keys())[:20]}")
        logger.error(f"   - gpu_resident: {list(self.gpu_resident_models.keys())[:20]}")
        raise RuntimeError(f"[DSM] GPU ë©”ëª¨ë¦¬ í™•ë³´ ì‹¤íŒ¨: {max_attempts}íšŒ ì‹œë„ í›„ì—ë„ {required_mb:.1f}MB í™•ë³´ ë¶ˆê°€")
            
    async def _predictive_preload(self, current_task: str):
        """ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”©"""
        try:
            predictions = self.task_predictor.predict_next_tasks(current_task, top_k=2)
            
            for next_task, probability in predictions:
                if probability > 0.3 and next_task in self.models:  # 30% ì´ìƒ í™•ë¥ 
                    if next_task not in self.gpu_resident_models:
                        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í”„ë¦¬ë¡œë“œ
                        asyncio.create_task(self._background_preload(next_task))
                        
        except Exception as e:
            logger.debug(f"ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”© ì˜¤ë¥˜: {str(e)}")
            
    async def _background_preload(self, model_name: str):
        """ë°±ê·¸ë¼ìš´ë“œ í”„ë¦¬ë¡œë”©"""
        try:
            # ì ì‹œ ëŒ€ê¸° (í˜„ì¬ ì‘ì—… ì™„ë£Œ í›„)
            await asyncio.sleep(0.5)
            
            # GPU ë©”ëª¨ë¦¬ ì—¬ìœ  í™•ì¸
            gpu_info = get_gpu_memory_info()
            if gpu_info and gpu_info['usage_percent'] < 55:  # 55% ë¯¸ë§Œ ì‚¬ìš© ì‹œì—ë§Œ
                await self.load_model_to_gpu(model_name)
                self.stats['preload_hits'] += 1
                logger.debug(f"ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”© ì„±ê³µ: {model_name}")
            else:
                self.stats['preload_misses'] += 1
                
        except Exception as e:
            self.stats['preload_misses'] += 1
            logger.debug(f"ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”© ì‹¤íŒ¨: {model_name}, {str(e)}")
            
    async def _compress_model_background(self, name: str):
        """ë°±ê·¸ë¼ìš´ë“œ ëª¨ë¸ ì••ì¶•"""
        try:
            if name in self.models:
                model = self.models[name].model
                compressed_data = self.model_compressor.compress_model(model)
                self.models[name].compressed_data = compressed_data
                logger.debug(f"ëª¨ë¸ ì••ì¶• ì™„ë£Œ: {name}")
        except Exception as e:
            logger.debug(f"ëª¨ë¸ ì••ì¶• ì‹¤íŒ¨: {name}, {str(e)}")
    
    async def _predictive_head_preload(self, current_head: str):
        """í—¤ë“œ íŠ¹í™” ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”©"""
        try:
            # í—¤ë“œ ê°„ ê´€ë ¨ì„± ê¸°ë°˜ ì˜ˆì¸¡ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            related_heads = self._get_related_heads(current_head)
            
            for related_head in related_heads:
                if related_head in self.models and related_head not in self.gpu_resident_models:
                    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê´€ë ¨ í—¤ë“œ í”„ë¦¬ë¡œë“œ
                    asyncio.create_task(self._background_head_preload(related_head))
                    
        except Exception as e:
            logger.debug(f"í—¤ë“œ ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”© ì˜¤ë¥˜: {str(e)}")
            
    def _get_related_heads(self, head_name: str) -> List[str]:
        """ê´€ë ¨ í—¤ë“œ ëª©ë¡ ë°˜í™˜ (í—¤ë“œ ê°„ ì‹œë„ˆì§€ ê³ ë ¤)"""
        related_map = {
            'emotion_head': ['empathy_head', 'sentiment_head'],
            'empathy_head': ['emotion_head', 'social_head'],
            'bentham_head': ['ethical_head', 'utility_head'],
            'semantic_head': ['linguistic_head', 'context_head'],
            'surd_head': ['causal_head', 'uncertainty_head']
        }
        
        return related_map.get(head_name, [])
    
    def _get_dynamic_preload_threshold(self, gpu_usage_percent: float) -> float:
        """GPU ì‚¬ìš©ë¥ ì— ë”°ë¥¸ ë™ì  í”„ë¦¬ë¡œë”© ì„ê³„ê°’ ê³„ì‚°"""
        if gpu_usage_percent < 60:
            # 60% ë¯¸ë§Œ: ì ê·¹ì  í”„ë¦¬ë¡œë”© í—ˆìš©
            return 75.0
        elif gpu_usage_percent < 70:
            # 60-70%: ë³´ìˆ˜ì  í”„ë¦¬ë¡œë”© 
            return 65.0  
        elif gpu_usage_percent < 80:
            # 70-80%: ë§¤ìš° ì œí•œì  í”„ë¦¬ë¡œë”©
            return 60.0
        else:
            # 80% ì´ˆê³¼: í”„ë¦¬ë¡œë”© ê¸ˆì§€
            return 0.0
    
    async def _smart_memory_management(self, current_usage_percent: float):
        """ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ - 80% ì´ˆê³¼ì‹œ ì ê·¹ì  ì–¸ë¡œë“œ"""
        if current_usage_percent > 80:
            logger.info(f"GPU ì‚¬ìš©ë¥  ìœ„í—˜ ìˆ˜ì¤€: {current_usage_percent:.1f}%, ì ê·¹ì  ì–¸ë¡œë“œ ì‹œì‘")
            
            # ìš°ì„ ìˆœìœ„ ë‚®ì€ ëª¨ë¸ë“¤ ì–¸ë¡œë“œ (CRITICAL ì œì™¸)
            models_to_unload = []
            for name, model in list(self.gpu_resident_models.items()):
                if self.models[name].priority != SwapPriority.CRITICAL:
                    models_to_unload.append((
                        name, 
                        self.models[name].last_access,
                        self.models[name].priority
                    ))
            
            # ì ‘ê·¼ ì‹œê°„ê³¼ ìš°ì„ ìˆœìœ„ ê¸°ì¤€ ì •ë ¬ (ì˜¤ë˜ë˜ê³  ìš°ì„ ìˆœìœ„ ë‚®ì€ ê²ƒë¶€í„°)
            models_to_unload.sort(key=lambda x: (x[2].value, x[1]))
            
            # 75% ì´í•˜ë¡œ ë–¨ì–´ì§ˆ ë•Œê¹Œì§€ ì–¸ë¡œë“œ
            target_usage = 75.0
            unloaded_count = 0
            
            for name, _, priority in models_to_unload:
                if current_usage_percent <= target_usage:
                    break
                    
                await self.unload_model_from_gpu(name)
                unloaded_count += 1
                
                # ì–¸ë¡œë“œ í›„ ì‚¬ìš©ë¥  ì¬í™•ì¸
                gpu_info = get_gpu_memory_info()
                if gpu_info:
                    current_usage_percent = gpu_info['usage_percent']
                    logger.debug(f"ì–¸ë¡œë“œ í›„ GPU ì‚¬ìš©ë¥ : {current_usage_percent:.1f}%")
            
            if unloaded_count > 0:
                logger.info(f"ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì™„ë£Œ: {unloaded_count}ê°œ ëª¨ë¸ ì–¸ë¡œë“œ")
                
            # ì¶”ê°€ ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
    
    async def _background_head_preload(self, head_name: str):
        """ë°±ê·¸ë¼ìš´ë“œ í—¤ë“œ í”„ë¦¬ë¡œë”© - ë™ì  ì„ê³„ê°’ ì ìš©"""
        try:
            # ì ì‹œ ëŒ€ê¸° (í˜„ì¬ í—¤ë“œ ë¡œë”© ì™„ë£Œ í›„)
            await asyncio.sleep(1.0)
            
            # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            gpu_info = get_gpu_memory_info()
            if not gpu_info:
                self.stats['preload_misses'] += 1
                return
                
            current_usage = gpu_info['usage_percent']
            
            # 80% ì´ˆê³¼ì‹œ ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ ìˆ˜í–‰
            if current_usage > 80:
                await self._smart_memory_management(current_usage)
                self.stats['preload_misses'] += 1
                return
            
            # ë™ì  ì„ê³„ê°’ ê³„ì‚°
            threshold = self._get_dynamic_preload_threshold(current_usage)
            
            if threshold > 0 and current_usage < threshold:
                await self.load_head_to_gpu(head_name)
                self.stats['preload_hits'] += 1
                logger.debug(f"í—¤ë“œ ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”© ì„±ê³µ: {head_name} (ì‚¬ìš©ë¥ : {current_usage:.1f}%, ì„ê³„ê°’: {threshold:.1f}%)")
            else:
                self.stats['preload_misses'] += 1
                logger.debug(f"í—¤ë“œ ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”© ì œí•œ: {head_name} (ì‚¬ìš©ë¥ : {current_usage:.1f}%, ì„ê³„ê°’: {threshold:.1f}%)")
                
        except Exception as e:
            self.stats['preload_misses'] += 1
            logger.debug(f"í—¤ë“œ ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”© ì‹¤íŒ¨: {head_name}, {str(e)}")
            
    async def _background_cleanup(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‘ì—…"""
        while not self._shutdown_event.is_set():
            try:
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì‹¤í–‰
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
                # í†µê³„ ë¡œê¹…
                if hasattr(self, 'stats'):
                    hit_rate = self.stats['cache_hits'] / max(1, self.stats['cache_hits'] + self.stats['cache_misses'])
                    logger.debug(f"ìŠ¤ì™‘ í†µê³„ - íˆíŠ¸ìœ¨: {hit_rate:.1%}, ì´ ìŠ¤ì™‘: {self.stats['total_swaps']}")
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.debug(f"ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì˜¤ë¥˜: {str(e)}")
                
    def _calculate_model_size(self, model: nn.Module) -> float:
        """ëª¨ë¸ í¬ê¸° ê³„ì‚° (MB)"""
        if model is None:
            return 0.0
            
        # wrapper í´ë˜ìŠ¤ ì²˜ë¦¬: ë‚´ë¶€ì˜ ì‹¤ì œ nn.Module ì°¾ê¸°
        actual_model = model
        
        # parameters() ë©”ì„œë“œê°€ ì—†ìœ¼ë©´ ë‚´ë¶€ ëª¨ë¸ ì°¾ê¸°
        if not hasattr(model, 'parameters'):
            # ì¼ë°˜ì ì¸ ì†ì„±ëª…ë“¤ ì‹œë„
            for attr_name in ['model', 'models', 'network', 'net', '_model', '_network']:
                if hasattr(model, attr_name):
                    candidate = getattr(model, attr_name)
                    # dictì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‹œë„
                    if isinstance(candidate, dict) and len(candidate) > 0:
                        candidate = next(iter(candidate.values()))
                    # nn.Moduleì¸ì§€ í™•ì¸
                    if hasattr(candidate, 'parameters'):
                        actual_model = candidate
                        logger.debug(f"Wrapper í´ë˜ìŠ¤ì—ì„œ ì‹¤ì œ ëª¨ë¸ ì°¾ìŒ: {attr_name}")
                        break
            else:
                # NO FALLBACK - nn.Moduleì´ ì•„ë‹ˆë©´ ë“±ë¡ ë¶ˆê°€
                raise RuntimeError(f"nn.Moduleì´ ì•„ë‹Œ ê°ì²´ëŠ” ë“±ë¡í•  ìˆ˜ ì—†ìŒ: {type(model).__name__}")
        
        try:
            param_size = sum(p.numel() * p.element_size() for p in actual_model.parameters())
            buffer_size = sum(b.numel() * b.element_size() for b in actual_model.buffers())
            return (param_size + buffer_size) / (1024 ** 2)
        except Exception as e:
            logger.error(f"ëª¨ë¸ í¬ê¸° ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œì—ë„ GPU ëª¨ë¸ì´ë©´ ì¶”ì •ì¹˜ ë°˜í™˜
            if hasattr(model, 'device') and str(model.device).startswith('cuda'):
                return 700.0
            return 0.0
        
    def _update_access_stats(self, name: str, hit: bool):
        """ì ‘ê·¼ í†µê³„ ì—…ë°ì´íŠ¸"""
        if hit:
            self.stats['cache_hits'] += 1
        else:
            self.stats['cache_misses'] += 1
            
    def _update_swap_stats(self, success: bool, swap_time: float):
        """ìŠ¤ì™‘ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats['total_swaps'] += 1
        self.stats['total_swap_time'] += swap_time
        
        if success:
            self.stats['successful_swaps'] += 1
        else:
            self.stats['failed_swaps'] += 1
            
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        hit_rate = self.stats['cache_hits'] / max(1, self.stats['cache_hits'] + self.stats['cache_misses'])
        avg_swap_time = self.stats['total_swap_time'] / max(1, self.stats['total_swaps'])
        
        return {
            **self.stats,
            'cache_hit_rate': hit_rate,
            'average_swap_time': avg_swap_time,
            'gpu_resident_models': list(self.gpu_resident_models.keys()),
            'ram_models': list(self.ram_models.keys()),
            'total_models': len(self.models)
        }
        
    def get_memory_status(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìƒíƒœ ë°˜í™˜"""
        gpu_info = get_gpu_memory_info()
        
        gpu_model_sizes = sum(
            self.models[name].size_mb for name in self.gpu_resident_models
        )
        
        ram_model_sizes = sum(
            model.size_mb for model in self.ram_models.values()
        )
        
        return {
            'gpu_info': gpu_info,
            'gpu_model_total_mb': gpu_model_sizes,
            'ram_model_total_mb': ram_model_sizes,
            'gpu_utilization_percent': gpu_info['usage_percent'] if gpu_info else 0,
            'models_on_gpu': len(self.gpu_resident_models),
            'models_on_ram': len(self.ram_models)
        }
        
    async def free_gpu_space_intelligent(self, required_mb: int, 
                                          target_usage: float = 0.85, 
                                          hard_timeout_s: float = 45.0) -> bool:
        """
        ì§€ëŠ¥ì  GPU ê³µê°„ í™•ë³´ - LRU + ìš°ì„ ìˆœìœ„ ê¸°ë°˜
        ëª©í‘œ ì‚¬ìš©ë¥ ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ìš°ì„ ìˆœìœ„ ì–¸ë¡œë“œë¥¼ ë°˜ë³µ
        
        Args:
            required_mb: í•„ìš”í•œ ë©”ëª¨ë¦¬ (MB)
            target_usage: ëª©í‘œ GPU ì‚¬ìš©ë¥  (0.0~1.0)
            hard_timeout_s: íƒ€ì„ì•„ì›ƒ ì‹œê°„ (ì´ˆ)
            
        Returns:
            bool: ì„±ê³µ ì‹œ True, íƒ€ì„ì•„ì›ƒ/ì‹¤íŒ¨ ì‹œ False
        """
        logger.info(f"[ì§€ëŠ¥ì  GPU ê³µê°„ í™•ë³´] {required_mb}MB í•„ìš”, ëª©í‘œ ì‚¬ìš©ë¥ : {target_usage*100:.1f}%")
        
        start_time = time.time()
        
        # íƒ€ì„ì•„ì›ƒê¹Œì§€ ë°˜ë³µ
        while True:
            # íƒ€ì„ì•„ì›ƒ ì²´í¬
            if (time.time() - start_time) > hard_timeout_s:
                logger.warning(f"[íƒ€ì„ì•„ì›ƒ] {hard_timeout_s}ì´ˆ ì´ˆê³¼")
                return False
            
            # í˜„ì¬ GPU ìƒíƒœ í™•ì¸
            gpu_info = get_gpu_memory_info()
            if not gpu_info:
                logger.error("GPU ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
                
            current_free_mb = gpu_info['free_mb']
            current_usage = gpu_info['usage_percent'] / 100.0  # 0.0~1.0 ë¹„ìœ¨ë¡œ ë³€í™˜
            logger.info(f"[í˜„ì¬ GPU] ì—¬ìœ : {current_free_mb}MB, ì‚¬ìš©ë¥ : {current_usage*100:.1f}%")
            
            # ëª©í‘œ ì‚¬ìš©ë¥  ë‹¬ì„± ë° ì¶©ë¶„í•œ ê³µê°„ í™•ì¸
            if current_usage <= target_usage and current_free_mb >= required_mb:
                logger.info(f"[ëª©í‘œ ë‹¬ì„±] ì‚¬ìš©ë¥ : {current_usage*100:.1f}% <= {target_usage*100:.1f}%, ì—¬ìœ : {current_free_mb}MB >= {required_mb}MB")
                return True
            
            # í•´ì œí•´ì•¼ í•  ë©”ëª¨ë¦¬ (int ìºìŠ¤íŒ…ìœ¼ë¡œ ì•ˆì •í™”) + safety_margin
            safety_margin_mb = 256  # ì—¬ìœ  ë²„í¼ í™•ë³´
            need_to_free_mb = int(max(required_mb - current_free_mb + safety_margin_mb, 
                                      (current_usage - target_usage) * gpu_info['total_mb']))  # ëª©í‘œ ì‚¬ìš©ë¥  ê³ ë ¤
            logger.info(f"[í•´ì œ í•„ìš”] {need_to_free_mb}MB")
            
            # ì–¸ë¡œë“œ í›„ë³´ ì„ ì • (LRU + ìš°ì„ ìˆœìœ„)
            unload_candidates = []
            
            # ì›Œí¬í”Œë¡œìš° ë³´í˜¸ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            protected_models = set()
            if self.workflow_aware and hasattr(self.workflow_memory_manager, 'workflow_tracker'):
                protected_models = self.workflow_memory_manager.workflow_tracker.get_protected_models()
                logger.debug(f"[ì›Œí¬í”Œë¡œìš° ë³´í˜¸] {len(protected_models)}ê°œ ëª¨ë¸")
            
            # GPUì— ìˆëŠ” ëª¨ë¸ë“¤ ì¤‘ í›„ë³´ ì„ ì •
            for name, model_info in self.models.items():
                if name not in self.gpu_resident_models:
                    continue
                    
                # CRITICAL ìš°ì„ ìˆœìœ„ëŠ” ì–¸ë¡œë“œ ë¶ˆê°€ (ë°±ë³¸ê³¼ translatorë§Œ)
                if model_info.priority == SwapPriority.CRITICAL:
                    logger.debug(f"[CRITICAL ë³´í˜¸] {name}ì€ ì–¸ë¡œë“œ ë¶ˆê°€")
                    continue
                    
                # ì›Œí¬í”Œë¡œìš° ë³´í˜¸ ëª¨ë¸ì€ ê±´ë„ˆë›°ê¸°
                if name in protected_models:
                    continue
                    
                # device_policyê°€ cpu_preloadì¸ ëª¨ë¸ ìš°ì„  ì–¸ë¡œë“œ
                is_cpu_preload = getattr(model_info, 'device_policy', '') == 'cpu_preload'
                
                unload_candidates.append({
                    'name': name,
                    'size_mb': model_info.size_mb,
                    'priority': model_info.priority.value,
                    'last_access': model_info.last_access,
                    'is_cpu_preload': is_cpu_preload
                })
            
            # í›„ë³´ê°€ ì—†ìœ¼ë©´ ë” ì´ìƒ ì–¸ë¡œë“œ ë¶ˆê°€ - ë¬´í•œ ë£¨í”„ ë°©ì§€
            if not unload_candidates:
                logger.warning("[ì–¸ë¡œë“œ ë¶ˆê°€] ë” ì´ìƒ ì–¸ë¡œë“œí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                logger.warning(f"[DSM] GPU ì‚¬ìš©ë¥  {current_usage*100:.1f}%ë¡œ ëª©í‘œ {target_usage*100:.1f}% ë‹¬ì„± ì‹¤íŒ¨")
                return False
            
            # ìš°ì„ ìˆœìœ„ ì•ˆì •ì  ë³€í™˜ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
            def _prio_rank(p):
                s = str(getattr(p, "value", p)).lower()
                if "critical" in s: return 0
                if "high" in s: return 1
                if "mid" in s or "medium" in s: return 2
                if "low" in s: return 3
                try:
                    return int(p)  # ìˆ«ìí˜•ì´ë©´ ê·¸ëŒ€ë¡œ
                except Exception:
                    return 4
            
            # ì •ë ¬: cpu_preload > LOW ìš°ì„ ìˆœìœ„ > LRU
            unload_candidates.sort(key=lambda x: (
                not x['is_cpu_preload'],  # cpu_preload ë¨¼ì €
                _prio_rank(x['priority']),  # ìš°ì„ ìˆœìœ„ (ì•ˆì •í™”ëœ ì •ë ¬)
                x['last_access']          # ì˜¤ë˜ëœ ê²ƒ ë¨¼ì €
            ))
            
            # ì²« ë²ˆì§¸ í›„ë³´ í•˜ë‚˜ë§Œ ì–¸ë¡œë“œ (ë°˜ë³µ ë£¨í”„ì—ì„œ ì ì§„ì ìœ¼ë¡œ ì²˜ë¦¬)
            candidate = unload_candidates[0]
            name = candidate['name']
            size_mb = candidate['size_mb']
            
            logger.info(f"[ì–¸ë¡œë“œ] {name} ({size_mb}MB) - {candidate['priority']}")
            
            try:
                # GPUì—ì„œ CPUë¡œ ì´ë™
                await self.unload_model_from_gpu(name)
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                torch.cuda.empty_cache()
                gc.collect()
                
                # ìƒíƒœ í™•ì¸
                gpu_info = get_gpu_memory_info()
                logger.info(f"[ì–¸ë¡œë“œ í›„] ì—¬ìœ : {gpu_info['free_mb']}MB (+{size_mb}MB)")
                logger.info(f"[GPU MEM] after swap-out {name}: alloc={gpu_info['allocated_mb']/1024:.1f}GB reserved={gpu_info['cached_mb']/1024:.1f}GB util={gpu_info['usage_percent']:.1f}%")
                
                # ë‹¤ìŒ ë£¨í”„ë¡œ ê³„ì†
                await asyncio.sleep(0.1)  # ì§§ì€ ëŒ€ê¸°ë¡œ ì‹œìŠ¤í…œ ì•ˆì •í™”
                # break ì œê±° - while ë£¨í”„ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ usage ì¬í‰ê°€
                
            except Exception as e:
                logger.error(f"{name} ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ë‹¤ìŒ í›„ë³´ë¡œ ê³„ì† ì‹œë„
            
            # ë£¨í”„ ëì—ì„œ ì‚¬ìš©ë¥ ì´ ì—¬ì „íˆ ë†’ìœ¼ë©´ CRITICAL ì™¸ ëª¨ë¸ ê°•ì œ ì–¸ë¡œë“œ
            # (while ë£¨í”„ ì•ˆì— ìˆì–´ì•¼ í•¨ - ë“¤ì—¬ì“°ê¸° ìˆ˜ì •)
            gpu_info = get_gpu_memory_info()
            if gpu_info:
                current_usage = gpu_info['usage_percent'] / 100.0
                current_free_mb = gpu_info['free_mb']
                
                # ëª©í‘œ ë‹¬ì„± í™•ì¸ (ê°•ì œ ì–¸ë¡œë“œ í›„ ì¬í™•ì¸)
                if current_usage <= target_usage and current_free_mb >= required_mb:
                    logger.info(f"[ëª©í‘œ ë‹¬ì„±] ì‚¬ìš©ë¥ : {current_usage*100:.1f}% <= {target_usage*100:.1f}%, ì—¬ìœ : {current_free_mb}MB >= {required_mb}MB")
                    return True
                
                # ì•„ì§ ëª©í‘œ ë¯¸ë‹¬ì„±ì‹œ ê°•ì œ ì–¸ë¡œë“œ
                if current_usage > target_usage:
                    logger.warning(f"[ê°•ì œ ì–¸ë¡œë“œ] ì‚¬ìš©ë¥  {current_usage*100:.1f}% > ëª©í‘œ {target_usage*100:.1f}%")
                    
                    # CRITICALì´ ì•„ë‹Œ ì²« ë²ˆì§¸ ëª¨ë¸ ê°•ì œ ì–¸ë¡œë“œ
                    for name, model_info in self.models.items():
                        if name not in self.gpu_resident_models:
                            continue
                        if model_info.priority != SwapPriority.CRITICAL:
                            logger.info(f"[ê°•ì œ ì–¸ë¡œë“œ] {name} (priority: {model_info.priority.value})")
                            await self.unload_model_from_gpu(name)
                            # while ë£¨í”„ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ usage ì¬í‰ê°€
                            logger.debug(f"[DSM] Re-check GPU usage after forced unload")
                            break  # for ë£¨í”„ë§Œ íƒˆì¶œ, while ë£¨í”„ëŠ” ê³„ì†
        
    @classmethod
    def get_instance(cls):
        """ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if not hasattr(cls, '_instance'):
            cls._instance = cls()
        return cls._instance

# ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
class SwapContext:
    """ìŠ¤ì™‘ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € - with ë¬¸ìœ¼ë¡œ ì‚¬ìš©"""
    
    def __init__(self, swap_manager: RedHeartDynamicSwapManager, model_names: List[str]):
        self.swap_manager = swap_manager
        self.model_names = model_names
        self.loaded_models = {}
        
    async def __aenter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ì§„ì…"""
        for name in self.model_names:
            model = await self.swap_manager.get_model(name)
            self.loaded_models[name] = model
        return self.loaded_models
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ"""
        # í•„ìš”ì— ë”°ë¼ ëª¨ë¸ë“¤ì„ ì–¸ë¡œë“œí•  ìˆ˜ ìˆì§€ë§Œ, 
        # ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”©ì„ ìœ„í•´ ì¦‰ì‹œ ì–¸ë¡œë“œí•˜ì§€ ì•ŠìŒ
        pass

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
async def example_usage():
    """ë™ì  ìŠ¤ì™‘ ë§¤ë‹ˆì € ì‚¬ìš© ì˜ˆì‹œ"""
    swap_manager = RedHeartDynamicSwapManager()
    await swap_manager.initialize()
    
    try:
        # ê°€ìƒì˜ ëª¨ë¸ ë“±ë¡
        dummy_model = nn.Linear(1000, 1000)
        swap_manager.register_model("test_model", dummy_model, SwapPriority.HIGH)
        
        # ëª¨ë¸ ì‚¬ìš©
        async with SwapContext(swap_manager, ["test_model"]) as models:
            test_model = models["test_model"]
            # ëª¨ë¸ ì‚¬ìš©...
            
        # í†µê³„ ì¶œë ¥
        stats = swap_manager.get_stats()
        memory_status = swap_manager.get_memory_status()
        
        print("ìŠ¤ì™‘ í†µê³„:", stats)
        print("ë©”ëª¨ë¦¬ ìƒíƒœ:", memory_status)
        
    finally:
        await swap_manager.shutdown()

# í˜¸í™˜ì„±ì„ ìœ„í•œ alias ì œê³µ
DynamicSwapManager = RedHeartDynamicSwapManager

# ì „ì—­ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_swap_mgr = None

def set_swap_manager(inst: Union["RedHeartDynamicSwapManager", "DynamicSwapManager"]):
    """ì™¸ë¶€(ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°)ì—ì„œ ìƒì„±í•œ DSMì„ ì „ì—­ ê³µìœ  ì¸ìŠ¤í„´ìŠ¤ë¡œ ê³ ì •."""
    global _swap_mgr
    _swap_mgr = inst
    logger.info(f"[DSM] Global swap manager set -> id={id(inst)}")

def get_swap_manager() -> DynamicSwapManager:
    """ì „ì—­ ìŠ¤ì™‘ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì™¸ë¶€ í˜¸ì¶œë¶€ ê¸°ëŒ€ ì‹¬ë³¼)"""
    global _swap_mgr
    if _swap_mgr is None:
        _swap_mgr = DynamicSwapManager.get_instance()
    return _swap_mgr

__all__ = ["RedHeartDynamicSwapManager", "DynamicSwapManager", "get_swap_manager"]

if __name__ == "__main__":
    asyncio.run(example_usage())