#!/usr/bin/env python3
"""
Red Heart AI ì›ë³¸ ë°ì´í„°ì…‹ íŒŒì‹± ìœ í‹¸ë¦¬í‹°
ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¹„ì •í˜• ë°ì´í„°ë¥¼ í†µí•© í˜•ì‹ìœ¼ë¡œ ë³€í™˜
"""

import os
import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatasetParser:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë°ì´í„°ì…‹ íŒŒì„œ"""
    
    def __init__(self, base_path: str = "for_learn_dataset"):
        self.base_path = Path(base_path)
        self.parsed_data = []
    
    def parse_ebs_format(self, file_path: Path) -> List[Dict]:
        """EBS ë¬¸í•™ ë°ì´í„° íŒŒì‹±"""
        results = []
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ì‘í’ˆë³„ë¡œ ë¶„ë¦¬ (êµ¬ë¶„ì: ê¸´ ëŒ€ì‹œ ë¼ì¸)
        works = re.split(r'ã…¡{40,}', content)
        
        for work in works:
            if not work.strip():
                continue
            
            lines = work.strip().split('\n')
            data = {
                'source': 'ebs_literature',
                'type': 'korean_literature',
                'text': '',
                'metadata': {}
            }
            
            current_field = None
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # í•„ë“œ ì¶”ì¶œ
                if line.endswith(')') and '(' in line and not ':' in line:
                    # ì‘í’ˆëª…
                    data['title'] = line
                elif line.startswith('ìƒí™©ì„¤ëª…:'):
                    current_field = 'situation'
                    data['text'] = line.replace('ìƒí™©ì„¤ëª…:', '').strip()
                elif line.startswith('ì´í•´ê´€ê³„ì:'):
                    data['metadata']['stakeholders'] = line.replace('ì´í•´ê´€ê³„ì:', '').strip()
                elif line.startswith('ìœ¤ë¦¬ì  ë”œë ˆë§ˆ:'):
                    data['metadata']['ethical_dilemma'] = line.replace('ìœ¤ë¦¬ì  ë”œë ˆë§ˆ:', '').strip()
                elif line.startswith('ì£¼ìš” ê°ì •:'):
                    current_field = 'emotions'
                elif line.startswith('ê°ì • ì›ì¸:'):
                    current_field = 'emotion_causes'
                elif line.startswith('ì„ íƒì§€'):
                    current_field = 'choices'
                elif line.startswith('ë•ëª©ìœ¤ë¦¬:'):
                    data['metadata']['virtue_ethics'] = line.replace('ë•ëª©ìœ¤ë¦¬:', '').strip()
                elif line.startswith('ì˜ë¬´ìœ¤ë¦¬:'):
                    data['metadata']['duty_ethics'] = line.replace('ì˜ë¬´ìœ¤ë¦¬:', '').strip()
                elif line.startswith('ê²°ê³¼ìœ¤ë¦¬:'):
                    data['metadata']['consequence_ethics'] = line.replace('ê²°ê³¼ìœ¤ë¦¬:', '').strip()
                else:
                    # ê°ì • ì ìˆ˜ íŒŒì‹±
                    if current_field == 'emotions' and ':' in line:
                        emotion, score = line.split(':')
                        if 'emotions' not in data['metadata']:
                            data['metadata']['emotions'] = {}
                        try:
                            data['metadata']['emotions'][emotion.strip()] = int(score.strip())
                        except:
                            pass
            
            if data['text']:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                results.append(data)
        
        return results
    
    def parse_claude_format(self, file_path: Path) -> List[Dict]:
        """Claudeê°€ ìƒì„±í•œ ë°ì´í„° íŒŒì‹±"""
        results = []
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ì‚¬ë¡€ë³„ë¡œ ë¶„ë¦¬
        cases = re.split(r'ì‚¬ë¡€ \d+:', content)
        
        for case in cases[1:]:  # ì²« ë²ˆì§¸ëŠ” ë¹ˆ ë¬¸ìì—´
            if not case.strip():
                continue
            
            data = {
                'source': 'ai_generated',
                'type': 'ethical_dilemma',
                'text': '',
                'metadata': {}
            }
            
            # ì œëª© ì¶”ì¶œ
            title_match = re.search(r'ìƒí™© ì œëª©:\s*(.+)', case)
            if title_match:
                data['title'] = title_match.group(1).strip()
            
            # ìƒí™© ì„¤ëª… ì¶”ì¶œ
            desc_match = re.search(r'ìƒí™© ì„¤ëª…:\s*(.+?)(?=ê´€ë ¨ëœ í•µì‹¬|$)', case, re.DOTALL)
            if desc_match:
                data['text'] = desc_match.group(1).strip()
            
            # í•µì‹¬ ê°€ì¹˜ ì¶”ì¶œ
            values_match = re.search(r'ê´€ë ¨ëœ í•µì‹¬ ê°€ì¹˜/ì›ì¹™:\s*(.+?)(?=\d+\.|$)', case, re.DOTALL)
            if values_match:
                values = values_match.group(1).strip()
                data['metadata']['core_values'] = [v.strip() for v in values.split('\n') if v.strip()]
            
            # ê°ì • ìƒíƒœ ì¶”ì¶œ
            emotion_match = re.search(r'ì£¼ìš” ê°ì •:\s*(.+)', case)
            if emotion_match:
                data['metadata']['emotions'] = emotion_match.group(1).strip()
            
            emotion_intensity_match = re.search(r'ê°ì • ê°•ë„:\s*(\d+)/10', case)
            if emotion_intensity_match:
                data['metadata']['emotion_intensity'] = int(emotion_intensity_match.group(1))
            
            if data['text']:
                results.append(data)
        
        return results
    
    def parse_scruples_format(self, file_path: Path) -> List[Dict]:
        """Scruples JSONL ë°ì´í„° íŒŒì‹±"""
        results = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    item = json.loads(line.strip())
                    data = {
                        'source': 'scruples',
                        'type': 'moral_judgment',
                        'title': item.get('title', ''),
                        'text': item.get('text', ''),
                        'metadata': {
                            'action': item.get('action', {}),
                            'label': item.get('label', ''),
                            'binarized_label': item.get('binarized_label', ''),
                            'post_type': item.get('post_type', '')
                        }
                    }
                    
                    if data['text']:
                        results.append(data)
                except:
                    continue
        
        return results
    
    def parse_all_datasets(self) -> List[Dict]:
        """ëª¨ë“  ë°ì´í„°ì…‹ íŒŒì‹±"""
        all_data = []
        
        # EBS ë°ì´í„°
        ebs_dir = self.base_path / "ai_ebs"
        if ebs_dir.exists():
            for file in ebs_dir.glob("*.txt"):
                logger.info(f"íŒŒì‹± ì¤‘: {file}")
                data = self.parse_ebs_format(file)
                all_data.extend(data)
                logger.info(f"  - {len(data)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ")
        
        # AI ìƒì„± ë°ì´í„°
        ai_dir = self.base_path / "ai_generated_dataset"
        if ai_dir.exists():
            for file in ai_dir.glob("*.txt"):
                logger.info(f"íŒŒì‹± ì¤‘: {file}")
                data = self.parse_claude_format(file)
                all_data.extend(data)
                logger.info(f"  - {len(data)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ")
        
        # Scruples ë°ì´í„°
        scruples_dir = self.base_path / "scruples_real_data"
        if scruples_dir.exists():
            for subdir in ['anecdotes', 'dilemmas']:
                sub_path = scruples_dir / subdir
                if sub_path.exists():
                    for file in sub_path.glob("*.jsonl"):
                        if 'train' in file.name:  # í•™ìŠµ ë°ì´í„°ë§Œ
                            logger.info(f"íŒŒì‹± ì¤‘: {file}")
                            data = self.parse_scruples_format(file)
                            # ë” ë§ì€ ë°ì´í„° ì‚¬ìš©
                            sample_size = min(1000, len(data))  # ìµœëŒ€ 1000ê°œ
                            all_data.extend(data[:sample_size])
                            logger.info(f"  - {sample_size}ê°œ ìƒ˜í”Œ ì¶”ì¶œ")
        
        logger.info(f"ì´ {len(all_data)}ê°œ ìƒ˜í”Œ íŒŒì‹± ì™„ë£Œ")
        return all_data
    
    def save_parsed_data(self, output_path: str = "parsed_raw_datasets.json"):
        """íŒŒì‹±ëœ ë°ì´í„° ì €ì¥"""
        data = self.parse_all_datasets()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")
        return data

if __name__ == "__main__":
    parser = DatasetParser()
    data = parser.save_parsed_data()
    
    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ í†µê³„:")
    print(f"  - ì´ ìƒ˜í”Œ ìˆ˜: {len(data)}")
    
    sources = {}
    for item in data:
        source = item.get('source', 'unknown')
        sources[source] = sources.get(source, 0) + 1
    
    print(f"  - ì†ŒìŠ¤ë³„ ë¶„í¬:")
    for source, count in sources.items():
        print(f"    â€¢ {source}: {count}ê°œ")