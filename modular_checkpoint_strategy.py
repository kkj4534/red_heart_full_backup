#!/usr/bin/env python3
"""
Red Heart AI ëª¨ë“ˆë³„ ì²´í¬í¬ì¸íŠ¸ ì „ëµ
ì—°ë™ ê·¸ë£¹ê³¼ ë…ë¦½ ëª¨ë“ˆì„ ë¶„ë¦¬í•˜ì—¬ ì €ì¥/ë¡œë“œ
"""

import torch
import torch.nn as nn
from pathlib import Path
from typing import Dict, List, Optional, Any
import logging
import json
from datetime import datetime

logger = logging.getLogger('RedHeart.ModularCheckpoint')


class ModularCheckpointStrategy:
    """
    ëª¨ë“ˆë³„ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ì „ëµ
    - ì—°ë™ ê·¸ë£¹: í•¨ê»˜ ì—…ë°ì´íŠ¸ë˜ëŠ” ëª¨ë“ˆë“¤
    - ë…ë¦½ ëª¨ë“ˆ: ê°œë³„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ëŠ” ëª¨ë“ˆë“¤
    """
    
    # ì—°ë™ ê·¸ë£¹ ì •ì˜ (í•¨ê»˜ ì—…ë°ì´íŠ¸)
    INTEGRATED_GROUPS = {
        'core_backbone_heads': [
            'backbone',
            'emotion_head', 
            'bentham_head',
            'regret_head',
            'surd_head'
        ],
        'emotion_analyzers': [
            'neural_emotion_analyzer',
            'neural_multimodal_emotion',
            'advanced_emotion',
            'phase0_net',
            'phase2_net',
            'hierarchical_integrator'
        ],
        'ethical_analyzers': [
            'neural_bentham_analyzer',
            'neural_moral_analyzer',
            'advanced_bentham'
        ],
        'decision_analyzers': [
            'neural_regret_analyzer',
            'neural_decision_analyzer',
            'advanced_regret'
        ],
        'surd_analyzers': [
            'neural_surd_analyzer',
            'neural_semantic_analyzer',
            'advanced_surd'
        ],
        'signal_processors': [
            'dsp_simulator',
            'kalman_filter'
        ]
    }
    
    # ë…ë¦½ ëª¨ë“ˆ ì •ì˜ (ê°œë³„ ì—…ë°ì´íŠ¸)
    INDEPENDENT_MODULES = [
        'neural_social_analyzer',
        'neural_cultural_analyzer',
        'neural_context_analyzer',
        'neural_narrative_analyzer'
    ]
    
    def __init__(self, checkpoint_dir: str = './checkpoints_modular'):
        """
        Args:
            checkpoint_dir: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # ê° ê·¸ë£¹/ëª¨ë“ˆë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        for group_name in self.INTEGRATED_GROUPS:
            (self.checkpoint_dir / group_name).mkdir(exist_ok=True)
        
        (self.checkpoint_dir / 'independent').mkdir(exist_ok=True)
        
        logger.info(f"âœ… ModularCheckpointStrategy ì´ˆê¸°í™” ì™„ë£Œ: {self.checkpoint_dir}")
    
    def save_modular_checkpoint(
        self, 
        modules: Dict[str, nn.Module],
        epoch: int,
        global_step: int,
        loss: float,
        optimizer: Optional[torch.optim.Optimizer] = None,
        metadata: Optional[Dict] = None
    ):
        """
        ëª¨ë“ˆë³„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        
        Args:
            modules: ëª¨ë“ˆ ë”•ì…”ë„ˆë¦¬
            epoch: í˜„ì¬ ì—í¬í¬
            global_step: ì „ì—­ ìŠ¤í…
            loss: í˜„ì¬ ì†ì‹¤
            optimizer: ì˜µí‹°ë§ˆì´ì € (ì„ íƒ)
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì„ íƒ)
        """
        timestamp = datetime.now().isoformat()
        
        # 1. ì—°ë™ ê·¸ë£¹ë³„ ì €ì¥
        for group_name, module_names in self.INTEGRATED_GROUPS.items():
            group_modules = {}
            
            for module_name in module_names:
                # ëª¨ë“ˆ ì´ë¦„ ë³€í˜• ì²˜ë¦¬ (head suffix ë“±)
                actual_name = self._find_module(modules, module_name)
                if actual_name and modules[actual_name] is not None:
                    group_modules[actual_name] = modules[actual_name].state_dict()
            
            if group_modules:
                group_checkpoint = {
                    'epoch': epoch,
                    'global_step': global_step,
                    'loss': loss,
                    'timestamp': timestamp,
                    'modules': group_modules,
                    'metadata': metadata or {}
                }
                
                # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ì €ì¥ (ê·¸ë£¹ë³„)
                if optimizer and group_name == 'core_backbone_heads':
                    group_checkpoint['optimizer'] = optimizer.state_dict()
                
                checkpoint_path = self.checkpoint_dir / group_name / f"checkpoint_epoch_{epoch+1}.pt"
                torch.save(group_checkpoint, checkpoint_path)
                logger.info(f"  ğŸ’¾ {group_name} ê·¸ë£¹ ì €ì¥: {checkpoint_path}")
        
        # 2. ë…ë¦½ ëª¨ë“ˆ ê°œë³„ ì €ì¥
        for module_name in self.INDEPENDENT_MODULES:
            actual_name = self._find_module(modules, module_name)
            if actual_name and modules[actual_name] is not None:
                module_checkpoint = {
                    'epoch': epoch,
                    'global_step': global_step,
                    'loss': loss,
                    'timestamp': timestamp,
                    'module_state': modules[actual_name].state_dict(),
                    'metadata': metadata or {}
                }
                
                checkpoint_path = self.checkpoint_dir / 'independent' / f"{actual_name}_epoch_{epoch+1}.pt"
                torch.save(module_checkpoint, checkpoint_path)
                logger.info(f"  ğŸ’¾ ë…ë¦½ ëª¨ë“ˆ {actual_name} ì €ì¥: {checkpoint_path}")
        
        # 3. ì „ì²´ ë©”íƒ€ë°ì´í„° ì €ì¥
        meta_info = {
            'epoch': epoch,
            'global_step': global_step,
            'loss': loss,
            'timestamp': timestamp,
            'groups_saved': list(self.INTEGRATED_GROUPS.keys()),
            'independent_saved': self.INDEPENDENT_MODULES,
            'metadata': metadata or {}
        }
        
        meta_path = self.checkpoint_dir / f"meta_epoch_{epoch+1}.json"
        with open(meta_path, 'w') as f:
            json.dump(meta_info, f, indent=2)
        
        logger.info(f"âœ… ëª¨ë“ˆë³„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ (Epoch {epoch+1})")
    
    def load_modular_checkpoint(
        self,
        modules: Dict[str, nn.Module],
        checkpoint_epoch: Optional[int] = None,
        load_optimizer: bool = True
    ) -> Dict[str, Any]:
        """
        ëª¨ë“ˆë³„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        
        Args:
            modules: ëª¨ë“ˆ ë”•ì…”ë„ˆë¦¬
            checkpoint_epoch: ë¡œë“œí•  ì—í¬í¬ (Noneì´ë©´ ìµœì‹ )
            load_optimizer: ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ ì—¬ë¶€
            
        Returns:
            ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ ì •ë³´
        """
        # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        if checkpoint_epoch is None:
            meta_files = list(self.checkpoint_dir.glob("meta_epoch_*.json"))
            if not meta_files:
                logger.warning("ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {}
            
            latest_meta = max(meta_files, key=lambda p: int(p.stem.split('_')[-1]))
            checkpoint_epoch = int(latest_meta.stem.split('_')[-1]) - 1
        
        logger.info(f"ğŸ“¥ Epoch {checkpoint_epoch+1} ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘...")
        
        loaded_info = {
            'epoch': checkpoint_epoch,
            'modules_loaded': [],
            'modules_failed': []
        }
        
        # 1. ì—°ë™ ê·¸ë£¹ ë¡œë“œ
        for group_name, module_names in self.INTEGRATED_GROUPS.items():
            checkpoint_path = self.checkpoint_dir / group_name / f"checkpoint_epoch_{checkpoint_epoch+1}.pt"
            
            if checkpoint_path.exists():
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                for module_name, state_dict in checkpoint['modules'].items():
                    actual_name = self._find_module(modules, module_name)
                    if actual_name and modules[actual_name] is not None:
                        try:
                            modules[actual_name].load_state_dict(state_dict)
                            loaded_info['modules_loaded'].append(actual_name)
                            logger.info(f"  âœ… {actual_name} ë¡œë“œ ì™„ë£Œ")
                        except Exception as e:
                            loaded_info['modules_failed'].append(actual_name)
                            logger.error(f"  âŒ {actual_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ
                if load_optimizer and 'optimizer' in checkpoint:
                    loaded_info['optimizer_state'] = checkpoint['optimizer']
                
                loaded_info['loss'] = checkpoint.get('loss', 0.0)
                loaded_info['global_step'] = checkpoint.get('global_step', 0)
        
        # 2. ë…ë¦½ ëª¨ë“ˆ ë¡œë“œ
        for module_name in self.INDEPENDENT_MODULES:
            actual_name = self._find_module(modules, module_name)
            if actual_name:
                checkpoint_path = self.checkpoint_dir / 'independent' / f"{actual_name}_epoch_{checkpoint_epoch+1}.pt"
                
                if checkpoint_path.exists() and modules[actual_name] is not None:
                    checkpoint = torch.load(checkpoint_path, map_location='cpu')
                    try:
                        modules[actual_name].load_state_dict(checkpoint['module_state'])
                        loaded_info['modules_loaded'].append(actual_name)
                        logger.info(f"  âœ… ë…ë¦½ ëª¨ë“ˆ {actual_name} ë¡œë“œ ì™„ë£Œ")
                    except Exception as e:
                        loaded_info['modules_failed'].append(actual_name)
                        logger.error(f"  âŒ ë…ë¦½ ëª¨ë“ˆ {actual_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
        logger.info(f"  - ë¡œë“œ ì„±ê³µ: {len(loaded_info['modules_loaded'])} ëª¨ë“ˆ")
        logger.info(f"  - ë¡œë“œ ì‹¤íŒ¨: {len(loaded_info['modules_failed'])} ëª¨ë“ˆ")
        
        return loaded_info
    
    def _find_module(self, modules: Dict[str, nn.Module], module_name: str) -> Optional[str]:
        """
        ëª¨ë“ˆ ì´ë¦„ ì°¾ê¸° (ìœ ì—°í•œ ë§¤ì¹­)
        
        Args:
            modules: ëª¨ë“ˆ ë”•ì…”ë„ˆë¦¬
            module_name: ì°¾ì„ ëª¨ë“ˆ ì´ë¦„
            
        Returns:
            ì‹¤ì œ ëª¨ë“ˆ ì´ë¦„ ë˜ëŠ” None
        """
        # ì •í™•í•œ ë§¤ì¹­
        if module_name in modules:
            return module_name
        
        # ë¶€ë¶„ ë§¤ì¹­ (ì˜ˆ: 'emotion_head' -> 'emotion')
        for actual_name in modules:
            if module_name.replace('_head', '') == actual_name:
                return actual_name
            if module_name.replace('neural_', '') == actual_name.replace('neural_', ''):
                return actual_name
            if module_name in actual_name or actual_name in module_name:
                return actual_name
        
        return None
    
    def get_optimizer_groups(self, modules: Dict[str, nn.Module]) -> List[Dict]:
        """
        ì—°ë™/ê°œë³„ ëª¨ë“ˆë³„ ì˜µí‹°ë§ˆì´ì € íŒŒë¼ë¯¸í„° ê·¸ë£¹ ìƒì„±
        
        Args:
            modules: ëª¨ë“ˆ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            ì˜µí‹°ë§ˆì´ì € íŒŒë¼ë¯¸í„° ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸
        """
        param_groups = []
        
        # 1. ì—°ë™ ê·¸ë£¹ë³„ íŒŒë¼ë¯¸í„° ê·¸ë£¹ ìƒì„±
        for group_name, module_names in self.INTEGRATED_GROUPS.items():
            group_params = []
            
            for module_name in module_names:
                actual_name = self._find_module(modules, module_name)
                if actual_name and modules[actual_name] is not None:
                    group_params.extend(list(modules[actual_name].parameters()))
            
            if group_params:
                # ê·¸ë£¹ë³„ í•™ìŠµë¥  ì¡°ì • ê°€ëŠ¥
                lr_scale = 1.0
                if 'analyzers' in group_name:
                    lr_scale = 0.5  # ë¶„ì„ê¸°ëŠ” ë‚®ì€ í•™ìŠµë¥ 
                elif 'signal' in group_name:
                    lr_scale = 0.8  # ì‹ í˜¸ ì²˜ë¦¬ê¸°ëŠ” ì¤‘ê°„ í•™ìŠµë¥ 
                
                param_groups.append({
                    'params': group_params,
                    'lr_scale': lr_scale,
                    'name': group_name
                })
                
                param_count = sum(p.numel() for p in group_params)
                logger.info(f"  íŒŒë¼ë¯¸í„° ê·¸ë£¹ '{group_name}': {param_count:,} íŒŒë¼ë¯¸í„° (lr_scale={lr_scale})")
        
        # 2. ë…ë¦½ ëª¨ë“ˆ ê°œë³„ íŒŒë¼ë¯¸í„° ê·¸ë£¹
        for module_name in self.INDEPENDENT_MODULES:
            actual_name = self._find_module(modules, module_name)
            if actual_name and modules[actual_name] is not None:
                module_params = list(modules[actual_name].parameters())
                if module_params:
                    param_groups.append({
                        'params': module_params,
                        'lr_scale': 0.3,  # ë…ë¦½ ëª¨ë“ˆì€ ë” ë‚®ì€ í•™ìŠµë¥ 
                        'name': actual_name
                    })
                    
                    param_count = sum(p.numel() for p in module_params)
                    logger.info(f"  ë…ë¦½ ëª¨ë“ˆ '{actual_name}': {param_count:,} íŒŒë¼ë¯¸í„° (lr_scale=0.3)")
        
        return param_groups