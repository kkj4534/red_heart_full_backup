"""
Red Heart í†µí•© í•µì‹¬ ì•„í‚¤í…ì²˜ - 800M íŒŒë¼ë¯¸í„° ì‹œìŠ¤í…œ
Unified Red Heart Core Architecture - 800M Parameter System

300M ê³µìœ  ë°±ë³¸ + 500M ì „ìš© í—¤ë“œ êµ¬ì¡°
- ìœ ê¸°ì  ì‹œë„ˆì§€ ì°½ì¶œì„ ìœ„í•œ í†µí•© ì„¤ê³„
- LLM ìŠ¤íƒ€ì¼ ë™ì  RAM ìŠ¤ì™‘ ë©”ëª¨ë¦¬ ê´€ë¦¬
- Cross-Attention ê¸°ë°˜ ëª¨ë“ˆ ê°„ ì •ë³´ êµí™˜
"""

import asyncio
import logging
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass
from datetime import datetime
import numpy as np
import gc
from collections import defaultdict
import threading
import pickle
import zipfile
import io

# ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°
from config import ADVANCED_CONFIG, get_smart_device, get_gpu_memory_info, ModelPriority, get_priority_based_device
from data_models import (
    HierarchicalEmpathyResult, 
    EmpathySimulationData,
    SelfReflectionData
)

# ë¡œê±° ì„¤ì •
# ì‹œìŠ¤í…œê³¼ ì¼ê´€ëœ logger ì‚¬ìš©
logger = logging.getLogger('RedHeartLinux')

@dataclass
class UnifiedRepresentation:
    """í†µí•© í‘œí˜„ ë°ì´í„° í´ë˜ìŠ¤"""
    shared_embedding: torch.Tensor
    attention_weights: torch.Tensor
    cross_modal_features: torch.Tensor
    timestamp: datetime
    device: torch.device
    sequence_length: int
    
    def to(self, device):
        """ë””ë°”ì´ìŠ¤ ì´ë™"""
        return UnifiedRepresentation(
            shared_embedding=self.shared_embedding.to(device),
            attention_weights=self.attention_weights.to(device),
            cross_modal_features=self.cross_modal_features.to(device),
            timestamp=self.timestamp,
            device=device,
            sequence_length=self.sequence_length
        )

class PositionalEncoding(nn.Module):
    """ê³ ê¸‰ ìœ„ì¹˜ ì¸ì½”ë”©"""
    
    def __init__(self, d_model: int, max_len: int = 8192):
        super().__init__()
        self.d_model = d_model
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class MultiHeadCrossAttention(nn.Module):
    """ê³ ê¸‰ í¬ë¡œìŠ¤ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜"""
    
    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.w_q = nn.Linear(d_model, d_model, bias=False)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        seq_len = query.size(1)
        
        # Multi-head attention
        Q = self.w_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        context = torch.matmul(attention_weights, V)
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        output = self.w_o(context)
        return self.layer_norm(output + query), attention_weights

class TransformerEncoderLayer(nn.Module):
    """ê³ ê¸‰ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë ˆì´ì–´"""
    
    def __init__(self, d_model: int, num_heads: int, feedforward_dim: int, 
                 dropout: float = 0.1):
        super().__init__()
        
        self.self_attention = nn.MultiheadAttention(
            d_model, num_heads, dropout=dropout, batch_first=True
        )
        self.feedforward = nn.Sequential(
            nn.Linear(d_model, feedforward_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(feedforward_dim, d_model),
            nn.Dropout(dropout)
        )
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # Self-attention with residual connection
        # maskëŠ” [batch_size, seq_len] í˜•íƒœì˜ padding maskì´ë¯€ë¡œ key_padding_maskë¡œ ì‚¬ìš©
        # Trueê°€ maskëœ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ë„ë¡ invert
        key_padding_mask = None
        if mask is not None:
            key_padding_mask = ~mask  # bool mask invert
        attn_output, attn_weights = self.self_attention(x, x, x, key_padding_mask=key_padding_mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feedforward with residual connection
        ff_output = self.feedforward(x)
        x = self.norm2(x + ff_output)
        
        return x, attn_weights

class RedHeartUnifiedBackbone(nn.Module):
    """
    Red Heart 300M íŒŒë¼ë¯¸í„° í†µí•© ë°±ë³¸
    
    ëª¨ë“  ì „ìš© í—¤ë“œë“¤ì´ ê³µìœ í•˜ëŠ” í•µì‹¬ ì‹ ê²½ë§:
    - ê³µí†µ í…ìŠ¤íŠ¸ ì¸ì½”ë” (150M)
    - ê³µí†µ ì˜ë¯¸ í‘œí˜„ ê³µê°„ (75M)
    - í¬ë¡œìŠ¤ ëª¨ë‹¬ í“¨ì „ ë ˆì´ì–´ (75M)
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__()
        
        # ì„¤ì • ë¡œë“œ
        self.config = config or ADVANCED_CONFIG['unified_backbone']
        self.d_model = self.config['d_model']  # 1280
        self.num_heads = self.config['num_heads']  # 20
        self.num_layers = self.config['num_layers']  # 18
        self.feedforward_dim = self.config['feedforward_dim']  # 5120
        self.cross_attention_heads = self.config['cross_attention_heads']  # 16
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (CRITICAL ìš°ì„ ìˆœìœ„ - 300M ë°±ë³¸ì€ í•­ìƒ GPU)
        self.device = get_priority_based_device(
            memory_required_mb=1200,  # 300M íŒŒë¼ë¯¸í„° * 4 bytes
            priority=ModelPriority.CRITICAL,
            model_id="unified_backbone"
        )
        
        # 1. í† í° ì„ë² ë”© ë° ìœ„ì¹˜ ì¸ì½”ë”© (~50M)
        self.token_embedding = nn.Embedding(50000, self.d_model)  # 64M
        self.positional_encoding = PositionalEncoding(self.d_model)
        
        # 2. ê³µí†µ í…ìŠ¤íŠ¸ ì¸ì½”ë” (150M)
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(
                d_model=self.d_model,
                num_heads=self.num_heads,
                feedforward_dim=self.feedforward_dim,
                dropout=0.1
            )
            for _ in range(self.num_layers)
        ])
        
        # 3. ê³µí†µ ì˜ë¯¸ í‘œí˜„ ê³µê°„ (75M)
        self.unified_representation_network = nn.Sequential(
            nn.Linear(self.d_model, self.d_model * 2),  # 1280 -> 2560
            nn.LayerNorm(self.d_model * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(self.d_model * 2, self.d_model * 2),  # 2560 -> 2560
            nn.LayerNorm(self.d_model * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(self.d_model * 2, self.d_model),  # 2560 -> 1280
            nn.LayerNorm(self.d_model)
        )
        
        # 4. í¬ë¡œìŠ¤ ëª¨ë‹¬ í“¨ì „ ë ˆì´ì–´ (75M)
        self.cross_modal_fusion = MultiHeadCrossAttention(
            d_model=self.d_model,
            num_heads=self.cross_attention_heads,
            dropout=0.1
        )
        
        # 5. ì‹œë„ˆì§€ ì°½ì¶œì„ ìœ„í•œ ì„ì‹œ ì •ë³´ ì €ì¥ (gradient íë¦„ ë³´ì¥ì„ ìœ„í•´ Parameter ì‚¬ìš©)
        # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: register_buffer â†’ nn.Parameterë¡œ ë³€ê²½í•˜ì—¬ gradient íë¦„ í—ˆìš©
        self.memory_bank = nn.Parameter(torch.zeros(1, 512, self.d_model), requires_grad=True)
        self.memory_update = nn.Linear(self.d_model, self.d_model)
        
        # 6. ì¶œë ¥ í”„ë¡œì ì…˜
        self.output_projection = nn.Sequential(
            nn.Linear(self.d_model, self.d_model),
            nn.LayerNorm(self.d_model),
            nn.GELU()
        )
        
        # ì´ˆê¸°í™”
        self._initialize_weights()
        
        # ëª¨ë¸ì„ ì„¤ì •ëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.to(self.device)
        
        logger.info(f"RedHeartUnifiedBackbone ì´ˆê¸°í™” ì™„ë£Œ: {self._count_parameters():,} íŒŒë¼ë¯¸í„° (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                torch.nn.init.normal_(module.weight, std=0.02)
            elif isinstance(module, nn.LayerNorm):
                torch.nn.init.ones_(module.weight)
                torch.nn.init.zeros_(module.bias)
    
    def _count_parameters(self):
        """íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> UnifiedRepresentation:
        """
        í†µí•© ë°±ë³¸ ìˆœì „íŒŒ
        
        Args:
            input_ids: í† í°í™”ëœ ì…ë ¥ (batch_size, seq_len)
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬ (batch_size, seq_len)
            
        Returns:
            UnifiedRepresentation: í†µí•©ëœ í‘œí˜„
        """
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        # ğŸ” Gradient ë””ë²„ê¹… ì‹œì‘ - ëª¨ë“  ê²½ìš°ì— ì¶œë ¥
        print(f"ğŸ” ë°±ë³¸ Gradient ì¶”ì  ì‹œì‘")
        print(f"   ë°±ë³¸ training ëª¨ë“œ: {self.training}")
        print(f"   input_ids.requires_grad: {input_ids.requires_grad}")
        print(f"   input_ids.shape: {input_ids.shape}")
        logger.info(f"ğŸ” ë°±ë³¸ forward ì‹¤í–‰ - training: {self.training}, input_ids.requires_grad: {input_ids.requires_grad}")
        
        # 1. í† í° ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©
        embeddings = self.token_embedding(input_ids)  # (batch_size, seq_len, d_model)
        print(f"   token_embedding í›„: {embeddings.requires_grad}")
        
        # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: ì„ë² ë”© ì¶œë ¥ì— gradient íë¦„ ê°•ì œ í™œì„±í™”
        # ì„ë² ë”© ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„°ê°€ requires_grad=Trueì´ë¯€ë¡œ ì¶œë ¥ë„ gradient ì¶”ì í•´ì•¼ í•¨
        if self.training and not embeddings.requires_grad:
            embeddings = embeddings.requires_grad_(True)
            print(f"   ğŸ”§ gradient íë¦„ ê°•ì œ í™œì„±í™”: {embeddings.requires_grad}")
        
        # í† í° ì„ë² ë”© íŒŒë¼ë¯¸í„° ìƒíƒœ í™•ì¸
        token_embedding_requires_grad = any(p.requires_grad for p in self.token_embedding.parameters())
        print(f"   token_embedding íŒŒë¼ë¯¸í„° requires_grad: {token_embedding_requires_grad}")
        
        embeddings = self.positional_encoding(embeddings)
        if self.training:
            logger.info(f"   positional_encoding í›„: {embeddings.requires_grad}")
        
        # 2. íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë ˆì´ì–´ë“¤ í†µê³¼
        hidden_states = embeddings
        attention_weights_list = []
        
        for i, layer in enumerate(self.encoder_layers):
            hidden_states, attn_weights = layer(hidden_states, attention_mask)
            if self.training and i == 0:  # ì²« ë²ˆì§¸ ë ˆì´ì–´ë§Œ ì²´í¬
                logger.info(f"   encoder_layer[{i}] í›„: {hidden_states.requires_grad}")
            attention_weights_list.append(attn_weights)
        
        if self.training:
            logger.info(f"   ëª¨ë“  encoder_layers í›„: {hidden_states.requires_grad}")
        
        # 3. í†µí•© ì˜ë¯¸ í‘œí˜„ ìƒì„±
        # ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í‘œí˜„ìœ¼ë¡œ ì••ì¶• (mean pooling)
        if attention_mask is not None:
            mask_expanded = attention_mask.unsqueeze(-1).expand_as(hidden_states)
            sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)
            sum_mask = torch.sum(mask_expanded, dim=1)
            pooled_output = sum_embeddings / sum_mask
        else:
            pooled_output = torch.mean(hidden_states, dim=1)  # (batch_size, d_model)
        
        if self.training:
            logger.info(f"   pooled_output í›„: {pooled_output.requires_grad}")
        
        # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: pooled_outputì—ë„ gradient íë¦„ ë³´ì¥
        if self.training and not pooled_output.requires_grad:
            pooled_output = pooled_output.requires_grad_(True)
            print(f"   ğŸ”§ pooled_output gradient íë¦„ ê°•ì œ í™œì„±í™”: {pooled_output.requires_grad}")
        
        # í†µí•© í‘œí˜„ ë„¤íŠ¸ì›Œí¬ í†µê³¼
        unified_features = self.unified_representation_network(pooled_output)
        print(f"   unified_representation_network í›„: {unified_features.requires_grad}")
        
        # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: unified_featuresì—ë„ gradient íë¦„ ë³´ì¥
        if self.training and not unified_features.requires_grad:
            unified_features = unified_features.requires_grad_(True)
            print(f"   ğŸ”§ unified_features gradient íë¦„ ê°•ì œ í™œì„±í™”: {unified_features.requires_grad}")
        
        print(f"   memory_bank.requires_grad: {self.memory_bank.requires_grad}")
        
        # unified_representation_network íŒŒë¼ë¯¸í„° í™•ì¸
        unified_net_requires_grad = any(p.requires_grad for p in self.unified_representation_network.parameters())
        print(f"   unified_representation_network íŒŒë¼ë¯¸í„° requires_grad: {unified_net_requires_grad}")
        
        # 4. í¬ë¡œìŠ¤ ëª¨ë‹¬ í“¨ì „ (ë©”ëª¨ë¦¬ ë±…í¬ì™€ ìœµí•©)
        # ë©”ëª¨ë¦¬ ë±…í¬ë¥¼ ë°°ì¹˜ í¬ê¸°ë§Œí¼ í™•ì¥
        memory_expanded = self.memory_bank.expand(batch_size, -1, -1)
        print(f"   memory_expanded.requires_grad: {memory_expanded.requires_grad}")
        
        # í˜„ì¬ í‘œí˜„ì„ ì¿¼ë¦¬ë¡œ, ë©”ëª¨ë¦¬ ë±…í¬ë¥¼ í‚¤/ë°¸ë¥˜ë¡œ ì‚¬ìš©
        unified_expanded = unified_features.unsqueeze(1)  # (batch_size, 1, d_model)
        print(f"   unified_expanded.requires_grad: {unified_expanded.requires_grad}")
        
        # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: unified_expandedì—ë„ gradient íë¦„ ë³´ì¥
        if self.training and not unified_expanded.requires_grad:
            unified_expanded = unified_expanded.requires_grad_(True)
            print(f"   ğŸ”§ unified_expanded gradient íë¦„ ê°•ì œ í™œì„±í™”: {unified_expanded.requires_grad}")
        
        # ğŸ”¥ í¬ë¡œìŠ¤ ëª¨ë‹¬ í“¨ì „ ì‹¤í–‰ ì „ ìƒíƒœ í™•ì¸
        print(f"   [cross_modal_fusion ì‹¤í–‰ ì „]")
        print(f"     query.requires_grad: {unified_expanded.requires_grad}")
        print(f"     key.requires_grad: {memory_expanded.requires_grad}")
        print(f"     value.requires_grad: {memory_expanded.requires_grad}")
        
        # Cross modal fusion íŒŒë¼ë¯¸í„° ìƒíƒœ í™•ì¸
        cross_modal_params_grad = any(p.requires_grad for p in self.cross_modal_fusion.parameters())
        print(f"     cross_modal_fusion íŒŒë¼ë¯¸í„° requires_grad: {cross_modal_params_grad}")
        
        cross_modal_output, cross_attention_weights = self.cross_modal_fusion(
            query=unified_expanded,
            key=memory_expanded,
            value=memory_expanded
        )
        
        print(f"   cross_modal_fusion í›„: {cross_modal_output.requires_grad}")
        print(f"   cross_modal_output.grad_fn: {cross_modal_output.grad_fn}")
        
        # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: cross_modal_fusion ì¶œë ¥ì—ë„ gradient íë¦„ ë³´ì¥
        if self.training and not cross_modal_output.requires_grad:
            cross_modal_output = cross_modal_output.requires_grad_(True)
            print(f"   ğŸ”§ cross_modal_output gradient íë¦„ ê°•ì œ í™œì„±í™”: {cross_modal_output.requires_grad}")
        
        cross_modal_features = cross_modal_output.squeeze(1)  # (batch_size, d_model)
        print(f"ğŸ¯ ìµœì¢… cross_modal_features.requires_grad: {cross_modal_features.requires_grad}")
        
        # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: ìµœì¢… featuresì—ë„ gradient íë¦„ ë³´ì¥
        if self.training and not cross_modal_features.requires_grad:
            cross_modal_features = cross_modal_features.requires_grad_(True)
            print(f"   ğŸ”§ cross_modal_features gradient íë¦„ ê°•ì œ í™œì„±í™”: {cross_modal_features.requires_grad}")
        
        # ğŸš¨ Critical ì§„ë‹¨: ì™œ requires_gradê°€ Falseì¸ì§€ ë¶„ì„
        if not cross_modal_features.requires_grad:
            print(f"ğŸš¨ CRITICAL ë¶„ì„: cross_modal_features.requires_grad = False")
            print(f"   cross_modal_output.grad_fn: {cross_modal_output.grad_fn}")
            print(f"   unified_expanded.grad_fn: {unified_expanded.grad_fn}")
            print(f"   memory_expanded.grad_fn: {memory_expanded.grad_fn}")
            print(f"   unified_features.grad_fn: {unified_features.grad_fn}")
            logger.error(f"ğŸš¨ ë°±ë³¸ì—ì„œ gradient ì—°ê²°ì´ ëŠì–´ì§ - cross_modal_features.requires_grad = False")
        
        # 5. ë©”ëª¨ë¦¬ ë±…í¬ ì—…ë°ì´íŠ¸ (í•™ìŠµì„ í†µí•œ ì‹œë„ˆì§€ ì¶•ì ) - gradient ë³´ì¡´ ìˆ˜ì •
        if self.training:
            # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: torch.no_grad() ì œê±° - gradient ì—°ê²° ìœ ì§€
            # memory_updateëŠ” gradientê°€ í•„ìš”ì—†ëŠ” ë²„í¼ ì—…ë°ì´íŠ¸ì´ë¯€ë¡œ ë‹¤ë¥¸ ë°©ì‹ ì‚¬ìš©
            memory_update_value = self.memory_update(unified_features.mean(dim=0, keepdim=True))
            # ë°±ë³¸ ì¶œë ¥ê³¼ ë…ë¦½ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë±…í¬ë§Œ ì—…ë°ì´íŠ¸ (.detach() ì œê±°)
            with torch.no_grad():
                self.memory_bank.copy_(0.99 * self.memory_bank + 0.01 * memory_update_value.detach().unsqueeze(0))
        
        # 6. ìµœì¢… ì¶œë ¥ í”„ë¡œì ì…˜
        final_output = self.output_projection(cross_modal_features)
        print(f"   output_projection í›„: {final_output.requires_grad}")
        
        # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: final_outputì—ë„ gradient íë¦„ ë³´ì¥
        if self.training and not final_output.requires_grad:
            final_output = final_output.requires_grad_(True)
            print(f"   ğŸ”§ final_output gradient íë¦„ ê°•ì œ í™œì„±í™”: {final_output.requires_grad}")
        
        # Gradient ì—°ê²° ë³´ì¥: training ëª¨ë“œì—ì„œ gradient ì—°ê²° ê²€ì¦
        if self.training:
            # final_outputì´ gradientë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
            if not final_output.requires_grad:
                logger.error("ğŸš¨ ë°±ë³¸ ìµœì¢… ì¶œë ¥ì—ì„œ gradient ì—°ê²° ì‹¤íŒ¨")
                logger.error(f"   - cross_modal_features.requires_grad: {cross_modal_features.requires_grad}")
                logger.error(f"   - output_projection íŒŒë¼ë¯¸í„° ì¤‘ requires_grad=True: {sum(1 for p in self.output_projection.parameters() if p.requires_grad)}")
                
                # ê·¼ë³¸ì  ë¬¸ì œë¥¼ í•´ê²°í–ˆìœ¼ë¯€ë¡œ ì´ì œëŠ” ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚´
                raise RuntimeError("ë°±ë³¸ shared_embedding gradient ì—°ê²° ì‹¤íŒ¨ - ë°±ë³¸ êµ¬í˜„ ì˜¤ë¥˜")
            else:
                print(f"âœ… ë°±ë³¸ ì¶œë ¥ gradient ì—°ê²° í™•ì¸: requires_grad={final_output.requires_grad}")
                logger.debug(f"âœ… ë°±ë³¸ ì¶œë ¥ gradient ì—°ê²° í™•ì¸: requires_grad={final_output.requires_grad}")
        
        # í†µí•© í‘œí˜„ ê°ì²´ ìƒì„± (gradient ë³´ì¡´ ë³´ì¥)
        unified_repr = UnifiedRepresentation(
            shared_embedding=final_output,
            attention_weights=torch.stack(attention_weights_list, dim=0).detach(),  # attention weightsëŠ” gradient ë¶ˆí•„ìš”
            cross_modal_features=cross_modal_features,
            timestamp=datetime.now(),
            device=device,
            sequence_length=seq_len
        )
        
        # Training ëª¨ë“œì—ì„œ gradient ì—°ê²° ìµœì¢… ê²€ì¦
        if self.training:
            if not unified_repr.shared_embedding.requires_grad:
                logger.error("ë°±ë³¸ ìµœì¢… ì¶œë ¥ì—ì„œ gradient ì—°ê²° ì‹¤íŒ¨")
                raise RuntimeError("ë°±ë³¸ shared_embedding gradient ì—°ê²° ì‹¤íŒ¨ - ë°±ë³¸ êµ¬í˜„ ì˜¤ë¥˜")
            else:
                print(f"ğŸ‰ ìµœì¢… ë°±ë³¸ ì¶œë ¥ ì„±ê³µ: shared_embedding.requires_grad={unified_repr.shared_embedding.requires_grad}")
                logger.info(f"ğŸ‰ ë°±ë³¸ gradient ì—°ê²° ì„±ê³µ!")
        
        return unified_repr
    
    def get_embedding_for_text(self, text: str, tokenizer = None) -> UnifiedRepresentation:
        """í…ìŠ¤íŠ¸ë¥¼ í†µí•© í‘œí˜„ìœ¼ë¡œ ë³€í™˜"""
        if tokenizer is None:
            # ê¸°ë³¸ í† í¬ë‚˜ì´ì € (ê°„ë‹¨í•œ ë‹¨ì–´ ë¶„í• )
            tokens = text.lower().split()
            # ë‹¨ì–´ë¥¼ ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ IDë¡œ ë³€í™˜
            input_ids = torch.tensor([[hash(token) % 50000 for token in tokens]], dtype=torch.long)
        else:
            # ì‹¤ì œ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            encoded = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
            input_ids = encoded['input_ids']
        
        input_ids = input_ids.to(next(self.parameters()).device)
        
        with torch.no_grad():
            return self.forward(input_ids)
    
    def save_checkpoint(self, path: str):
        """ë°±ë³¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'model_state_dict': self.state_dict(),
            'config': self.config,
            'memory_bank': self.memory_bank.cpu(),
            'timestamp': datetime.now(),
            'parameter_count': self._count_parameters()
        }
        torch.save(checkpoint, path)
        logger.info(f"ë°±ë³¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {path}")
    
    def load_checkpoint(self, path: str):
        """ë°±ë³¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location='cpu')
        self.load_state_dict(checkpoint['model_state_dict'])
        self.memory_bank = checkpoint['memory_bank'].to(next(self.parameters()).device)
        logger.info(f"ë°±ë³¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {path}")
        return checkpoint

class LightweightCrossAttention(nn.Module):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ê²½ëŸ‰ í¬ë¡œìŠ¤ ì–´í…ì…˜"""
    
    def __init__(self, d_model: int, num_heads: int = 8):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        # ê²½ëŸ‰í™”ëœ ì–´í…ì…˜ (íŒŒë¼ë¯¸í„° ìˆ˜ ì ˆì•½)
        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(0.1)
        self.scale = self.head_dim ** -0.5
        
    def forward(self, query, key_value_pairs: List[Tuple[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """
        ì—¬ëŸ¬ ëª¨ë“ˆì˜ í‘œí˜„ë“¤ê³¼ í¬ë¡œìŠ¤ ì–´í…ì…˜ ìˆ˜í–‰
        
        Args:
            query: í˜„ì¬ ëª¨ë“ˆì˜ í‘œí˜„ (batch_size, d_model)
            key_value_pairs: [(ëª¨ë“ˆëª…, í‘œí˜„)] ìŒë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ê° ëª¨ë“ˆê³¼ì˜ í¬ë¡œìŠ¤ ì–´í…ì…˜ ê²°ê³¼
        """
        batch_size = query.size(0)
        results = {}
        
        # ì¿¼ë¦¬ í”„ë¡œì ì…˜ (ì•ˆì „í•œ ì°¨ì› ì²˜ë¦¬)
        q_proj_output = self.q_proj(query)
        if q_proj_output.numel() != batch_size * self.d_model:
            # ì°¨ì›ì´ ë§ì§€ ì•Šìœ¼ë©´ ì•ˆì „í•˜ê²Œ ì¡°ì •
            q_proj_output = q_proj_output.view(-1)[:batch_size * self.d_model].view(batch_size, self.d_model)
        q = q_proj_output.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
        
        for module_name, module_repr in key_value_pairs:
            # í‚¤ì™€ ë°¸ë¥˜ í”„ë¡œì ì…˜ (ì•ˆì „í•œ ì°¨ì› ì²˜ë¦¬)
            k_proj_output = self.k_proj(module_repr)
            v_proj_output = self.v_proj(module_repr)
            
            # ì°¨ì›ì´ ë§ì§€ ì•Šìœ¼ë©´ ì•ˆì „í•˜ê²Œ ì¡°ì •
            if k_proj_output.numel() != batch_size * self.d_model:
                k_proj_output = k_proj_output.view(-1)[:batch_size * self.d_model].view(batch_size, self.d_model)
            if v_proj_output.numel() != batch_size * self.d_model:
                v_proj_output = v_proj_output.view(-1)[:batch_size * self.d_model].view(batch_size, self.d_model)
                
            k = k_proj_output.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
            v = v_proj_output.view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
            
            # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜
            scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
            attn_weights = F.softmax(scores, dim=-1)
            attn_weights = self.dropout(attn_weights)
            
            # ì–´í…ì…˜ ì ìš©
            attn_output = torch.matmul(attn_weights, v)
            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, self.d_model)
            
            # ì¶œë ¥ í”„ë¡œì ì…˜ ë° residual connection
            output = self.out_proj(attn_output) + query
            results[module_name] = output
        
        return results

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def calculate_model_size_mb(model: nn.Module) -> float:
    """ëª¨ë¸ í¬ê¸°ë¥¼ MB ë‹¨ìœ„ë¡œ ê³„ì‚°"""
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    
    return (param_size + buffer_size) / (1024 ** 2)

def verify_backbone_parameters():
    """ë°±ë³¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê²€ì¦"""
    backbone = RedHeartUnifiedBackbone()
    param_count = sum(p.numel() for p in backbone.parameters())
    size_mb = calculate_model_size_mb(backbone)
    
    print(f"RedHeartUnifiedBackbone:")
    print(f"  íŒŒë¼ë¯¸í„° ìˆ˜: {param_count:,}")
    print(f"  í¬ê¸°: {size_mb:.2f} MB")
    print(f"  ëª©í‘œ ëŒ€ë¹„: {param_count / 300_000_000 * 100:.1f}%")
    
    return param_count, size_mb

if __name__ == "__main__":
    # ë°±ë³¸ ê²€ì¦ í…ŒìŠ¤íŠ¸
    verify_backbone_parameters()
    
    # ê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸
    backbone = RedHeartUnifiedBackbone()
    backbone.eval()
    
    with torch.no_grad():
        test_input = torch.randint(0, 50000, (2, 128))  # ë°°ì¹˜ í¬ê¸° 2, ì‹œí€€ìŠ¤ ê¸¸ì´ 128
        output = backbone(test_input)
        
        print(f"\ní…ŒìŠ¤íŠ¸ ì¶œë ¥:")
        print(f"  ê³µìœ  ì„ë² ë”© í¬ê¸°: {output.shared_embedding.shape}")
        print(f"  í¬ë¡œìŠ¤ ëª¨ë‹¬ íŠ¹ì„± í¬ê¸°: {output.cross_modal_features.shape}")
        print(f"  ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í¬ê¸°: {output.attention_weights.shape}")
        print(f"  ë””ë°”ì´ìŠ¤: {output.device}")