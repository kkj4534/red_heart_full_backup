#!/usr/bin/env python3
"""
Red Heart AI í†µí•© ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬
ì„±ëŠ¥ ì¸¡ì • ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
"""

import asyncio
import time
import torch
import psutil
import numpy as np
import argparse
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple
import logging
from dataclasses import dataclass, field
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
import sys
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from main_unified import UnifiedInferenceSystem, InferenceConfig, MemoryMode

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger('Benchmark')


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    memory_mode: str
    num_iterations: int
    batch_size: int
    
    # ì‹œê°„ ì¸¡ì •
    total_time: float = 0.0
    avg_time_per_request: float = 0.0
    min_time: float = float('inf')
    max_time: float = 0.0
    std_time: float = 0.0
    percentile_50: float = 0.0
    percentile_95: float = 0.0
    percentile_99: float = 0.0
    
    # ë©”ëª¨ë¦¬ ì¸¡ì •
    peak_memory_mb: float = 0.0
    avg_memory_mb: float = 0.0
    min_memory_mb: float = float('inf')
    max_memory_mb: float = 0.0
    
    # GPU ì¸¡ì • (ìˆì„ ê²½ìš°)
    peak_gpu_memory_mb: float = 0.0
    avg_gpu_memory_mb: float = 0.0
    
    # ì²˜ë¦¬ëŸ‰
    throughput: float = 0.0  # requests per second
    
    # ëª¨ë“ˆë³„ ì‹œê°„
    module_times: Dict[str, float] = field(default_factory=dict)
    
    # ì˜¤ë¥˜ìœ¨
    error_count: int = 0
    error_rate: float = 0.0
    
    # ê°œë³„ ì¸¡ì •ê°’
    individual_times: List[float] = field(default_factory=list)
    individual_memories: List[float] = field(default_factory=list)


class UnifiedSystemBenchmark:
    """í†µí•© ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self, memory_mode: MemoryMode, batch_size: int = 1):
        self.memory_mode = memory_mode
        self.batch_size = batch_size
        self.system = None
        self.process = psutil.Process()
        
        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ì„¸íŠ¸
        self.test_texts = [
            "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ì•„ì„œ ê¸°ë¶„ì´ ì¢‹ìŠµë‹ˆë‹¤.",
            "ì‹œí—˜ì— ë–¨ì–´ì ¸ì„œ ë„ˆë¬´ ì†ìƒí•˜ê³  ìš°ìš¸í•©ë‹ˆë‹¤.",
            "ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜ ì‹œê°„ì„ ë³´ë‚´ë‹ˆ í–‰ë³µí•´ìš”.",
            "ë¯¸ë˜ê°€ ë¶ˆí™•ì‹¤í•´ì„œ ê±±ì •ì´ ë§ì´ ë©ë‹ˆë‹¤.",
            "ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ê²Œ ë˜ì–´ ì„¤ë ˆê³  ê¸°ëŒ€ë©ë‹ˆë‹¤.",
            "ì‹¤ìˆ˜ë¥¼ í•´ì„œ í›„íšŒê°€ ë˜ê³  ìì±…ê°ì´ ë“­ë‹ˆë‹¤.",
            "ê°€ì¡±ê³¼ í•¨ê»˜ ë³´ë‚´ëŠ” ì‹œê°„ì´ ì†Œì¤‘í•˜ê³  ê°ì‚¬í•©ë‹ˆë‹¤.",
            "ì¼ì´ ì˜ í’€ë¦¬ì§€ ì•Šì•„ì„œ ë‹µë‹µí•˜ê³  í™”ê°€ ë‚©ë‹ˆë‹¤.",
            "ëª©í‘œë¥¼ ë‹¬ì„±í•´ì„œ ë¿Œë“¯í•˜ê³  ìë‘ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.",
            "í˜¼ì ìˆìœ¼ë‹ˆ ì™¸ë¡­ê³  ì“¸ì“¸í•œ ê¸°ë¶„ì´ ë“­ë‹ˆë‹¤.",
            "ì¢‹ì€ ì†Œì‹ì„ ë“¤ì–´ì„œ ê¸°ì˜ê³  ì‹ ì´ ë‚©ë‹ˆë‹¤.",
            "ì‹¤ë§ìŠ¤ëŸ¬ìš´ ê²°ê³¼ì— ì¢Œì ˆê°ì„ ëŠë‚ë‹ˆë‹¤.",
            "ë„ì „ì ì¸ ê³¼ì œë¥¼ ì•ë‘ê³  ê¸´ì¥ë˜ì§€ë§Œ ë™ê¸°ë¶€ì—¬ê°€ ë©ë‹ˆë‹¤.",
            "í‰í™”ë¡œìš´ ìˆœê°„ì„ ì¦ê¸°ë©° í¸ì•ˆí•¨ì„ ëŠë‚ë‹ˆë‹¤.",
            "ì˜ˆìƒì¹˜ ëª»í•œ ì„ ë¬¼ì„ ë°›ì•„ì„œ ë†€ëê³  ê°ë™ì ì…ë‹ˆë‹¤."
        ]
        
    async def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ëª¨ë“œ: {self.memory_mode.value})")
        
        config = InferenceConfig(
            memory_mode=self.memory_mode,
            auto_memory_mode=False,
            batch_size=self.batch_size,
            enable_monitoring=False,  # ëª¨ë‹ˆí„°ë§ ë¹„í™œì„±í™”ë¡œ ìˆœìˆ˜ ì„±ëŠ¥ ì¸¡ì •
            verbose=False
        )
        
        self.system = UnifiedInferenceSystem(config)
        await self.system.initialize()
        
        # ì›Œë°ì—…
        await self._warmup()
        
        logger.info("ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def _warmup(self):
        """ì›Œë°ì—… ì‹¤í–‰"""
        logger.info("ì›Œë°ì—… ì¤‘...")
        for i in range(3):
            await self.system.analyze(self.test_texts[0])
        logger.info("ì›Œë°ì—… ì™„ë£Œ")
    
    def _get_memory_usage(self) -> Tuple[float, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        # CPU ë©”ëª¨ë¦¬
        cpu_memory = self.process.memory_info().rss / 1024 / 1024
        
        # GPU ë©”ëª¨ë¦¬
        gpu_memory = 0
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024
        
        return cpu_memory, gpu_memory
    
    async def run_benchmark(self, num_iterations: int = 100) -> BenchmarkResult:
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {num_iterations}íšŒ ë°˜ë³µ")
        
        result = BenchmarkResult(
            memory_mode=self.memory_mode.value,
            num_iterations=num_iterations,
            batch_size=self.batch_size
        )
        
        # ì¸¡ì •ê°’ ì €ì¥
        times = []
        memories = []
        gpu_memories = []
        errors = 0
        
        # ì§„í–‰ í‘œì‹œ
        progress_interval = max(1, num_iterations // 10)
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        start_total = time.time()
        
        for i in range(num_iterations):
            # í…ìŠ¤íŠ¸ ì„ íƒ (ìˆœí™˜)
            text = self.test_texts[i % len(self.test_texts)]
            
            # ë©”ëª¨ë¦¬ ì¸¡ì • (ì‹œì‘)
            mem_before, gpu_mem_before = self._get_memory_usage()
            
            # ì‹œê°„ ì¸¡ì •
            start = time.perf_counter()
            
            try:
                # ë¶„ì„ ì‹¤í–‰
                output = await self.system.analyze(text)
                
                # ì‹œê°„ ê¸°ë¡
                elapsed = time.perf_counter() - start
                times.append(elapsed)
                
                # ëª¨ë“ˆë³„ ì‹œê°„ ê¸°ë¡ (ì²« ë²ˆì§¸ ì‹¤í–‰ì—ì„œë§Œ)
                if i == 0 and 'processing_time' in output:
                    result.module_times['total'] = output['processing_time']
                
            except Exception as e:
                logger.warning(f"ë°˜ë³µ {i} ì‹¤íŒ¨: {e}")
                errors += 1
                elapsed = time.perf_counter() - start
                times.append(elapsed)
            
            # ë©”ëª¨ë¦¬ ì¸¡ì • (ì¢…ë£Œ)
            mem_after, gpu_mem_after = self._get_memory_usage()
            memories.append(mem_after)
            gpu_memories.append(gpu_mem_after)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (i + 1) % progress_interval == 0:
                progress = ((i + 1) / num_iterations) * 100
                logger.info(f"  ì§„í–‰: {progress:.0f}% ({i+1}/{num_iterations})")
        
        total_time = time.time() - start_total
        
        # í†µê³„ ê³„ì‚°
        times_array = np.array(times)
        memories_array = np.array(memories)
        gpu_memories_array = np.array(gpu_memories) if gpu_memories else np.array([0])
        
        result.total_time = total_time
        result.avg_time_per_request = np.mean(times_array)
        result.min_time = np.min(times_array)
        result.max_time = np.max(times_array)
        result.std_time = np.std(times_array)
        result.percentile_50 = np.percentile(times_array, 50)
        result.percentile_95 = np.percentile(times_array, 95)
        result.percentile_99 = np.percentile(times_array, 99)
        
        result.avg_memory_mb = np.mean(memories_array)
        result.min_memory_mb = np.min(memories_array)
        result.max_memory_mb = np.max(memories_array)
        result.peak_memory_mb = np.max(memories_array)
        
        result.avg_gpu_memory_mb = np.mean(gpu_memories_array)
        result.peak_gpu_memory_mb = np.max(gpu_memories_array)
        
        result.throughput = num_iterations / total_time
        result.error_count = errors
        result.error_rate = errors / num_iterations
        
        result.individual_times = times
        result.individual_memories = memories.tolist()
        
        return result
    
    def print_results(self, result: BenchmarkResult):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 70)
        print(f"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ - {result.memory_mode} ëª¨ë“œ")
        print("=" * 70)
        
        print(f"\nâ±ï¸ ì‹œê°„ ì„±ëŠ¥:")
        print(f"  ì´ ì‹¤í–‰ ì‹œê°„: {result.total_time:.2f}ì´ˆ")
        print(f"  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {result.avg_time_per_request*1000:.2f}ms")
        print(f"  ìµœì†Œ ì‹œê°„: {result.min_time*1000:.2f}ms")
        print(f"  ìµœëŒ€ ì‹œê°„: {result.max_time*1000:.2f}ms")
        print(f"  í‘œì¤€í¸ì°¨: {result.std_time*1000:.2f}ms")
        print(f"  ì¤‘ì•™ê°’ (P50): {result.percentile_50*1000:.2f}ms")
        print(f"  P95: {result.percentile_95*1000:.2f}ms")
        print(f"  P99: {result.percentile_99*1000:.2f}ms")
        print(f"  ì²˜ë¦¬ëŸ‰: {result.throughput:.2f} req/s")
        
        print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©:")
        print(f"  í‰ê·  CPU ë©”ëª¨ë¦¬: {result.avg_memory_mb:.1f}MB")
        print(f"  ìµœëŒ€ CPU ë©”ëª¨ë¦¬: {result.peak_memory_mb:.1f}MB")
        print(f"  ìµœì†Œ CPU ë©”ëª¨ë¦¬: {result.min_memory_mb:.1f}MB")
        
        if result.avg_gpu_memory_mb > 0:
            print(f"  í‰ê·  GPU ë©”ëª¨ë¦¬: {result.avg_gpu_memory_mb:.1f}MB")
            print(f"  ìµœëŒ€ GPU ë©”ëª¨ë¦¬: {result.peak_gpu_memory_mb:.1f}MB")
        
        if result.error_count > 0:
            print(f"\nâš ï¸ ì˜¤ë¥˜:")
            print(f"  ì˜¤ë¥˜ íšŸìˆ˜: {result.error_count}")
            print(f"  ì˜¤ë¥˜ìœ¨: {result.error_rate*100:.2f}%")
        
        print("=" * 70)
    
    def save_results(self, result: BenchmarkResult, filepath: Path):
        """ê²°ê³¼ ì €ì¥"""
        result_dict = {
            'timestamp': datetime.now().isoformat(),
            'memory_mode': result.memory_mode,
            'num_iterations': result.num_iterations,
            'batch_size': result.batch_size,
            'metrics': {
                'avg_time_ms': result.avg_time_per_request * 1000,
                'min_time_ms': result.min_time * 1000,
                'max_time_ms': result.max_time * 1000,
                'std_time_ms': result.std_time * 1000,
                'p50_ms': result.percentile_50 * 1000,
                'p95_ms': result.percentile_95 * 1000,
                'p99_ms': result.percentile_99 * 1000,
                'throughput_rps': result.throughput,
                'avg_memory_mb': result.avg_memory_mb,
                'peak_memory_mb': result.peak_memory_mb,
                'avg_gpu_memory_mb': result.avg_gpu_memory_mb,
                'peak_gpu_memory_mb': result.peak_gpu_memory_mb,
                'error_rate': result.error_rate
            },
            'raw_data': {
                'times': result.individual_times[:100],  # ì²˜ìŒ 100ê°œë§Œ
                'memories': result.individual_memories[:100]
            }
        }
        
        with open(filepath, 'w') as f:
            json.dump(result_dict, f, indent=2)
        
        logger.info(f"ê²°ê³¼ ì €ì¥: {filepath}")
    
    def plot_results(self, result: BenchmarkResult, save_path: Path = None):
        """ê²°ê³¼ ì‹œê°í™”"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(12, 8))
            
            # 1. ì‘ë‹µ ì‹œê°„ ë¶„í¬
            axes[0, 0].hist(np.array(result.individual_times) * 1000, bins=30, edgecolor='black')
            axes[0, 0].axvline(result.avg_time_per_request * 1000, color='red', linestyle='--', label='í‰ê· ')
            axes[0, 0].axvline(result.percentile_95 * 1000, color='orange', linestyle='--', label='P95')
            axes[0, 0].set_xlabel('ì‘ë‹µ ì‹œê°„ (ms)')
            axes[0, 0].set_ylabel('ë¹ˆë„')
            axes[0, 0].set_title('ì‘ë‹µ ì‹œê°„ ë¶„í¬')
            axes[0, 0].legend()
            
            # 2. ì‹œê°„ë³„ ì‘ë‹µ ì‹œê°„ ì¶”ì´
            axes[0, 1].plot(np.array(result.individual_times) * 1000)
            axes[0, 1].set_xlabel('ìš”ì²­ ë²ˆí˜¸')
            axes[0, 1].set_ylabel('ì‘ë‹µ ì‹œê°„ (ms)')
            axes[0, 1].set_title('ì‘ë‹µ ì‹œê°„ ì¶”ì´')
            axes[0, 1].grid(True, alpha=0.3)
            
            # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì´
            axes[1, 0].plot(result.individual_memories)
            axes[1, 0].axhline(result.avg_memory_mb, color='red', linestyle='--', label='í‰ê· ')
            axes[1, 0].set_xlabel('ìš”ì²­ ë²ˆí˜¸')
            axes[1, 0].set_ylabel('ë©”ëª¨ë¦¬ (MB)')
            axes[1, 0].set_title('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì´')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
            
            # 4. ì„±ëŠ¥ ìš”ì•½
            axes[1, 1].axis('off')
            summary_text = f"""
            ì„±ëŠ¥ ìš”ì•½ ({result.memory_mode} ëª¨ë“œ)
            
            í‰ê·  ì‘ë‹µ: {result.avg_time_per_request*1000:.1f}ms
            P95: {result.percentile_95*1000:.1f}ms
            P99: {result.percentile_99*1000:.1f}ms
            
            ì²˜ë¦¬ëŸ‰: {result.throughput:.1f} req/s
            
            í‰ê·  ë©”ëª¨ë¦¬: {result.avg_memory_mb:.0f}MB
            ìµœëŒ€ ë©”ëª¨ë¦¬: {result.peak_memory_mb:.0f}MB
            
            ì˜¤ë¥˜ìœ¨: {result.error_rate*100:.1f}%
            """
            axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center')
            
            plt.suptitle(f'Red Heart AI ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ - {result.memory_mode} ëª¨ë“œ', fontsize=14)
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=150)
                logger.info(f"ê·¸ë˜í”„ ì €ì¥: {save_path}")
            
            plt.show()
            
        except ImportError:
            logger.warning("matplotlib ì—†ìŒ - ì‹œê°í™” ê±´ë„ˆëœ€")


async def compare_memory_modes(num_iterations: int = 50):
    """ë©”ëª¨ë¦¬ ëª¨ë“œë³„ ë¹„êµ ë²¤ì¹˜ë§ˆí¬"""
    
    modes_to_test = [
        MemoryMode.MINIMAL,
        MemoryMode.LIGHT,
        MemoryMode.NORMAL,
        MemoryMode.HEAVY
    ]
    
    results = {}
    
    for mode in modes_to_test:
        logger.info(f"\n{'='*70}")
        logger.info(f"í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {mode.value}")
        logger.info(f"{'='*70}")
        
        try:
            benchmark = UnifiedSystemBenchmark(mode)
            await benchmark.initialize()
            
            result = await benchmark.run_benchmark(num_iterations)
            benchmark.print_results(result)
            
            # ê²°ê³¼ ì €ì¥
            filepath = Path(f"benchmark_{mode.value}_{int(time.time())}.json")
            benchmark.save_results(result, filepath)
            
            # ì‹œê°í™”
            plot_path = Path(f"benchmark_{mode.value}_{int(time.time())}.png")
            benchmark.plot_results(result, plot_path)
            
            results[mode.value] = result
            
        except Exception as e:
            logger.error(f"{mode.value} ëª¨ë“œ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            continue
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # ëª¨ë“œ ê°„ ëŒ€ê¸°
        await asyncio.sleep(5)
    
    # ë¹„êµ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“Š ë©”ëª¨ë¦¬ ëª¨ë“œë³„ ë¹„êµ")
    print("=" * 70)
    
    print(f"\n{'ëª¨ë“œ':<10} {'í‰ê· (ms)':<12} {'P95(ms)':<12} {'ì²˜ë¦¬ëŸ‰(req/s)':<15} {'ë©”ëª¨ë¦¬(MB)':<12}")
    print("-" * 70)
    
    for mode_name, result in results.items():
        print(f"{mode_name:<10} "
              f"{result.avg_time_per_request*1000:<12.1f} "
              f"{result.percentile_95*1000:<12.1f} "
              f"{result.throughput:<15.1f} "
              f"{result.peak_memory_mb:<12.0f}")
    
    return results


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Red Heart AI ë²¤ì¹˜ë§ˆí¬')
    parser.add_argument('--memory-mode', default='normal',
                       choices=['minimal', 'light', 'normal', 'heavy', 'ultra', 'extreme'],
                       help='ë©”ëª¨ë¦¬ ëª¨ë“œ')
    parser.add_argument('--num-iterations', type=int, default=100,
                       help='ë°˜ë³µ íšŸìˆ˜')
    parser.add_argument('--batch-size', type=int, default=1,
                       help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--compare', action='store_true',
                       help='ëª¨ë“  ë©”ëª¨ë¦¬ ëª¨ë“œ ë¹„êµ')
    
    args = parser.parse_args()
    
    if args.compare:
        # ëª¨ë“œ ë¹„êµ
        await compare_memory_modes(args.num_iterations)
    else:
        # ë‹¨ì¼ ëª¨ë“œ ë²¤ì¹˜ë§ˆí¬
        mode = MemoryMode[args.memory_mode.upper()]
        benchmark = UnifiedSystemBenchmark(mode, args.batch_size)
        
        await benchmark.initialize()
        result = await benchmark.run_benchmark(args.num_iterations)
        
        benchmark.print_results(result)
        
        # ê²°ê³¼ ì €ì¥
        filepath = Path(f"benchmark_{mode.value}_{int(time.time())}.json")
        benchmark.save_results(result, filepath)
        
        # ì‹œê°í™”
        plot_path = Path(f"benchmark_{mode.value}_{int(time.time())}.png")
        benchmark.plot_results(result, plot_path)


if __name__ == '__main__':
    asyncio.run(main())