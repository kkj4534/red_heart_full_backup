"""
ì‹¬ì¸µ ì‚¬ê³ ìš© ë‹¤ì°¨ì› ìœ¤ë¦¬ ì¶”ë¡  ì‹œìŠ¤í…œ
Deep Multi-Dimensional Ethics Reasoning System

ë‹¤ì–‘í•œ ìœ¤ë¦¬ì  ê´€ì ì„ í†µí•©í•˜ì—¬ ê¹Šì´ ìˆëŠ” ìœ¤ë¦¬ì  ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
ê³µë¦¬ì£¼ì˜, ë• ìœ¤ë¦¬í•™, ì˜ë¬´ë¡ ì  ìœ¤ë¦¬í•™, ëŒë´„ ìœ¤ë¦¬í•™, ì •ì˜ë¡  ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
1. ë‹¤ì°¨ì› ìœ¤ë¦¬í•™íŒŒ í†µí•© ì¶”ë¡ 
2. ë¬¸í™”ì  ë§¥ë½ ê³ ë ¤ ìœ¤ë¦¬ íŒë‹¨
3. ì´í•´ê´€ê³„ì ê´€ì  ë‹¤ê°ë„ ë¶„ì„
4. ì¥ë‹¨ê¸° ê²°ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ ìœ¤ë¦¬ í‰ê°€
"""

import numpy as np
import torch
import logging
import json
import time
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod

try:
    from config import ADVANCED_CONFIG, DEVICE
except ImportError:
    # config.pyì— ë¬¸ì œê°€ ìˆì„ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
    ADVANCED_CONFIG = {'enable_gpu': False}
    DEVICE = 'cpu'
    print("âš ï¸  config.py ì„í¬íŠ¸ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
from data_models import EmotionData
from mixture_of_experts import create_ethics_moe, MixtureOfExperts

logger = logging.getLogger('DeepMultiDimensionalEthics')

class EthicsSchool(Enum):
    """ìœ¤ë¦¬í•™íŒŒ"""
    UTILITARIANISM = "utilitarianism"      # ê³µë¦¬ì£¼ì˜
    VIRTUE_ETHICS = "virtue_ethics"        # ë• ìœ¤ë¦¬í•™
    DEONTOLOGICAL = "deontological"        # ì˜ë¬´ë¡ ì  ìœ¤ë¦¬í•™
    CARE_ETHICS = "care_ethics"            # ëŒë´„ ìœ¤ë¦¬í•™
    JUSTICE_THEORY = "justice_theory"      # ì •ì˜ë¡ 
    NARRATIVE_ETHICS = "narrative_ethics"  # ì„œì‚¬ ìœ¤ë¦¬í•™
    FEMINIST_ETHICS = "feminist_ethics"    # í˜ë¯¸ë‹ˆìŠ¤íŠ¸ ìœ¤ë¦¬í•™
    ENVIRONMENTAL_ETHICS = "environmental_ethics"  # í™˜ê²½ ìœ¤ë¦¬í•™

@dataclass
class StakeholderPerspective:
    """ì´í•´ê´€ê³„ì ê´€ì """
    stakeholder_id: str
    name: str
    role: str
    power_level: float  # 0-1, ê¶Œë ¥/ì˜í–¥ë ¥ ìˆ˜ì¤€
    vulnerability: float  # 0-1, ì·¨ì•½ì„± ìˆ˜ì¤€
    
    # ê´€ì ë³„ ê°€ì¹˜
    values: Dict[str, float] = field(default_factory=dict)
    concerns: List[str] = field(default_factory=list)
    
    # ì˜ˆìƒ ì˜í–¥
    expected_benefits: float = 0.0
    expected_harms: float = 0.0
    
    # ì˜ê²¬ ê°€ì¤‘ì¹˜
    voice_weight: float = 1.0

@dataclass
class CulturalContext:
    """ë¬¸í™”ì  ë§¥ë½"""
    culture_id: str
    cultural_values: Dict[str, float] = field(default_factory=dict)
    social_norms: List[str] = field(default_factory=list)
    moral_priorities: Dict[str, float] = field(default_factory=dict)
    
    # ë¬¸í™”ì  íŠ¹ì„±
    individualism_collectivism: float = 0.5  # 0: ì§‘ë‹¨ì£¼ì˜, 1: ê°œì¸ì£¼ì˜
    power_distance: float = 0.5  # ê¶Œë ¥ ê±°ë¦¬
    uncertainty_avoidance: float = 0.5  # ë¶ˆí™•ì‹¤ì„± íšŒí”¼
    long_term_orientation: float = 0.5  # ì¥ê¸° ì§€í–¥ì„±

@dataclass
class EthicalDilemma:
    """ìœ¤ë¦¬ì  ë”œë ˆë§ˆ"""
    dilemma_id: str
    scenario: str
    context: str
    
    # ë”œë ˆë§ˆ íŠ¹ì„±
    complexity_level: float = 0.5
    urgency_level: float = 0.5
    reversibility: float = 0.5  # ê²°ì •ì˜ ê°€ì—­ì„±
    
    # ê´€ë ¨ ì •ë³´
    stakeholders: List[StakeholderPerspective] = field(default_factory=list)
    cultural_context: Optional[CulturalContext] = None
    available_options: List[str] = field(default_factory=list)
    
    # ì œì•½ì‚¬í•­
    legal_constraints: List[str] = field(default_factory=list)
    resource_constraints: Dict[str, float] = field(default_factory=dict)
    time_constraints: float = 1.0  # ë¬´ì œí•œ ì‹œê°„ = 1.0

@dataclass
class EthicsReasoning:
    """ìœ¤ë¦¬ ì¶”ë¡  ê²°ê³¼"""
    school: EthicsSchool
    reasoning_process: List[str] = field(default_factory=list)
    ethical_score: float = 0.0
    confidence: float = 0.0
    key_principles: List[str] = field(default_factory=list)
    potential_conflicts: List[str] = field(default_factory=list)

@dataclass
class IntegratedEthicsResult:
    """í†µí•© ìœ¤ë¦¬ ì¶”ë¡  ê²°ê³¼"""
    dilemma: EthicalDilemma
    school_reasonings: Dict[EthicsSchool, EthicsReasoning] = field(default_factory=dict)
    
    # í†µí•© ê²°ê³¼
    overall_recommendation: str = ""
    confidence_score: float = 0.0
    ethical_consensus: float = 0.0  # í•™íŒŒ ê°„ í•©ì˜ ì •ë„
    
    # ìƒì„¸ ë¶„ì„
    stakeholder_analysis: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    cultural_considerations: List[str] = field(default_factory=list)
    temporal_analysis: Dict[str, float] = field(default_factory=dict)
    
    # ë©”íƒ€ ì •ë³´
    processing_time: float = 0.0
    reasoning_depth: int = 0

class EthicsReasoningEngine(ABC):
    """ìœ¤ë¦¬ ì¶”ë¡  ì—”ì§„ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def reason(self, dilemma: EthicalDilemma) -> EthicsReasoning:
        """ìœ¤ë¦¬ì  ì¶”ë¡  ìˆ˜í–‰"""
        pass
    
    @abstractmethod
    def get_school(self) -> EthicsSchool:
        """ìœ¤ë¦¬í•™íŒŒ ë°˜í™˜"""
        pass

class UtilitarianEngine(EthicsReasoningEngine):
    """ê³µë¦¬ì£¼ì˜ ì¶”ë¡  ì—”ì§„"""
    
    def get_school(self) -> EthicsSchool:
        return EthicsSchool.UTILITARIANISM
    
    def reason(self, dilemma: EthicalDilemma) -> EthicsReasoning:
        """ê³µë¦¬ì£¼ì˜ì  ì¶”ë¡ """
        
        reasoning_process = ["ê³µë¦¬ì£¼ì˜ì  ê´€ì ì—ì„œ ë¶„ì„ ì‹œì‘"]
        
        # 1. ì „ì²´ íš¨ìš© ê³„ì‚°
        total_utility = 0.0
        stakeholder_count = len(dilemma.stakeholders)
        
        if stakeholder_count > 0:
            for stakeholder in dilemma.stakeholders:
                net_utility = stakeholder.expected_benefits - stakeholder.expected_harms
                # ì·¨ì•½ì„± ê°€ì¤‘ì¹˜ ì ìš©
                weighted_utility = net_utility * (1.0 + stakeholder.vulnerability)
                total_utility += weighted_utility
            
            average_utility = total_utility / stakeholder_count
            reasoning_process.append(f"í‰ê·  íš¨ìš© ê³„ì‚°: {average_utility:.3f}")
        else:
            average_utility = 0.5
            reasoning_process.append("ì´í•´ê´€ê³„ì ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ê¸°ë³¸ íš¨ìš© ì ìš©")
        
        # 2. ìµœëŒ€ í–‰ë³µ ì›ì¹™ ì ìš©
        reasoning_process.append("ìµœëŒ€ ë‹¤ìˆ˜ì˜ ìµœëŒ€ í–‰ë³µ ì›ì¹™ ì ìš©")
        
        # 3. ê²°ê³¼ ê¸°ë°˜ í‰ê°€
        if average_utility > 0.6:
            recommendation = "ê³µë¦¬ì£¼ì˜ì  ê´€ì ì—ì„œ ê¸ì •ì  ê²°ê³¼ ì˜ˆìƒ"
            ethical_score = min(average_utility, 1.0)
        elif average_utility < 0.4:
            recommendation = "ê³µë¦¬ì£¼ì˜ì  ê´€ì ì—ì„œ ë¶€ì •ì  ê²°ê³¼ ì˜ˆìƒ"
            ethical_score = max(average_utility, 0.0)
        else:
            recommendation = "ê³µë¦¬ì£¼ì˜ì  ê´€ì ì—ì„œ ì¤‘ë¦½ì  ê²°ê³¼ ì˜ˆìƒ"
            ethical_score = 0.5
        
        reasoning_process.append(recommendation)
        
        # ì ì¬ì  ê°ˆë“±
        conflicts = []
        if stakeholder_count > 5:
            conflicts.append("ë‹¤ìˆ˜ì˜ ì´í•´ê´€ê³„ìë¡œ ì¸í•œ íš¨ìš© ê³„ì‚° ë³µì¡ì„±")
        
        # ì·¨ì•½ ê³„ì¸µ ë³´í˜¸ í•„ìš”ì„±
        vulnerable_stakeholders = [s for s in dilemma.stakeholders if s.vulnerability > 0.7]
        if vulnerable_stakeholders:
            conflicts.append(f"{len(vulnerable_stakeholders)}ëª…ì˜ ì·¨ì•½ ê³„ì¸µ íŠ¹ë³„ ê³ ë ¤ í•„ìš”")
        
        return EthicsReasoning(
            school=self.get_school(),
            reasoning_process=reasoning_process,
            ethical_score=ethical_score,
            confidence=0.8 if stakeholder_count > 0 else 0.5,
            key_principles=["ìµœëŒ€ ë‹¤ìˆ˜ì˜ ìµœëŒ€ í–‰ë³µ", "ê²°ê³¼ ì¤‘ì‹¬ íŒë‹¨", "íš¨ìš© ìµœëŒ€í™”"],
            potential_conflicts=conflicts
        )

class VirtueEthicsEngine(EthicsReasoningEngine):
    """ë• ìœ¤ë¦¬í•™ ì¶”ë¡  ì—”ì§„"""
    
    def get_school(self) -> EthicsSchool:
        return EthicsSchool.VIRTUE_ETHICS
    
    def reason(self, dilemma: EthicalDilemma) -> EthicsReasoning:
        """ë• ìœ¤ë¦¬í•™ì  ì¶”ë¡ """
        
        reasoning_process = ["ë• ìœ¤ë¦¬í•™ì  ê´€ì ì—ì„œ ë¶„ì„ ì‹œì‘"]
        
        # í•µì‹¬ ë•ëª©ë“¤
        virtues = {
            'courage': 0.0,      # ìš©ê¸°
            'justice': 0.0,      # ì •ì˜
            'temperance': 0.0,   # ì ˆì œ
            'wisdom': 0.0,       # ì§€í˜œ
            'compassion': 0.0,   # ì—°ë¯¼
            'integrity': 0.0,    # ì„±ì‹¤ì„±
            'humility': 0.0      # ê²¸ì†
        }
        
        # ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ìœ¼ë¡œ ë•ëª© í‰ê°€
        scenario_lower = dilemma.scenario.lower()
        
        # ìš©ê¸° í‰ê°€
        if any(word in scenario_lower for word in ['ìœ„í—˜', 'ë„ì „', 'ì–´ë ¤ìš´', 'ê³¤ë€']):
            virtues['courage'] = 0.8
            reasoning_process.append("ìƒí™©ì´ ìš©ê¸°ë¥¼ ìš”êµ¬í•¨")
        
        # ì •ì˜ í‰ê°€
        if any(word in scenario_lower for word in ['ê³µì •', 'ê³µí‰', 'í‰ë“±', 'ì°¨ë³„']):
            virtues['justice'] = 0.9
            reasoning_process.append("ì •ì˜ë¡œìš´ íŒë‹¨ì´ í•µì‹¬")
        
        # ì ˆì œ í‰ê°€
        if any(word in scenario_lower for word in ['ìš•ì‹¬', 'íƒìš•', 'ê³¼ë„']):
            virtues['temperance'] = 0.7
            reasoning_process.append("ì ˆì œê°€ í•„ìš”í•œ ìƒí™©")
        
        # ì§€í˜œ í‰ê°€
        if dilemma.complexity_level > 0.7:
            virtues['wisdom'] = 0.8
            reasoning_process.append("ë³µì¡í•œ ìƒí™©ìœ¼ë¡œ ì§€í˜œê°€ ìš”êµ¬ë¨")
        
        # ì—°ë¯¼ í‰ê°€
        vulnerable_count = len([s for s in dilemma.stakeholders if s.vulnerability > 0.6])
        if vulnerable_count > 0:
            virtues['compassion'] = min(0.9, 0.5 + vulnerable_count * 0.2)
            reasoning_process.append(f"ì·¨ì•½ ê³„ì¸µ {vulnerable_count}ëª…ìœ¼ë¡œ ì—°ë¯¼ì´ ì¤‘ìš”")
        
        # ì„±ì‹¤ì„± í‰ê°€ (ê¸°ë³¸ê°’ ë†’ê²Œ)
        virtues['integrity'] = 0.8
        reasoning_process.append("ì„±ì‹¤ì„±ì€ ëª¨ë“  ìœ¤ë¦¬ì  í–‰ë™ì˜ ê¸°ë°˜")
        
        # ê²¸ì† í‰ê°€
        if any(word in scenario_lower for word in ['ê¶Œë ¥', 'ì§€ìœ„', 'ìš°ì›”']):
            virtues['humility'] = 0.7
            reasoning_process.append("ê¶Œë ¥ ê´€ë ¨ ìƒí™©ì—ì„œ ê²¸ì†ì´ í•„ìš”")
        
        # ì „ì²´ ë•ëª© ì ìˆ˜ ê³„ì‚°
        virtue_scores = [score for score in virtues.values() if score > 0]
        if virtue_scores:
            ethical_score = np.mean(virtue_scores)
            confidence = len(virtue_scores) / len(virtues)  # í‰ê°€ëœ ë•ëª© ë¹„ìœ¨
        else:
            ethical_score = 0.5
            confidence = 0.3
        
        # í•µì‹¬ ì›ì¹™
        key_principles = []
        for virtue, score in virtues.items():
            if score > 0.6:
                key_principles.append(virtue)
        
        if not key_principles:
            key_principles = ["ì„±ì‹¤ì„±", "í’ˆì„± ì¤‘ì‹¬ íŒë‹¨"]
        
        # ì ì¬ì  ê°ˆë“±
        conflicts = []
        high_virtues = [v for v, s in virtues.items() if s > 0.7]
        if len(high_virtues) > 3:
            conflicts.append("ë‹¤ìˆ˜ ë•ëª© ê°„ ìš°ì„ ìˆœìœ„ ê²°ì • í•„ìš”")
        
        if virtues['justice'] > 0.7 and virtues['compassion'] > 0.7:
            conflicts.append("ì •ì˜ì™€ ì—°ë¯¼ ì‚¬ì´ì˜ ê· í˜• í•„ìš”")
        
        return EthicsReasoning(
            school=self.get_school(),
            reasoning_process=reasoning_process,
            ethical_score=ethical_score,
            confidence=confidence,
            key_principles=key_principles,
            potential_conflicts=conflicts
        )

class DeontologicalEngine(EthicsReasoningEngine):
    """ì˜ë¬´ë¡ ì  ìœ¤ë¦¬í•™ ì¶”ë¡  ì—”ì§„"""
    
    def get_school(self) -> EthicsSchool:
        return EthicsSchool.DEONTOLOGICAL
    
    def reason(self, dilemma: EthicalDilemma) -> EthicsReasoning:
        """ì˜ë¬´ë¡ ì  ì¶”ë¡ """
        
        reasoning_process = ["ì˜ë¬´ë¡ ì  ê´€ì ì—ì„œ ë¶„ì„ ì‹œì‘"]
        
        # í•µì‹¬ ì˜ë¬´ë“¤
        duties = {
            'no_harm': 0.0,           # í•´ë¥¼ ë¼ì¹˜ì§€ ë§ë¼
            'truth_telling': 0.0,     # ì§„ì‹¤ì„ ë§í•˜ë¼
            'promise_keeping': 0.0,   # ì•½ì†ì„ ì§€ì¼œë¼
            'respect_autonomy': 0.0,  # ììœ¨ì„±ì„ ì¡´ì¤‘í•˜ë¼
            'fairness': 0.0,          # ê³µì •í•˜ê²Œ ëŒ€í•˜ë¼
            'respect_dignity': 0.0    # ì¸ê°„ ì¡´ì—„ì„±ì„ ì¡´ì¤‘í•˜ë¼
        }
        
        scenario_lower = dilemma.scenario.lower()
        
        # ë¬´í•´ ì›ì¹™
        if any(word in scenario_lower for word in ['í•´ë¡­', 'í”¼í•´', 'ì†ìƒ', 'ìœ„í—˜']):
            duties['no_harm'] = 0.9
            reasoning_process.append("ë¬´í•´ ì›ì¹™(do no harm) ì ìš©")
        
        # ì§„ì‹¤ ì˜ë¬´
        if any(word in scenario_lower for word in ['ê±°ì§“', 'ì†ì„', 'ì§„ì‹¤', 'ì •ì§']):
            duties['truth_telling'] = 0.8
            reasoning_process.append("ì§„ì‹¤ ì˜ë¬´ í™•ì¸")
        
        # ì•½ì† ì¤€ìˆ˜
        if any(word in scenario_lower for word in ['ì•½ì†', 'ê³„ì•½', 'í•©ì˜', 'ì„œì•½']):
            duties['promise_keeping'] = 0.8
            reasoning_process.append("ì•½ì† ì¤€ìˆ˜ ì˜ë¬´ í™•ì¸")
        
        # ììœ¨ì„± ì¡´ì¤‘
        autonomous_stakeholders = len([s for s in dilemma.stakeholders if s.vulnerability < 0.3])
        if autonomous_stakeholders > 0:
            duties['respect_autonomy'] = 0.8
            reasoning_process.append(f"{autonomous_stakeholders}ëª…ì˜ ììœ¨ì  ê°œì¸ì— ëŒ€í•œ ì¡´ì¤‘")
        
        # ê³µì •ì„±
        if any(word in scenario_lower for word in ['ê³µì •', 'í‰ë“±', 'ì°¨ë³„']):
            duties['fairness'] = 0.9
            reasoning_process.append("ê³µì •ì„± ì˜ë¬´ ì ìš©")
        
        # ì¸ê°„ ì¡´ì—„ì„± (í•­ìƒ ë†’ì€ ì ìˆ˜)
        duties['respect_dignity'] = 0.9
        reasoning_process.append("ì¸ê°„ ì¡´ì—„ì„± ì¡´ì¤‘ì€ ì ˆëŒ€ì  ì˜ë¬´")
        
        # ì¹¸íŠ¸ì˜ ì •ì–¸ëª…ë ¹ ì ìš©
        reasoning_process.append("ì¹¸íŠ¸ì˜ ì •ì–¸ëª…ë ¹ ì›ì¹™ ì ìš©")
        
        # ë³´í¸í™” ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸
        if dilemma.complexity_level < 0.5:
            universalizability = 0.8
            reasoning_process.append("í–‰ë™ ì›ì¹™ì˜ ë³´í¸í™” ê°€ëŠ¥ì„± ë†’ìŒ")
        else:
            universalizability = 0.6
            reasoning_process.append("ë³µì¡í•œ ìƒí™©ìœ¼ë¡œ ë³´í¸í™” ê°€ëŠ¥ì„± ì œí•œì ")
        
        # ì „ì²´ ì˜ë¬´ ì¤€ìˆ˜ ì ìˆ˜
        duty_scores = [score for score in duties.values() if score > 0]
        if duty_scores:
            ethical_score = np.mean(duty_scores) * universalizability
            confidence = 0.9  # ì˜ë¬´ë¡ ì€ í™•ì‹¤í•œ ê·œì¹™ ê¸°ë°˜
        else:
            ethical_score = 0.7  # ê¸°ë³¸ì ìœ¼ë¡œ ì˜ë¬´ ì¤€ìˆ˜ ì§€í–¥
            confidence = 0.7
        
        # í•µì‹¬ ì›ì¹™
        key_principles = ["ì •ì–¸ëª…ë ¹", "ì˜ë¬´ ê¸°ë°˜ íŒë‹¨", "ë³´í¸í™” ê°€ëŠ¥ì„±"]
        active_duties = [duty for duty, score in duties.items() if score > 0.6]
        key_principles.extend(active_duties)
        
        # ì ì¬ì  ê°ˆë“±
        conflicts = []
        if len(active_duties) > 3:
            conflicts.append("ë‹¤ìˆ˜ ì˜ë¬´ ê°„ ìš°ì„ ìˆœìœ„ ì¶©ëŒ")
        
        if duties['no_harm'] > 0.7 and duties['truth_telling'] > 0.7:
            conflicts.append("ì§„ì‹¤ ë§í•˜ê¸°ì™€ í•´ì•… ë°©ì§€ ì˜ë¬´ ê°„ ê°ˆë“± ê°€ëŠ¥")
        
        return EthicsReasoning(
            school=self.get_school(),
            reasoning_process=reasoning_process,
            ethical_score=ethical_score,
            confidence=confidence,
            key_principles=key_principles,
            potential_conflicts=conflicts
        )

class CareEthicsEngine(EthicsReasoningEngine):
    """ëŒë´„ ìœ¤ë¦¬í•™ ì¶”ë¡  ì—”ì§„"""
    
    def get_school(self) -> EthicsSchool:
        return EthicsSchool.CARE_ETHICS
    
    def reason(self, dilemma: EthicalDilemma) -> EthicsReasoning:
        """ëŒë´„ ìœ¤ë¦¬í•™ì  ì¶”ë¡ """
        
        reasoning_process = ["ëŒë´„ ìœ¤ë¦¬í•™ì  ê´€ì ì—ì„œ ë¶„ì„ ì‹œì‘"]
        
        # ëŒë´„ ì¤‘ì‹¬ ê°€ì¹˜ë“¤
        care_values = {
            'responsiveness': 0.0,    # ë°˜ì‘ì„±
            'responsibility': 0.0,    # ì±…ì„ê°
            'competence': 0.0,        # ëŠ¥ë ¥
            'attentiveness': 0.0,     # ì£¼ì˜ê¹ŠìŒ
            'trust': 0.0              # ì‹ ë¢°
        }
        
        # ê´€ê³„ ì¤‘ì‹¬ ë¶„ì„
        relationships = []
        vulnerable_stakeholders = [s for s in dilemma.stakeholders if s.vulnerability > 0.5]
        powerful_stakeholders = [s for s in dilemma.stakeholders if s.power_level > 0.7]
        
        # ë°˜ì‘ì„± í‰ê°€
        if vulnerable_stakeholders:
            care_values['responsiveness'] = min(0.9, 0.5 + len(vulnerable_stakeholders) * 0.2)
            reasoning_process.append(f"ì·¨ì•½ ê³„ì¸µ {len(vulnerable_stakeholders)}ëª…ì— ëŒ€í•œ ë°˜ì‘ì„± ì¤‘ìš”")
            relationships.append("ì·¨ì•½ì-ë³´í˜¸ì ê´€ê³„")
        
        # ì±…ì„ê° í‰ê°€
        if powerful_stakeholders:
            care_values['responsibility'] = min(0.9, 0.6 + len(powerful_stakeholders) * 0.1)
            reasoning_process.append(f"ê¶Œë ¥ì {len(powerful_stakeholders)}ëª…ì˜ ì±…ì„ê° ì¤‘ìš”")
            relationships.append("ê¶Œë ¥ì-ì•½ì ê´€ê³„")
        
        # ëŠ¥ë ¥ í‰ê°€
        if dilemma.resource_constraints:
            available_resources = np.mean(list(dilemma.resource_constraints.values()))
            care_values['competence'] = available_resources
            reasoning_process.append(f"ëŒë´„ ëŠ¥ë ¥ í‰ê°€: ìì› ê°€ìš©ì„± {available_resources:.2f}")
        else:
            care_values['competence'] = 0.7
        
        # ì£¼ì˜ê¹ŠìŒ í‰ê°€
        care_values['attentiveness'] = 0.8  # ëŒë´„ ìœ¤ë¦¬ëŠ” í•­ìƒ ì„¸ì‹¬í•¨ ìš”êµ¬
        reasoning_process.append("ìƒí™©ì— ëŒ€í•œ ì„¸ì‹¬í•œ ì£¼ì˜ í•„ìš”")
        
        # ì‹ ë¢° í‰ê°€
        scenario_lower = dilemma.scenario.lower()
        if any(word in scenario_lower for word in ['ì‹ ë¢°', 'ë¯¿ìŒ', 'ì˜ì¡´']):
            care_values['trust'] = 0.8
            reasoning_process.append("ì‹ ë¢° ê´€ê³„ ì¤‘ìš”ì„± í™•ì¸")
        else:
            care_values['trust'] = 0.6
        
        # ëŒë´„ ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
        reasoning_process.append("ëŒë´„ ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
        if len(relationships) > 0:
            reasoning_process.append(f"ì‹ë³„ëœ ê´€ê³„: {', '.join(relationships)}")
        
        # ë§¥ë½ì  ì ‘ê·¼
        reasoning_process.append("ì¶”ìƒì  ì›ì¹™ë³´ë‹¤ êµ¬ì²´ì  ë§¥ë½ ì¤‘ì‹œ")
        
        # ì „ì²´ ëŒë´„ ì ìˆ˜
        care_scores = list(care_values.values())
        ethical_score = np.mean(care_scores)
        
        # ì·¨ì•½ì ë³´í˜¸ ê°€ì¤‘ì¹˜
        if vulnerable_stakeholders:
            vulnerability_bonus = len(vulnerable_stakeholders) * 0.05
            ethical_score = min(1.0, ethical_score + vulnerability_bonus)
            reasoning_process.append(f"ì·¨ì•½ì ë³´í˜¸ ê°€ì¤‘ì¹˜ ì ìš©: +{vulnerability_bonus:.2f}")
        
        confidence = 0.8 if vulnerable_stakeholders else 0.6
        
        # í•µì‹¬ ì›ì¹™
        key_principles = ["ê´€ê³„ ì¤‘ì‹¬ ìœ¤ë¦¬", "ì·¨ì•½ì ë³´í˜¸", "ëŒë´„ê³¼ ë°°ë ¤"]
        active_values = [value for value, score in care_values.items() if score > 0.6]
        key_principles.extend(active_values)
        
        # ì ì¬ì  ê°ˆë“±
        conflicts = []
        if len(powerful_stakeholders) > 0 and len(vulnerable_stakeholders) > 0:
            conflicts.append("ê¶Œë ¥ ë¶ˆê· í˜• ìƒí™©ì—ì„œ ëŒë´„ ê´€ê³„ ë³µì¡ì„±")
        
        if dilemma.resource_constraints and np.mean(list(dilemma.resource_constraints.values())) < 0.5:
            conflicts.append("ì œí•œëœ ìì›ìœ¼ë¡œ ì¸í•œ ëŒë´„ ëŠ¥ë ¥ ì œì•½")
        
        return EthicsReasoning(
            school=self.get_school(),
            reasoning_process=reasoning_process,
            ethical_score=ethical_score,
            confidence=confidence,
            key_principles=key_principles,
            potential_conflicts=conflicts
        )

class DeepMultiDimensionalEthicsSystem:
    """ì‹¬ì¸µ ë‹¤ì°¨ì› ìœ¤ë¦¬ ì¶”ë¡  ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.logger = logger
        
        # ìœ¤ë¦¬ ì¶”ë¡  ì—”ì§„ë“¤
        self.reasoning_engines = {
            EthicsSchool.UTILITARIANISM: UtilitarianEngine(),
            EthicsSchool.VIRTUE_ETHICS: VirtueEthicsEngine(),
            EthicsSchool.DEONTOLOGICAL: DeontologicalEngine(),
            EthicsSchool.CARE_ETHICS: CareEthicsEngine()
        }
        
        # í•™íŒŒë³„ ê°€ì¤‘ì¹˜ (ë¬¸í™”ì /ìƒí™©ì ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥)
        self.school_weights = {
            EthicsSchool.UTILITARIANISM: 0.3,
            EthicsSchool.VIRTUE_ETHICS: 0.25,
            EthicsSchool.DEONTOLOGICAL: 0.25,
            EthicsSchool.CARE_ETHICS: 0.2
        }
        
        # ì¶”ë¡  íˆìŠ¤í† ë¦¬
        self.reasoning_history = []
        
        # ë¬¸í™”ì  ì ì‘ ë©”ëª¨ë¦¬
        self.cultural_adaptations = {}
        
        # Mixture of Experts for ìœ¤ë¦¬ ë¶„ì„
        self.moe_enabled = True
        if self.moe_enabled:
            try:
                # ìœ¤ë¦¬ ë¶„ì„ìš© MoE ì´ˆê¸°í™”
                ethics_input_dim = 512  # ìœ¤ë¦¬ì  ë§¥ë½ ì„ë² ë”© ì°¨ì›
                ethics_output_dim = len(EthicsSchool)  # ìœ¤ë¦¬í•™íŒŒ ìˆ˜
                
                self.ethics_moe = create_ethics_moe(
                    input_dim=ethics_input_dim,
                    output_dim=ethics_output_dim,
                    num_experts=4
                )
                
                # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ ì´ë™
                if torch.cuda.is_available() and ADVANCED_CONFIG.get('enable_gpu', False):
                    self.ethics_moe = self.ethics_moe.cuda()
                
                self.logger.info("ìœ¤ë¦¬ ë¶„ì„ìš© MoE ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (4ê°œ ì „ë¬¸ê°€)")
            except Exception as e:
                self.logger.warning(f"ìœ¤ë¦¬ MoE ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ ì‹œìŠ¤í…œ ì‚¬ìš©: {e}")
                self.moe_enabled = False
        
        self.logger.info("ì‹¬ì¸µ ë‹¤ì°¨ì› ìœ¤ë¦¬ ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def comprehensive_ethical_analysis(self, dilemma: EthicalDilemma) -> IntegratedEthicsResult:
        """ì¢…í•©ì  ìœ¤ë¦¬ ë¶„ì„"""
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: ê° ìœ¤ë¦¬í•™íŒŒë³„ ì¶”ë¡ 
        school_reasonings = {}
        for school, engine in self.reasoning_engines.items():
            try:
                reasoning = engine.reason(dilemma)
                school_reasonings[school] = reasoning
                self.logger.debug(f"{school.value} ì¶”ë¡  ì™„ë£Œ: ì ìˆ˜ {reasoning.ethical_score:.3f}")
            except Exception as e:
                self.logger.error(f"{school.value} ì¶”ë¡  ì‹¤íŒ¨: {e}")
                continue
        
        # 2ë‹¨ê³„: MoE ê¸°ë°˜ ìœ¤ë¦¬í•™íŒŒ ê°€ì¤‘ì¹˜ ì¡°ì •
        if self.moe_enabled:
            school_reasonings = self._apply_moe_ethics_analysis(dilemma, school_reasonings)
        
        # 3ë‹¨ê³„: ì´í•´ê´€ê³„ì ê´€ì  ë¶„ì„
        stakeholder_analysis = self._analyze_stakeholder_perspectives(dilemma, school_reasonings)
        
        # 3ë‹¨ê³„: ë¬¸í™”ì  ë§¥ë½ ê³ ë ¤
        cultural_considerations = self._consider_cultural_context(dilemma, school_reasonings)
        
        # 4ë‹¨ê³„: ì‹œê°„ì  ë¶„ì„ (ë‹¨ê¸°/ì¥ê¸° ì˜í–¥)
        temporal_analysis = self._analyze_temporal_implications(dilemma, school_reasonings)
        
        # 5ë‹¨ê³„: í•™íŒŒ ê°„ í•©ì˜ ë° ê°ˆë“± ë¶„ì„
        ethical_consensus = self._calculate_ethical_consensus(school_reasonings)
        
        # 6ë‹¨ê³„: í†µí•© ì¶”ì²œ ìƒì„±
        overall_recommendation = self._generate_integrated_recommendation(
            dilemma, school_reasonings, stakeholder_analysis, cultural_considerations
        )
        
        # 7ë‹¨ê³„: ì‹ ë¢°ë„ ê³„ì‚°
        confidence_score = self._calculate_overall_confidence(school_reasonings, ethical_consensus)
        
        # ê²°ê³¼ ìƒì„±
        result = IntegratedEthicsResult(
            dilemma=dilemma,
            school_reasonings=school_reasonings,
            overall_recommendation=overall_recommendation,
            confidence_score=confidence_score,
            ethical_consensus=ethical_consensus,
            stakeholder_analysis=stakeholder_analysis,
            cultural_considerations=cultural_considerations,
            temporal_analysis=temporal_analysis,
            processing_time=time.time() - start_time,
            reasoning_depth=len(school_reasonings)
        )
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.reasoning_history.append(result)
        if len(self.reasoning_history) > 100:
            self.reasoning_history = self.reasoning_history[-100:]
        
        self.logger.info(
            f"ì¢…í•© ìœ¤ë¦¬ ë¶„ì„ ì™„ë£Œ: {len(school_reasonings)}ê°œ í•™íŒŒ, "
            f"í•©ì˜ë„ {ethical_consensus:.3f}, ì‹ ë¢°ë„ {confidence_score:.3f}"
        )
        
        return result
    
    def _apply_moe_ethics_analysis(self, dilemma: EthicalDilemma, 
                                 school_reasonings: Dict[EthicsSchool, Any]) -> Dict[EthicsSchool, Any]:
        """
        MoE ê¸°ë°˜ ìœ¤ë¦¬í•™íŒŒ ë¶„ì„ ë° ê°€ì¤‘ì¹˜ ì¡°ì •
        
        Args:
            dilemma: ìœ¤ë¦¬ì  ë”œë ˆë§ˆ
            school_reasonings: ê¸°ë³¸ ìœ¤ë¦¬í•™íŒŒë³„ ì¶”ë¡  ê²°ê³¼
            
        Returns:
            MoEë¡œ ë³´ì •ëœ ìœ¤ë¦¬í•™íŒŒë³„ ì¶”ë¡  ê²°ê³¼
        """
        try:
            # ìœ¤ë¦¬ì  ë§¥ë½ ì„ë² ë”© ìƒì„±
            context_embedding = self._create_ethics_context_embedding(dilemma)
            
            if context_embedding is None:
                return school_reasonings
            
            # MoE ì¶”ë¡ 
            moe_result = self.ethics_moe(context_embedding, temperature=0.7, return_expert_outputs=True)
            
            # MoE ê²°ê³¼ë¥¼ ìœ¤ë¦¬í•™íŒŒ ìš°ì„ ìˆœìœ„ë¡œ ë³€í™˜
            ethics_probs = torch.softmax(moe_result.final_output, dim=-1).squeeze(0)
            
            # ìœ¤ë¦¬í•™íŒŒ ë§¤í•‘
            ethics_schools = list(EthicsSchool)
            
            # MoE ê¸°ë°˜ í•™íŒŒë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
            enhanced_reasonings = {}
            
            for i, school in enumerate(ethics_schools):
                if school in school_reasonings and i < len(ethics_probs):
                    original_reasoning = school_reasonings[school]
                    moe_weight = ethics_probs[i].item()
                    
                    # ì›ë³¸ ì ìˆ˜ì™€ MoE ê°€ì¤‘ì¹˜ ê²°í•©
                    original_score = getattr(original_reasoning, 'ethical_score', 0.5)
                    enhanced_score = original_score * (0.7 + 0.3 * moe_weight)
                    
                    # ìƒˆë¡œìš´ ì¶”ë¡  ê²°ê³¼ ìƒì„± (ì›ë³¸ ë³µì‚¬ í›„ ìˆ˜ì •)
                    enhanced_reasoning = original_reasoning
                    enhanced_reasoning.ethical_score = enhanced_score
                    enhanced_reasoning.confidence *= (0.8 + 0.2 * moe_weight)
                    
                    # MoE ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    if hasattr(enhanced_reasoning, 'metadata'):
                        enhanced_reasoning.metadata.update({
                            'moe_weight': moe_weight,
                            'moe_enhanced': True,
                            'original_score': original_score
                        })
                    
                    enhanced_reasonings[school] = enhanced_reasoning
                    
                    self.logger.debug(f"{school.value} MoE ê°€ì¤‘ì¹˜: {moe_weight:.3f}, "
                                    f"ì¡°ì •ëœ ì ìˆ˜: {enhanced_score:.3f}")
                
                elif school in school_reasonings:
                    # MoE ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì›ë³¸ ìœ ì§€
                    enhanced_reasonings[school] = school_reasonings[school]
            
            # MoE ë‹¤ì–‘ì„± ì •ë³´ ë¡œê¹…
            self.logger.info(f"ìœ¤ë¦¬ MoE ë¶„ì„ ì™„ë£Œ - ì „ë¬¸ê°€ {moe_result.total_experts_used}ê°œ ì‚¬ìš©, "
                           f"ë‹¤ì–‘ì„± ì ìˆ˜: {moe_result.diversity_score:.3f}")
            
            return enhanced_reasonings
            
        except Exception as e:
            self.logger.error(f"ìœ¤ë¦¬ MoE ë¶„ì„ ì‹¤íŒ¨: {e}")
            return school_reasonings
    
    def _create_ethics_context_embedding(self, dilemma: EthicalDilemma) -> Optional[torch.Tensor]:
        """
        ìœ¤ë¦¬ì  ë§¥ë½ ì„ë² ë”© ìƒì„±
        
        Args:
            dilemma: ìœ¤ë¦¬ì  ë”œë ˆë§ˆ
            
        Returns:
            ìœ¤ë¦¬ì  ë§¥ë½ ì„ë² ë”© í…ì„œ
        """
        try:
            # ë”œë ˆë§ˆì˜ í•µì‹¬ ìš”ì†Œë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜
            context_features = []
            
            # 1. ë”œë ˆë§ˆ ë³µì¡ë„
            complexity = getattr(dilemma, 'complexity_level', 0.5)
            context_features.extend([complexity])
            
            # 2. ì´í•´ê´€ê³„ì ìˆ˜
            stakeholder_count = len(getattr(dilemma, 'stakeholders', []))
            normalized_stakeholder_count = min(stakeholder_count / 10.0, 1.0)
            context_features.extend([normalized_stakeholder_count])
            
            # 3. ì‹œê°„ì  ê¸´ê¸‰ì„±
            urgency = getattr(dilemma, 'urgency_level', 0.5)
            context_features.extend([urgency])
            
            # 4. ë¬¸í™”ì  ë¯¼ê°ì„±
            cultural_sensitivity = getattr(dilemma, 'cultural_sensitivity', 0.5)
            context_features.extend([cultural_sensitivity])
            
            # 5. ê²°ê³¼ì˜ ê°€ì—­ì„±
            reversibility = getattr(dilemma, 'consequence_reversibility', 0.5)
            context_features.extend([reversibility])
            
            # 6. ê°œì¸ vs ì§‘ë‹¨ ì˜í–¥
            personal_vs_collective = getattr(dilemma, 'personal_vs_collective_impact', 0.5)
            context_features.extend([personal_vs_collective])
            
            # ë²¡í„°ë¥¼ ì›í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ í™•ì¥ (512ì°¨ì›)
            while len(context_features) < 512:
                # ê¸°ì¡´ íŠ¹ì„±ë“¤ì„ ë³€í˜•í•˜ì—¬ í™•ì¥
                base_idx = len(context_features) % 6
                if base_idx < len(context_features):
                    # ê¸°ì¡´ ê°’ì— ì‘ì€ ë³€í˜• ì¶”ê°€
                    variation = context_features[base_idx] * (0.9 + 0.2 * np.random.random())
                    context_features.append(variation)
                else:
                    context_features.append(0.5)  # ê¸°ë³¸ê°’
            
            # í…ì„œë¡œ ë³€í™˜
            embedding = torch.tensor(context_features[:512], dtype=torch.float32)
            
            # ì •ê·œí™”
            embedding = torch.nn.functional.normalize(embedding, p=2, dim=0)
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            embedding = embedding.unsqueeze(0)
            
            # GPUë¡œ ì´ë™ (í•„ìš”ì‹œ)
            if torch.cuda.is_available() and ADVANCED_CONFIG.get('enable_gpu', False):
                embedding = embedding.cuda()
            
            return embedding
            
        except Exception as e:
            self.logger.error(f"ìœ¤ë¦¬ ë§¥ë½ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _analyze_stakeholder_perspectives(
        self,
        dilemma: EthicalDilemma,
        school_reasonings: Dict[EthicsSchool, EthicsReasoning]
    ) -> Dict[str, Dict[str, Any]]:
        """ì´í•´ê´€ê³„ì ê´€ì  ë¶„ì„"""
        
        stakeholder_analysis = {}
        
        for stakeholder in dilemma.stakeholders:
            analysis = {
                'perspective': stakeholder.name,
                'vulnerability_level': stakeholder.vulnerability,
                'power_level': stakeholder.power_level,
                'expected_impact': stakeholder.expected_benefits - stakeholder.expected_harms,
                'voice_weight': stakeholder.voice_weight,
                'ethical_priorities': {}
            }
            
            # ì·¨ì•½ì„±ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„
            if stakeholder.vulnerability > 0.7:
                analysis['ethical_priorities']['care_ethics'] = 0.9
                analysis['ethical_priorities']['protection_needed'] = True
            
            # ê¶Œë ¥ ìˆ˜ì¤€ì— ë”°ë¥¸ ì±…ì„
            if stakeholder.power_level > 0.7:
                analysis['ethical_priorities']['responsibility'] = 0.8
                analysis['ethical_priorities']['leadership_expected'] = True
            
            # ìœ¤ë¦¬í•™íŒŒë³„ ê´€ì‹¬ë„
            for school, reasoning in school_reasonings.items():
                if school == EthicsSchool.CARE_ETHICS and stakeholder.vulnerability > 0.5:
                    analysis['ethical_priorities'][school.value] = reasoning.ethical_score
                elif school == EthicsSchool.UTILITARIANISM:
                    # ëª¨ë“  ì´í•´ê´€ê³„ìê°€ ê³µë¦¬ì£¼ì˜ì  ê³„ì‚°ì— í¬í•¨
                    analysis['ethical_priorities'][school.value] = reasoning.ethical_score
                elif school == EthicsSchool.DEONTOLOGICAL and stakeholder.power_level > 0.6:
                    # ê¶Œë ¥ ìˆëŠ” ì´í•´ê´€ê³„ìëŠ” ì˜ë¬´ ì¤€ìˆ˜ ì¤‘ìš”
                    analysis['ethical_priorities'][school.value] = reasoning.ethical_score
            
            stakeholder_analysis[stakeholder.stakeholder_id] = analysis
        
        return stakeholder_analysis
    
    def _consider_cultural_context(
        self,
        dilemma: EthicalDilemma,
        school_reasonings: Dict[EthicsSchool, EthicsReasoning]
    ) -> List[str]:
        """ë¬¸í™”ì  ë§¥ë½ ê³ ë ¤"""
        
        considerations = []
        
        if dilemma.cultural_context:
            culture = dilemma.cultural_context
            
            # ê°œì¸ì£¼ì˜ vs ì§‘ë‹¨ì£¼ì˜
            if culture.individualism_collectivism < 0.3:
                considerations.append("ì§‘ë‹¨ì£¼ì˜ ë¬¸í™”ì—ì„œ ê³µë™ì²´ ì´ìµ ìš°ì„ ì‹œ")
                # ê³µë¦¬ì£¼ì˜ì™€ ëŒë´„ ìœ¤ë¦¬ ê°€ì¤‘ì¹˜ ì¦ê°€
                self.school_weights[EthicsSchool.UTILITARIANISM] *= 1.2
                self.school_weights[EthicsSchool.CARE_ETHICS] *= 1.3
                
            elif culture.individualism_collectivism > 0.7:
                considerations.append("ê°œì¸ì£¼ì˜ ë¬¸í™”ì—ì„œ ê°œì¸ ê¶Œë¦¬ì™€ ììœ  ì¤‘ì‹œ")
                # ë• ìœ¤ë¦¬ì™€ ì˜ë¬´ë¡  ê°€ì¤‘ì¹˜ ì¦ê°€
                self.school_weights[EthicsSchool.VIRTUE_ETHICS] *= 1.2
                self.school_weights[EthicsSchool.DEONTOLOGICAL] *= 1.1
            
            # ê¶Œë ¥ ê±°ë¦¬
            if culture.power_distance > 0.7:
                considerations.append("ë†’ì€ ê¶Œë ¥ ê±°ë¦¬ ë¬¸í™”ì—ì„œ ìœ„ê³„ ì§ˆì„œ ì¤‘ì‹œ")
            elif culture.power_distance < 0.3:
                considerations.append("ë‚®ì€ ê¶Œë ¥ ê±°ë¦¬ ë¬¸í™”ì—ì„œ í‰ë“± ì¤‘ì‹œ")
            
            # ë¶ˆí™•ì‹¤ì„± íšŒí”¼
            if culture.uncertainty_avoidance > 0.7:
                considerations.append("ë¶ˆí™•ì‹¤ì„± íšŒí”¼ ë¬¸í™”ì—ì„œ ëª…í™•í•œ ê·œì¹™ ì„ í˜¸")
                # ì˜ë¬´ë¡ ì  ìœ¤ë¦¬ ê°€ì¤‘ì¹˜ ì¦ê°€
                self.school_weights[EthicsSchool.DEONTOLOGICAL] *= 1.2
            
            # ì¥ê¸° ì§€í–¥ì„±
            if culture.long_term_orientation > 0.7:
                considerations.append("ì¥ê¸° ì§€í–¥ ë¬¸í™”ì—ì„œ ë¯¸ë˜ ê²°ê³¼ ì¤‘ì‹œ")
            
            # ë¬¸í™”ì  ê°€ì¹˜ ë°˜ì˜
            for value, importance in culture.cultural_values.items():
                if importance > 0.7:
                    considerations.append(f"ë¬¸í™”ì  ê°€ì¹˜ '{value}' ë†’ì€ ì¤‘ìš”ì„±")
        
        else:
            considerations.append("ë¬¸í™”ì  ë§¥ë½ ì •ë³´ ì—†ìŒ - ì¼ë°˜ì  ìœ¤ë¦¬ ì›ì¹™ ì ìš©")
        
        return considerations
    
    def _analyze_temporal_implications(
        self,
        dilemma: EthicalDilemma,
        school_reasonings: Dict[EthicsSchool, EthicsReasoning]
    ) -> Dict[str, float]:
        """ì‹œê°„ì  ì˜í–¥ ë¶„ì„"""
        
        temporal_analysis = {
            'immediate_impact': 0.0,    # ì¦‰ì‹œ ì˜í–¥
            'short_term_impact': 0.0,   # ë‹¨ê¸° ì˜í–¥ (1ê°œì›”-1ë…„)
            'long_term_impact': 0.0,    # ì¥ê¸° ì˜í–¥ (1ë…„ ì´ìƒ)
            'reversibility': dilemma.reversibility,
            'urgency_factor': dilemma.urgency_level
        }
        
        # ê° ìœ¤ë¦¬í•™íŒŒì˜ ì‹œê°„ì  ê°€ì¤‘ì¹˜
        temporal_weights = {
            EthicsSchool.UTILITARIANISM: {'immediate': 0.3, 'short': 0.4, 'long': 0.3},
            EthicsSchool.VIRTUE_ETHICS: {'immediate': 0.2, 'short': 0.3, 'long': 0.5},
            EthicsSchool.DEONTOLOGICAL: {'immediate': 0.5, 'short': 0.3, 'long': 0.2},
            EthicsSchool.CARE_ETHICS: {'immediate': 0.4, 'short': 0.4, 'long': 0.2}
        }
        
        # ê° ì‹œê°„ëŒ€ë³„ ì˜í–¥ ê³„ì‚°
        for school, reasoning in school_reasonings.items():
            if school in temporal_weights:
                weights = temporal_weights[school]
                score = reasoning.ethical_score
                
                temporal_analysis['immediate_impact'] += score * weights['immediate']
                temporal_analysis['short_term_impact'] += score * weights['short']
                temporal_analysis['long_term_impact'] += score * weights['long']
        
        # ì •ê·œí™”
        num_schools = len(school_reasonings)
        if num_schools > 0:
            temporal_analysis['immediate_impact'] /= num_schools
            temporal_analysis['short_term_impact'] /= num_schools
            temporal_analysis['long_term_impact'] /= num_schools
        
        return temporal_analysis
    
    def _calculate_ethical_consensus(self, school_reasonings: Dict[EthicsSchool, EthicsReasoning]) -> float:
        """ìœ¤ë¦¬í•™íŒŒ ê°„ í•©ì˜ë„ ê³„ì‚°"""
        
        if len(school_reasonings) < 2:
            return 1.0  # í•™íŒŒê°€ í•˜ë‚˜ë¿ì´ë©´ í•©ì˜ë„ ìµœëŒ€
        
        scores = [reasoning.ethical_score for reasoning in school_reasonings.values()]
        
        # ì ìˆ˜ ë¶„ì‚°ìœ¼ë¡œ í•©ì˜ë„ ì¸¡ì • (ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ í•©ì˜ë„ ë†’ìŒ)
        score_variance = np.var(scores)
        consensus = 1.0 - min(score_variance * 4, 1.0)  # ë¶„ì‚° 4ë°° í›„ ì—­ì‚°
        
        return np.clip(consensus, 0.0, 1.0)
    
    def _generate_integrated_recommendation(
        self,
        dilemma: EthicalDilemma,
        school_reasonings: Dict[EthicsSchool, EthicsReasoning],
        stakeholder_analysis: Dict[str, Dict[str, Any]],
        cultural_considerations: List[str]
    ) -> str:
        """í†µí•© ì¶”ì²œ ìƒì„±"""
        
        recommendations = []
        
        # 1. í•™íŒŒë³„ í•µì‹¬ ë©”ì‹œì§€
        high_scoring_schools = [(school, reasoning) for school, reasoning in school_reasonings.items() 
                               if reasoning.ethical_score > 0.6]
        
        if high_scoring_schools:
            recommendations.append("ìœ¤ë¦¬ì  ê´€ì ë³„ ë¶„ì„ ê²°ê³¼:")
            for school, reasoning in high_scoring_schools:
                recommendations.append(f"- {school.value}: {reasoning.reasoning_process[-1]}")
        
        # 2. ì´í•´ê´€ê³„ì ê³ ë ¤ì‚¬í•­
        vulnerable_stakeholders = [sid for sid, analysis in stakeholder_analysis.items() 
                                 if analysis['vulnerability_level'] > 0.6]
        
        if vulnerable_stakeholders:
            recommendations.append(f"íŠ¹ë³„ ê³ ë ¤ ëŒ€ìƒ: {len(vulnerable_stakeholders)}ëª…ì˜ ì·¨ì•½ ê³„ì¸µ")
        
        # 3. ë¬¸í™”ì  ê¶Œê³ ì‚¬í•­
        if cultural_considerations:
            recommendations.append("ë¬¸í™”ì  ë§¥ë½ ê³ ë ¤ì‚¬í•­:")
            for consideration in cultural_considerations[:2]:  # ìƒìœ„ 2ê°œë§Œ
                recommendations.append(f"- {consideration}")
        
        # 4. ìµœì¢… ê¶Œê³ 
        overall_scores = [reasoning.ethical_score for reasoning in school_reasonings.values()]
        if overall_scores:
            avg_score = np.mean(overall_scores)
            if avg_score > 0.7:
                final_rec = "ìœ¤ë¦¬ì ìœ¼ë¡œ ë°”ëŒì§í•œ ê²°ì •ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤."
            elif avg_score > 0.5:
                final_rec = "ì‹ ì¤‘í•œ ê³ ë ¤ í•˜ì— ì§„í–‰ ê°€ëŠ¥í•œ ê²°ì •ì…ë‹ˆë‹¤."
            else:
                final_rec = "ìœ¤ë¦¬ì  ìš°ë ¤ê°€ ìˆì–´ ì¬ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            final_rec = "ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ í›„ ì¬í‰ê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        
        recommendations.append(f"\nì¢…í•© ê¶Œê³ : {final_rec}")
        
        return "\n".join(recommendations)
    
    def _calculate_overall_confidence(
        self,
        school_reasonings: Dict[EthicsSchool, EthicsReasoning],
        consensus: float
    ) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        
        if not school_reasonings:
            return 0.0
        
        # ê° í•™íŒŒì˜ ì‹ ë¢°ë„ í‰ê· 
        avg_confidence = np.mean([reasoning.confidence for reasoning in school_reasonings.values()])
        
        # í•©ì˜ë„ë¥¼ ì‹ ë¢°ë„ì— ë°˜ì˜
        overall_confidence = avg_confidence * (0.7 + 0.3 * consensus)
        
        return np.clip(overall_confidence, 0.0, 1.0)
    
    def get_ethics_analytics(self) -> Dict[str, Any]:
        """ìœ¤ë¦¬ ë¶„ì„ ì •ë³´ ë°˜í™˜"""
        
        if not self.reasoning_history:
            return {"message": "ë¶„ì„ íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        recent_analyses = self.reasoning_history[-10:]
        
        analytics = {
            'total_analyses': len(self.reasoning_history),
            'average_confidence': np.mean([analysis.confidence_score for analysis in recent_analyses]),
            'average_consensus': np.mean([analysis.ethical_consensus for analysis in recent_analyses]),
            'average_processing_time': np.mean([analysis.processing_time for analysis in recent_analyses]),
            'school_performance': {},
            'common_conflicts': []
        }
        
        # í•™íŒŒë³„ ì„±ëŠ¥
        for school in EthicsSchool:
            school_scores = []
            for analysis in recent_analyses:
                if school in analysis.school_reasonings:
                    school_scores.append(analysis.school_reasonings[school].ethical_score)
            
            if school_scores:
                analytics['school_performance'][school.value] = {
                    'average_score': np.mean(school_scores),
                    'consistency': 1.0 - np.std(school_scores),
                    'usage_rate': len(school_scores) / len(recent_analyses)
                }
        
        # ê³µí†µ ê°ˆë“± ìš”ì†Œ
        all_conflicts = []
        for analysis in recent_analyses:
            for reasoning in analysis.school_reasonings.values():
                all_conflicts.extend(reasoning.potential_conflicts)
        
        if all_conflicts:
            from collections import Counter
            conflict_counts = Counter(all_conflicts)
            analytics['common_conflicts'] = [conflict for conflict, count in conflict_counts.most_common(3)]
        
        return analytics


# í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ í•¨ìˆ˜
def test_deep_multi_dimensional_ethics():
    """ì‹¬ì¸µ ë‹¤ì°¨ì› ìœ¤ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§  ì‹¬ì¸µ ë‹¤ì°¨ì› ìœ¤ë¦¬ ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    ethics_system = DeepMultiDimensionalEthicsSystem()
    
    # í…ŒìŠ¤íŠ¸ ì´í•´ê´€ê³„ìë“¤
    stakeholders = [
        StakeholderPerspective(
            stakeholder_id="employees",
            name="ì§ì›ë“¤",
            role="ê·¼ë¡œì",
            power_level=0.3,
            vulnerability=0.7,
            expected_benefits=0.2,
            expected_harms=0.8,
            voice_weight=0.8
        ),
        StakeholderPerspective(
            stakeholder_id="shareholders",
            name="ì£¼ì£¼ë“¤",
            role="íˆ¬ìì",
            power_level=0.9,
            vulnerability=0.2,
            expected_benefits=0.8,
            expected_harms=0.1,
            voice_weight=0.6
        ),
        StakeholderPerspective(
            stakeholder_id="customers",
            name="ê³ ê°ë“¤",
            role="ì†Œë¹„ì",
            power_level=0.5,
            vulnerability=0.4,
            expected_benefits=0.6,
            expected_harms=0.3,
            voice_weight=0.7
        )
    ]
    
    # ë¬¸í™”ì  ë§¥ë½
    cultural_context = CulturalContext(
        culture_id="korean",
        cultural_values={"hierarchy_respect": 0.8, "group_harmony": 0.9},
        individualism_collectivism=0.3,  # ì§‘ë‹¨ì£¼ì˜ ì„±í–¥
        power_distance=0.7,
        long_term_orientation=0.8
    )
    
    # í…ŒìŠ¤íŠ¸ ë”œë ˆë§ˆ
    test_dilemma = EthicalDilemma(
        dilemma_id="corporate_layoff",
        scenario="ê²½ì œì  ì–´ë ¤ì›€ìœ¼ë¡œ ì¸í•´ íšŒì‚¬ê°€ ì§ì› 30%ë¥¼ í•´ê³ í•´ì•¼ í•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤. "
                "ì´ëŠ” íšŒì‚¬ì˜ ìƒì¡´ì„ ìœ„í•´ í•„ìš”í•˜ì§€ë§Œ ë§ì€ ê°€ì •ì— ê²½ì œì  íƒ€ê²©ì„ ì¤„ ê²ƒì…ë‹ˆë‹¤.",
        context="ê¸€ë¡œë²Œ ê²½ì œ ì¹¨ì²´ë¡œ ì¸í•œ êµ¬ì¡°ì¡°ì •",
        complexity_level=0.8,
        urgency_level=0.7,
        reversibility=0.3,
        stakeholders=stakeholders,
        cultural_context=cultural_context,
        available_options=[
            "ì „ì²´ ì§ì› 30% í•´ê³ ",
            "ì„ê¸ˆ ì‚­ê°ìœ¼ë¡œ í•´ê³  ìµœì†Œí™”",
            "ë‹¨ê³„ì  êµ¬ì¡°ì¡°ì •",
            "ì‚¬ì—…ë¶€ ë§¤ê°"
        ],
        resource_constraints={"financial": 0.3, "time": 0.4, "human": 0.6}
    )
    
    print(f"í…ŒìŠ¤íŠ¸ ë”œë ˆë§ˆ: {test_dilemma.scenario}")
    print(f"ë³µì¡ë„: {test_dilemma.complexity_level}, ê¸´ê¸‰ë„: {test_dilemma.urgency_level}")
    print(f"ì´í•´ê´€ê³„ì: {len(stakeholders)}ëª…")
    
    # ì¢…í•© ìœ¤ë¦¬ ë¶„ì„ ì‹¤í–‰
    result = ethics_system.comprehensive_ethical_analysis(test_dilemma)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼:")
    print(f"- ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.3f}ì´ˆ")
    print(f"- ì¶”ë¡  ê¹Šì´: {result.reasoning_depth}ê°œ í•™íŒŒ")
    print(f"- ìœ¤ë¦¬ì  í•©ì˜ë„: {result.ethical_consensus:.3f}")
    print(f"- ì „ì²´ ì‹ ë¢°ë„: {result.confidence_score:.3f}")
    
    print(f"\nğŸ« í•™íŒŒë³„ ì¶”ë¡  ê²°ê³¼:")
    for school, reasoning in result.school_reasonings.items():
        print(f"\n--- {school.value} ---")
        print(f"ìœ¤ë¦¬ ì ìˆ˜: {reasoning.ethical_score:.3f}")
        print(f"ì‹ ë¢°ë„: {reasoning.confidence:.3f}")
        print(f"í•µì‹¬ ì›ì¹™: {', '.join(reasoning.key_principles[:3])}")
        print(f"ì¶”ë¡  ê³¼ì •:")
        for i, process in enumerate(reasoning.reasoning_process[-3:], 1):
            print(f"  {i}. {process}")
        
        if reasoning.potential_conflicts:
            print(f"ì ì¬ì  ê°ˆë“±: {', '.join(reasoning.potential_conflicts)}")
    
    print(f"\nğŸ‘¥ ì´í•´ê´€ê³„ì ë¶„ì„:")
    for stakeholder_id, analysis in result.stakeholder_analysis.items():
        print(f"- {analysis['perspective']}: "
              f"ì·¨ì•½ì„± {analysis['vulnerability_level']:.2f}, "
              f"ê¶Œë ¥ {analysis['power_level']:.2f}, "
              f"ì˜ˆìƒ ì˜í–¥ {analysis['expected_impact']:.2f}")
    
    print(f"\nğŸŒ ë¬¸í™”ì  ê³ ë ¤ì‚¬í•­:")
    for consideration in result.cultural_considerations:
        print(f"- {consideration}")
    
    print(f"\nâ° ì‹œê°„ì  ë¶„ì„:")
    for time_aspect, value in result.temporal_analysis.items():
        print(f"- {time_aspect}: {value:.3f}")
    
    print(f"\nğŸ’¡ ì¢…í•© ê¶Œê³ ì‚¬í•­:")
    print(result.overall_recommendation)
    
    # ë¶„ì„ ì •ë³´
    analytics = ethics_system.get_ethics_analytics()
    print(f"\nğŸ“ˆ ì‹œìŠ¤í…œ ë¶„ì„:")
    print(f"- ì´ ë¶„ì„ ìˆ˜: {analytics['total_analyses']}")
    print(f"- í‰ê·  ì‹ ë¢°ë„: {analytics['average_confidence']:.3f}")
    print(f"- í‰ê·  í•©ì˜ë„: {analytics['average_consensus']:.3f}")
    
    if analytics['school_performance']:
        print(f"- í•™íŒŒë³„ ì„±ëŠ¥:")
        for school, perf in analytics['school_performance'].items():
            print(f"  {school}: í‰ê·  ì ìˆ˜ {perf['average_score']:.3f}, "
                  f"ì¼ê´€ì„± {perf['consistency']:.3f}")
    
    print("âœ… ì‹¬ì¸µ ë‹¤ì°¨ì› ìœ¤ë¦¬ ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    return ethics_system, result


if __name__ == "__main__":
    test_deep_multi_dimensional_ethics()
