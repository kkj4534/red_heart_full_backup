"""
ë²¤ë‹´ ê³„ì‚°ê¸° ML ëª¨ë¸ í›ˆë ¨ ì‹œìŠ¤í…œ
Bentham Calculator ML Model Training System

ë²¤ë‹´ ê°€ì¤‘ì¹˜ ë ˆì´ì–´ë³„ ML ëª¨ë¸ì„ ì‹¤ì œ ë°ì´í„°ë¡œ í›ˆë ¨
"""

import os
import json
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any
import time
from datetime import datetime
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
# SentenceTransformerëŠ” sentence_transformer_singletonì„ í†µí•´ ì‚¬ìš©
from transformers import pipeline

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('BenthamMLTrainer')

class BenthamDataset(Dataset):
    """PyTorch ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, features, targets):
        self.features = torch.FloatTensor(features)
        self.targets = torch.FloatTensor(targets)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]


class MultiHeadAttention(nn.Module):
    """Multi-head attention mechanism"""
    
    def __init__(self, dim, num_heads=8):
        super().__init__()
        self.num_heads = num_heads
        self.dim = dim
        self.head_dim = dim // num_heads
        
        self.q_linear = nn.Linear(dim, dim)
        self.k_linear = nn.Linear(dim, dim)
        self.v_linear = nn.Linear(dim, dim)
        self.out_linear = nn.Linear(dim, dim)
        
    def forward(self, x):
        batch_size = x.size(0)
        
        # Linear transformations
        Q = self.q_linear(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.k_linear(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_linear(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attention_weights = F.softmax(scores, dim=-1)
        attended = torch.matmul(attention_weights, V)
        
        # Concatenate heads
        attended = attended.transpose(1, 2).contiguous().view(batch_size, -1, self.dim)
        
        return self.out_linear(attended)


class TransformerBlock(nn.Module):
    """Transformer block with attention and feed-forward"""
    
    def __init__(self, dim, num_heads=8, ff_dim=None, dropout=0.1):
        super().__init__()
        if ff_dim is None:
            ff_dim = dim * 4
            
        self.attention = MultiHeadAttention(dim, num_heads)
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        
        self.ff = nn.Sequential(
            nn.Linear(dim, ff_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ff_dim, dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, x):
        # Self-attention with residual
        attended = self.attention(x.unsqueeze(1)).squeeze(1)
        x = self.norm1(x + attended)
        
        # Feed-forward with residual
        ff_out = self.ff(x)
        x = self.norm2(x + ff_out)
        
        return x


class BenthamDNN(nn.Module):
    """ë§¤ìš° ë†’ì€ ë³µì¡ë„ì˜ ë²¤ë‹´ ê³„ì‚°ìš© ë”¥ëŸ¬ë‹ ëª¨ë¸ (2M íŒŒë¼ë¯¸í„°)"""
    
    def __init__(self, input_dim, hidden_dims=[2048, 1024, 512, 256, 128], dropout=0.5):
        super().__init__()
        
        # ì…ë ¥ ì„ë² ë”© ë ˆì´ì–´
        self.input_embedding = nn.Sequential(
            nn.Linear(input_dim, hidden_dims[0]),
            nn.LayerNorm(hidden_dims[0]),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # Transformer ë¸”ë¡ë“¤
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(hidden_dims[0], num_heads=16, dropout=dropout),
            TransformerBlock(hidden_dims[0], num_heads=16, dropout=dropout)
        ])
        
        # ë©”ì¸ ë„¤íŠ¸ì›Œí¬ (Residual connections í¬í•¨)
        self.layers = nn.ModuleList()
        prev_dim = hidden_dims[0]
        
        for i, hidden_dim in enumerate(hidden_dims[1:]):
            self.layers.append(nn.Sequential(
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout)
            ))
            
            # Skip connection layers (ì°¨ì›ì´ ë§ì„ ë•Œë§Œ)
            if prev_dim == hidden_dim:
                setattr(self, f'skip_{i}', nn.Identity())
            else:
                setattr(self, f'skip_{i}', nn.Linear(prev_dim, hidden_dim))
                
            prev_dim = hidden_dim
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.output_layer = nn.Sequential(
            nn.Linear(prev_dim, prev_dim // 2),
            nn.LayerNorm(prev_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(prev_dim // 2, 1)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.LayerNorm):
            nn.init.constant_(module.bias, 0)
            nn.init.constant_(module.weight, 1.0)
    
    def forward(self, x):
        # ì…ë ¥ ì„ë² ë”©
        x = self.input_embedding(x)
        
        # Transformer ë¸”ë¡ë“¤
        for transformer in self.transformer_blocks:
            x = transformer(x)
        
        # ë©”ì¸ ë„¤íŠ¸ì›Œí¬ (Residual connections)
        prev_x = x
        for i, layer in enumerate(self.layers):
            new_x = layer(prev_x)
            
            # Skip connection
            skip_layer = getattr(self, f'skip_{i}')
            if hasattr(skip_layer, 'weight'):  # Linear layer
                skip_x = skip_layer(prev_x)
            else:  # Identity
                skip_x = prev_x
                
            # Residual connection (ì°¨ì›ì´ ë§ì„ ë•Œë§Œ)
            if new_x.shape == skip_x.shape:
                x = new_x + skip_x
            else:
                x = new_x
                
            prev_x = x
        
        # ì¶œë ¥
        return self.output_layer(x)


class BenthamMLTrainer:
    """ë²¤ë‹´ ê³„ì‚°ê¸° ML ëª¨ë¸ í›ˆë ¨"""
    
    def __init__(self):
        self.data_dir = Path("/mnt/c/large_project/linux_red_heart/processed_datasets")
        self.model_dir = Path("/mnt/c/large_project/linux_red_heart/models/bentham_models")
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # 6ê°œ ê°€ì¤‘ì¹˜ ë ˆì´ì–´
        self.weight_layers = [
            'contextual',  # ìƒí™©ì  ë§¥ë½
            'temporal',    # ì‹œê°„ì  ì˜í–¥
            'social',      # ì‚¬íšŒì  íŒŒê¸‰
            'ethical',     # ìœ¤ë¦¬ì  ì¤‘ìš”ë„
            'emotional',   # ê°ì •ì  ê°•ë„
            'cognitive'    # ì¸ì§€ì  ë³µì¡ë„
        ]
        
        self.trained_models = {}
        self.scalers = {}
        
        # GPU ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Using device: {self.device}")
        
        # í›ˆë ¨ ì„¤ì • (AdamW adaptive gradient í™œìš©)
        self.epochs = 300
        self.batch_size = 16  # ë©”ëª¨ë¦¬ ì•ˆì •ì„±ì„ ìœ„í•´ ë” ì‘ê²Œ
        self.learning_rate = 0.0005  # ì ë‹¹í•œ í•™ìŠµë¥ 
        self.patience = 40  # ì¶©ë¶„í•œ patienceë¡œ ìµœì ì  ì°¾ê¸°
        
        # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì„ ìœ„í•œ ëª¨ë¸ë“¤
        logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        from sentence_transformer_singleton import get_sentence_transformer
        self.sentence_transformer = get_sentence_transformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')
        self.emotion_analyzer = pipeline('text-classification', 
                                        model='j-hartmann/emotion-english-distilroberta-base',
                                        device=0 if torch.cuda.is_available() else -1)
        
    def load_training_data(self) -> pd.DataFrame:
        """í›ˆë ¨ ë°ì´í„° ë¡œë“œ"""
        logger.info("ë²¤ë‹´ í›ˆë ¨ ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        training_data = []
        
        # ìŠ¤í¬ëŸ¬í”Œ ë°ì´í„° ë¡œë“œ
        scruples_files = list(self.data_dir.glob("scruples/scruples_batch_*.json"))
        logger.info(f"ìŠ¤í¬ëŸ¬í”Œ íŒŒì¼ {len(scruples_files)}ê°œ ë°œê²¬")
        
        for file_path in scruples_files[:2]:  # ì²˜ìŒ 2ê°œë§Œ ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if 'scenarios' in data:
                    for scenario in data['scenarios']:
                        if isinstance(scenario, dict) and 'description' in scenario:
                            # ë²¤ë‹´ íŠ¹ì„± ì¶”ì¶œ
                            features = self._extract_bentham_features(scenario)
                            if features:
                                training_data.append(features)
                        else:
                            logger.warning(f"ì˜ëª»ëœ ì‹œë‚˜ë¦¬ì˜¤ í˜•ì‹: {type(scenario)}")
                                
            except Exception as e:
                logger.warning(f"íŒŒì¼ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}")
                continue
        
        # í†µí•© ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ë¡œë“œ (êµ¬ì¡° ìˆ˜ì •)
        integrated_files = [
            self.data_dir / "integrated_scenarios.json",
            self.data_dir / "final_integrated_with_batch7_20250619_213234.json"
        ]
        
        for file_path in integrated_files:
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # íŒŒì¼ êµ¬ì¡° ì²´í¬ ë° ì²˜ë¦¬
                    if isinstance(data, dict) and 'integrated_scenarios' in data:
                        # final_integrated_with_batch7 í˜•ì‹
                        scenarios = data['integrated_scenarios']
                        logger.info(f"í†µí•© íŒŒì¼ {file_path}ì—ì„œ {len(scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë°œê²¬")
                    elif isinstance(data, list):
                        # integrated_scenarios.json í˜•ì‹
                        scenarios = data
                        logger.info(f"í†µí•© íŒŒì¼ {file_path}ì—ì„œ {len(scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë°œê²¬")
                    else:
                        logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í†µí•© íŒŒì¼ í˜•ì‹: {type(data)}")
                        continue
                    
                    for idx, scenario in enumerate(scenarios[:10]):  # ê° íŒŒì¼ì—ì„œ 10ê°œë§Œ
                        if isinstance(scenario, dict) and 'description' in scenario:
                            features = self._extract_bentham_features(scenario)
                            if features:
                                training_data.append(features)
                        else:
                            logger.warning(f"ì˜ëª»ëœ í†µí•© ì‹œë‚˜ë¦¬ì˜¤ í˜•ì‹: {type(scenario)}")
                            
                except Exception as e:
                    logger.warning(f"í†µí•© íŒŒì¼ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}")
                    import traceback
                    logger.warning(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        
        df = pd.DataFrame(training_data)
        logger.info(f"ì´ {len(df)}ê°œ í›ˆë ¨ ìƒ˜í”Œ ìƒì„±")
        return df
    
    def _extract_bentham_features(self, scenario: Dict) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ê³ ê¸‰ ë²¤ë‹´ íŠ¹ì„± ì¶”ì¶œ (íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ í¬í•¨)"""
        try:
            text = scenario.get('description', '')
            if not text:
                return None
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ì„ë² ë”© ëª¨ë¸ 512 í† í° í•œê³„, ì•ˆì „ ë§ˆì§„ ì ìš©)
            if len(text) > 400:
                text = text[:400]
            
            # ê¸°ë³¸ íŠ¹ì„±ë“¤
            features = {
                'text_length': len(text),
                'word_count': len(text.split()),
                'complexity': scenario.get('complexity', 0.5),
                
                # ìƒí™©ì  ë§¥ë½ íŠ¹ì„±
                'has_context': 1 if scenario.get('context') else 0,
                'stakeholder_count': len(scenario.get('stakeholders', {})),
                'moral_complexity': scenario.get('moral_complexity', 0.5),
                
                # ì‹œê°„ì  íŠ¹ì„±
                'urgency_keywords': self._count_urgency_keywords(text),
                'future_impact_keywords': self._count_future_keywords(text),
                
                # ì‚¬íšŒì  íŠ¹ì„±
                'social_keywords': self._count_social_keywords(text),
                'relationship_keywords': self._count_relationship_keywords(text),
                
                # ìœ¤ë¦¬ì  íŠ¹ì„±
                'ethical_keywords': self._count_ethical_keywords(text),
                'moral_judgment': self._encode_moral_judgment(scenario.get('context', {})),
                
                # ê°ì •ì  íŠ¹ì„±
                'emotion_keywords': self._count_emotion_keywords(text),
                'emotional_intensity': self._estimate_emotional_intensity(text),
                
                # ì¸ì§€ì  íŠ¹ì„±
                'cognitive_load': self._estimate_cognitive_load(text),
                'decision_complexity': self._estimate_decision_complexity(text)
            }
            
            # ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (GPU ë©”ëª¨ë¦¬ ì•ˆì •ì„± ê°œì„ )
            try:
                # GPU ë©”ëª¨ë¦¬ ì²´í¬
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
                    if gpu_memory > 6.0:  # 6GB ì´ìƒ ì‚¬ìš© ì¤‘ì´ë©´ GPU ì •ë¦¬
                        torch.cuda.empty_cache()
                        logger.info(f"GPU ë©”ëª¨ë¦¬ ì •ë¦¬: {gpu_memory:.2f}GB")
                
                # 1. ì„ë² ë”© íŠ¹ì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬)
                try:
                    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
                        embedding = self.sentence_transformer.encode(text, convert_to_numpy=True, show_progress_bar=False)
                    
                    # í†µê³„ì  íŠ¹ì„± ì¶”ì¶œ
                    features.update({
                        'embedding_mean': float(np.mean(embedding)),
                        'embedding_std': float(np.std(embedding)),
                        'embedding_max': float(np.max(embedding)),
                        'embedding_min': float(np.min(embedding)),
                        'embedding_skew': float(np.percentile(embedding, 75) - np.percentile(embedding, 25))
                    })
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    del embedding
                    
                except Exception as embed_e:
                    logger.warning(f"ì„ë² ë”© ì‹¤íŒ¨: {embed_e}")
                    features.update({
                        'embedding_mean': 0.0, 'embedding_std': 0.0, 'embedding_max': 0.0, 
                        'embedding_min': 0.0, 'embedding_skew': 0.0
                    })
                
                # 2. ê°ì • ë¶„ì„ íŠ¹ì„± (ë°°ì¹˜ í¬ê¸° ì œí•œ)
                try:
                    # ì§§ì€ í…ìŠ¤íŠ¸ë¡œ ì˜ë¼ì„œ ì²˜ë¦¬ (ì•ˆì „ ë§ˆì§„)
                    text_for_emotion = text[:300] if len(text) > 300 else text
                    
                    with torch.no_grad():
                        emotion_result = self.emotion_analyzer(text_for_emotion)
                    
                    emotion_scores = {result['label']: result['score'] for result in emotion_result}
                    
                    # ì£¼ìš” ê°ì •ë“¤ì˜ ì ìˆ˜
                    emotions = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']
                    for emotion in emotions:
                        features[f'emotion_{emotion}'] = emotion_scores.get(emotion, 0.0)
                    
                    # ê°ì • ë‹¤ì–‘ì„± ì§€ìˆ˜
                    emotion_values = list(emotion_scores.values())
                    features['emotion_diversity'] = float(np.std(emotion_values))
                    features['emotion_dominance'] = float(np.max(emotion_values))
                    
                except Exception as emotion_e:
                    logger.warning(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {emotion_e}")
                    features.update({
                        'emotion_anger': 0.0, 'emotion_disgust': 0.0, 'emotion_fear': 0.0,
                        'emotion_joy': 0.0, 'emotion_neutral': 0.5, 'emotion_sadness': 0.0,
                        'emotion_surprise': 0.0, 'emotion_diversity': 0.0, 'emotion_dominance': 0.5
                    })
                
            except Exception as e:
                logger.warning(f"ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ ì „ì²´ ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ê°’ë“¤ë¡œ ì±„ìš°ê¸°
                features.update({
                    'embedding_mean': 0.0, 'embedding_std': 0.0, 'embedding_max': 0.0, 
                    'embedding_min': 0.0, 'embedding_skew': 0.0,
                    'emotion_anger': 0.0, 'emotion_disgust': 0.0, 'emotion_fear': 0.0,
                    'emotion_joy': 0.0, 'emotion_neutral': 0.5, 'emotion_sadness': 0.0,
                    'emotion_surprise': 0.0, 'emotion_diversity': 0.0, 'emotion_dominance': 0.5
                })
            
            # ëª©í‘œ ê°€ì¤‘ì¹˜ ìƒì„± (ê°œì„ ëœ ê·œì¹™ ê¸°ë°˜)
            targets = self._generate_target_weights(features, text)
            features.update(targets)
            
            return features
            
        except Exception as e:
            logger.warning(f"íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _count_urgency_keywords(self, text: str) -> int:
        urgency_words = ['ê¸‰í•˜', 'ë¹¨ë¦¬', 'ì¦‰ì‹œ', 'ê¸´ê¸‰', 'ì„œë‘˜', 'ì§€ê¸ˆ', 'ë‹¹ì¥', 'ë¹¨ë¦¬ë¹¨ë¦¬']
        return sum(1 for word in urgency_words if word in text)
    
    def _count_future_keywords(self, text: str) -> int:
        future_words = ['ë¯¸ë˜', 'ì•ìœ¼ë¡œ', 'ë‚˜ì¤‘', 'ì¥ê¸°', 'ì˜êµ¬', 'ê³„ì†', 'í‰ìƒ', 'ì˜¤ë˜']
        return sum(1 for word in future_words if word in text)
    
    def _count_social_keywords(self, text: str) -> int:
        social_words = ['ì‚¬ëŒ', 'ì¹œêµ¬', 'ê°€ì¡±', 'ë™ë£Œ', 'ì‚¬íšŒ', 'ê³µë™ì²´', 'ê´€ê³„', 'íƒ€ì¸']
        return sum(1 for word in social_words if word in text)
    
    def _count_relationship_keywords(self, text: str) -> int:
        rel_words = ['ì¹œêµ¬', 'ì—°ì¸', 'ë¶€ëª¨', 'ìì‹', 'í˜•ì œ', 'ìë§¤', 'ë™ë£Œ', 'ì´ì›ƒ']
        return sum(1 for word in rel_words if word in text)
    
    def _count_ethical_keywords(self, text: str) -> int:
        ethical_words = ['ìœ¤ë¦¬', 'ë„ë•', 'ì˜³', 'ê·¸ë¥´', 'ì •ì˜', 'ê³µì •', 'ì„ ì•…', 'ì±…ì„']
        return sum(1 for word in ethical_words if word in text)
    
    def _encode_moral_judgment(self, context: Dict) -> float:
        judgment = context.get('moral_judgment', 'NOBODY')
        mapping = {
            'AUTHOR': 0.2,    # ì‘ì„±ìê°€ ì˜ëª»
            'OTHER': 0.8,     # íƒ€ì¸ì´ ì˜ëª»
            'EVERYBODY': 0.5, # ëª¨ë‘ ì˜ëª»
            'NOBODY': 0.6,    # ì•„ë¬´ë„ ì˜ëª» ì•ˆí•¨
            'INFO': 0.5       # ì •ë³´ ë¶€ì¡±
        }
        return mapping.get(judgment, 0.5)
    
    def _count_emotion_keywords(self, text: str) -> int:
        emotion_words = ['ê¸°ì˜', 'ìŠ¬í”„', 'í™”ë‚˜', 'ë¬´ì„œ', 'ë†€ë¼', 'ì§œì¦', 'í–‰ë³µ', 'ìš°ìš¸']
        return sum(1 for word in emotion_words if word in text)
    
    def _estimate_emotional_intensity(self, text: str) -> float:
        intensity_words = ['ë§¤ìš°', 'ì •ë§', 'ë„ˆë¬´', 'ì™„ì „', 'ì—„ì²­', 'êµ‰ì¥', 'ê·¹ë„ë¡œ']
        count = sum(1 for word in intensity_words if word in text)
        return min(1.0, count * 0.2)
    
    def _estimate_cognitive_load(self, text: str) -> float:
        # ë¬¸ì¥ ë³µì¡ë„ ì¶”ì •
        sentences = text.split('.')
        avg_length = np.mean([len(s.split()) for s in sentences if s.strip()])
        return min(1.0, avg_length / 20.0)
    
    def _estimate_decision_complexity(self, text: str) -> float:
        decision_words = ['ì„ íƒ', 'ê²°ì •', 'ê³ ë¯¼', 'íŒë‹¨', 'ë”œë ˆë§ˆ', 'ê°ˆë“±']
        count = sum(1 for word in decision_words if word in text)
        return min(1.0, count * 0.3)
    
    def _generate_target_weights(self, features: Dict, text: str) -> Dict[str, float]:
        """ê°œì„ ëœ ëª©í‘œ ê°€ì¤‘ì¹˜ ìƒì„± (ë°ì´í„° ë¦¬ì¼€ì§€ ë°©ì§€)"""
        targets = {}
        
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜ì— ë…¸ì´ì¦ˆ ì¶”ê°€í•˜ì—¬ ë°ì´í„° ë¦¬ì¼€ì§€ ë°©ì§€
        base_weight = 1.0
        noise_factor = 0.1  # 10% ë…¸ì´ì¦ˆ
        
        # ìƒí™©ì  ë§¥ë½ ê°€ì¤‘ì¹˜ (ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„)
        contextual = base_weight
        # í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ë³µì¡ë„ì˜ ë¹„ì„ í˜• ê´€ê³„
        length_factor = np.tanh(features['text_length'] / 1000.0)  
        complexity_factor = features.get('complexity', 0.5) ** 2
        contextual += length_factor * complexity_factor * 0.8
        # ëœë¤ ë…¸ì´ì¦ˆ ì¶”ê°€
        contextual += np.random.normal(0, noise_factor)
        targets['target_contextual'] = np.clip(contextual, 0.5, 3.0)
        
        # ì‹œê°„ì  ì˜í–¥ ê°€ì¤‘ì¹˜ (í‚¤ì›Œë“œ ë°€ë„ ê¸°ë°˜)
        temporal = base_weight
        word_count = max(1, features['word_count'])
        urgency_density = features['urgency_keywords'] / word_count
        future_density = features['future_impact_keywords'] / word_count
        temporal += (urgency_density * 2.0 + future_density * 1.5) * 10
        temporal += np.random.normal(0, noise_factor)
        targets['target_temporal'] = np.clip(temporal, 0.5, 3.0)
        
        # ì‚¬íšŒì  íŒŒê¸‰ ê°€ì¤‘ì¹˜ (ìƒí˜¸ì‘ìš© íš¨ê³¼)
        social = base_weight
        social_density = features['social_keywords'] / word_count
        rel_density = features['relationship_keywords'] / word_count
        # ë¹„ì„ í˜• ìƒí˜¸ì‘ìš©
        social += np.sqrt(social_density * rel_density) * 5.0
        social += np.random.normal(0, noise_factor)
        targets['target_social'] = np.clip(social, 0.5, 3.0)
        
        # ìœ¤ë¦¬ì  ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ (ë³µí•© ì§€ìˆ˜)
        ethical = base_weight
        ethical_density = features['ethical_keywords'] / word_count
        moral_extremeness = abs(features['moral_judgment'] - 0.5) * 2
        # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì ìš©
        ethical += np.log1p(ethical_density * 10) + moral_extremeness
        ethical += np.random.normal(0, noise_factor)
        targets['target_ethical'] = np.clip(ethical, 0.5, 3.0)
        
        # ê°ì •ì  ê°•ë„ ê°€ì¤‘ì¹˜ (ê°ì • ì§€ìˆ˜)
        emotional = base_weight
        emotion_density = features['emotion_keywords'] / word_count
        intensity = features['emotional_intensity']
        # ì œê³±ê·¼ ê´€ê³„
        emotional += np.sqrt(emotion_density * intensity) * 3.0
        emotional += np.random.normal(0, noise_factor)
        targets['target_emotional'] = np.clip(emotional, 0.5, 3.0)
        
        # ì¸ì§€ì  ë³µì¡ë„ ê°€ì¤‘ì¹˜ (ë³µì¡ë„ ì§€ìˆ˜)
        cognitive = base_weight
        cog_load = features['cognitive_load']
        decision_comp = features['decision_complexity']
        # ì§€ìˆ˜ í•¨ìˆ˜ ê´€ê³„
        cognitive += np.exp(cog_load * decision_comp) - 1.0
        cognitive += np.random.normal(0, noise_factor)
        targets['target_cognitive'] = np.clip(cognitive, 0.5, 3.0)
        
        return targets
    
    def train_deep_model(self, X_train, X_test, y_train, y_test, layer_name):
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨"""
        input_dim = X_train.shape[1]
        
        # ëª¨ë¸ ìƒì„±
        model = BenthamDNN(input_dim).to(self.device)
        
        # ìµœì í™”ê¸° (AdamW with weight decay)
        optimizer = optim.AdamW(model.parameters(), 
                               lr=self.learning_rate, 
                               weight_decay=0.01)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=10
        )
        
        # ì†ì‹¤ í•¨ìˆ˜
        criterion = nn.MSELoss()
        
        # ë°ì´í„°ë¡œë”
        train_dataset = BenthamDataset(X_train, y_train.values)
        test_dataset = BenthamDataset(X_test, y_test.values)
        
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)
        
        # í›ˆë ¨ ë£¨í”„
        best_loss = float('inf')
        patience_counter = 0
        train_losses = []
        val_losses = []
        
        for epoch in range(self.epochs):
            # í›ˆë ¨
            model.train()
            train_loss = 0.0
            for batch_features, batch_targets in train_loader:
                batch_features = batch_features.to(self.device)
                batch_targets = batch_targets.to(self.device)
                
                optimizer.zero_grad()
                outputs = model(batch_features).squeeze()
                loss = criterion(outputs, batch_targets)
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item()
            
            # ê²€ì¦
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for batch_features, batch_targets in test_loader:
                    batch_features = batch_features.to(self.device)
                    batch_targets = batch_targets.to(self.device)
                    
                    outputs = model(batch_features).squeeze()
                    loss = criterion(outputs, batch_targets)
                    val_loss += loss.item()
            
            train_loss /= len(train_loader)
            val_loss /= len(test_loader)
            
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            scheduler.step(val_loss)
            
            # Early stopping
            if val_loss < best_loss:
                best_loss = val_loss
                patience_counter = 0
                # ìµœê³  ëª¨ë¸ ì €ì¥
                best_model_state = model.state_dict()
            else:
                patience_counter += 1
            
            if epoch % 10 == 0:
                logger.info(f"  Epoch {epoch:3d}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}")
            
            if patience_counter >= self.patience:
                logger.info(f"  Early stopping at epoch {epoch}")
                break
        
        # ìµœê³  ëª¨ë¸ ë³µì›
        model.load_state_dict(best_model_state)
        
        # ìµœì¢… í‰ê°€
        model.eval()
        with torch.no_grad():
            y_pred = []
            for batch_features, _ in test_loader:
                batch_features = batch_features.to(self.device)
                outputs = model(batch_features).squeeze()
                y_pred.extend(outputs.cpu().numpy())
        
        y_pred = np.array(y_pred)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        return model, mse, r2, len(train_losses)

    def train_models(self, df: pd.DataFrame):
        """ê° ê°€ì¤‘ì¹˜ ë ˆì´ì–´ë³„ ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨"""
        logger.info("ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        # íŠ¹ì„± ì»¬ëŸ¼ë“¤
        feature_cols = [col for col in df.columns if not col.startswith('target_')]
        
        for layer in self.weight_layers:
            target_col = f'target_{layer}'
            if target_col not in df.columns:
                logger.warning(f"ëª©í‘œ ì»¬ëŸ¼ {target_col} ì—†ìŒ, ê±´ë„ˆëœ€")
                continue
            
            logger.info(f"{layer} ë ˆì´ì–´ ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            
            # ë°ì´í„° ì¤€ë¹„
            X = df[feature_cols].fillna(0)
            y = df[target_col]
            
            # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # íŠ¹ì„± ì •ê·œí™”
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨
            model, mse, r2, epochs_trained = self.train_deep_model(
                X_train_scaled, X_test_scaled, y_train, y_test, layer
            )
            
            logger.info(f"{layer} ëª¨ë¸ ì„±ëŠ¥:")
            logger.info(f"  - MSE: {mse:.6f}")
            logger.info(f"  - RÂ²: {r2:.6f}")
            logger.info(f"  - í›ˆë ¨ ì—í¬í¬: {epochs_trained}")
            
            # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
            self.trained_models[layer] = model
            self.scalers[layer] = scaler
            
            # íŒŒì¼ë¡œ ì €ì¥
            model_path = self.model_dir / f"{layer}_model.pth"
            scaler_path = self.model_dir / f"{layer}_scaler.joblib"
            
            torch.save(model.state_dict(), model_path)
            joblib.dump(scaler, scaler_path)
            
            logger.info(f"{layer} ëª¨ë¸ ì €ì¥: {model_path}")
        
        # í›ˆë ¨ ì™„ë£Œ í”Œë˜ê·¸ ì €ì¥
        training_info = {
            'model_type': 'deep_learning',
            'trained_layers': self.weight_layers,
            'training_date': datetime.now().isoformat(),
            'training_samples': len(df),
            'feature_columns': feature_cols,
            'epochs': self.epochs,
            'batch_size': self.batch_size,
            'learning_rate': self.learning_rate
        }
        
        info_path = self.model_dir / "training_info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(training_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ëª¨ë“  ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ! ì •ë³´ ì €ì¥: {info_path}")

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    trainer = BenthamMLTrainer()
    
    # ë°ì´í„° ë¡œë“œ
    df = trainer.load_training_data()
    
    if len(df) < 10:
        logger.error("í›ˆë ¨ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 10ê°œ í•„ìš”)")
        return
    
    # ëª¨ë¸ í›ˆë ¨
    trainer.train_models(df)
    
    logger.info("ğŸ‰ ë²¤ë‹´ ML ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")

if __name__ == "__main__":
    main()