# Red Heart AI ê³ ê¸‰ í•™ìŠµ ì „ëµ ë° ì‹¤í–‰ ê°€ì´ë“œ

## ğŸ“Œ Executive Summary

10,000ê°œ ë°ì´í„°ì…‹ìœ¼ë¡œ 730M íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ë¡œì»¬ GPU(8GB VRAM)ì—ì„œ í•™ìŠµí•˜ëŠ” ê³ ê¸‰ ì „ëµ. 
ì˜ë„ì  ê³¼ì í•© í›„ í¬ë¡œìŠ¤ì˜¤ë²„ë¥¼ í†µí•œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒì„ ëª©í‘œë¡œ í•¨.

**í•µì‹¬ ì „ëµ:**
- **Phase 1**: LR Sweep (5 candidates Ã— 5 epochs) 
- **Phase 2**: ë³¸ í•™ìŠµ (60 epochs, 2 epochë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸)
- **Phase 3**: Sweet Spot ë¶„ì„ (ëª¨ë“ˆë³„ ìµœì  ì—í­ íƒì§€)
- **Phase 4**: íŒŒë¼ë¯¸í„° í¬ë¡œìŠ¤ì˜¤ë²„ (ìµœì  ì¡°í•© ìƒì„±)
- **ì˜ˆìƒ ì‹œê°„**: 170-200ì‹œê°„ (7-8ì¼)
- **ì˜ˆìƒ ì„±ëŠ¥**: ë‹¨ì¼ ëª¨ë¸ 75-80% â†’ í¬ë¡œìŠ¤ì˜¤ë²„ 85-90%

---

## 1. ëª¨ë“ˆ ì˜ì¡´ì„± êµ¬ì¡° ë° ê·¸ë£¹í™”

### 1.1 ì—°ë™ ëª¨ë“ˆ ê·¸ë£¹ (ê°™ì€ ì—í­ ì²´í¬í¬ì¸íŠ¸ í•„ìˆ˜)

#### ê·¸ë£¹ A: Backbone-Heads ìƒí˜¸ ì˜ì¡´ì„±
```python
DEPENDENCY_GROUP_A = {
    'backbone': 'RedHeartUnifiedBackbone',  # 90.6M
    'heads': {
        'emotion': 'EmotionHead',  # 17.3M  
        'bentham': 'BenthamHead',  # 13.9M
        'regret': 'RegretHead',    # 19.9M
        'surd': 'SURDHead'         # 12.0M
    }
}
# ì´ 153.7M - ë°±ë³¸ì´ í—¤ë“œì— íƒœìŠ¤í¬ë³„ íŠ¹ì§• ì§ì ‘ ì „ë‹¬
```

**ì˜ì¡´ì„± ì´ìœ :**
- ë°±ë³¸ì˜ `task_projections`ê°€ ê° í—¤ë“œë³„ë¡œ ìµœì í™”ë¨
- í—¤ë“œ ì†ì‹¤ì´ ë°±ë³¸ íŒŒë¼ë¯¸í„°ì— ì§ì ‘ ì—­ì „íŒŒ
- ì¶”ë¡  ì‹œ ë°±ë³¸ ì¶œë ¥ì´ í—¤ë“œ ì…ë ¥ìœ¼ë¡œ ì§ê²°

#### ê·¸ë£¹ B: Neural Analyzers ìƒí˜¸ ì°¸ì¡°
```python
DEPENDENCY_GROUP_B = {
    'neural_emotion': 'NeuralEmotionAnalyzer',  # 122.6M
    'neural_bentham': 'NeuralBenthamCalculator', # 78.3M
    'neural_regret': 'NeuralRegretAnalyzer',    # 153.9M
    'neural_surd': 'NeuralSURDAnalyzer'         # 13.5M
}
# ì´ 368.3M - ì„œë¡œ ê°„ attention ë©”ì»¤ë‹ˆì¦˜ ê³µìœ  ê°€ëŠ¥
```

#### ê·¸ë£¹ C: DSP-Kalman ìœµí•© ì˜ì¡´ì„±
```python
DEPENDENCY_GROUP_C = {
    'dsp': 'EmotionDSPSimulator',    # 2.3M
    'kalman': 'DynamicKalmanFilter'  # 0.7K
}
# ì´ 2.3M - Kalmanì´ DSP ì¶œë ¥ì„ í•„ìˆ˜ ì…ë ¥ìœ¼ë¡œ ìš”êµ¬
```

### 1.2 ë…ë¦½ ëª¨ë“ˆ (ê°œë³„ ìµœì  ì—í­ ê°€ëŠ¥)

```python
INDEPENDENT_MODULES = {
    'advanced_emotion': 'AdvancedEmotionAnalyzer',  # 63.0M
    'advanced_regret': 'AdvancedRegretAnalyzer',   # 44.2M  
    'advanced_bentham': 'AdvancedBenthamCalculator', # 2.5M
    'advanced_surd': 'AdvancedSURDAnalyzer',       # ì‹¤ì œ ë¯¸êµ¬í˜„
    'phase0': 'Phase0EmotionCalibrator',           # 2.0M
    'phase1': 'Phase1EmpathyLearner',              # 0.2M
    'phase2': 'Phase2CommunityNetwork'             # ë¯¸êµ¬í˜„
}
```

---

## 2. Phase 1: Learning Rate Sweep ì „ëµ

### 2.1 ì‹¤í–‰ ì„¤ì •
```python
# lr_sweep_config.py
LR_CANDIDATES = [1e-5, 5e-5, 1e-4, 2e-4, 5e-4]
SWEEP_CONFIG = {
    'epochs': 5,
    'batch_size': 4,
    'gradient_accumulation_steps': 16,  # Effective batch: 64
    'mixed_precision': True,
    'gradient_checkpointing': False,  # 730Mì€ ë¶ˆí•„ìš”
    'save_metrics': True,
    'metrics_dir': 'C:/large_project/linux_red_heart/docs/data/lr_sweep/'
}
```

### 2.2 ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
```python
class LRSweepLogger:
    def __init__(self, lr, output_dir):
        self.lr = lr
        self.metrics = {
            'lr': lr,
            'epochs': [],
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': [],
            'gradient_norm': [],
            'learning_curve': [],
            'gpu_memory': [],
            'time_per_epoch': []
        }
        
    def log_epoch(self, epoch, metrics):
        """ì—í­ë³„ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.metrics['epochs'].append(epoch)
        self.metrics['train_loss'].append(metrics['train_loss'])
        self.metrics['val_loss'].append(metrics['val_loss'])
        self.metrics['gradient_norm'].append(metrics['grad_norm'])
        self.metrics['gpu_memory'].append(torch.cuda.max_memory_allocated() / 1e9)
        
    def save(self):
        """JSON ë° CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ë…¼ë¬¸ìš©)"""
        # ë©”ì¸ ìœ„ì¹˜
        with open(f'{self.output_dir}/lr_{self.lr}_metrics.json', 'w') as f:
            json.dump(self.metrics, f, indent=2)
            
        # ë°±ì—… ìœ„ì¹˜
        backup_path = 'C:/large_project/linux_red_heart/docs/data/lr_sweep/'
        os.makedirs(backup_path, exist_ok=True)
        shutil.copy2(
            f'{self.output_dir}/lr_{self.lr}_metrics.json',
            f'{backup_path}/lr_{self.lr}_backup.json'
        )
        
        # CSV ë³€í™˜ (ë…¼ë¬¸ ê·¸ë˜í”„ìš©)
        pd.DataFrame(self.metrics).to_csv(
            f'{backup_path}/lr_{self.lr}_table.csv',
            index=False
        )
```

### 2.3 ìµœì  LR ìë™ ì„ íƒ
```python
def select_best_lr(sweep_results_dir):
    """ê²€ì¦ ì†ì‹¤ ê¸°ì¤€ ìµœì  LR ì„ íƒ"""
    best_lr = None
    best_val_loss = float('inf')
    
    for lr in LR_CANDIDATES:
        metrics_file = f'{sweep_results_dir}/lr_{lr}_metrics.json'
        with open(metrics_file, 'r') as f:
            metrics = json.load(f)
            
        # ë§ˆì§€ë§‰ 3 ì—í­ì˜ í‰ê·  ê²€ì¦ ì†ì‹¤
        avg_val_loss = np.mean(metrics['val_loss'][-3:])
        
        # ê³¼ì í•© ì§€í‘œ í™•ì¸ (train vs val gap)
        overfit_score = metrics['train_loss'][-1] - metrics['val_loss'][-1]
        
        # ì¢…í•© ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        score = avg_val_loss + 0.1 * abs(overfit_score)
        
        if score < best_val_loss:
            best_val_loss = score
            best_lr = lr
            
    logger.info(f"ìµœì  LR ì„ íƒ: {best_lr} (score: {best_val_loss:.4f})")
    return best_lr
```

---

## 3. Phase 2: ë³¸ í•™ìŠµ (60 Epochs)

### 3.1 í•™ìŠµ ì„¤ì •
```python
# main_training_config.py
MAIN_CONFIG = {
    'epochs': 60,
    'batch_size': 4,
    'gradient_accumulation_steps': 16,  # Effective: 64
    'learning_rate': None,  # LR Sweepì—ì„œ ìë™ ê²°ì •
    'scheduler': 'cosine_annealing_with_restarts',
    'warmup_ratio': 0.1,
    'weight_decay': 0.01,
    'mixed_precision': True,
    'save_every_n_epochs': 2,  # 30ê°œ ì²´í¬í¬ì¸íŠ¸ ìƒì„±
    'early_stopping': False,  # ì˜ë„ì  ë¹„í™œì„±í™” (ê³¼ì í•© ìœ ë„)
    'dropout': 0.1,  # ë‚®ê²Œ ì„¤ì • (ê³¼ì í•© ìœ ë„)
}
```

### 3.2 ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì „ëµ
```python
class ModularCheckpointManager:
    def __init__(self, base_dir='checkpoints'):
        self.base_dir = base_dir
        self.backup_dir = 'C:/large_project/linux_red_heart/docs/data/checkpoints/'
        os.makedirs(self.base_dir, exist_ok=True)
        os.makedirs(self.backup_dir, exist_ok=True)
        
    def save_checkpoint(self, epoch, model, optimizer, metrics):
        """ëª¨ë“ˆë³„ ê°œë³„ ì €ì¥ + ë©”íŠ¸ë¦­ ê¸°ë¡"""
        checkpoint = {
            'epoch': epoch,
            'timestamp': datetime.now().isoformat(),
            'metrics': metrics,
            'model_state': {},
            'optimizer_state': optimizer.state_dict()
        }
        
        # 1. ì—°ë™ ê·¸ë£¹ë³„ ì €ì¥
        checkpoint['model_state']['group_a'] = {
            'backbone': model.backbone.state_dict(),
            'heads': {
                name: head.state_dict() 
                for name, head in model.heads.items()
            }
        }
        
        checkpoint['model_state']['group_b'] = {
            name: analyzer.state_dict()
            for name, analyzer in model.analyzers.items()
            if 'neural_' in name
        }
        
        checkpoint['model_state']['group_c'] = {
            'dsp': model.analyzers.get('dsp', {}).state_dict() if 'dsp' in model.analyzers else {},
            'kalman': model.analyzers.get('kalman', {}).state_dict() if 'kalman' in model.analyzers else {}
        }
        
        # 2. ë…ë¦½ ëª¨ë“ˆ ì €ì¥
        checkpoint['model_state']['independent'] = {
            name: module.state_dict()
            for name, module in model.analyzers.items()
            if 'advanced_' in name
        }
        
        # 3. ë©”íŠ¸ë¦­ ìƒì„¸ ê¸°ë¡ (ë…¼ë¬¸ìš©)
        checkpoint['detailed_metrics'] = {
            'train_loss': metrics['train_loss'],
            'val_loss': metrics['val_loss'],
            'train_acc': metrics.get('train_acc', 0),
            'val_acc': metrics.get('val_acc', 0),
            'per_module_loss': self._calculate_per_module_loss(model),
            'gradient_statistics': self._get_gradient_stats(model),
            'activation_statistics': self._get_activation_stats(model),
            'weight_statistics': self._get_weight_stats(model),
            'gpu_memory': torch.cuda.max_memory_allocated() / 1e9,
            'learning_rate': optimizer.param_groups[0]['lr']
        }
        
        # 4. ì €ì¥ (ë©”ì¸ + ë°±ì—…)
        checkpoint_path = f'{self.base_dir}/epoch_{epoch:03d}.pt'
        torch.save(checkpoint, checkpoint_path)
        
        # ë°±ì—… (ì••ì¶•)
        backup_path = f'{self.backup_dir}/epoch_{epoch:03d}_backup.pt.gz'
        with gzip.open(backup_path, 'wb') as f:
            torch.save(checkpoint, f)
            
        # 5. ë©”íŠ¸ë¦­ë§Œ ë³„ë„ JSON ì €ì¥ (ë¹ ë¥¸ ë¶„ì„ìš©)
        metrics_path = f'{self.backup_dir}/metrics_epoch_{epoch:03d}.json'
        with open(metrics_path, 'w') as f:
            json.dump(checkpoint['detailed_metrics'], f, indent=2)
            
        logger.info(f"âœ… Epoch {epoch} ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ")
        
    def _calculate_per_module_loss(self, model):
        """ëª¨ë“ˆë³„ ì†ì‹¤ ê³„ì‚°"""
        losses = {}
        # ê° ëª¨ë“ˆë³„ë¡œ ë”ë¯¸ ì…ë ¥ìœ¼ë¡œ ì†ì‹¤ ê³„ì‚°
        with torch.no_grad():
            dummy_input = torch.randn(2, 768).to(model.device)
            dummy_target = torch.randint(0, 7, (2,)).to(model.device)
            
            for name, module in model.analyzers.items():
                try:
                    output = module(dummy_input)
                    if isinstance(output, dict):
                        output = output.get('logits', output.get('scores', None))
                    if output is not None:
                        loss = F.cross_entropy(output, dummy_target)
                        losses[name] = loss.item()
                except:
                    losses[name] = -1
        return losses
        
    def _get_gradient_stats(self, model):
        """ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„"""
        stats = {}
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad = param.grad.data
                stats[name] = {
                    'mean': grad.mean().item(),
                    'std': grad.std().item(),
                    'max': grad.max().item(),
                    'min': grad.min().item(),
                    'norm': grad.norm().item()
                }
        return stats
        
    def _get_activation_stats(self, model):
        """í™œì„±í™” í†µê³„ (hook ì‚¬ìš©)"""
        # êµ¬í˜„ ìƒëµ - í•„ìš”ì‹œ ì¶”ê°€
        return {}
        
    def _get_weight_stats(self, model):
        """ê°€ì¤‘ì¹˜ í†µê³„"""
        stats = {}
        for name, param in model.named_parameters():
            stats[name] = {
                'mean': param.data.mean().item(),
                'std': param.data.std().item(),
                'norm': param.data.norm().item(),
                'sparsity': (param.data.abs() < 1e-6).float().mean().item()
            }
        return stats
```

### 3.3 DSM ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ëµ
```python
class AdaptiveDSMScheduler:
    """GPU ë©”ëª¨ë¦¬ 70-90% ìœ ì§€í•˜ë©° ë™ì  ìŠ¤ì™‘"""
    
    def __init__(self, target_memory_usage=0.85, max_memory_gb=8.0):
        self.target_usage = target_memory_usage
        self.max_memory = max_memory_gb * 1e9
        self.swap_history = []
        self.module_priorities = {
            'backbone': 100,  # ì ˆëŒ€ ìŠ¤ì™‘ ì•ˆí•¨
            'heads': 90,      # ê±°ì˜ ìŠ¤ì™‘ ì•ˆí•¨
            'neural_': 50,    # í•„ìš”ì‹œ ìŠ¤ì™‘
            'advanced_': 20,  # ìš°ì„  ìŠ¤ì™‘
            'phase': 10       # ê°€ì¥ ë¨¼ì € ìŠ¤ì™‘
        }
        
    def check_and_swap(self, model, current_epoch):
        """ë©”ëª¨ë¦¬ ì²´í¬ ë° ìŠ¤ì™‘ ê²°ì •"""
        current_memory = torch.cuda.memory_allocated()
        usage_ratio = current_memory / self.max_memory
        
        if usage_ratio > self.target_usage:
            # ìŠ¤ì™‘ í•„ìš”
            modules_to_swap = self._select_swap_candidates(model, usage_ratio)
            for module_name in modules_to_swap:
                self._swap_to_cpu(model, module_name)
                self.swap_history.append({
                    'epoch': current_epoch,
                    'module': module_name,
                    'memory_before': usage_ratio,
                    'action': 'swap_out'
                })
                
        elif usage_ratio < self.target_usage - 0.1:
            # ì—¬ìœ  ìˆìœ¼ë©´ ë³µì›
            if self.swap_history:
                last_swapped = self.swap_history[-1]['module']
                self._swap_to_gpu(model, last_swapped)
                self.swap_history.append({
                    'epoch': current_epoch,
                    'module': last_swapped,
                    'memory_before': usage_ratio,
                    'action': 'swap_in'
                })
                
    def _select_swap_candidates(self, model, usage_ratio):
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ìŠ¤ì™‘ ëŒ€ìƒ ì„ íƒ"""
        candidates = []
        
        # Advanced modules first
        for name in model.analyzers.keys():
            if 'advanced_' in name:
                candidates.append((self.module_priorities.get('advanced_', 20), name))
                
        # Sort by priority (lower = swap first)
        candidates.sort(key=lambda x: x[0])
        
        # ë©”ëª¨ë¦¬ ì´ˆê³¼ëŸ‰ì— ë”°ë¼ ìŠ¤ì™‘ ê°œìˆ˜ ê²°ì •
        overflow = usage_ratio - self.target_usage
        num_to_swap = max(1, int(overflow * 10))  # ì´ˆê³¼ 10%ë‹¹ 1ê°œ
        
        return [name for _, name in candidates[:num_to_swap]]
        
    def _swap_to_cpu(self, model, module_name):
        """GPU â†’ CPU ìŠ¤ì™‘"""
        if hasattr(model, 'analyzers') and module_name in model.analyzers:
            model.analyzers[module_name] = model.analyzers[module_name].cpu()
            torch.cuda.empty_cache()
            
    def _swap_to_gpu(self, model, module_name):
        """CPU â†’ GPU ë³µì›"""
        if hasattr(model, 'analyzers') and module_name in model.analyzers:
            model.analyzers[module_name] = model.analyzers[module_name].cuda()
```

---

## 4. Phase 3: Sweet Spot ë¶„ì„

### 4.1 ìë™ Sweet Spot íƒì§€
```python
class SweetSpotAnalyzer:
    def __init__(self, checkpoint_dir):
        self.checkpoint_dir = checkpoint_dir
        self.metrics = self._load_all_metrics()
        
    def _load_all_metrics(self):
        """ëª¨ë“  ì—í­ì˜ ë©”íŠ¸ë¦­ ë¡œë“œ"""
        metrics = {}
        for epoch in range(2, 62, 2):  # 2, 4, 6, ..., 60
            metrics_file = f'{self.checkpoint_dir}/metrics_epoch_{epoch:03d}.json'
            if os.path.exists(metrics_file):
                with open(metrics_file, 'r') as f:
                    metrics[epoch] = json.load(f)
        return metrics
        
    def find_module_sweet_spots(self):
        """ëª¨ë“ˆ ê·¸ë£¹ë³„ ìµœì  ì—í­ ì°¾ê¸°"""
        sweet_spots = {}
        
        # 1. ì—°ë™ ê·¸ë£¹ A (Backbone-Heads)
        sweet_spots['group_a'] = self._find_best_epoch_for_group(
            ['backbone', 'emotion_head', 'bentham_head', 'regret_head', 'surd_head']
        )
        
        # 2. ì—°ë™ ê·¸ë£¹ B (Neural Analyzers)
        sweet_spots['group_b'] = self._find_best_epoch_for_group(
            ['neural_emotion', 'neural_bentham', 'neural_regret', 'neural_surd']
        )
        
        # 3. ì—°ë™ ê·¸ë£¹ C (DSP-Kalman)
        sweet_spots['group_c'] = self._find_best_epoch_for_group(
            ['dsp', 'kalman']
        )
        
        # 4. ë…ë¦½ ëª¨ë“ˆë“¤
        for module in ['advanced_emotion', 'advanced_regret', 'advanced_bentham']:
            sweet_spots[module] = self._find_best_epoch_for_module(module)
            
        return sweet_spots
        
    def _find_best_epoch_for_group(self, module_names):
        """ê·¸ë£¹ ë‚´ ëª¨ë“ˆë“¤ì˜ í‰ê·  ì„±ëŠ¥ ê¸°ì¤€ ìµœì  ì—í­"""
        best_epoch = None
        best_score = float('inf')
        
        for epoch, metrics in self.metrics.items():
            # ê·¸ë£¹ ë‚´ ëª¨ë“  ëª¨ë“ˆì˜ ì†ì‹¤ í‰ê· 
            group_losses = []
            for module_name in module_names:
                if module_name in metrics.get('per_module_loss', {}):
                    loss = metrics['per_module_loss'][module_name]
                    if loss > 0:  # ìœ íš¨í•œ ì†ì‹¤ë§Œ
                        group_losses.append(loss)
                        
            if group_losses:
                avg_loss = np.mean(group_losses)
                
                # ê³¼ì í•© í˜ë„í‹° ê³„ì‚°
                train_val_gap = abs(metrics['train_loss'] - metrics['val_loss'])
                overfit_penalty = 0.1 * train_val_gap if train_val_gap > 0.5 else 0
                
                # ì¢…í•© ì ìˆ˜
                score = avg_loss + overfit_penalty
                
                if score < best_score:
                    best_score = score
                    best_epoch = epoch
                    
        return best_epoch
        
    def _find_best_epoch_for_module(self, module_name):
        """ê°œë³„ ëª¨ë“ˆì˜ ìµœì  ì—í­"""
        best_epoch = None
        best_loss = float('inf')
        
        for epoch, metrics in self.metrics.items():
            if module_name in metrics.get('per_module_loss', {}):
                loss = metrics['per_module_loss'][module_name]
                if 0 < loss < best_loss:
                    best_loss = loss
                    best_epoch = epoch
                    
        return best_epoch
        
    def generate_analysis_report(self, sweet_spots):
        """ë¶„ì„ ë³´ê³ ì„œ ìƒì„± (ë…¼ë¬¸ìš©)"""
        report = {
            'analysis_timestamp': datetime.now().isoformat(),
            'sweet_spots': sweet_spots,
            'detailed_analysis': {}
        }
        
        for group_or_module, best_epoch in sweet_spots.items():
            if best_epoch and best_epoch in self.metrics:
                metrics = self.metrics[best_epoch]
                report['detailed_analysis'][group_or_module] = {
                    'best_epoch': best_epoch,
                    'train_loss': metrics['train_loss'],
                    'val_loss': metrics['val_loss'],
                    'overfit_score': metrics['train_loss'] - metrics['val_loss'],
                    'gradient_norm_stats': self._summarize_gradient_stats(metrics),
                    'weight_sparsity': self._calculate_sparsity(metrics)
                }
                
        # ì €ì¥
        report_path = 'C:/large_project/linux_red_heart/docs/data/sweet_spot_analysis.json'
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
            
        # ì‹œê°í™”ìš© ë°ì´í„° ìƒì„±
        self._generate_visualization_data(report)
        
        return report
        
    def _summarize_gradient_stats(self, metrics):
        """ê·¸ë˜ë””ì–¸íŠ¸ í†µê³„ ìš”ì•½"""
        if 'gradient_statistics' not in metrics:
            return {}
            
        all_norms = []
        for param_stats in metrics['gradient_statistics'].values():
            if 'norm' in param_stats:
                all_norms.append(param_stats['norm'])
                
        return {
            'mean_norm': np.mean(all_norms) if all_norms else 0,
            'max_norm': np.max(all_norms) if all_norms else 0,
            'min_norm': np.min(all_norms) if all_norms else 0
        }
        
    def _calculate_sparsity(self, metrics):
        """ê°€ì¤‘ì¹˜ í¬ì†Œì„± ê³„ì‚°"""
        if 'weight_statistics' not in metrics:
            return 0
            
        sparsities = []
        for param_stats in metrics['weight_statistics'].values():
            if 'sparsity' in param_stats:
                sparsities.append(param_stats['sparsity'])
                
        return np.mean(sparsities) if sparsities else 0
        
    def _generate_visualization_data(self, report):
        """Matplotlibìš© ì‹œê°í™” ë°ì´í„° ìƒì„±"""
        # í•™ìŠµ ê³¡ì„  ë°ì´í„°
        epochs = sorted(self.metrics.keys())
        train_losses = [self.metrics[e]['train_loss'] for e in epochs]
        val_losses = [self.metrics[e]['val_loss'] for e in epochs]
        
        viz_data = {
            'epochs': epochs,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'sweet_spots': report['sweet_spots']
        }
        
        viz_path = 'C:/large_project/linux_red_heart/docs/data/visualization_data.json'
        with open(viz_path, 'w') as f:
            json.dump(viz_data, f)
```

---

## 5. Phase 4: íŒŒë¼ë¯¸í„° í¬ë¡œìŠ¤ì˜¤ë²„

### 5.1 í¬ë¡œìŠ¤ì˜¤ë²„ ì „ëµ
```python
class ParameterCrossover:
    def __init__(self, checkpoint_dir, sweet_spots):
        self.checkpoint_dir = checkpoint_dir
        self.sweet_spots = sweet_spots
        
    def create_optimal_model(self):
        """Sweet spot ì¡°í•©ìœ¼ë¡œ ìµœì  ëª¨ë¸ ìƒì„±"""
        
        # ë¹ˆ ëª¨ë¸ ì´ˆê¸°í™”
        from unified_training_v2 import UnifiedTrainingSystemV2
        optimal_model = UnifiedTrainingSystemV2(args)
        
        # 1. ì—°ë™ ê·¸ë£¹ A (Backbone-Heads) ë¡œë“œ
        if 'group_a' in self.sweet_spots:
            epoch = self.sweet_spots['group_a']
            checkpoint = torch.load(f'{self.checkpoint_dir}/epoch_{epoch:03d}.pt')
            
            # Backbone
            optimal_model.backbone.load_state_dict(
                checkpoint['model_state']['group_a']['backbone']
            )
            
            # Heads
            for head_name, head_state in checkpoint['model_state']['group_a']['heads'].items():
                if head_name in optimal_model.heads:
                    optimal_model.heads[head_name].load_state_dict(head_state)
                    
        # 2. ì—°ë™ ê·¸ë£¹ B (Neural Analyzers) ë¡œë“œ
        if 'group_b' in self.sweet_spots:
            epoch = self.sweet_spots['group_b']
            checkpoint = torch.load(f'{self.checkpoint_dir}/epoch_{epoch:03d}.pt')
            
            for analyzer_name, analyzer_state in checkpoint['model_state']['group_b'].items():
                if analyzer_name in optimal_model.analyzers:
                    optimal_model.analyzers[analyzer_name].load_state_dict(analyzer_state)
                    
        # 3. ì—°ë™ ê·¸ë£¹ C (DSP-Kalman) ë¡œë“œ
        if 'group_c' in self.sweet_spots:
            epoch = self.sweet_spots['group_c']
            checkpoint = torch.load(f'{self.checkpoint_dir}/epoch_{epoch:03d}.pt')
            
            if 'dsp' in optimal_model.analyzers:
                optimal_model.analyzers['dsp'].load_state_dict(
                    checkpoint['model_state']['group_c']['dsp']
                )
            if 'kalman' in optimal_model.analyzers:
                optimal_model.analyzers['kalman'].load_state_dict(
                    checkpoint['model_state']['group_c']['kalman']
                )
                
        # 4. ë…ë¦½ ëª¨ë“ˆë“¤ ê°œë³„ ë¡œë“œ
        for module_name in ['advanced_emotion', 'advanced_regret', 'advanced_bentham']:
            if module_name in self.sweet_spots and module_name in optimal_model.analyzers:
                epoch = self.sweet_spots[module_name]
                checkpoint = torch.load(f'{self.checkpoint_dir}/epoch_{epoch:03d}.pt')
                
                if module_name in checkpoint['model_state']['independent']:
                    optimal_model.analyzers[module_name].load_state_dict(
                        checkpoint['model_state']['independent'][module_name]
                    )
                    
        return optimal_model
        
    def create_ensemble_variants(self, delta_epochs=4):
        """Sweet spot Â±2 ì—í­ ë²”ìœ„ì—ì„œ ì•™ìƒë¸”ìš© ë³€í˜• ìƒì„±"""
        variants = []
        
        for delta in [-4, -2, 0, 2, 4]:
            adjusted_spots = {}
            for key, epoch in self.sweet_spots.items():
                if epoch:
                    # ì—í­ ë²”ìœ„ ì œí•œ (2-60)
                    adjusted_epoch = max(2, min(60, epoch + delta))
                    adjusted_spots[key] = adjusted_epoch
                    
            # ì¡°ì •ëœ sweet spotìœ¼ë¡œ ëª¨ë¸ ìƒì„±
            self.sweet_spots = adjusted_spots
            variant_model = self.create_optimal_model()
            variants.append({
                'delta': delta,
                'model': variant_model,
                'sweet_spots': adjusted_spots.copy()
            })
            
        return variants
        
    def evaluate_crossover_quality(self, model, test_loader):
        """í¬ë¡œìŠ¤ì˜¤ë²„ ëª¨ë¸ í’ˆì§ˆ í‰ê°€"""
        model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in test_loader:
                outputs = model(batch)
                loss = model.compute_loss(outputs, batch)
                total_loss += loss.item()
                
                # ì •í™•ë„ ê³„ì‚° (ê°ì • ë¶„ë¥˜ ê¸°ì¤€)
                if 'emotion' in outputs:
                    predictions = outputs['emotion'].argmax(dim=1)
                    targets = batch['emotion_target']
                    correct += (predictions == targets).sum().item()
                    total += targets.size(0)
                    
        metrics = {
            'avg_loss': total_loss / len(test_loader),
            'accuracy': correct / total if total > 0 else 0
        }
        
        return metrics
```

---

## 6. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

### 6.1 í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# run_advanced_training.sh

# í™˜ê²½ ì„¤ì •
export PYTHONPATH=/mnt/c/large_project/linux_red_heart
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
LOG_DIR="logs/training_$(date +%Y%m%d_%H%M%S)"
mkdir -p $LOG_DIR

echo "ğŸš€ Red Heart AI ê³ ê¸‰ í•™ìŠµ ì‹œì‘"
echo "ë¡œê·¸ ë””ë ‰í† ë¦¬: $LOG_DIR"

# Phase 1: LR Sweep
echo "Phase 1: Learning Rate Sweep ì‹œì‘..."
for lr in 1e-5 5e-5 1e-4 2e-4 5e-4; do
    echo "  LR $lr í…ŒìŠ¤íŠ¸ ì¤‘..."
    python unified_training_v2.py \
        --mode lr-sweep \
        --lr $lr \
        --epochs 5 \
        --batch-size 4 \
        --gradient-accumulation 16 \
        --mixed-precision \
        --save-metrics \
        > $LOG_DIR/lr_sweep_${lr}.log 2>&1
done

# ìµœì  LR ì„ íƒ
echo "ìµœì  LR ë¶„ì„ ì¤‘..."
python analyze_lr_sweep.py --sweep-dir $LOG_DIR > $LOG_DIR/best_lr.txt
BEST_LR=$(cat $LOG_DIR/best_lr.txt | grep "Best LR:" | cut -d: -f2)
echo "ì„ íƒëœ ìµœì  LR: $BEST_LR"

# Phase 2: ë³¸ í•™ìŠµ
echo "Phase 2: ë³¸ í•™ìŠµ ì‹œì‘ (60 epochs)..."
nohup python unified_training_v2.py \
    --mode full-train \
    --lr $BEST_LR \
    --epochs 60 \
    --batch-size 4 \
    --gradient-accumulation 16 \
    --mixed-precision \
    --use-dsm \
    --save-every 2 \
    --checkpoint-dir checkpoints \
    > $LOG_DIR/main_training.log 2>&1 &

TRAIN_PID=$!
echo "í•™ìŠµ í”„ë¡œì„¸ìŠ¤ PID: $TRAIN_PID"

# í•™ìŠµ ëª¨ë‹ˆí„°ë§
while kill -0 $TRAIN_PID 2>/dev/null; do
    echo "í•™ìŠµ ì§„í–‰ ì¤‘... ($(date))"
    
    # GPU ë©”ëª¨ë¦¬ ì²´í¬
    nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits \
        >> $LOG_DIR/gpu_memory.log
    
    # ìµœê·¼ ë¡œê·¸ í™•ì¸
    tail -n 5 $LOG_DIR/main_training.log
    
    sleep 300  # 5ë¶„ë§ˆë‹¤ ì²´í¬
done

echo "í•™ìŠµ ì™„ë£Œ!"

# Phase 3: Sweet Spot ë¶„ì„
echo "Phase 3: Sweet Spot ë¶„ì„ ì‹œì‘..."
python analyze_sweet_spots.py \
    --checkpoint-dir checkpoints \
    --output-dir analysis \
    > $LOG_DIR/sweet_spot_analysis.log 2>&1

# Phase 4: í¬ë¡œìŠ¤ì˜¤ë²„
echo "Phase 4: íŒŒë¼ë¯¸í„° í¬ë¡œìŠ¤ì˜¤ë²„ ì‹œì‘..."
python parameter_crossover.py \
    --checkpoint-dir checkpoints \
    --sweet-spots analysis/sweet_spots.json \
    --output-model final_model.pt \
    > $LOG_DIR/crossover.log 2>&1

echo "âœ… ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!"
echo "ìµœì¢… ëª¨ë¸: final_model.pt"
echo "ë¶„ì„ ê²°ê³¼: analysis/"
echo "ë°±ì—… ë°ì´í„°: C:/large_project/linux_red_heart/docs/data/"
```

### 6.2 Python ë©”ì¸ ì‹¤í–‰ íŒŒì¼
```python
# main_training_pipeline.py
import argparse
import json
import os
from pathlib import Path
import torch
from datetime import datetime

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data-path', default='claude_api_preprocessing/claude_preprocessed_complete.json')
    parser.add_argument('--max-samples', type=int, default=10000)
    parser.add_argument('--gpu-id', type=int, default=0)
    args = parser.parse_args()
    
    # GPU ì„¤ì •
    torch.cuda.set_device(args.gpu_id)
    
    # ë°ì´í„° ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
    backup_base = Path('C:/large_project/linux_red_heart/docs/data')
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    session_dir = backup_base / f'session_{timestamp}'
    session_dir.mkdir(parents=True, exist_ok=True)
    
    # ì„¸ì…˜ ì •ë³´ ì €ì¥
    session_info = {
        'start_time': datetime.now().isoformat(),
        'data_path': args.data_path,
        'max_samples': args.max_samples,
        'gpu_id': args.gpu_id,
        'cuda_device': torch.cuda.get_device_name(args.gpu_id),
        'total_gpu_memory': torch.cuda.get_device_properties(args.gpu_id).total_memory / 1e9
    }
    
    with open(session_dir / 'session_info.json', 'w') as f:
        json.dump(session_info, f, indent=2)
    
    print(f"ğŸ“ ì„¸ì…˜ ë””ë ‰í† ë¦¬: {session_dir}")
    
    # Phase 1: LR Sweep
    from lr_sweep import run_lr_sweep
    best_lr = run_lr_sweep(args, session_dir)
    
    # Phase 2: Main Training  
    from main_training import run_main_training
    checkpoints = run_main_training(args, best_lr, session_dir)
    
    # Phase 3: Sweet Spot Analysis
    from sweet_spot_analysis import analyze_sweet_spots
    sweet_spots = analyze_sweet_spots(checkpoints, session_dir)
    
    # Phase 4: Crossover
    from parameter_crossover import create_crossover_model
    final_model = create_crossover_model(checkpoints, sweet_spots, session_dir)
    
    # ìµœì¢… ì €ì¥
    torch.save(final_model.state_dict(), session_dir / 'final_model.pt')
    
    # ì„¸ì…˜ ì¢…ë£Œ ì •ë³´
    session_info['end_time'] = datetime.now().isoformat()
    session_info['final_model_path'] = str(session_dir / 'final_model.pt')
    session_info['sweet_spots'] = sweet_spots
    
    with open(session_dir / 'session_info_final.json', 'w') as f:
        json.dump(session_info, f, indent=2)
    
    print(f"âœ… í•™ìŠµ ì™„ë£Œ! ìµœì¢… ëª¨ë¸: {session_dir / 'final_model.pt'}")

if __name__ == '__main__':
    main()
```

---

## 7. ëª¨ë‹ˆí„°ë§ ë° ë¬¸ì œ í•´ê²°

### 7.1 ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```python
# monitor_training.py
class TrainingMonitor:
    def __init__(self, log_file, checkpoint_dir):
        self.log_file = log_file
        self.checkpoint_dir = checkpoint_dir
        
    def monitor(self, interval=60):
        """ì‹¤ì‹œê°„ í•™ìŠµ ëª¨ë‹ˆí„°ë§"""
        while True:
            # GPU ë©”ëª¨ë¦¬ ì²´í¬
            gpu_memory = torch.cuda.memory_allocated() / 1e9
            gpu_util = torch.cuda.utilization()
            
            # ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ í™•ì¸
            latest_checkpoint = self._get_latest_checkpoint()
            
            # í•™ìŠµ ì§„í–‰ë„
            if latest_checkpoint:
                epoch = int(latest_checkpoint.stem.split('_')[1])
                progress = epoch / 60 * 100
                
                print(f"[{datetime.now().strftime('%H:%M:%S')}]")
                print(f"  ì§„í–‰ë„: {progress:.1f}% (Epoch {epoch}/60)")
                print(f"  GPU: {gpu_memory:.1f}GB / {gpu_util}%")
                
            # ì—ëŸ¬ ì²´í¬
            self._check_for_errors()
            
            time.sleep(interval)
            
    def _get_latest_checkpoint(self):
        checkpoints = list(Path(self.checkpoint_dir).glob('epoch_*.pt'))
        return max(checkpoints, key=lambda p: p.stat().st_mtime) if checkpoints else None
        
    def _check_for_errors(self):
        """ë¡œê·¸ì—ì„œ ì—ëŸ¬ í™•ì¸"""
        with open(self.log_file, 'r') as f:
            lines = f.readlines()[-100:]  # ë§ˆì§€ë§‰ 100ì¤„
            
        for line in lines:
            if 'ERROR' in line or 'CUDA out of memory' in line:
                print(f"âš ï¸ ì—ëŸ¬ ê°ì§€: {line.strip()}")
```

### 7.2 ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

#### OOM (Out of Memory) ë°œìƒ ì‹œ
```python
# 1. ë°°ì¹˜ í¬ê¸° ê°ì†Œ
config['batch_size'] = 2  # 4 â†’ 2

# 2. Gradient accumulation ì¦ê°€
config['gradient_accumulation_steps'] = 32  # 16 â†’ 32

# 3. DSM ê³µê²©ì  ìŠ¤ì™‘
dsm_scheduler.target_usage = 0.75  # 0.85 â†’ 0.75
```

#### í•™ìŠµ ì¤‘ë‹¨ ì‹œ ì¬ê°œ
```python
def resume_training(checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state'])
    optimizer.load_state_dict(checkpoint['optimizer_state'])
    start_epoch = checkpoint['epoch'] + 1
    return start_epoch
```

---

## 8. ì˜ˆìƒ ê²°ê³¼ ë° ë…¼ë¬¸ í™œìš©

### 8.1 ì˜ˆìƒ ì„±ëŠ¥
- **ë‹¨ì¼ ëª¨ë¸**: 75-80% accuracy (ê³¼ì í•©)
- **í¬ë¡œìŠ¤ì˜¤ë²„**: 85-88% accuracy  
- **5-ë³€í˜• ì•™ìƒë¸”**: 90%+ accuracy

### 8.2 ë…¼ë¬¸ìš© ë°ì´í„°
1. **LR Sweep ê·¸ë˜í”„**: 5ê°œ í•™ìŠµë¥ ë³„ ìˆ˜ë ´ ê³¡ì„ 
2. **í•™ìŠµ ê³¡ì„ **: 60 ì—í­ train/val loss
3. **Sweet Spot ë¶„ì„**: ëª¨ë“ˆë³„ ìµœì  ì—í­ íˆíŠ¸ë§µ
4. **í¬ë¡œìŠ¤ì˜¤ë²„ íš¨ê³¼**: ë‹¨ì¼ vs í¬ë¡œìŠ¤ì˜¤ë²„ ì„±ëŠ¥ ë¹„êµ
5. **DSM íš¨ìœ¨ì„±**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ìŠ¤ì™‘ íŒ¨í„´

### 8.3 ë°±ì—… ìœ„ì¹˜
```
C:/large_project/linux_red_heart/docs/data/
â”œâ”€â”€ lr_sweep/           # LR ì‹¤í—˜ ë°ì´í„°
â”œâ”€â”€ checkpoints/        # 30ê°œ ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ metrics/           # ì—í­ë³„ ë©”íŠ¸ë¦­
â”œâ”€â”€ analysis/          # Sweet spot ë¶„ì„
â””â”€â”€ final/            # ìµœì¢… ëª¨ë¸ ë° ë³´ê³ ì„œ
```

---

## 9. ê²°ë¡ 

ì´ ì „ëµì€ ì œí•œëœ ë°ì´í„°(10K)ì™€ GPU(8GB)ë¡œ 730M ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ì°½ì˜ì  ì ‘ê·¼ë²•ì…ë‹ˆë‹¤.
ì˜ë„ì  ê³¼ì í•© í›„ í¬ë¡œìŠ¤ì˜¤ë²„ë¥¼ í†µí•´ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©°,
DSMì„ í™œìš©í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ë¡œ ëŒ€ê·œëª¨ ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

ì´ ì†Œìš” ì‹œê°„ì€ 170-200ì‹œê°„(7-8ì¼)ì´ë©°, 
ì² ì €í•œ ë°ì´í„° ë°±ì—…ê³¼ ë©”íŠ¸ë¦­ ê¸°ë¡ìœ¼ë¡œ ë…¼ë¬¸ ì‘ì„±ì— í•„ìš”í•œ ëª¨ë“  ìë£Œë¥¼ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.