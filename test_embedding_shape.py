#!/usr/bin/env python3
"""
SentenceTransformer ì„ë² ë”© ì°¨ì› í…ŒìŠ¤íŠ¸
"""

import sys
import torch
import numpy as np
sys.path.append('/mnt/c/large_project/linux_red_heart')

from sentence_transformer_singleton import get_sentence_transformer

def test_embedding_shape():
    """ì„ë² ë”© í˜•íƒœ í…ŒìŠ¤íŠ¸"""
    print("ğŸ“Š SentenceTransformer ì„ë² ë”© í˜•íƒœ í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    # ëª¨ë¸ ë¡œë“œ
    print("1. ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = get_sentence_transformer(
        model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_text = "ì¹œêµ¬ê°€ ì–´ë ¤ìš´ ìƒí™©ì— ì²˜í–ˆì„ ë•Œ ì–´ë–»ê²Œ ë„ì™€ì•¼ í• ê¹Œ?"
    print(f"2. í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸: '{test_text}'")
    print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(test_text)} ë¬¸ì")
    print(f"   ë‹¨ì–´ ìˆ˜: {len(test_text.split())} ë‹¨ì–´")
    
    # ì„ë² ë”© ìƒì„±
    print("\n3. ì„ë² ë”© ìƒì„±...")
    embeddings = model.encode([test_text])
    
    # í˜•íƒœ í™•ì¸
    print(f"\n4. ì„ë² ë”© ì •ë³´:")
    print(f"   - type(embeddings): {type(embeddings)}")
    
    if isinstance(embeddings, list):
        print(f"   - len(embeddings): {len(embeddings)}")
        print(f"   - type(embeddings[0]): {type(embeddings[0])}")
        
        # ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œ ë¶„ì„
        first_embedding = embeddings[0]
        if hasattr(first_embedding, 'shape'):
            print(f"   - embeddings[0].shape: {first_embedding.shape}")
            print(f"   - embeddings[0].dtype: {first_embedding.dtype}")
        else:
            print(f"   - embeddings[0] ë‚´ìš© ìƒ˜í”Œ: {first_embedding[:10] if len(first_embedding) > 10 else first_embedding}")
            print(f"   - len(embeddings[0]): {len(first_embedding) if hasattr(first_embedding, '__len__') else 'N/A'}")
    elif isinstance(embeddings, np.ndarray):
        print(f"   - embeddings.shape: {embeddings.shape}")
        print(f"   - embeddings.dtype: {embeddings.dtype}")
        print(f"   - embeddings[0][:10]: {embeddings[0][:10]}")
    
    # í…ì„œ ë³€í™˜ í…ŒìŠ¤íŠ¸
    print("\n5. í…ì„œ ë³€í™˜ í…ŒìŠ¤íŠ¸:")
    embedding_tensor = torch.tensor(embeddings[0], dtype=torch.float32)
    print(f"   - embedding_tensor.shape: {embedding_tensor.shape}")
    print(f"   - embedding_tensor.dim(): {embedding_tensor.dim()}")
    
    # ì°¨ì› í™•ì¥ í…ŒìŠ¤íŠ¸
    print("\n6. ì°¨ì› í™•ì¥ í…ŒìŠ¤íŠ¸:")
    if len(embedding_tensor.shape) == 1:
        print(f"   - ì›ë³¸: {embedding_tensor.shape}")
        expanded = embedding_tensor.unsqueeze(0).unsqueeze(0)
        print(f"   - unsqueeze(0).unsqueeze(0): {expanded.shape}")
        
        # íŒ¨ë”© í…ŒìŠ¤íŠ¸
        max_seq_length = 128
        padded_tensor = torch.zeros(1, max_seq_length, embedding_tensor.shape[-1])
        padded_tensor[:, 0, :] = expanded[0, 0, :]
        print(f"   - íŒ¨ë”© í›„: {padded_tensor.shape}")
    
    print("\n7. ì˜ˆìƒ ë°±ë³¸ ì…ë ¥:")
    print(f"   - ê¸°ëŒ€: (batch_size=1, seq_len={max_seq_length}, input_dim=768)")
    print(f"   - ì‹¤ì œ: {padded_tensor.shape}")
    
    # ë°±ë³¸ í”„ë¡œì ì…˜ í…ŒìŠ¤íŠ¸
    print("\n8. ë°±ë³¸ í”„ë¡œì ì…˜ ì‹œë®¬ë ˆì´ì…˜:")
    input_projection = torch.nn.Linear(768, 896)
    try:
        # ì •ìƒ ì¼€ì´ìŠ¤
        output = input_projection(padded_tensor)
        print(f"   âœ… ì •ìƒ ì²˜ë¦¬: ì…ë ¥ {padded_tensor.shape} -> ì¶œë ¥ {output.shape}")
    except Exception as e:
        print(f"   âŒ ì—ëŸ¬: {e}")
    
    # ë¬¸ì œ ì¬í˜„ í…ŒìŠ¤íŠ¸ 
    print("\n9. ë¬¸ì œ ì¬í˜„ í…ŒìŠ¤íŠ¸ (1x7 í˜•íƒœ):")
    wrong_tensor = torch.randn(1, 7)  # ë¬¸ì œê°€ ëœ í˜•íƒœ
    print(f"   - ì˜ëª»ëœ ì…ë ¥: {wrong_tensor.shape}")
    try:
        output = input_projection(wrong_tensor)
        print(f"   ì¶œë ¥: {output.shape}")
    except Exception as e:
        print(f"   âŒ ì˜ˆìƒëœ ì—ëŸ¬: {e}")
    
    print("\n" + "="*50)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    test_embedding_shape()