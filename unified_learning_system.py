"""
í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ - Week 4 í•µì‹¬ êµ¬í˜„
Unified Learning System - Week 4 Core Implementation

ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ 800M íŒŒë¼ë¯¸í„° í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ:
- ê³µìœ  ë°±ë³¸ì„ í†µí•œ ë‹¤ì¤‘ í—¤ë“œ ë™ì‹œ í•™ìŠµ
- ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ë° í˜¼í•© ì •ë°€ë„ (FP16) í›ˆë ¨
- ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ ë° ë©”ëª¨ë¦¬ ìµœì í™”
- í—¤ë“œë³„ í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ ë° ì†ì‹¤ í•¨ìˆ˜ ê· í˜•
- íš¨ìœ¨ì  ë°±í”„ë¡œíŒŒê²Œì´ì…˜ ë° ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
"""

import asyncio
import logging
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.amp import GradScaler, autocast
from torch.utils.checkpoint import checkpoint
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque, defaultdict, OrderedDict
from enum import Enum
import numpy as np
# pathlib ì œê±° - WSL í˜¸í™˜ì„±ì„ ìœ„í•´ os.path ì‚¬ìš©
import json
import threading
from abc import ABC, abstractmethod
import math
import gc
import psutil
import os

# í•µì‹¬ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from config import ADVANCED_CONFIG, get_smart_device, get_gpu_memory_info, ModelPriority, get_priority_based_device, _force_swap_low_priority_models, _emergency_gpu_cleanup
from head_compatibility_interface import HeadType, HeadProcessingResult
from unified_red_heart_core import RedHeartUnifiedBackbone, UnifiedRepresentation
from dynamic_swap_manager import RedHeartDynamicSwapManager
from intelligent_synergy_system import IntelligentSynergySystem

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

class LearningPhase(Enum):
    """í•™ìŠµ ë‹¨ê³„"""
    WARM_UP = "warm_up"                    # ì›Œë°ì—… ë‹¨ê³„
    COLLABORATIVE = "collaborative"        # í˜‘ë ¥ í•™ìŠµ ë‹¨ê³„
    SPECIALIZED = "specialized"            # ì „ë¬¸í™” í•™ìŠµ ë‹¨ê³„
    INTEGRATION = "integration"            # í†µí•© í•™ìŠµ ë‹¨ê³„
    FINE_TUNING = "fine_tuning"           # íŒŒì¸íŠœë‹ ë‹¨ê³„

class TrainingStrategy(Enum):
    """í›ˆë ¨ ì „ëµ"""
    ROUND_ROBIN = "round_robin"           # ìˆœì°¨ì  í—¤ë“œ í›ˆë ¨
    PARALLEL = "parallel"                 # ë³‘ë ¬ í—¤ë“œ í›ˆë ¨
    PRIORITY_BASED = "priority_based"     # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ í›ˆë ¨
    ADAPTIVE = "adaptive"                 # ì ì‘ì  í›ˆë ¨

@dataclass
class TrainingMetrics:
    """í›ˆë ¨ ë©”íŠ¸ë¦­"""
    epoch: int = 0
    step: int = 0
    total_loss: float = 0.0
    head_losses: Dict[HeadType, float] = field(default_factory=dict)
    learning_rates: Dict[str, float] = field(default_factory=dict)
    memory_usage: float = 0.0
    gpu_utilization: float = 0.0
    training_time: float = 0.0
    gradient_norm: float = 0.0
    synergy_gain: float = 0.0
    
    # íš¨ìœ¨ì„± ë©”íŠ¸ë¦­
    samples_per_second: float = 0.0
    memory_efficiency: float = 0.0  # ë©”ëª¨ë¦¬ ì‚¬ìš© íš¨ìœ¨ì„±
    convergence_rate: float = 0.0   # ìˆ˜ë ´ ì†ë„

@dataclass
class HeadTrainingConfig:
    """í—¤ë“œë³„ í›ˆë ¨ ì„¤ì •"""
    head_type: HeadType
    learning_rate: float = 1e-4
    weight_decay: float = 1e-5
    gradient_clip_value: float = 1.0
    loss_weight: float = 1.0
    update_frequency: int = 1  # Në²ˆì˜ ìŠ¤í…ë§ˆë‹¤ ì—…ë°ì´íŠ¸
    freeze_until_epoch: int = 0  # ì´ ì—í¬í¬ê¹Œì§€ ë™ê²°
    
    # ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
    warmup_steps: int = 1000
    scheduler_type: str = "cosine"  # cosine, linear, exponential
    
    # ì •ê·œí™” ì„¤ì •
    dropout_rate: float = 0.1
    label_smoothing: float = 0.0

class MemoryEfficientTrainer(nn.Module):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í›ˆë ¨ê¸°"""
    
    def __init__(self, unified_backbone: RedHeartUnifiedBackbone,
                 head_configs: Dict[HeadType, HeadTrainingConfig],
                 unified_system=None):  # UnifiedLearningSystem ì°¸ì¡° ì¶”ê°€
        super().__init__()
        
        self.unified_backbone = unified_backbone
        self.head_configs = head_configs
        self.unified_system = unified_system  # cached_head_modules ì ‘ê·¼ì„ ìœ„í•´
        
        # í›ˆë ¨ ìƒíƒœ - ë©”ëª¨ë¦¬ ì•ˆì „ì„±ì„ ìœ„í•´ ROUND_ROBIN ê³ ì •
        self.current_phase = LearningPhase.WARM_UP
        self.training_strategy = TrainingStrategy.ROUND_ROBIN
        logger.info(f"í›ˆë ¨ ì „ëµ ì„¤ì •: {self.training_strategy.value} (ë©”ëª¨ë¦¬ ì•ˆì „ì„± ìš°ì„ )")
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        self.use_gradient_checkpointing = True
        self.use_mixed_precision = True
        self.gradient_accumulation_steps = 4
        self.max_batch_size = 8
        self.adaptive_batch_sizing = True
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ëŸ¬ (í˜¼í•© ì •ë°€ë„ìš©)
        self.scaler = GradScaler('cuda')
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ (ì§€ì—° ì´ˆê¸°í™”)
        self.optimizers = {}
        self.schedulers = {}
        self._optimizers_initialized = False
        # self._initialize_optimizers()  # ì‹¤ì œ í•™ìŠµ ì‹œì‘ ì‹œ ì´ˆê¸°í™”
        
        # í›ˆë ¨ ë©”íŠ¸ë¦­
        self.metrics_history = deque(maxlen=1000)
        self.current_metrics = TrainingMetrics()
        
        # ë™ì  ë°°ì¹˜ í¬ê¸° ê´€ë¦¬
        self.adaptive_batch_manager = AdaptiveBatchSizeManager()
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
        self.memory_monitor = MemoryMonitor()
        
        # ë°±ë³¸ì„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ì— ë“±ë¡
        self.memory_monitor.register_gpu_module("unified_backbone", self.unified_backbone)
        
        logger.info("MemoryEfficientTrainer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def forward_with_checkpointing(self, batch_data: Dict[str, Any], 
                                  active_heads: List[HeadType]) -> Dict[str, Any]:
        """ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ì„ ì‚¬ìš©í•œ ìˆœì „íŒŒ"""
        
        # 1. ë°±ë³¸ ìˆœì „íŒŒ
        backbone_output = self.unified_backbone(batch_data)
        
        # 2. í—¤ë“œë³„ ì²˜ë¦¬
        head_outputs = {}
        for head_type in active_heads:
            if self.unified_system and hasattr(self.unified_system, 'cached_head_modules'):
                head_module = self.unified_system.cached_head_modules.get(head_type)
                if head_module is not None:
                    try:
                        # ë°±ë³¸ ì¶œë ¥ì„ í—¤ë“œì— ì „ë‹¬
                        head_input = backbone_output.shared_embedding
                        logger.debug(f"í—¤ë“œ {head_type.value} - ë°±ë³¸ ì¶œë ¥ shape: {head_input.shape}")
                        
                        # í—¤ë“œ forward ì‹¤í–‰ (forward ë©”ì„œë“œê°€ ë‚´ë¶€ì ìœ¼ë¡œ input_adapter ì²˜ë¦¬)
                        if hasattr(head_module, 'forward'):
                            logger.debug(f"í—¤ë“œ {head_type.value} - forward ì‹¤í–‰")
                            try:
                                raw_output = head_module.forward(head_input)
                                head_output = self._normalize_head_output(head_module, head_input, raw_output, head_type)
                                head_outputs[head_type] = head_output
                                logger.debug(f"í—¤ë“œ {head_type.value} - forward ì„±ê³µ, ì¶œë ¥ shape: {tuple(head_output.shape)}")
                            except RuntimeError as re:
                                if "mat1 and mat2" in str(re):
                                    logger.error(f"Shape mismatch ìƒì„¸ ì •ë³´:")
                                    logger.error(f"  - í—¤ë“œ íƒ€ì…: {head_type.value}")
                                    logger.error(f"  - í—¤ë“œ í´ë˜ìŠ¤: {head_module.__class__.__name__}")
                                    logger.error(f"  - ì…ë ¥ shape: {head_input.shape}")
                                    
                                    # PyTorch ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„¸ ë¶„ì„
                                    if hasattr(head_module, 'get_pytorch_network'):
                                        pytorch_net = head_module.get_pytorch_network()
                                        if pytorch_net:
                                            logger.error(f"  - PyTorch ë„¤íŠ¸ì›Œí¬ íƒ€ì…: {type(pytorch_net).__name__}")
                                            
                                            # Sequentialì´ë‚˜ ë‹¤ë¥¸ ì»¨í…Œì´ë„ˆì¸ ê²½ìš°
                                            if hasattr(pytorch_net, 'modules'):
                                                for idx, module in enumerate(pytorch_net.modules()):
                                                    if isinstance(module, torch.nn.Linear):
                                                        logger.error(f"    - Linear ë ˆì´ì–´ {idx}: in_features={module.in_features}, out_features={module.out_features}")
                                            
                                            # ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ shape ì¶œë ¥
                                            for name, param in pytorch_net.named_parameters():
                                                if 'weight' in name:
                                                    logger.error(f"    - {name}: shape={param.shape}")
                                raise
                        else:
                            logger.warning(f"í—¤ë“œ {head_type.value}ì— forward ë©”ì„œë“œê°€ ì—†ìŒ")
                    except Exception as e:
                        logger.error(f"í—¤ë“œ {head_type.value} ìˆœì „íŒŒ ì˜¤ë¥˜: {str(e)}")
                        logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                        import traceback
                        logger.error(f"íŠ¸ë ˆì´ìŠ¤ë°±:\n{traceback.format_exc()}")
                        logger.error(f"ì…ë ¥ shape: {head_input.shape if hasattr(head_input, 'shape') else 'N/A'}")
                        
                        # ë” ìì„¸í•œ ë””ë²„ê¹… ì •ë³´
                        if hasattr(head_module, '__class__'):
                            logger.error(f"í—¤ë“œ í´ë˜ìŠ¤: {head_module.__class__.__name__}")
                        if hasattr(head_module, 'get_pytorch_network'):
                            pytorch_net = head_module.get_pytorch_network()
                            if pytorch_net:
                                logger.error(f"PyTorch ë„¤íŠ¸ì›Œí¬ íƒ€ì…: {type(pytorch_net)}")
        
        return {
            'backbone_output': backbone_output,
            'head_outputs': head_outputs
        }
    
    def _normalize_head_output(self, head_module, head_input, raw_output, head_type):
        """
        ë‹¤ì–‘í•œ í—¤ë“œ ì¶œë ¥(raw_output)ì„ í•™ìŠµìš© í…ì„œë¡œ í‘œì¤€í™”:
        - dict: logits/probabilities/last_hidden_state ìš°ì„ 
        - tuple/list: ì²« ë²ˆì§¸ í…ì„œ
        - numpyâ†’tensor ìë™ ë³€í™˜
        - device/float dtype ì •ë ¬
        - requires_grad ë³´ì¥(í•„ìš” ì‹œ ì¬ê³„ì‚°)
        """
        import torch
        import numpy as np
        
        def _pick_tensor(x):
            if isinstance(x, torch.Tensor):
                return x
            if isinstance(x, (list, tuple)):
                for v in x:
                    if isinstance(v, torch.Tensor):
                        return v
            if isinstance(x, dict):
                for k in ("logits", "logit", "scores", "last_hidden_state", "hidden_states", "embeddings"):
                    v = x.get(k)
                    if isinstance(v, torch.Tensor):
                        return v
                # dict ì•ˆì˜ ì²« í…ì„œ íƒìƒ‰
                for v in x.values():
                    if isinstance(v, torch.Tensor):
                        return v
            # numpy -> tensor
            try:
                if isinstance(x, np.ndarray):
                    return torch.from_numpy(x)
            except Exception:
                pass
            raise RuntimeError(f"{head_type.value} í—¤ë“œ ì¶œë ¥ì—ì„œ í•™ìŠµìš© í…ì„œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (type={type(x)}).")

        # ë¨¼ì € ì¶œë ¥ íƒ€ì… ë¡œê¹…
        logger.debug(f"í—¤ë“œ {head_type.value} - ì¶œë ¥ íƒ€ì…: {type(raw_output)}")
        
        y = _pick_tensor(raw_output)
        
        # device ì •ë ¬
        if y.device != head_input.device:
            y = y.to(head_input.device)
        
        # dtype ì •ë ¬(AMP ì‚¬ìš© ì‹œ fp16/torch.float32 ë“± í”„ë¡œì íŠ¸ ì •ì±…ì— ë§ì¶° ì •ë¦¬)
        if y.dtype not in (torch.float16, torch.float32, torch.bfloat16):
            y = y.float()
        
        # gradient ì—°ê²° ë³´ì¥: ë§Œì•½ leaf í…ì„œë¡œ gradê°€ ëŠê²¼ë‹¤ë©´ ì•ˆì „í•˜ê²Œ ì¬ê³„ì‚°
        if (not y.requires_grad) or (y.grad_fn is None):
            logger.warning(f"í—¤ë“œ {head_type.value} - gradient ëŠê¹€ ê°ì§€, ì¬ê³„ì‚° ì‹œë„...")
            try:
                # ì¬ê³„ì‚°ìœ¼ë¡œ ê·¸ë˜í”„ ì—°ê²°(í—¤ë“œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê°€ëŠ¥ ìƒíƒœ ê°€ì •)
                if hasattr(head_module, "train"):
                    head_module.train()
                y = head_module(head_input)  # í•œ ë²ˆ ë” ê³„ì‚° (í…ì„œ ê²½ë¡œ ë³´ì¥)
                # ë‹¤ì‹œ dict/tupleì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¬ì¶”ì¶œ
                y = _pick_tensor(y)
                if y.device != head_input.device:
                    y = y.to(head_input.device)
                if (not y.requires_grad) or (y.grad_fn is None):
                    # ë§ˆì§€ë§‰ ì‹œë„: requires_grad ê°•ì œ í™œì„±í™”
                    y = y.requires_grad_(True)
                    if not y.requires_grad:
                        raise RuntimeError("ì¬ê³„ì‚° í›„ì—ë„ gradientê°€ ì—°ê²°ë˜ì§€ ì•ŠìŒ")
                logger.info(f"í—¤ë“œ {head_type.value} - gradient ì—°ê²° ë³µêµ¬ ì„±ê³µ")
            except Exception as e:
                logger.error(f"í—¤ë“œ {head_type.value} - gradient ì—°ê²° ì‹¤íŒ¨: {e}")
                # gradientê°€ ì—†ì–´ë„ í•™ìŠµ ì§„í–‰ì„ ìœ„í•´ requires_grad ê°•ì œ ì„¤ì •
                y = y.detach().requires_grad_(True)
                logger.warning(f"í—¤ë“œ {head_type.value} - detach í›„ requires_grad ê°•ì œ ì„¤ì •")
        
        logger.debug(f"í—¤ë“œ {head_type.value} - ì •ê·œí™” ì™„ë£Œ: device={y.device}, requires_grad={y.requires_grad}")
        return y
    
    def set_optimizer(self, optimizer, scheduler=None):
        """ì™¸ë¶€ì—ì„œ ìƒì„±ëœ ì˜µí‹°ë§ˆì´ì € ì£¼ì…"""
        self._optimizers = optimizer
        self._schedulers = scheduler
        self._optimizers_initialized = True
        logger.info("âœ… ì˜µí‹°ë§ˆì´ì € ì£¼ì… ì™„ë£Œ")
    
    def _initialize_optimizers(self):
        """ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” (deprecated - set_optimizer ì‚¬ìš©)"""
        logger.warning("âš ï¸ _initialize_optimizersëŠ” deprecatedë¨. set_optimizerë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        return  # ì´ì¤‘ ìƒì„± ë°©ì§€ë¥¼ ìœ„í•´ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ
        
        # ë°±ë³¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •í•˜ê³  gradient ì—°ê²° ê°•í™”
        self.unified_backbone.train()
        
        # ë°±ë³¸ íŒŒë¼ë¯¸í„°ë“¤ì˜ requires_grad í™•ì¸ ë° ê°•í™”
        backbone_grad_params = sum(1 for p in self.unified_backbone.parameters() if p.requires_grad)
        backbone_total_params = sum(1 for p in self.unified_backbone.parameters())
        
        if backbone_grad_params != backbone_total_params:
            logger.warning(f"ë°±ë³¸ íŒŒë¼ë¯¸í„° gradient ì„¤ì • ë¶ˆì¼ì¹˜: {backbone_grad_params}/{backbone_total_params}")
            for param in self.unified_backbone.parameters():
                param.requires_grad_(True)
        
        logger.info(f"ë°±ë³¸ ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”: {backbone_total_params}ê°œ íŒŒë¼ë¯¸í„° ì¤‘ {backbone_grad_params}ê°œ gradient í™œì„±í™”")
        
        # í—¤ë“œë³„ ì˜µí‹°ë§ˆì´ì € (ë‚˜ì¤‘ì— í—¤ë“œê°€ ë¡œë“œë  ë•Œ ì¶”ê°€)
        for head_type, config in self.head_configs.items():
            # í—¤ë“œë³„ íŒŒë¼ë¯¸í„°ëŠ” ì‹¤ì œ í—¤ë“œê°€ ë¡œë“œë  ë•Œ ì„¤ì •
            pass
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        self.schedulers['backbone'] = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizers['backbone'],
            T_0=1000,  # ì²« ë²ˆì§¸ ì¬ì‹œì‘ê¹Œì§€ì˜ ìŠ¤í… ìˆ˜
            T_mult=2,  # ì¬ì‹œì‘ ì£¼ê¸° ë°°ìˆ˜
            eta_min=1e-6
        )
    
    def add_head_optimizer(self, head_type: HeadType, head_module: nn.Module):
        """í—¤ë“œë³„ ì˜µí‹°ë§ˆì´ì € ì¶”ê°€"""
        config = self.head_configs[head_type]
        
        # í—¤ë“œ ì˜µí‹°ë§ˆì´ì € ë©”ëª¨ë¦¬ ì˜ˆì¸¡
        head_params = list(head_module.parameters())
        param_count = sum(p.numel() for p in head_params)
        dtype_size = 4  # float32
        optimizer_mb = (param_count * 2 * dtype_size) / (1024 * 1024)
        logger.info(f"ğŸ“Š {head_type.value} í—¤ë“œ ì˜µí‹°ë§ˆì´ì € ì˜ˆìƒ ë©”ëª¨ë¦¬: {optimizer_mb:.1f}MB")
        
        # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ë¥¼ í†µí•œ GPU ë©”ëª¨ë¦¬ ìš”ì²­
        if hasattr(self, 'workflow_memory_manager') and self.workflow_memory_manager:
            logger.info(f"ğŸ“¦ {head_type.value} í—¤ë“œ ì˜µí‹°ë§ˆì´ì €ë¥¼ ìœ„í•œ GPU ë©”ëª¨ë¦¬ ìš”ì²­ ì¤‘...")
            try:
                import asyncio
                loop = asyncio.get_event_loop()
                success = loop.run_until_complete(
                    self.workflow_memory_manager.request_gpu(
                        module_name=f"{head_type.value}_optimizer",
                        required_mb=optimizer_mb,
                        deps=[head_type.value],
                        target_util=0.85
                    )
                )
                if not success:
                    logger.warning(f"âš ï¸ {head_type.value} í—¤ë“œì— ëŒ€í•œ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±")
                    # í•™ìŠµë¥  ê°ì†Œë¡œ ëŒ€ì‘
                    config.learning_rate *= 0.5
                    logger.info(f"í•™ìŠµë¥ ì„ {config.learning_rate}ë¡œ ê°ì†Œ")
            except Exception as e:
                logger.error(f"í—¤ë“œ ì˜µí‹°ë§ˆì´ì € ë©”ëª¨ë¦¬ ìš”ì²­ ì˜¤ë¥˜: {e}")
        
        self.optimizers[head_type.value] = torch.optim.AdamW(
            head_module.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            betas=(0.9, 0.999)
        )
        
        # í—¤ë“œë¥¼ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ì— ë“±ë¡
        self.memory_monitor.register_gpu_module(f"head_{head_type.value}", head_module)
    
    def reduce_batch_size(self):
        """ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ - ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ í˜¸ì¶œ"""
        if hasattr(self, 'adaptive_batch_manager'):
            old_size = self.adaptive_batch_manager.current_batch_size
            self.adaptive_batch_manager.current_batch_size = max(
                self.adaptive_batch_manager.min_batch_size,
                old_size // 2
            )
            logger.info(f"ğŸ“‰ ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ: {old_size} -> {self.adaptive_batch_manager.current_batch_size}")
        
        # ì „ì—­ ì„¤ì •ë„ ì—…ë°ì´íŠ¸
        if hasattr(self, 'config') and 'batch_size' in self.config:
            old_size = self.config['batch_size']
            self.config['batch_size'] = max(1, old_size // 2)
            logger.info(f"ğŸ“‰ ì „ì—­ ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ: {old_size} -> {self.config['batch_size']}")
    
    def forward_with_checkpointing(self, input_data: Dict[str, Any],
                                 active_heads: List[HeadType]) -> Dict[str, torch.Tensor]:
        """ì²´í¬í¬ì¸íŒ…ì„ ì‚¬ìš©í•œ ìˆœì „íŒŒ"""
        
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        input_text = input_data.get('text', '')
        batch_size = input_data.get('batch_size', 1)
        
        # í† í¬ë‚˜ì´ì§• (ê°€ìƒì˜ í† í¬ë‚˜ì´ì§•)
        input_ids = torch.randint(0, 30000, (batch_size, 128), device=self.unified_backbone.device)
        attention_mask = torch.ones_like(input_ids, dtype=torch.bool)
        
        # ë°±ë³¸ì´ í•™ìŠµ ëª¨ë“œì¸ì§€ í™•ì¸í•˜ê³  ì„¤ì •
        if not self.unified_backbone.training:
            logger.warning("ë°±ë³¸ì´ eval ëª¨ë“œì˜€ìŠµë‹ˆë‹¤. í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
            self.unified_backbone.train()
        
        # input_idsëŠ” ì •ìˆ˜í˜•ì´ë¯€ë¡œ requires_grad ì„¤ì •í•˜ì§€ ì•ŠìŒ (ì˜¤ë¥˜ ë°©ì§€)
        logger.debug(f"input_ids dtype: {input_ids.dtype}, shape: {input_ids.shape}")
        logger.debug(f"ë°±ë³¸ training ëª¨ë“œ: {self.unified_backbone.training}")
        
        # ë°±ë³¸ íŒŒë¼ë¯¸í„°ë“¤ì˜ gradient ìƒíƒœ í™•ì¸
        backbone_grad_params = sum(1 for p in self.unified_backbone.parameters() if p.requires_grad)
        backbone_total_params = sum(1 for p in self.unified_backbone.parameters())
        logger.debug(f"ë°±ë³¸ gradient íŒŒë¼ë¯¸í„°: {backbone_grad_params}/{backbone_total_params}")
        
        if self.use_gradient_checkpointing:
            # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš©
            def create_custom_forward(module):
                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            
            # ë°±ë³¸ ì²˜ë¦¬ (ì²´í¬í¬ì¸íŒ… ì ìš©)
            unified_repr = checkpoint(
                create_custom_forward(self.unified_backbone),
                input_ids,
                attention_mask
            )
        else:
            # ì¼ë°˜ ìˆœì „íŒŒ
            unified_repr = self.unified_backbone(input_ids, attention_mask)
        
        # ê° í—¤ë“œë³„ ì¶œë ¥ ê³„ì‚° - ë°±ë³¸ë§Œ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        head_outputs = {}
        for head_type in active_heads:
            # ìºì‹œëœ ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆ ì‚¬ìš©
            if self.unified_system is not None:
                real_head = self.unified_system.cached_head_modules.get(head_type)
            else:
                real_head = None
            
            if real_head is None:
                raise RuntimeError(f"ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆ {head_type.value}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ì´ˆê¸°í™”ê°€ ì œëŒ€ë¡œ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # ì‹¤ì œ í—¤ë“œë¥¼ í†µí•œ ìˆœì „íŒŒ - DSMì„ í†µí•´ GPUë¡œ ë¡œë“œ
            head_input = unified_repr.shared_embedding  # ë°±ë³¸ ì¶œë ¥ì„ í—¤ë“œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
            
            # í—¤ë“œëŠ” ì´ë¯¸ _load_active_headsì—ì„œ GPUë¡œ ë¡œë“œë¨
            # ì—¬ê¸°ì„œëŠ” ì´ë¯¸ ë¡œë“œëœ í—¤ë“œë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
            if real_head is None:
                # ìºì‹œì— ì—†ìœ¼ë©´ DSMì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                if self.swap_manager and head_type.value in self.swap_manager.models:
                    real_head = self.swap_manager.models[head_type.value].model
                    self.cached_head_modules[head_type] = real_head  # ìºì‹œ ê°±ì‹ 
                    logger.debug(f"DSMì—ì„œ {head_type.value} í—¤ë“œ íšë“")
                else:
                    raise RuntimeError(f"í—¤ë“œ {head_type.value}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (NO FALLBACK)")
            
            # GPUì—ì„œ ì‹¤í–‰ (CPU ìš°íšŒ ì œê±°)
            head_input_gpu = head_input  # GPUì—ì„œ ê³„ì‚°
            logger.debug(f"í—¤ë“œ {head_type.value}: GPUì—ì„œ ì‹¤í–‰")
            
            # gradient ì—°ê²° ìƒíƒœ í™•ì¸ ë° ë¶„ì„
            logger.debug(f"ë°±ë³¸ ì¶œë ¥ ë¶„ì„ - requires_grad: {head_input_gpu.requires_grad}, dtype: {head_input_gpu.dtype}, shape: {head_input_gpu.shape}")
            logger.debug(f"ë°±ë³¸ ì¶œë ¥ grad_fn: {head_input_gpu.grad_fn}")
            
            if not head_input_gpu.requires_grad:
                # ë°±ë³¸ íŒŒë¼ë¯¸í„°ë“¤ì˜ requires_grad ìƒíƒœ ì¬í™•ì¸
                backbone_params_requiring_grad = [p for p in self.unified_backbone.parameters() if p.requires_grad]
                logger.warning(f"ë°±ë³¸ ì¶œë ¥ì´ requires_grad=Falseì…ë‹ˆë‹¤!")
                logger.warning(f"ë°±ë³¸ requires_grad íŒŒë¼ë¯¸í„°: {len(backbone_params_requiring_grad)}ê°œ")
                
                # ì‹¤ì œ gradient ì—°ê²° ë¬¸ì œ í•´ê²°
                if len(backbone_params_requiring_grad) > 0:
                    logger.warning("ë°±ë³¸ training ëª¨ë“œ ë° gradient ì„¤ì •ì„ ê°•ì œë¡œ ë³µêµ¬í•©ë‹ˆë‹¤.")
                    
                    # ë°±ë³¸ì„ ëª…ì‹œì ìœ¼ë¡œ training ëª¨ë“œë¡œ ì„¤ì •
                    self.unified_backbone.train()
                    
                    # ëª¨ë“  ë°±ë³¸ íŒŒë¼ë¯¸í„°ì˜ requires_grad ì¬ì„¤ì •
                    for param in self.unified_backbone.parameters():
                        param.requires_grad_(True)
                    
                    # ë°±ë³¸ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ gradientê°€ ì—°ê²°ëœ ì¶œë ¥ ìƒì„±
                    logger.warning("ë°±ë³¸ ìˆœì „íŒŒë¥¼ ì¬ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    if self.use_gradient_checkpointing:
                        unified_repr = checkpoint(
                            create_custom_forward(self.unified_backbone),
                            input_ids,
                            attention_mask
                        )
                    else:
                        unified_repr = self.unified_backbone(input_ids, attention_mask)
                    
                    head_input = unified_repr.shared_embedding
                    
                    # ì¬ì‹¤í–‰ í›„ì—ë„ gradientê°€ ì—†ìœ¼ë©´ ì¹˜ëª…ì  ì˜¤ë¥˜
                    if not head_input.requires_grad:
                        logger.error("ë°±ë³¸ ìˆœì „íŒŒ ì¬ì‹¤í–‰ í›„ì—ë„ gradientê°€ ì—°ê²°ë˜ì§€ ì•ŠìŒ")
                        raise RuntimeError("ë°±ë³¸ gradient ì—°ê²° ì‹¤íŒ¨ - ë°±ë³¸ êµ¬í˜„ ê²°í•¨")
                    else:
                        logger.info("ë°±ë³¸ gradient ì—°ê²° ë³µêµ¬ ì„±ê³µ")
                else:
                    logger.error("ë°±ë³¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ requires_grad=Falseì…ë‹ˆë‹¤.")
                    # ë°±ë³¸ íŒŒë¼ë¯¸í„°ë“¤ì„ ê°•ì œë¡œ í™œì„±í™”
                    for param in self.unified_backbone.parameters():
                        param.requires_grad_(True)
                    logger.warning("ë°±ë³¸ íŒŒë¼ë¯¸í„° requires_gradë¥¼ ê°•ì œë¡œ Trueë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
                    raise RuntimeError("ë°±ë³¸ íŒŒë¼ë¯¸í„° gradient ì„¤ì • ë¬¸ì œ - ì¬ì‹œë„ í•„ìš”")
            else:
                logger.debug(f"í—¤ë“œ {head_type.value}: gradient ì—°ê²° ì •ìƒ")
            
            # í—¤ë“œ ëª¨ë“ˆì´ í•™ìŠµ ëª¨ë“œì¸ì§€ í™•ì¸
            if hasattr(real_head, 'training'):
                if not real_head.training:
                    logger.warning(f"í—¤ë“œ {head_type.value}ì´ eval ëª¨ë“œì…ë‹ˆë‹¤. í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
                    real_head.train()
            
            # í—¤ë“œ íŒŒë¼ë¯¸í„°ì˜ requires_grad í™•ì¸
            head_params_with_grad = sum(1 for p in real_head.parameters() if p.requires_grad)
            head_params_total = sum(1 for p in real_head.parameters())
            if head_params_with_grad == 0:
                logger.error(f"í—¤ë“œ {head_type.value}ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ requires_grad=Falseì…ë‹ˆë‹¤")
                raise RuntimeError(f"í—¤ë“œ {head_type.value} íŒŒë¼ë¯¸í„°ì— gradientê°€ ì—°ê²°ë˜ì§€ ì•ŠìŒ")
            
            logger.debug(f"í—¤ë“œ {head_type.value}: {head_params_with_grad}/{head_params_total} íŒŒë¼ë¯¸í„°ê°€ requires_grad=True")
            
            # ì‹¤ì œ í—¤ë“œ ìˆœì „íŒŒ (GPUì—ì„œ ìˆ˜í–‰ - NO FALLBACK)
            try:
                raw_output = real_head(head_input_gpu)  # GPUì—ì„œ ì§ì ‘ ì‹¤í–‰
                # ì¶œë ¥ ì •ê·œí™” (dict/tuple -> tensor, gradient ì—°ê²° ë³´ì¥)
                head_output = self._normalize_head_output(real_head, head_input_gpu, raw_output, head_type)
            except AttributeError as e:
                if "'dict' object has no attribute 'shape'" in str(e):
                    # ì˜ˆì™¸ ì²˜ë¦¬: shape ë¡œê¹… ì‹œë„ì—ì„œ ë°œìƒí•œ ì˜¤ë¥˜
                    logger.error(f"{head_type.value} forward ì‹¤íŒ¨: 'dict' object has no attribute 'shape'")
                    logger.error(f"ì…ë ¥ shape: {head_input_gpu.shape}")
                    # dict ì¶œë ¥ì„ í…ì„œë¡œ ë³€í™˜ ì‹œë„
                    if isinstance(raw_output, dict):
                        for k in ("logits", "logit", "scores", "last_hidden_state", "hidden_states", "embeddings"):
                            if k in raw_output and torch.is_tensor(raw_output[k]):
                                head_output = raw_output[k]
                                break
                        else:
                            # dictì˜ ì²« ë²ˆì§¸ í…ì„œ ì‚¬ìš©
                            for v in raw_output.values():
                                if torch.is_tensor(v):
                                    head_output = v
                                    break
                            else:
                                raise RuntimeError(f"{head_type.value} í—¤ë“œ ì¶œë ¥ì—ì„œ í…ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    else:
                        raise
                else:
                    raise
            
            # ì¶œë ¥ì˜ gradient ì—°ê²° í™•ì¸
            if not head_output.requires_grad:
                logger.error(f"í—¤ë“œ {head_type.value} ì¶œë ¥ì´ requires_grad=Falseì…ë‹ˆë‹¤")
                # gradient ì—°ê²° ë³µêµ¬ ì‹œë„
                head_output = head_output.detach().requires_grad_(True)
                logger.warning(f"í—¤ë“œ {head_type.value} - detach í›„ requires_grad ê°•ì œ ì„¤ì •")
            
            logger.debug(f"í—¤ë“œ {head_type.value}: GPUì—ì„œ ì§ì ‘ ì‹¤í–‰ ì™„ë£Œ (device: {head_output.device})")
            head_outputs[head_type.value] = head_output
        
        return {
            'unified_representation': unified_repr,
            'head_outputs': head_outputs
        }
    
    async def train_step(self, batch_data: Dict[str, Any],
                        active_heads: List[HeadType]) -> TrainingMetrics:
        """ë‹¨ì¼ í›ˆë ¨ ìŠ¤í…"""
        
        step_start_time = time.time()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        memory_before = self.memory_monitor.get_memory_usage()
        
        # ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ
        if self.adaptive_batch_sizing:
            batch_size = self.adaptive_batch_manager.get_optimal_batch_size(
                memory_before, len(active_heads)
            )
            batch_data['batch_size'] = batch_size
        
        # í˜¼í•© ì •ë°€ë„ ì»¨í…ìŠ¤íŠ¸
        with autocast(device_type='cuda', enabled=self.use_mixed_precision):
            # ìˆœì „íŒŒ
            outputs = self.forward_with_checkpointing(batch_data, active_heads)
            
            # ì†ì‹¤ ê³„ì‚°
            losses = self._calculate_losses(outputs, batch_data, active_heads)
            total_loss = sum(losses.values()) / len(losses)
        
        # ì—­ì „íŒŒ (ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì‚¬ìš©)
        scaled_loss = total_loss / self.gradient_accumulation_steps
        
        if self.use_mixed_precision:
            self.scaler.scale(scaled_loss).backward()
        else:
            scaled_loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ (ëˆ„ì  ìŠ¤í…ë§ˆë‹¤)
        if (self.current_metrics.step + 1) % self.gradient_accumulation_steps == 0:
            await self._update_gradients(active_heads)
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        memory_after = self.memory_monitor.get_memory_usage()
        step_time = time.time() - step_start_time
        
        self._update_metrics(
            losses, total_loss, memory_after - memory_before, 
            step_time, batch_data.get('batch_size', 1)
        )
        
        return self.current_metrics
    
    async def _update_gradients(self, active_heads: List[HeadType]):
        """ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸"""
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        total_norm = 0.0
        
        if self.use_mixed_precision:
            # ë°±ë³¸ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            self.scaler.unscale_(self.optimizers['backbone'])
            backbone_norm = torch.nn.utils.clip_grad_norm_(
                self.unified_backbone.parameters(), 1.0
            )
            total_norm += backbone_norm
            
            # í—¤ë“œë³„ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            for head_type in active_heads:
                if head_type.value in self.optimizers:
                    self.scaler.unscale_(self.optimizers[head_type.value])
                    config = self.head_configs[head_type]
                    # ì‹¤ì œ í—¤ë“œ íŒŒë¼ë¯¸í„°ê°€ ìˆë‹¤ë©´ í´ë¦¬í•‘
                    # head_norm = torch.nn.utils.clip_grad_norm_(head_params, config.gradient_clip_value)
                    # total_norm += head_norm
        
        # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
        if self.use_mixed_precision:
            self.scaler.step(self.optimizers['backbone'])
            for head_type in active_heads:
                if head_type.value in self.optimizers:
                    self.scaler.step(self.optimizers[head_type.value])
            self.scaler.update()
        else:
            self.optimizers['backbone'].step()
            for head_type in active_heads:
                if head_type.value in self.optimizers:
                    self.optimizers[head_type.value].step()
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        for scheduler in self.schedulers.values():
            scheduler.step()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        for optimizer in self.optimizers.values():
            optimizer.zero_grad()
        
        self.current_metrics.gradient_norm = total_norm
    
    def _calculate_losses(self, outputs: Dict[str, torch.Tensor],
                         batch_data: Dict[str, Any],
                         active_heads: List[HeadType]) -> Dict[str, torch.Tensor]:
        """ì•ˆì •ì ì¸ ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚° (NaN ë°©ì§€)"""
        
        # ì•ˆì •ì ì¸ loss ê³„ì‚°ê¸° ì´ˆê¸°í™”
        if not hasattr(self, '_stable_loss_calculator'):
            from fix_nan_loss import StableLossCalculator
            self._stable_loss_calculator = StableLossCalculator()
        
        # ì•ˆì •ì ì¸ loss ê³„ì‚°
        losses = self._stable_loss_calculator.calculate_stable_losses(
            outputs, batch_data, active_heads, self.head_configs
        )
        
        # ê°€ì¤‘ì¹˜ ì ìš© ë° ì¶”ê°€ ì•ˆì „ ê²€ì‚¬
        weighted_losses = {}
        for head_type in active_heads:
            config = self.head_configs[head_type]
            loss = losses[head_type.value]
            
            # ê°€ì¤‘ì¹˜ ì ìš©
            weighted_loss = loss * config.loss_weight
            
            # ìµœì¢… ì•ˆì „ ê²€ì‚¬
            if torch.isnan(weighted_loss) or torch.isinf(weighted_loss):
                logger.warning(f"{head_type.value} ê°€ì¤‘ loss NaN/Inf ê°ì§€, 1.0ìœ¼ë¡œ ëŒ€ì²´")
                weighted_loss = torch.tensor(1.0, device=loss.device, requires_grad=True)
            
            weighted_losses[head_type.value] = weighted_loss
        
        return weighted_losses
    
    def _update_metrics(self, losses: Dict[str, torch.Tensor], total_loss: torch.Tensor,
                       memory_delta: float, step_time: float, batch_size: int):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        
        self.current_metrics.step += 1
        self.current_metrics.total_loss = float(total_loss.item())
        self.current_metrics.head_losses = {k: float(v.item()) for k, v in losses.items()}
        self.current_metrics.memory_usage = memory_delta
        self.current_metrics.training_time = step_time
        self.current_metrics.samples_per_second = batch_size / step_time if step_time > 0 else 0.0
        
        # í•™ìŠµë¥  ê¸°ë¡
        for name, optimizer in self.optimizers.items():
            self.current_metrics.learning_rates[name] = optimizer.param_groups[0]['lr']
        
        # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        metrics_copy = TrainingMetrics(
            epoch=self.current_metrics.epoch,
            step=self.current_metrics.step,
            total_loss=self.current_metrics.total_loss,
            head_losses=self.current_metrics.head_losses.copy(),
            learning_rates=self.current_metrics.learning_rates.copy(),
            memory_usage=self.current_metrics.memory_usage,
            training_time=self.current_metrics.training_time,
            samples_per_second=self.current_metrics.samples_per_second
        )
        self.metrics_history.append(metrics_copy)

class AdaptiveBatchSizeManager:
    """ì ì‘ì  ë°°ì¹˜ í¬ê¸° ê´€ë¦¬ì - 103% GPU ë©”ëª¨ë¦¬ ë¬¸ì œ ëŒ€ì‘"""
    
    def __init__(self, initial_batch_size: int = 4, max_batch_size: int = 16):
        self.current_batch_size = initial_batch_size
        self.max_batch_size = max_batch_size
        self.min_batch_size = 1
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
        self.memory_history = deque(maxlen=10)
        self.oom_count = 0
        self.successful_steps = 0
        
        # ì ì‘ íŒŒë¼ë¯¸í„° (í˜„ì‹¤ì  ì¡°ì • - GPU threshold 85% ë³µì›ì— ë§ì¶¤)
        self.memory_threshold = 0.65  # GPU ë©”ëª¨ë¦¬ 65% ì‚¬ìš©ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ (50% â†’ 65%)
        self.increase_threshold = 0.50  # GPU ë©”ëª¨ë¦¬ 50% ë¯¸ë§Œì‹œ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê³ ë ¤ (35% â†’ 50%)
        
    def get_optimal_batch_size(self, current_memory_usage: float, num_active_heads: int) -> int:
        """ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œê³¼ í†µí•©ëœ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        memory_info = get_gpu_memory_info()
        if memory_info:
            memory_utilization = memory_info.get('usage_percent', 50) / 100.0
        else:
            memory_utilization = 0.5  # ê¸°ë³¸ê°’
        
        self.memory_history.append(memory_utilization)
        
        # ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œê³¼ í†µí•©ëœ ë‹¨ê³„ë³„ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        # 70% ë¯¸ë§Œ: ì •ìƒ ìš´ì˜
        if memory_utilization < 0.70:
            target_batch_size = 4  # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°
            
        # 70-75%: ê²½ê³  ë ˆë²¨ - ë°°ì¹˜ í¬ê¸° ê°ì†Œ
        elif memory_utilization < 0.75:
            target_batch_size = 2
            logger.warning(f"GPU ë©”ëª¨ë¦¬ ê²½ê³  ë ˆë²¨({memory_utilization:.1%}) - ë°°ì¹˜ í¬ê¸° 2ë¡œ ê°ì†Œ")
            
        # 75-80%: ìœ„í—˜ ë ˆë²¨ - ìµœì†Œ ë°°ì¹˜ í¬ê¸°
        elif memory_utilization < 0.80:
            target_batch_size = 1
            logger.error(f"GPU ë©”ëª¨ë¦¬ ìœ„í—˜ ë ˆë²¨({memory_utilization:.1%}) - ë°°ì¹˜ í¬ê¸° 1ë¡œ ìµœì†Œí™”")
            
        # 80% ì´ˆê³¼: ê¸´ê¸‰ ìƒí™© - ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨
        else:
            target_batch_size = 1
            logger.critical(f"GPU ë©”ëª¨ë¦¬ ê¸´ê¸‰ ìƒí™©({memory_utilization:.1%}) - ìµœì†Œ ë°°ì¹˜ë¡œ í•™ìŠµ ê³„ì†")
        
        # í™œì„± í—¤ë“œ ìˆ˜ì— ë”°ë¥¸ ì¶”ê°€ ì¡°ì •
        if num_active_heads > 2:
            target_batch_size = max(1, target_batch_size // 2)
            logger.info(f"ë‹¤ì¤‘ í—¤ë“œ í™œì„±({num_active_heads}ê°œ) - ë°°ì¹˜ í¬ê¸° ì¶”ê°€ ê°ì†Œ: {target_batch_size}")
        
        # ì ì§„ì  ë³€ê²½ (ê°‘ì‘ìŠ¤ëŸ° ë³€í™” ë°©ì§€)
        if target_batch_size > self.current_batch_size:
            self.current_batch_size = min(target_batch_size, self.current_batch_size + 1)
        elif target_batch_size < self.current_batch_size:
            self.current_batch_size = target_batch_size  # ê°ì†ŒëŠ” ì¦‰ì‹œ ì ìš©
        
        # ë²”ìœ„ ì œí•œ
        self.current_batch_size = max(self.min_batch_size, 
                                     min(self.max_batch_size, self.current_batch_size))
        
        self.successful_steps += 1
        return self.current_batch_size
    
    def report_oom(self):
        """Out of Memory ë°œìƒ ë³´ê³ """
        self.oom_count += 1
        if self.current_batch_size > self.min_batch_size:
            self.current_batch_size = max(self.min_batch_size, self.current_batch_size // 2)
            logger.error(f"OOM ë°œìƒ, ë°°ì¹˜ í¬ê¸° ëŒ€í­ ê°ì†Œ: {self.current_batch_size}")

class MemoryMonitor:
    """ê°•í™”ëœ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„° - GPU ëª¨ë“ˆë³„ ìƒì„¸ ì¶”ì """
    
    def __init__(self):
        self.cpu_memory_history = deque(maxlen=100)
        self.gpu_memory_history = deque(maxlen=100)
        
        # GPU ëª¨ë“ˆ ì¶”ì ì„ ìœ„í•œ ì¶”ê°€ ì €ì¥ì†Œ
        self.gpu_module_registry = {}  # ëª¨ë“ˆëª… -> ëª¨ë“ˆ ê°ì²´
        self.memory_snapshots = deque(maxlen=50)  # ìƒì„¸ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·
        self.high_memory_alerts = []  # ê³ ë©”ëª¨ë¦¬ ì‚¬ìš© ì•Œë¦¼ ê¸°ë¡
        
        # ë””ë²„ê·¸ ì„¤ì •
        self.debug_threshold = 0.90  # 90% ì´ìƒì—ì„œ ìƒì„¸ ë¡œê·¸
        self.critical_threshold = 1.0  # 100% ì´ìƒì—ì„œ ê¸´ê¸‰ ëŒ€ì‘
        
    def get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ ë° ê³ ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ ìë™ ë¶„ì„"""
        
        # CPU ë©”ëª¨ë¦¬
        cpu_memory = psutil.virtual_memory()
        cpu_usage_gb = (cpu_memory.total - cpu_memory.available) / (1024**3)
        
        # GPU ë©”ëª¨ë¦¬ (ìƒì„¸ ë¶„ì„)
        gpu_info = get_gpu_memory_info()
        gpu_usage_gb = gpu_info.get('allocated_mb', 0) / 1024 if gpu_info else 0
        gpu_total_gb = gpu_info.get('total_mb', 8192) / 1024 if gpu_info else 8
        gpu_percent = (gpu_usage_gb / gpu_total_gb * 100) if gpu_info else 0
        
        # torch.cuda ì§ì ‘ ì¡°íšŒë¡œ ë” ì •í™•í•œ ìˆ˜ì¹˜ ì–»ê¸°
        import torch
        if torch.cuda.is_available():
            allocated_gb = torch.cuda.memory_allocated(0) / (1024**3)
            reserved_gb = torch.cuda.memory_reserved(0) / (1024**3)
            free_gb = (torch.cuda.get_device_properties(0).total_memory / (1024**3)) - allocated_gb
            torch_gpu_percent = (allocated_gb / (torch.cuda.get_device_properties(0).total_memory / (1024**3))) * 100
            
            # ë” ì •í™•í•œ ìˆ˜ì¹˜ë¡œ ì—…ë°ì´íŠ¸
            gpu_usage_gb = allocated_gb
            gpu_percent = torch_gpu_percent
            
            usage = {
                'cpu_memory_gb': cpu_usage_gb,
                'gpu_memory_gb': gpu_usage_gb,
                'cpu_percent': cpu_memory.percent,
                'gpu_percent': gpu_percent,
                # ì¶”ê°€ ìƒì„¸ ì •ë³´
                'gpu_allocated_gb': allocated_gb,
                'gpu_reserved_gb': reserved_gb,
                'gpu_free_gb': free_gb,
                'gpu_total_gb': torch.cuda.get_device_properties(0).total_memory / (1024**3)
            }
        else:
            usage = {
                'cpu_memory_gb': cpu_usage_gb,
                'gpu_memory_gb': gpu_usage_gb,
                'cpu_percent': cpu_memory.percent,
                'gpu_percent': gpu_percent
            }
        
        self.cpu_memory_history.append(usage['cpu_memory_gb'])
        self.gpu_memory_history.append(usage['gpu_memory_gb'])
        
        # 90% ì´ìƒì´ë©´ ìƒì„¸ ë¶„ì„ ìˆ˜í–‰
        if usage['gpu_percent'] >= (self.debug_threshold * 100):
            self._analyze_high_memory_usage(usage)
        
        # 100% ì´ìƒì´ë©´ ê¸´ê¸‰ ëŒ€ì‘
        if usage['gpu_percent'] >= (self.critical_threshold * 100):
            self._emergency_memory_response(usage)
        
        return usage
    
    def get_memory_efficiency(self) -> float:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³„ì‚°"""
        if len(self.gpu_memory_history) < 2:
            return 1.0
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì˜ ì•ˆì •ì„±ì„ íš¨ìœ¨ì„± ì§€í‘œë¡œ ì‚¬ìš©
        recent_usage = list(self.gpu_memory_history)[-10:]
        if len(recent_usage) < 2:
            return 1.0
        
        std_dev = np.std(recent_usage)
        mean_usage = np.mean(recent_usage)
        
        # í‘œì¤€í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ íš¨ìœ¨ì  (ì•ˆì •ì )
        efficiency = 1.0 / (1.0 + std_dev / max(0.1, mean_usage))
        return min(1.0, efficiency)
    
    def register_gpu_module(self, name: str, module):
        """GPU ëª¨ë“ˆ ë“±ë¡ (ì¶”ì ìš©)"""
        self.gpu_module_registry[name] = module
        logger.debug(f"GPU ëª¨ë“ˆ ë“±ë¡: {name}")
    
    def _analyze_high_memory_usage(self, usage: Dict[str, float]):
        """ê³ ë©”ëª¨ë¦¬ ì‚¬ìš© ìƒì„¸ ë¶„ì„"""
        
        timestamp = datetime.now()
        gpu_percent = usage['gpu_percent']
        
        logger.critical(f"ğŸš¨ HIGH GPU MEMORY ALERT: {gpu_percent:.1f}% ì‚¬ìš© ì¤‘!")
        logger.critical(f"   í• ë‹¹ëœ ë©”ëª¨ë¦¬: {usage.get('gpu_allocated_gb', 0):.2f}GB")
        logger.critical(f"   ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {usage.get('gpu_reserved_gb', 0):.2f}GB")
        logger.critical(f"   ì—¬ìœ  ë©”ëª¨ë¦¬: {usage.get('gpu_free_gb', 0):.2f}GB")
        logger.critical(f"   ì „ì²´ ë©”ëª¨ë¦¬: {usage.get('gpu_total_gb', 8):.2f}GB")
        
        # torch.cuda ìƒì„¸ ì •ë³´
        import torch
        if torch.cuda.is_available():
            logger.critical("ğŸ“Š TORCH.CUDA ìƒì„¸ ì •ë³´:")
            logger.critical(f"   memory_allocated(): {torch.cuda.memory_allocated(0) / (1024**3):.3f}GB")
            logger.critical(f"   memory_reserved(): {torch.cuda.memory_reserved(0) / (1024**3):.3f}GB")
            logger.critical(f"   max_memory_allocated(): {torch.cuda.max_memory_allocated(0) / (1024**3):.3f}GB")
            logger.critical(f"   max_memory_reserved(): {torch.cuda.max_memory_reserved(0) / (1024**3):.3f}GB")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚° ë°©ì‹ë“¤ ë¹„êµ
            device_props = torch.cuda.get_device_properties(0)
            total_memory = device_props.total_memory / (1024**3)
            allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)
            reserved_memory = torch.cuda.memory_reserved(0) / (1024**3)
            
            allocated_percent = (allocated_memory / total_memory) * 100
            reserved_percent = (reserved_memory / total_memory) * 100
            
            logger.critical(f"ğŸ” ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚° ë¹„êµ:")
            logger.critical(f"   í• ë‹¹ ê¸°ì¤€: {allocated_percent:.2f}%")
            logger.critical(f"   ì˜ˆì•½ ê¸°ì¤€: {reserved_percent:.2f}%")
            logger.critical(f"   config.py ê¸°ì¤€: {usage.get('gpu_percent', 0):.2f}%")
        
        # ë“±ë¡ëœ GPU ëª¨ë“ˆë“¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        self._debug_gpu_modules()
        
        # ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ì €ì¥
        snapshot = {
            'timestamp': timestamp,
            'gpu_percent': gpu_percent,
            'usage_details': usage.copy(),
            'module_count': len(self.gpu_module_registry)
        }
        self.memory_snapshots.append(snapshot)
        self.high_memory_alerts.append(snapshot)
        
        # ìµœê·¼ ë©”ëª¨ë¦¬ íŒ¨í„´ ë¶„ì„
        if len(self.gpu_memory_history) >= 5:
            recent_usage = list(self.gpu_memory_history)[-5:]
            avg_usage = sum(recent_usage) / len(recent_usage)
            trend = "ì¦ê°€" if recent_usage[-1] > recent_usage[0] else "ê°ì†Œ"
            logger.critical(f"ğŸ“ˆ ìµœê·¼ ë©”ëª¨ë¦¬ íŒ¨í„´: í‰ê·  {avg_usage:.2f}GB, {trend} ì¶”ì„¸")
    
    def _emergency_memory_response(self, usage: Dict[str, float]):
        """100% ì´ìƒ ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ ê¸´ê¸‰ ëŒ€ì‘"""
        
        gpu_percent = usage['gpu_percent']
        
        logger.error(f"ğŸš¨ğŸš¨ğŸš¨ CRITICAL GPU MEMORY OVERFLOW: {gpu_percent:.1f}%!!")
        logger.error("ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘...")
        
        # torch ìºì‹œ ì •ë¦¬
        import torch
        if torch.cuda.is_available():
            logger.error("ğŸ§¹ torch.cuda.empty_cache() ì‹¤í–‰")
            torch.cuda.empty_cache()
            
            # ì •ë¦¬ í›„ ìƒíƒœ ì¬í™•ì¸
            after_allocated = torch.cuda.memory_allocated(0) / (1024**3)
            after_reserved = torch.cuda.memory_reserved(0) / (1024**3)
            after_percent = (after_allocated / (torch.cuda.get_device_properties(0).total_memory / (1024**3))) * 100
            
            logger.error(f"ğŸ”„ ì •ë¦¬ í›„ ìƒíƒœ: {after_percent:.1f}% (í• ë‹¹: {after_allocated:.2f}GB, ì˜ˆì•½: {after_reserved:.2f}GB)")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        import gc
        logger.error("ğŸ—‘ï¸  gc.collect() ì‹¤í–‰")
        gc.collect()
        
        # ìœ„í—˜ ìˆ˜ì¤€ ê¸°ë¡
        self.high_memory_alerts.append({
            'timestamp': datetime.now(),
            'type': 'CRITICAL_OVERFLOW',
            'gpu_percent': gpu_percent,
            'emergency_response': True
        })
    
    def _debug_gpu_modules(self):
        """ë“±ë¡ëœ GPU ëª¨ë“ˆë“¤ì˜ ìƒì„¸ ë©”ëª¨ë¦¬ ë¶„ì„"""
        
        if not self.gpu_module_registry:
            logger.warning("ğŸ” ë“±ë¡ëœ GPU ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. ì¶”ì  ë¶ˆê°€.")
            return
        
        logger.critical(f"ğŸ” GPU ëª¨ë“ˆë³„ ë©”ëª¨ë¦¬ ë¶„ì„ ({len(self.gpu_module_registry)}ê°œ ëª¨ë“ˆ):")
        
        import torch
        total_params = 0
        
        for name, module in self.gpu_module_registry.items():
            try:
                if hasattr(module, 'parameters'):
                    # PyTorch ëª¨ë“ˆì¸ ê²½ìš°
                    param_count = sum(p.numel() for p in module.parameters() if p.requires_grad)
                    param_memory_mb = sum(p.numel() * p.element_size() for p in module.parameters()) / (1024**2)
                    device = next(module.parameters()).device if param_count > 0 else "unknown"
                    
                    logger.critical(f"   ğŸ“¦ {name}:")
                    logger.critical(f"      íŒŒë¼ë¯¸í„°: {param_count:,}ê°œ")
                    logger.critical(f"      ë©”ëª¨ë¦¬: {param_memory_mb:.2f}MB")
                    logger.critical(f"      ë””ë°”ì´ìŠ¤: {device}")
                    logger.critical(f"      í›ˆë ¨ ëª¨ë“œ: {getattr(module, 'training', 'unknown')}")
                    
                    total_params += param_count
                
                else:
                    logger.critical(f"   â“ {name}: PyTorch ëª¨ë“ˆ ì•„ë‹˜")
            
            except Exception as e:
                logger.critical(f"   âŒ {name}: ë¶„ì„ ì‹¤íŒ¨ - {str(e)}")
        
        logger.critical(f"ğŸ”¢ ì „ì²´ ì¶”ì ëœ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
        
        # í˜„ì¬ GPU í…ì„œë“¤ ë¶„ì„ ì‹œë„
        try:
            if torch.cuda.is_available():
                logger.critical("ğŸ§  GPU í…ì„œ ë¶„ì„:")
                import gc
                gpu_tensors = []
                for obj in gc.get_objects():
                    if torch.is_tensor(obj) and obj.is_cuda:
                        gpu_tensors.append(obj)
                
                total_tensor_memory = sum(tensor.element_size() * tensor.numel() for tensor in gpu_tensors) / (1024**3)
                logger.critical(f"   GPU í…ì„œ ê°œìˆ˜: {len(gpu_tensors)}ê°œ")
                logger.critical(f"   GPU í…ì„œ ë©”ëª¨ë¦¬: {total_tensor_memory:.3f}GB")
        
        except Exception as e:
            logger.warning(f"GPU í…ì„œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
    
    def get_debug_summary(self) -> Dict[str, Any]:
        """ë””ë²„ê·¸ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        return {
            'registered_modules': len(self.gpu_module_registry),
            'high_memory_alerts': len(self.high_memory_alerts),
            'memory_snapshots': len(self.memory_snapshots),
            'latest_alert': self.high_memory_alerts[-1] if self.high_memory_alerts else None,
            'debug_threshold': self.debug_threshold,
            'critical_threshold': self.critical_threshold
        }

class UnifiedLearningScheduler:
    """í†µí•© í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, head_configs: Dict[HeadType, HeadTrainingConfig]):
        self.head_configs = head_configs
        self.current_phase = LearningPhase.WARM_UP
        self.phase_progress = 0.0
        
        # ìŠ¤ì¼€ì¤„ë§ ìƒíƒœ
        self.step_count = 0
        self.epoch_count = 0
        self.phase_step_count = 0
        
        # í•™ìŠµ ì „ëµ ë§¤ê°œë³€ìˆ˜
        self.strategy_params = {
            TrainingStrategy.ROUND_ROBIN: {'cycle_length': 4},
            TrainingStrategy.PARALLEL: {'weight_balance': 0.5},
            TrainingStrategy.PRIORITY_BASED: {'priority_weights': {}},
            TrainingStrategy.ADAPTIVE: {'adaptation_rate': 0.1}
        }
        
    def get_active_heads(self, current_strategy: TrainingStrategy, 
                        available_heads: List[HeadType]) -> List[HeadType]:
        """í˜„ì¬ ì „ëµì— ë”°ë¥¸ í™œì„± í—¤ë“œ ì„ íƒ"""
        
        if current_strategy == TrainingStrategy.ROUND_ROBIN:
            # ìˆœì°¨ì  í—¤ë“œ í›ˆë ¨
            cycle_length = self.strategy_params[TrainingStrategy.ROUND_ROBIN]['cycle_length']
            cycle_position = self.step_count % (len(available_heads) * cycle_length)
            head_index = cycle_position // cycle_length
            return [available_heads[head_index]]
        
        elif current_strategy == TrainingStrategy.PARALLEL:
            # ëª¨ë“  í—¤ë“œ ë³‘ë ¬ í›ˆë ¨
            return available_heads
        
        elif current_strategy == TrainingStrategy.PRIORITY_BASED:
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì„ íƒ
            priorities = self._calculate_head_priorities(available_heads)
            # ìƒìœ„ 50% í—¤ë“œ ì„ íƒ
            sorted_heads = sorted(priorities.items(), key=lambda x: x[1], reverse=True)
            top_heads = [head for head, _ in sorted_heads[:max(1, len(sorted_heads)//2)]]
            return top_heads
        
        elif current_strategy == TrainingStrategy.ADAPTIVE:
            # ì ì‘ì  ì„ íƒ (ì„±ëŠ¥ ê¸°ë°˜)
            return self._adaptive_head_selection(available_heads)
        
        return available_heads  # ê¸°ë³¸ê°’
    
    def _calculate_head_priorities(self, available_heads: List[HeadType]) -> Dict[HeadType, float]:
        """í—¤ë“œë³„ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        priorities = {}
        
        for head_type in available_heads:
            config = self.head_configs[head_type]
            
            # ê¸°ë³¸ ìš°ì„ ìˆœìœ„ëŠ” í•™ìŠµë¥ ê³¼ ì†ì‹¤ ê°€ì¤‘ì¹˜ì— ê¸°ë°˜
            base_priority = config.learning_rate * config.loss_weight
            
            # í˜„ì¬ ì—í¬í¬ì™€ freeze_until_epoch ê³ ë ¤
            if self.epoch_count < config.freeze_until_epoch:
                priority = 0.0  # ë™ê²°ëœ í—¤ë“œ
            else:
                # ì›Œë°ì—… ì™„ë£Œ ì •ë„ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„
                warmup_factor = min(1.0, self.step_count / max(1, config.warmup_steps))
                priority = base_priority * warmup_factor
            
            priorities[head_type] = priority
        
        return priorities
    
    def _adaptive_head_selection(self, available_heads: List[HeadType]) -> List[HeadType]:
        """ì ì‘ì  í—¤ë“œ ì„ íƒ"""
        
        # í˜„ì¬ ë‹¨ê³„ì™€ ì„±ëŠ¥ì„ ê³ ë ¤í•œ ë™ì  ì„ íƒ
        if self.current_phase == LearningPhase.WARM_UP:
            # ì›Œë°ì—… ë‹¨ê³„: ê¸°ë³¸ í—¤ë“œë“¤ë§Œ
            basic_heads = [HeadType.EMOTION_EMPATHY, HeadType.BENTHAM_FROMM]
            return [h for h in basic_heads if h in available_heads]
        
        elif self.current_phase == LearningPhase.COLLABORATIVE:
            # í˜‘ë ¥ í•™ìŠµ: ëª¨ë“  í—¤ë“œ
            return available_heads
        
        elif self.current_phase == LearningPhase.SPECIALIZED:
            # ì „ë¬¸í™” í•™ìŠµ: êµëŒ€ë¡œ ì „ë¬¸í™”
            cycle_length = 3
            head_index = (self.phase_step_count // cycle_length) % len(available_heads)
            return [available_heads[head_index]]
        
        elif self.current_phase == LearningPhase.INTEGRATION:
            # í†µí•© í•™ìŠµ: ì‹œë„ˆì§€ íš¨ê³¼ê°€ ë†’ì€ ì¡°í•©
            synergistic_combinations = [
                [HeadType.EMOTION_EMPATHY, HeadType.BENTHAM_FROMM],
                [HeadType.BENTHAM_FROMM, HeadType.REGRET_LEARNING],
                [HeadType.SEMANTIC_SURD, HeadType.META_INTEGRATION]
            ]
            combination_index = (self.phase_step_count // 5) % len(synergistic_combinations)
            selected_combination = synergistic_combinations[combination_index]
            return [h for h in selected_combination if h in available_heads]
        
        return available_heads
    
    def update_phase(self, performance_metrics: Dict[str, float]):
        """í•™ìŠµ ë‹¨ê³„ ì—…ë°ì´íŠ¸"""
        self.step_count += 1
        self.phase_step_count += 1
        
        # ë‹¨ê³„ ì „í™˜ ì¡°ê±´ í™•ì¸
        should_advance = False
        
        if self.current_phase == LearningPhase.WARM_UP:
            # ì›Œë°ì—… ì™„ë£Œ ì¡°ê±´: 1000 ìŠ¤í… ë˜ëŠ” ì†ì‹¤ ì•ˆì •í™”
            if (self.phase_step_count >= 1000 or 
                performance_metrics.get('loss_stability', 0) > 0.8):
                should_advance = True
                next_phase = LearningPhase.COLLABORATIVE
        
        elif self.current_phase == LearningPhase.COLLABORATIVE:
            # í˜‘ë ¥ í•™ìŠµ ì™„ë£Œ ì¡°ê±´: ì‹œë„ˆì§€ íš¨ê³¼ í™•ì¸
            if (self.phase_step_count >= 2000 or
                performance_metrics.get('synergy_gain', 0) > 0.15):
                should_advance = True
                next_phase = LearningPhase.SPECIALIZED
        
        elif self.current_phase == LearningPhase.SPECIALIZED:
            # ì „ë¬¸í™” ì™„ë£Œ ì¡°ê±´: ê° í—¤ë“œë³„ ì„±ëŠ¥ ì•ˆì •í™”
            if (self.phase_step_count >= 3000 or
                performance_metrics.get('head_stability', 0) > 0.9):
                should_advance = True
                next_phase = LearningPhase.INTEGRATION
        
        elif self.current_phase == LearningPhase.INTEGRATION:
            # í†µí•© í•™ìŠµ ì™„ë£Œ ì¡°ê±´: ì „ì²´ ì„±ëŠ¥ ìˆ˜ë ´
            if (self.phase_step_count >= 2000 or
                performance_metrics.get('convergence_rate', 0) > 0.95):
                should_advance = True
                next_phase = LearningPhase.FINE_TUNING
        
        if should_advance:
            logger.info(f"í•™ìŠµ ë‹¨ê³„ ì „í™˜: {self.current_phase.value} â†’ {next_phase.value}")
            self.current_phase = next_phase
            self.phase_step_count = 0
            self.phase_progress = 0.0

class RealTimeMemoryMonitor:
    """ì‹¤ì‹œê°„ GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìŠ¤ì™‘ ì œì–´"""
    
    def __init__(self):
        self.monitoring_enabled = True
        self.warning_threshold = 70  # 70% ì´ìƒì‹œ ê²½ê³ 
        self.force_swap_threshold = 75  # 75% ì´ìƒì‹œ ê°•ì œ ìŠ¤ì™‘
        self.emergency_threshold = 80  # 80% ì´ìƒì‹œ ê¸´ê¸‰ ì •ë¦¬
        self.memory_history = deque(maxlen=10)
        
    def check_memory_and_act(self, step_count: int = 0):
        """ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ë° í•„ìš”ì‹œ ìŠ¤ì™‘ ì•¡ì…˜ ìˆ˜í–‰"""
        if not self.monitoring_enabled:
            return True
            
        memory_info = get_gpu_memory_info()
        if memory_info is None:
            return True
            
        usage_percent = memory_info['usage_percent']
        self.memory_history.append(usage_percent)
        
        # 70% ë¯¸ë§Œ: ì •ìƒ ìš´ì˜
        if usage_percent < self.warning_threshold:
            if step_count % 20 == 0:  # 20ìŠ¤í…ë§ˆë‹¤ ì •ìƒ ìƒíƒœ ë¡œê·¸
                logger.debug(f"GPU ë©”ëª¨ë¦¬ ì •ìƒ: {usage_percent:.1f}% ì‚¬ìš©ì¤‘")
            return True
        
        # 70-75%: ê²½ê³  ë° ì˜ˆë°©ì  ìŠ¤ì™‘ ì¤€ë¹„
        elif usage_percent < self.force_swap_threshold:
            logger.warning(f"GPU ë©”ëª¨ë¦¬ ê²½ê³  ë ˆë²¨: {usage_percent:.1f}% - ì˜ˆë°©ì  ìŠ¤ì™‘ ì¤€ë¹„")
            self._prepare_preventive_swap()
            return True
        
        # 75-80%: ê°•ì œ ìŠ¤ì™‘ ìˆ˜í–‰
        elif usage_percent < self.emergency_threshold:
            logger.error(f"GPU ë©”ëª¨ë¦¬ ìœ„í—˜ ë ˆë²¨: {usage_percent:.1f}% - ê°•ì œ ìŠ¤ì™‘ ìˆ˜í–‰")
            self._perform_force_swap()
            return True
        
        # 80% ì´ˆê³¼: ê¸´ê¸‰ ì •ë¦¬
        else:
            logger.critical(f"GPU ë©”ëª¨ë¦¬ ê¸´ê¸‰ ìƒí™©: {usage_percent:.1f}% - ê¸´ê¸‰ ì •ë¦¬ ìˆ˜í–‰")
            self._perform_emergency_cleanup()
            return False  # í•™ìŠµ ì¼ì‹œ ì¤‘ë‹¨ ì‹ í˜¸
    
    def _prepare_preventive_swap(self):
        """ì˜ˆë°©ì  ìŠ¤ì™‘ ì¤€ë¹„"""
        # LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ CPU ì´ë™ ì¤€ë¹„
        logger.info("LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ CPU ì´ë™ ì¤€ë¹„ ì¤‘...")
    
    def _perform_force_swap(self):
        """ê°•ì œ ìŠ¤ì™‘ ìˆ˜í–‰"""
        logger.info("ë‚®ì€ ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ ê°•ì œ ìŠ¤ì™‘ ìˆ˜í–‰ ì¤‘...")
        _force_swap_low_priority_models()
        
        # ìŠ¤ì™‘ í›„ ë©”ëª¨ë¦¬ ì¬í™•ì¸
        memory_info = get_gpu_memory_info()
        if memory_info:
            logger.info(f"ê°•ì œ ìŠ¤ì™‘ ì™„ë£Œ: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {memory_info['usage_percent']:.1f}%")
    
    def _perform_emergency_cleanup(self):
        """ê¸´ê¸‰ GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        logger.critical("ê¸´ê¸‰ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ìˆ˜í–‰ ì¤‘...")
        _emergency_gpu_cleanup()
        
        # ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ ì¬í™•ì¸
        memory_info = get_gpu_memory_info()
        if memory_info:
            logger.critical(f"ê¸´ê¸‰ ì •ë¦¬ ì™„ë£Œ: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {memory_info['usage_percent']:.1f}%")
    
    def get_memory_trend(self) -> str:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¶”ì„¸ ë°˜í™˜"""
        if len(self.memory_history) < 3:
            return "insufficient_data"
        
        recent_avg = sum(self.memory_history[-3:]) / 3
        older_avg = sum(self.memory_history[:-3]) / max(1, len(self.memory_history) - 3)
        
        if recent_avg > older_avg + 5:
            return "increasing"
        elif recent_avg < older_avg - 5:
            return "decreasing"
        else:
            return "stable"

class UnifiedLearningSystem:
    """
    í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ - ë©”ì¸ í´ë˜ìŠ¤
    
    800M íŒŒë¼ë¯¸í„° ë‹¤ì¤‘ í—¤ë“œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, head_compatibility_manager=None, config: Dict[str, Any] = None, swap_manager=None):
        self.config = config or ADVANCED_CONFIG.get('unified_learning_config', {})
        
        # HeadCompatibilityManagerë¥¼ ì™¸ë¶€ì—ì„œ ì£¼ì…ë°›ìŒ
        if head_compatibility_manager is None:
            raise ValueError("HeadCompatibilityManager must be provided - ì¤‘ë³µ ìƒì„± ë°©ì§€ë¥¼ ìœ„í•´ ì™¸ë¶€ì—ì„œ ì „ë‹¬ í•„ìš”")
        self._external_head_compatibility_manager = head_compatibility_manager
        
        # í—¤ë“œ í›ˆë ¨ ì„¤ì •
        self.head_configs = self._initialize_head_configs()
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤
        self.unified_backbone = RedHeartUnifiedBackbone()
        
        # ë°±ë³¸ training ëª¨ë“œ ë° gradient ì„¤ì • ê°•í™”
        self.unified_backbone.train()
        for param in self.unified_backbone.parameters():
            param.requires_grad_(True)
        logger.info(f"ë°±ë³¸ gradient ì„¤ì • ì™„ë£Œ: {sum(1 for p in self.unified_backbone.parameters() if p.requires_grad)}/{sum(1 for p in self.unified_backbone.parameters())} íŒŒë¼ë¯¸í„°")
        
        self.trainer = MemoryEfficientTrainer(self.unified_backbone, self.head_configs, self)
        self.scheduler = UnifiedLearningScheduler(self.head_configs)
        
        # ìŠ¤ì™‘ ë§¤ë‹ˆì € - ì™¸ë¶€ ì£¼ì… ë˜ëŠ” ì „ì—­ì—ì„œ ê°€ì ¸ì˜´
        if swap_manager is not None:
            self.swap_manager = swap_manager
        else:
            from dynamic_swap_manager import get_swap_manager
            self.swap_manager = get_swap_manager()
        
        # DSM ì¼ì¹˜ì„± ê²€ì¦ (íŠ¸ë¦½ì™€ì´ì–´)
        from dynamic_swap_manager import get_swap_manager
        assert id(self.swap_manager) == id(get_swap_manager()), "ULS sees different DSM"
        logger.critical(f"[ULS] DSM id={id(self.swap_manager)}")
        self.synergy_system = IntelligentSynergySystem()
        
        # í—¤ë“œ í˜¸í™˜ì„± ë§¤ë‹ˆì € - ì™¸ë¶€ì—ì„œ ì£¼ì…ë°›ì€ ê²ƒ ì‚¬ìš©
        self.head_compatibility_manager = self._external_head_compatibility_manager
        
        # ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        self.memory_monitor = RealTimeMemoryMonitor()
        self.last_memory_check = 0
        self.memory_check_interval = 5  # 5ìŠ¤í…ë§ˆë‹¤ ë©”ëª¨ë¦¬ í™•ì¸
        
        # í›ˆë ¨ ìƒíƒœ
        self.is_training = False
        self.training_thread = None
        self.training_stats = {}
        
        # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
        self.checkpoint_dir = "checkpoints/unified_learning"
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆ ìºì‹œ
        self.cached_head_modules = {}
        
        # ì´ˆê¸°í™” ì™„ë£Œ í”Œë˜ê·¸
        self.initialized = False
        
        # ì˜µí‹°ë§ˆì´ì € ê´€ë ¨ ì†ì„± ì´ˆê¸°í™”
        self._optimizers_initialized = False
        self._optimizers = None
        self._schedulers = None
        self._scaler = None  # AMP scaler (ì˜µì…˜)
        
        logger.info("UnifiedLearningSystem ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def _build_optimizers(self):
        """ì˜µí‹°ë§ˆì´ì € ë¹Œë“œ - ë°±ë³¸ê³¼ í—¤ë“œ íŒŒë¼ë¯¸í„° ë¶„ë¦¬"""
        logger.info("ğŸ”§ ì˜µí‹°ë§ˆì´ì € ë¹Œë“œ ì‹œì‘...")
        
        # ë°±ë³¸ íŒŒë¼ë¯¸í„° ìˆ˜ì§‘
        backbone_params = []
        if self.unified_backbone:
            backbone_params = [p for p in self.unified_backbone.parameters() if p.requires_grad]
            logger.info(f"   ë°±ë³¸ íŒŒë¼ë¯¸í„°: {len(backbone_params)}ê°œ")
        
        # í—¤ë“œ íŒŒë¼ë¯¸í„° ìˆ˜ì§‘
        head_params = []
        for head_type, head_module in self.cached_head_modules.items():
            if head_module and hasattr(head_module, 'parameters'):
                params = [p for p in head_module.parameters() if p.requires_grad]
                head_params.extend(params)
                logger.info(f"   {head_type.value} í—¤ë“œ íŒŒë¼ë¯¸í„°: {len(params)}ê°œ")
        logger.info(f"   ì´ í—¤ë“œ íŒŒë¼ë¯¸í„°: {len(head_params)}ê°œ")
        
        # AdamW ì˜µí‹°ë§ˆì´ì € ìƒì„± - O3 ì œì•ˆ: í•œ ë²ˆë§Œ ìƒì„± í›„ state CPU ì´ë™
        from torch.optim import AdamW
        import torch
        
        # GPU ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· - ì˜µí‹°ë§ˆì´ì € ìƒì„± ì „
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            before_alloc = torch.cuda.memory_allocated()
            before_reserved = torch.cuda.memory_reserved()
            logger.info(f"[OPT_STATE_DEBUG] ìƒì„± ì „: alloc={before_alloc/1024**2:.1f}MB, reserved={before_reserved/1024**2:.1f}MB")
        
        # GPU íŒŒë¼ë¯¸í„°ë¡œ ë‹¨ì¼ AdamW ìƒì„±
        self._optimizers = AdamW([
            {"params": backbone_params, "lr": 1e-5, "weight_decay": 0.01},
            {"params": head_params, "lr": 5e-4, "weight_decay": 0.01}
        ], betas=(0.9, 0.999), foreach=False, capturable=False, fused=False)
        
        # GPU ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· - ì˜µí‹°ë§ˆì´ì € ìƒì„± í›„
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            after_alloc = torch.cuda.memory_allocated()
            after_reserved = torch.cuda.memory_reserved()
            diff_alloc = (after_alloc - before_alloc) / 1024**2
            diff_reserved = (after_reserved - before_reserved) / 1024**2
            logger.info(f"[OPT_STATE_DEBUG] ìƒì„± í›„: alloc={after_alloc/1024**2:.1f}MB, reserved={after_reserved/1024**2:.1f}MB")
            logger.info(f"[OPT_STATE_DEBUG] ì¦ê°€ëŸ‰: alloc=+{diff_alloc:.1f}MB, reserved=+{diff_reserved:.1f}MB")
            
            if diff_alloc > 10:  # 10MB ì´ìƒ ì¦ê°€ ì‹œ ê²½ê³ 
                logger.warning(f"âš ï¸ AdamW ìƒì„±ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ {diff_alloc:.1f}MB ì¦ê°€!")
                logger.warning("ì›ì¸: AdamW stateê°€ GPUì— ìƒì„±ë˜ì—ˆì„ ê°€ëŠ¥ì„±")
        
        # state lazy-init (GPU workspace íšŒí”¼)
        # torch.empty ì‚¬ìš©ìœ¼ë¡œ CUDA workspace í• ë‹¹ ë°©ì§€
        cpu_state_count = 0
        gpu_state_count = 0
        for param_group in self._optimizers.param_groups:
            for p in param_group['params']:
                if p not in self._optimizers.state:
                    # torch.emptyë¡œ CPU ë©”ëª¨ë¦¬ë§Œ í• ë‹¹ (GPU workspace ì—†ìŒ)
                    # ì•ˆì „í•œ CPU zeros ì´ˆê¸°í™” (GPU ìºì‹œ íšŒí”¼ + ì •ì˜ëœ ê°’)
                    self._optimizers.state[p] = {
                        'step': torch.zeros((), dtype=torch.int64, device='cpu'),
                        'exp_avg': torch.zeros(p.shape, dtype=p.dtype, device='cpu'),
                        'exp_avg_sq': torch.zeros(p.shape, dtype=p.dtype, device='cpu')
                    }
                    cpu_state_count += 3  # step, exp_avg, exp_avg_sq
        
        # ë¡œê·¸ ì¶œë ¥
        logger.info(f"   ğŸ“Š AdamW state ì´ˆê¸°í™” ì™„ë£Œ: CPUì— {cpu_state_count}ê°œ í…ì„œ ìƒì„±, GPUì— {gpu_state_count}ê°œ")
        
        # GPU ìºì‹œ ì™„ì „ ì •ë¦¬ (OSì— ë©”ëª¨ë¦¬ ë°˜í™˜)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()  # CUDA 11+: ë¹ˆ ìºì‹œ ë¸”ë¡ì„ OSì— ë°˜í™˜
            logger.info("   GPU ìºì‹œ ì™„ì „ ì •ë¦¬ ì™„ë£Œ (OS ë°˜í™˜)")
            
            # state ì´ˆê¸°í™” í›„ GPU ë©”ëª¨ë¦¬ ê²€ì¦
            torch.cuda.synchronize()
            final_alloc = torch.cuda.memory_allocated()
            final_reserved = torch.cuda.memory_reserved()
            # before_allocì€ ë¼ì¸ 1268ì—ì„œ ì •ì˜ë¨
            total_diff = (final_alloc - before_alloc) / 1024**2
            logger.info(f"[OPT_STATE_DEBUG] ìµœì¢…: alloc={final_alloc/1024**2:.1f}MB, reserved={final_reserved/1024**2:.1f}MB")
            logger.info(f"[OPT_STATE_DEBUG] ì „ì²´ ì¦ê°€ëŸ‰: {total_diff:.1f}MB")
            
            if total_diff > 100:  # 100MB ì´ìƒ ì¦ê°€ë©´ ì‹¬ê°í•œ ë¬¸ì œ
                logger.error(f"âŒ GPU ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€: {total_diff:.1f}MB ì¦ê°€!")
                logger.error("AdamW stateê°€ ì—¬ì „íˆ GPUì— ìˆê±°ë‚˜ ë‹¤ë¥¸ ë¬¸ì œ ë°œìƒ")
        
        logger.info("âœ… AdamW ìµœì í™” ì™„ë£Œ (stateëŠ” CPU, GPU ìºì‹œ ì •ë¦¬ë¨)")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ì™€ ìŠ¤ì¼€ì¼ëŸ¬ëŠ” í•„ìš”ì‹œ ì¶”ê°€
        self._schedulers = None
        self._scaler = None  
        
        # trainerì— ì˜µí‹°ë§ˆì´ì € ì£¼ì… (ì´ì¤‘ ìƒì„± ë°©ì§€)
        if hasattr(self.trainer, 'set_optimizer'):
            self.trainer.set_optimizer(self._optimizers, self._schedulers)
            logger.info("   Trainerì— ì˜µí‹°ë§ˆì´ì € ì£¼ì… ì™„ë£Œ")
        
        self._optimizers_initialized = True
        logger.info("âœ… ì˜µí‹°ë§ˆì´ì € ë¹Œë“œ ì™„ë£Œ")
    
    async def initialize_system(self):
        """ì‹œìŠ¤í…œ ë¹„ë™ê¸° ì´ˆê¸°í™” - ìŠ¤ì™‘ ë§¤ë‹ˆì € ë° í—¤ë“œë“¤ì„ ì‚¬ì „ ì´ˆê¸°í™”"""
        if self.initialized:
            return
        
        logger.info("í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹œì‘...")
        
        # 1. ìŠ¤ì™‘ ë§¤ë‹ˆì € ë¨¼ì € ì´ˆê¸°í™”
        await self.swap_manager.initialize()
        logger.info("ë™ì  ìŠ¤ì™‘ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 2. í—¤ë“œ í˜¸í™˜ì„± ë§¤ë‹ˆì €ëŠ” ì´ë¯¸ UnifiedSystemOrchestratorì—ì„œ ì´ˆê¸°í™”ë¨
        # ì¤‘ë³µ í˜¸ì¶œ ì œê±° - HeadCompatibilityManager.initialize_all_heads()ëŠ” í•œ ë²ˆë§Œ í˜¸ì¶œë˜ì–´ì•¼ í•¨
        logger.info("í—¤ë“œ í˜¸í™˜ì„± ë§¤ë‹ˆì €ëŠ” ì´ë¯¸ ì´ˆê¸°í™”ë¨ - ê±´ë„ˆëœ€")
        
        # 3. ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆë“¤ì„ ìºì‹œ
        logger.info("ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆ ìºì‹± ì¤‘...")
        for head_type in self.head_configs.keys():
            try:
                real_head = await self._get_real_head_module(head_type)
                if real_head is not None:
                    self.cached_head_modules[head_type] = real_head
                    logger.info(f"í—¤ë“œ {head_type.value} ìºì‹± ì™„ë£Œ")
                else:
                    logger.warning(f"í—¤ë“œ {head_type.value} ìºì‹± ì‹¤íŒ¨: None ë°˜í™˜")
            except Exception as e:
                logger.error(f"í—¤ë“œ {head_type.value} ìºì‹± ì˜¤ë¥˜: {str(e)}")
        
        logger.info(f"í—¤ë“œ ëª¨ë“ˆ ìºì‹± ì™„ë£Œ: {len(self.cached_head_modules)}/{len(self.head_configs)}ê°œ")
        
        # 4. ëª¨ë“  í—¤ë“œê°€ ì„±ê³µì ìœ¼ë¡œ ìºì‹±ë˜ì—ˆëŠ”ì§€ ê²€ì¦ (fallback ì—†ìŒ)
        if len(self.cached_head_modules) != len(self.head_configs):
            missing_heads = [head_type.value for head_type in self.head_configs.keys() 
                           if head_type not in self.cached_head_modules]
            logger.error(f"í—¤ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: {missing_heads} í—¤ë“œë“¤ì„ ë¡œë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            logger.error("í”„ë¡œì íŠ¸ ê·œì¹™ì— ë”°ë¼ fallback ì²˜ë¦¬ëŠ” ê¸ˆì§€ë©ë‹ˆë‹¤. ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            raise RuntimeError(f"í•„ìˆ˜ í—¤ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: {missing_heads}. HeadCompatibilityManager í™•ì¸ í•„ìš”.")
        
        # 5. ê° í—¤ë“œì˜ PyTorch ëª¨ë“ˆ ê²€ì¦
        for head_type, head_module in self.cached_head_modules.items():
            if not isinstance(head_module, nn.Module):
                logger.error(f"í—¤ë“œ {head_type.value}ì´ ìœ íš¨í•œ PyTorch ëª¨ë“ˆì´ ì•„ë‹™ë‹ˆë‹¤: {type(head_module)}")
                raise RuntimeError(f"í—¤ë“œ {head_type.value} ëª¨ë“ˆ íƒ€ì… ì˜¤ë¥˜")
            
            # íŒŒë¼ë¯¸í„° ìˆ˜ ê²€ì¦
            param_count = sum(p.numel() for p in head_module.parameters())
            if param_count == 0:
                logger.error(f"í—¤ë“œ {head_type.value}ì— íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                raise RuntimeError(f"í—¤ë“œ {head_type.value} íŒŒë¼ë¯¸í„° ë¶€ì¬")
            
            logger.info(f"í—¤ë“œ {head_type.value} ê²€ì¦ ì™„ë£Œ: {param_count:,}ê°œ íŒŒë¼ë¯¸í„°")
        
        # 6. GPU ìºì‹œ ì •ë¦¬ - reserved ë©”ëª¨ë¦¬ í•´ì œ
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()  # CUDA 11+: ë¹ˆ ìºì‹œ ë¸”ë¡ì„ OSì— ë°˜í™˜
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê·¸
            from config import get_gpu_memory_info
            gpu_info = get_gpu_memory_info()
            if gpu_info:
                logger.info(f"[GPU MEM] í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” í›„ ìºì‹œ ì •ë¦¬:")
                logger.info(f"   allocated: {gpu_info['allocated_mb']:.1f}MB")
                logger.info(f"   reserved: {gpu_info['cached_mb']:.1f}MB")
                logger.info(f"   usage: {gpu_info['usage_percent']:.1f}%")
        
        # 7. ì´ˆê¸°í™” ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
        self.initialized = True
        logger.info("í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë“  í—¤ë“œ ê²€ì¦ë¨")
    
    
    def estimate_optim_mb(self) -> float:
        """ì˜µí‹°ë§ˆì´ì €ê°€ ì‚¬ìš©í•  ì˜ˆìƒ ë©”ëª¨ë¦¬ í¬ê¸° (MB)"""
        # CPU ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©ì‹œ GPU ë©”ëª¨ë¦¬ëŠ” 0
        if self.config.get('optimizer_device', 'cpu') == 'cpu':
            logger.info("ğŸ”§ CPU ì˜µí‹°ë§ˆì´ì € ì‚¬ìš© - GPU ë©”ëª¨ë¦¬ ìš”ì²­ 0MB")
            return 0
        
        # GPU ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©ì‹œ (í˜„ì¬ëŠ” ì‚¬ìš© ì•ˆí•¨)
        backbone_params = sum(p.numel() for p in self.unified_backbone.parameters())
        optimizer_memory_bytes = backbone_params * 2 * 4
        optimizer_memory_mb = optimizer_memory_bytes / (1024 * 1024)
        return optimizer_memory_mb * 1.2
    
    def _initialize_head_configs(self) -> Dict[HeadType, HeadTrainingConfig]:
        """í—¤ë“œë³„ í›ˆë ¨ ì„¤ì • ì´ˆê¸°í™”"""
        configs = {}
        
        # ê°ì • ê³µê° í—¤ë“œ
        configs[HeadType.EMOTION_EMPATHY] = HeadTrainingConfig(
            head_type=HeadType.EMOTION_EMPATHY,
            learning_rate=3e-4,
            weight_decay=1e-4,
            loss_weight=1.2,  # ê°ì • ì²˜ë¦¬ ì¤‘ìš”ë„ ë†’ìŒ
            warmup_steps=500,
            scheduler_type="cosine",
            dropout_rate=0.1
        )
        
        # ë²¤ë‹´-í”„ë¡¬ ìœ¤ë¦¬ í—¤ë“œ
        configs[HeadType.BENTHAM_FROMM] = HeadTrainingConfig(
            head_type=HeadType.BENTHAM_FROMM,
            learning_rate=2e-4,
            weight_decay=2e-4,
            loss_weight=1.5,  # ìœ¤ë¦¬ íŒë‹¨ ê°€ì¥ ì¤‘ìš”
            warmup_steps=800,
            scheduler_type="cosine",
            dropout_rate=0.15
        )
        
        # ì˜ë¯¸ SURD í—¤ë“œ
        configs[HeadType.SEMANTIC_SURD] = HeadTrainingConfig(
            head_type=HeadType.SEMANTIC_SURD,
            learning_rate=1e-4,
            weight_decay=1e-5,
            loss_weight=0.8,
            warmup_steps=1000,
            scheduler_type="linear",
            dropout_rate=0.05
        )
        
        # í›„íšŒ í•™ìŠµ í—¤ë“œ
        configs[HeadType.REGRET_LEARNING] = HeadTrainingConfig(
            head_type=HeadType.REGRET_LEARNING,
            learning_rate=2.5e-4,
            weight_decay=1e-4,
            loss_weight=1.0,
            warmup_steps=600,
            scheduler_type="cosine",
            dropout_rate=0.12
        )
        
        # ë©”íƒ€ í†µí•© í—¤ë“œ
        configs[HeadType.META_INTEGRATION] = HeadTrainingConfig(
            head_type=HeadType.META_INTEGRATION,
            learning_rate=1.5e-4,
            weight_decay=5e-5,
            loss_weight=1.1,
            freeze_until_epoch=2,  # ë‹¤ë¥¸ í—¤ë“œë“¤ì´ ì–´ëŠ ì •ë„ í•™ìŠµëœ í›„ ì‹œì‘
            warmup_steps=1200,
            scheduler_type="cosine",
            dropout_rate=0.08
        )
        
        return configs
    
    async def train_unified_system(self, train_data_loader, 
                                 validation_data_loader=None,
                                 num_epochs: int = 10,
                                 training_strategy: TrainingStrategy = TrainingStrategy.ROUND_ROBIN):
        """í†µí•© ì‹œìŠ¤í…œ í›ˆë ¨"""
        
        logger.info(f"í†µí•© í•™ìŠµ ì‹œì‘: {num_epochs} ì—í¬í¬, ì „ëµ: {training_strategy.value}")
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì²˜ìŒ í›ˆë ¨ì‹œì—ë§Œ)
        if not self.initialized:
            logger.info("ğŸ”„ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ. initialize_system() í˜¸ì¶œ")
            await self.initialize_system()
        else:
            logger.info("âœ… ì‹œìŠ¤í…œ ì´ë¯¸ ì´ˆê¸°í™”ë¨")
        
        # cached_head_modules ìƒíƒœ í™•ì¸
        logger.info(f"ğŸ” cached_head_modules ìƒíƒœ: {len(self.cached_head_modules)}ê°œ í—¤ë“œ")
        for head_type, module in self.cached_head_modules.items():
            logger.info(f"   - {head_type.value}: {type(module).__name__}")
        
        if len(self.cached_head_modules) == 0:
            logger.error("âŒ cached_head_modulesê°€ ë¹„ì–´ìˆìŒ! ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise RuntimeError("cached_head_modulesê°€ ë¹„ì–´ìˆìŒ - initialize_system() ì‹¤íŒ¨")
        
        # ì˜µí‹°ë§ˆì´ì € ì§€ì—° ì´ˆê¸°í™”
        if not self._optimizers_initialized:
            logger.info("ğŸ”§ ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì‹œì‘...")
            
            # ì˜µí‹°ë§ˆì´ì € ë©”ëª¨ë¦¬ ì²´í¬ (CPU ì‚¬ìš©ì‹œ GPU ìš”ì²­ ìƒëµ)
            estimated_mb = self.estimate_optim_mb()
            logger.info(f"   ì˜ˆìƒ ì˜µí‹°ë§ˆì´ì € ë©”ëª¨ë¦¬: {estimated_mb:.1f}MB")
            
            success = True  # CPU ì˜µí‹°ë§ˆì´ì €ëŠ” ê¸°ë³¸ ì„±ê³µ
            max_retries = 3
            retry_delay = 1.0
            
            # CPU ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©ì‹œ GPU ë©”ëª¨ë¦¬ ìš”ì²­ ìƒëµ
            if estimated_mb > 0:
                from workflow_aware_memory_manager import WorkflowAwareMemoryManager
                mem_manager = WorkflowAwareMemoryManager()
                
                # ì¬ì‹œë„ ë£¨í”„ ì¶”ê°€
                for retry in range(max_retries):
                    success = await mem_manager.request_gpu(
                        module_name="optimizer_bundle", 
                        required_mb=estimated_mb,
                        deps=[],
                        target_util=0.85,
                        must_succeed=False  # ì‹¤íŒ¨í•´ë„ CPUë¡œ ì§„í–‰
                    )
                    
                    if success:
                        logger.info("âœ… GPU memory allocated for optimizer")
                        break
                    else:
                        logger.warning(f"âš ï¸ GPU memory allocation failed (attempt {retry + 1}/{max_retries})")
                        
                        if retry < max_retries - 1:
                            # ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„
                            await asyncio.sleep(retry_delay * (2 ** retry))
                        else:
                            # ë§ˆì§€ë§‰ ì‹œë„: ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ í›„ ì¬ì‹œë„
                            logger.warning("ğŸ”„ Attempting with reduced batch size...")
                            original_batch_size = self.config.get('batch_size', 16)
                            reduced_batch_size = max(1, original_batch_size // 2)
                            
                            if reduced_batch_size < original_batch_size:
                                self.config['batch_size'] = reduced_batch_size
                                # ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ì¬ê³„ì‚° (ë°°ì¹˜ í¬ê¸°ì— ë¹„ë¡€)
                                estimated_mb = estimated_mb * 0.7  # ë°°ì¹˜ í¬ê¸° ê°ì†Œë¡œ ì•½ 30% ì ˆê°
                                logger.info(f"ğŸ“‰ Batch size reduced: {original_batch_size} â†’ {reduced_batch_size}")
                                logger.info(f"ğŸ“‰ Memory requirement reduced to: {estimated_mb:.2f}MB")
                                
                                # ìµœì¢… ì¬ì‹œë„
                                success = await mem_manager.request_gpu(
                                    module_name="optimizer_bundle", 
                                    required_mb=estimated_mb,
                                    deps=[],
                                    target_util=0.85
                                )
            
            if not success:
                raise RuntimeError(
                    f"Failed to allocate GPU memory for optimizer after {max_retries} attempts. "
                    f"Required: {estimated_mb:.2f}MB. Consider reducing model size or batch size."
                )
            
            # ì˜µí‹°ë§ˆì´ì € ë¹Œë“œ
            await self._build_optimizers()
            logger.info("âœ… ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì™„ë£Œ")
        
        self.is_training = True
        self.trainer.training_strategy = training_strategy
        
        try:
            for epoch in range(num_epochs):
                self.trainer.current_metrics.epoch = epoch
                self.scheduler.epoch_count = epoch
                
                # ì—í¬í¬ ì‹œì‘
                epoch_start_time = time.time()
                epoch_losses = []
                epoch_metrics = []
                
                logger.info(f"ì—í¬í¬ {epoch+1}/{num_epochs} ì‹œì‘")
                
                # ì—í¬í¬ìš© í™œì„± í—¤ë“œ ì´ˆê¸°í™”
                available_heads = list(self.head_configs.keys())
                epoch_active_heads = self.scheduler.get_active_heads(training_strategy, available_heads)
                
                # ë°°ì¹˜ë³„ í›ˆë ¨
                for batch_idx, batch_data in enumerate(train_data_loader):
                    if not self.is_training:
                        break
                    
                    # ë°°ì¹˜ë³„ í™œì„± í—¤ë“œ ì„ íƒ (ë™ì  ìŠ¤ì¼€ì¤„ë§ ì§€ì›)
                    active_heads = self.scheduler.get_active_heads(training_strategy, available_heads)
                    
                    # í—¤ë“œ ë¡œë”© (ìŠ¤ì™‘ ë§¤ë‹ˆì € ì‚¬ìš©)
                    await self._load_active_heads(active_heads)
                    
                    try:
                        # í›ˆë ¨ ìŠ¤í… ì‹¤í–‰
                        step_metrics = await self.trainer.train_step(batch_data, active_heads)
                        epoch_metrics.append(step_metrics)
                        epoch_losses.append(step_metrics.total_loss)
                        
                        # ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìŠ¤ì™‘ ì œì–´
                        total_steps = epoch * len(train_data_loader) + batch_idx
                        if total_steps % self.memory_check_interval == 0:
                            memory_ok = self.memory_monitor.check_memory_and_act(total_steps)
                            if not memory_ok:
                                logger.warning(f"ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ í•™ìŠµ ìŠ¤í… {total_steps} ì¼ì‹œ ì •ì§€")
                                # ê¸´ê¸‰ ì •ë¦¬ í›„ ì ì‹œ ëŒ€ê¸°
                                await asyncio.sleep(2)
                                continue
                        
                        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                        performance_metrics = {
                            'loss_stability': self._calculate_loss_stability(epoch_losses),
                            'synergy_gain': step_metrics.synergy_gain,
                            'convergence_rate': self._calculate_convergence_rate(epoch_metrics)
                        }
                        self.scheduler.update_phase(performance_metrics)
                        
                        # ë¡œê¹… (ë§¤ 100 ìŠ¤í…ë§ˆë‹¤)
                        if batch_idx % 100 == 0:
                            await self._log_training_progress(step_metrics, batch_idx, active_heads)
                        
                    except RuntimeError as e:
                        if "out of memory" in str(e).lower():
                            logger.error("GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - ë°°ì¹˜ í¬ê¸° ì¡°ì •")
                            self.trainer.adaptive_batch_manager.report_oom()
                            torch.cuda.empty_cache()
                            gc.collect()
                            continue
                        else:
                            raise
                
                # ì—í¬í¬ ì™„ë£Œ ì²˜ë¦¬
                epoch_time = time.time() - epoch_start_time
                await self._complete_epoch(epoch, epoch_metrics, epoch_time)
                
                # ê²€ì¦ (ì„ íƒì )
                if validation_data_loader is not None:
                    # ê²€ì¦ì„ ìœ„í•œ í—¤ë“œ ì„ íƒ (ì—í¬í¬ ì „ì²´ì—ì„œ ì‚¬ìš©ëœ í—¤ë“œë“¤)
                    await self._validate_model(validation_data_loader, epoch_active_heads)
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if (epoch + 1) % 2 == 0:  # 2 ì—í¬í¬ë§ˆë‹¤ ì €ì¥
                    await self._save_checkpoint(epoch)
        
        except Exception as e:
            logger.error(f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            tb_str = traceback.format_exc()
            logger.error(f"ğŸ˜¨ ìƒì„¸ ì—ëŸ¬ ìŠ¤íƒ:\n{tb_str}")
            logger.error(f"ğŸ” ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ğŸ“ ì—íŸ¼í¬: {epoch+1}/{num_epochs}, ë°°ì¹˜: {batch_idx if 'batch_idx' in locals() else 'N/A'}")
            raise
        
        finally:
            self.is_training = False
            logger.info("í†µí•© í•™ìŠµ ì™„ë£Œ")
    
    async def _load_active_heads(self, active_heads: List[HeadType]):
        """í™œì„± í—¤ë“œë“¤ ë¡œë”©"""
        logger.info(f"ğŸ”¥ _load_active_heads ì‹œì‘ - {len(active_heads)}ê°œ í—¤ë“œ ë¡œë”© ì˜ˆì •")
        
        # ë””ë²„ê·¸: DSMì— ë“±ë¡ëœ í‚¤ í™•ì¸
        logger.info(f"[G9 DEBUG] DSM keys: {sorted(list(self.swap_manager.models.keys()))[:20]}...")
        
        for head_type in active_heads:
            # ìŠ¤ì™‘ ë§¤ë‹ˆì €ë¥¼ í†µí•´ í—¤ë“œ ë¡œë”©
            logger.info(f"ğŸ”„ {head_type.value} í—¤ë“œë¥¼ GPUë¡œ ë¡œë”© ì‹œë„...")
            await self.swap_manager.load_head_to_gpu(head_type.value)
            
            # ì˜µí‹°ë§ˆì´ì € ë“±ë¡ (ì²˜ìŒ ë¡œë”©ì‹œ)
            if head_type.value not in self.trainer.optimizers:
                # HeadCompatibilityManagerë¥¼ í†µí•´ ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
                real_head = await self._get_real_head_module(head_type)
                if real_head is not None:
                    self.trainer.add_head_optimizer(head_type, real_head)
                else:
                    logger.warning(f"í—¤ë“œ {head_type.value}ì˜ ì‹¤ì œ ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤")
    
    async def _get_real_head_module(self, head_type: HeadType) -> Optional[nn.Module]:
        """HeadCompatibilityManagerë¥¼ í†µí•´ í—¤ë“œ ì–´ëŒ‘í„° ê°€ì ¸ì˜¤ê¸°"""
        try:
            # í—¤ë“œ ì–´ëŒ‘í„° ê°€ì ¸ì˜¤ê¸°
            head_adapter = self.head_compatibility_manager.head_adapters.get(head_type)
            if head_adapter is None:
                logger.error(f"í—¤ë“œ ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {head_type.value}")
                return None
            
            # í—¤ë“œ ì–´ëŒ‘í„° ì´ˆê¸°í™” í™•ì¸
            if not head_adapter.initialized:
                await head_adapter.initialize_head()
            
            # í—¤ë“œ ì–´ëŒ‘í„°ê°€ forward ë©”ì„œë“œë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
            if hasattr(head_adapter, 'forward'):
                logger.info(f"í—¤ë“œ ì–´ëŒ‘í„° ì „ì²´ ë°˜í™˜: {head_type.value}")
                return head_adapter  # í—¤ë“œ ì–´ëŒ‘í„° ì „ì²´ë¥¼ ë°˜í™˜ (input_adapter í¬í•¨)
            else:
                # forward ë©”ì„œë“œê°€ ì—†ìœ¼ë©´ PyTorch ë„¤íŠ¸ì›Œí¬ë§Œ ë°˜í™˜
                pytorch_network = None
                if hasattr(head_adapter, 'get_pytorch_network'):
                    pytorch_network = head_adapter.get_pytorch_network()
                
                if pytorch_network is not None:
                    logger.info(f"PyTorch ë„¤íŠ¸ì›Œí¬ë§Œ ë°˜í™˜: {head_type.value}")
                    return pytorch_network
                else:
                    logger.warning(f"í—¤ë“œ {head_type.value}ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return None
                
        except Exception as e:
            logger.error(f"í—¤ë“œ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {head_type.value}: {str(e)}")
            return None
    
    def _calculate_loss_stability(self, recent_losses: List[float], window_size: int = 50) -> float:
        """ì†ì‹¤ ì•ˆì •ì„± ê³„ì‚°"""
        if len(recent_losses) < window_size:
            return 0.0
        
        recent_window = recent_losses[-window_size:]
        std_dev = np.std(recent_window)
        mean_loss = np.mean(recent_window)
        
        # í‘œì¤€í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ì•ˆì •ì 
        stability = 1.0 / (1.0 + std_dev / max(0.001, mean_loss))
        return min(1.0, stability)
    
    def _calculate_convergence_rate(self, metrics_history: List[TrainingMetrics]) -> float:
        """ìˆ˜ë ´ ì†ë„ ê³„ì‚°"""
        if len(metrics_history) < 10:
            return 0.0
        
        recent_losses = [m.total_loss for m in metrics_history[-10:]]
        
        # ì†ì‹¤ ê°ì†Œ ì¶”ì„¸ ë¶„ì„
        if len(recent_losses) >= 2:
            loss_trend = recent_losses[-1] - recent_losses[0]
            if loss_trend < 0:  # ì†ì‹¤ ê°ì†Œ ì¤‘
                convergence = min(1.0, abs(loss_trend) / recent_losses[0])
            else:
                convergence = 0.0
        else:
            convergence = 0.0
        
        return convergence
    
    async def _log_training_progress(self, metrics: TrainingMetrics, 
                                   batch_idx: int, active_heads: List[HeadType]):
        """í›ˆë ¨ ì§„í–‰ìƒí™© ë¡œê¹…"""
        
        head_names = [h.value for h in active_heads]
        memory_info = self.trainer.memory_monitor.get_memory_usage()
        
        logger.info(
            f"ìŠ¤í… {metrics.step}, ë°°ì¹˜ {batch_idx}: "
            f"ì†ì‹¤={metrics.total_loss:.4f}, "
            f"í™œì„±í—¤ë“œ={head_names}, "
            f"GPU={memory_info['gpu_percent']:.1f}%, "
            f"ì²˜ë¦¬ì†ë„={metrics.samples_per_second:.1f} samples/s"
        )
        
        # í—¤ë“œë³„ ì†ì‹¤ ë¡œê¹…
        for head_name, loss in metrics.head_losses.items():
            logger.debug(f"  {head_name} ì†ì‹¤: {loss:.4f}")
    
    async def _complete_epoch(self, epoch: int, epoch_metrics: List[TrainingMetrics], 
                            epoch_time: float):
        """ì—í¬í¬ ì™„ë£Œ ì²˜ë¦¬"""
        
        # ì—í¬í¬ í†µê³„ ê³„ì‚°
        avg_loss = np.mean([m.total_loss for m in epoch_metrics])
        avg_memory = np.mean([m.memory_usage for m in epoch_metrics])
        total_samples = sum(m.samples_per_second * m.training_time for m in epoch_metrics)
        
        # íš¨ìœ¨ì„± ë©”íŠ¸ë¦­
        memory_efficiency = self.trainer.memory_monitor.get_memory_efficiency()
        
        epoch_stats = {
            'epoch': epoch,
            'avg_loss': avg_loss,
            'epoch_time': epoch_time,
            'total_samples': total_samples,
            'avg_memory_usage': avg_memory,
            'memory_efficiency': memory_efficiency,
            'learning_phase': self.scheduler.current_phase.value,
            'training_strategy': self.trainer.training_strategy.value
        }
        
        self.training_stats[f'epoch_{epoch}'] = epoch_stats
        
        logger.info(
            f"ì—í¬í¬ {epoch} ì™„ë£Œ: "
            f"í‰ê· ì†ì‹¤={avg_loss:.4f}, "
            f"ì‹œê°„={epoch_time:.1f}s, "
            f"ë©”ëª¨ë¦¬íš¨ìœ¨={memory_efficiency:.2%}, "
            f"í•™ìŠµë‹¨ê³„={self.scheduler.current_phase.value}"
        )
    
    async def _validate_model(self, validation_data_loader, active_heads: List[HeadType]):
        """ëª¨ë¸ ê²€ì¦"""
        
        logger.info("ëª¨ë¸ ê²€ì¦ ì‹œì‘...")
        
        self.trainer.eval()
        validation_losses = []
        
        try:
            with torch.no_grad():
                for batch_data in validation_data_loader:
                    # ê²€ì¦ ìˆœì „íŒŒ
                    outputs = self.trainer.forward_with_checkpointing(batch_data, active_heads)
                    
                    # ê²€ì¦ ì†ì‹¤ ê³„ì‚°
                    losses = self.trainer._calculate_losses(outputs, batch_data, active_heads)
                    total_loss = sum(losses.values()) / len(losses)
                    validation_losses.append(float(total_loss.item()))
        
        finally:
            self.trainer.train()  # í›ˆë ¨ ëª¨ë“œë¡œ ë³µì›
        
        avg_val_loss = np.mean(validation_losses)
        logger.info(f"ê²€ì¦ ì™„ë£Œ: í‰ê·  ê²€ì¦ ì†ì‹¤ = {avg_val_loss:.4f}")
        
        return avg_val_loss
    
    async def _save_checkpoint(self, epoch: int):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        
        checkpoint_path = os.path.join(self.checkpoint_dir, f"unified_model_epoch_{epoch}.pth")
        
        checkpoint = {
            'epoch': epoch,
            'unified_backbone_state_dict': self.unified_backbone.state_dict(),
            'optimizers_state_dict': {name: opt.state_dict() for name, opt in self.trainer.optimizers.items()},
            'schedulers_state_dict': {name: sch.state_dict() for name, sch in self.trainer.schedulers.items()},
            'training_stats': self.training_stats,
            'current_phase': self.scheduler.current_phase.value,
            'step_count': self.scheduler.step_count,
            'config': self.config
        }
        
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    def get_training_statistics(self) -> Dict[str, Any]:
        """í›ˆë ¨ í†µê³„ ë°˜í™˜"""
        
        current_metrics = self.trainer.current_metrics
        memory_stats = self.trainer.memory_monitor.get_memory_usage()
        
        stats = {
            'current_metrics': {
                'epoch': current_metrics.epoch,
                'step': current_metrics.step,
                'total_loss': current_metrics.total_loss,
                'head_losses': current_metrics.head_losses,
                'learning_rates': current_metrics.learning_rates,
                'samples_per_second': current_metrics.samples_per_second,
                'memory_efficiency': self.trainer.memory_monitor.get_memory_efficiency()
            },
            'memory_stats': memory_stats,
            'training_phase': self.scheduler.current_phase.value,
            'training_strategy': self.trainer.training_strategy.value,
            'batch_size': self.trainer.adaptive_batch_manager.current_batch_size,
            'epoch_stats': self.training_stats
        }
        
        return stats

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
async def example_usage():
    """í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ"""
    
    # ê°€ìƒì˜ ë°ì´í„° ë¡œë” ìƒì„±
    class DummyDataLoader:
        def __init__(self, num_batches=100):
            self.num_batches = num_batches
        
        def __iter__(self):
            for i in range(self.num_batches):
                yield {
                    'text': f'ìƒ˜í”Œ í…ìŠ¤íŠ¸ {i}',
                    'batch_size': 4,
                    'labels': torch.randint(0, 10, (4,))
                }
    
    # í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ìƒì„±
    learning_system = UnifiedLearningSystem()
    
    # ë°ì´í„° ë¡œë”
    train_loader = DummyDataLoader(num_batches=500)
    val_loader = DummyDataLoader(num_batches=50)
    
    print("=== í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    # í›ˆë ¨ ì‹¤í–‰
    await learning_system.train_unified_system(
        train_data_loader=train_loader,
        validation_data_loader=val_loader,
        num_epochs=3,
        training_strategy=TrainingStrategy.ROUND_ROBIN
    )
    
    # í›ˆë ¨ í†µê³„ ì¶œë ¥
    stats = learning_system.get_training_statistics()
    print(f"\n=== í›ˆë ¨ ì™„ë£Œ í†µê³„ ===")
    print(f"ìµœì¢… ì†ì‹¤: {stats['current_metrics']['total_loss']:.4f}")
    print(f"í›ˆë ¨ ë‹¨ê³„: {stats['training_phase']}")
    print(f"ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {stats['current_metrics']['memory_efficiency']:.2%}")
    print(f"ì²˜ë¦¬ ì†ë„: {stats['current_metrics']['samples_per_second']:.1f} samples/s")
    
    # í—¤ë“œë³„ ì†ì‹¤
    print(f"\n=== í—¤ë“œë³„ ì†ì‹¤ ===")
    for head_name, loss in stats['current_metrics']['head_losses'].items():
        print(f"{head_name}: {loss:.4f}")

if __name__ == "__main__":
    asyncio.run(example_usage())