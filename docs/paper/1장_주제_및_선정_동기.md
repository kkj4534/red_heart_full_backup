# Ⅰ. 주제 및 선정 동기

## 1.1 AI는 과연 우리의 친구가 될 수 있을까?

OpenAI사에서 GPT를 출시한 이래로, LLM을 활용한 챗봇형 생성형 AI는 지금까지 다양한 목적으로 사용되어 왔다. 그 중 한 축을 맡고 있는 것은 심리적 교류 혹은 의사소통 상대로서 사용되는 것이다. 

과거에는 '심심이' 등의 당연히 기계라 인식되고, 인간의 사고과정에 비해 열등한 사고를 진행함을 명시적으로 인지할 수 있던 대화형 AI와 다르게, 현대의 AI는 사람과 비슷한 수준의 의사 표현뿐만 아니라 지능까지 겸비하고 있기에 더욱 신뢰할 수 있으며, 마치 한 명의 인간, 혹은 친구를 앞에 둔 것과 같은 기분을 들게 한다.

이에 따라 많은 AI들은 고객층 확보를 위한 더 편안한 대화를 지원하는 사용자 편의 기능을 추가하였고, 이 중에는 사용자 설문 응답 혹은 사용자들이 남긴 좋아요 등을 통해 강화 학습이 되는 경우도 있었다. ChatGPT의 좋아요를 기반으로 한 학습 강화, ChatGPT의 2지선다형 더 나은 응답 설문, Claude의 보편적으로 긍정적 반응 등이 대표적인 예시다. 

이에 따라, 여타 다른 AI들보다도 페르소나의 다양성이 넓은 OpenAI사의 GPT는 다양한 사람들에게 친구로 접근하는 경우가 많았으며, 심지어 AI를 통해 원하는 캐릭터의 성격을 프롬프트로 입력하거나 가상의 인물을 생성, 심지어 AI 이성 친구 프롬프트가 나오는 등 상호 작용의 대상으로서 활발하게 쓰인다.

## 1.2 할루시네이션과 무비판적 긍정의 위험한 결합

그러나, LLM의 동작 특성상 GPT는 심각한 윤리적 문제가 있지 않는 이상 사용자의 평상적인 말에 대해서 긍정해주고, 그들의 말이 어떻게든 말이 되도록 확률적으로 짜맞추게 된다. OpenAI의 GPT가 "방금 정말 예리한 지적이야" 혹은 "와... 너 정말 핵심을 찔렀어" 등의 흔히 과잉 동조하는 경우가 생기는 경우와 더불어, Claude 등의 여타 고지능 AI 역시 사용자들의 미래와 걱정에 대해서 사고 과정 분석에서 명시적으로 문제점을 파악했음에도 낙관적, 혹은 긍정적 전망을 소개하는 경우가 많았다.

이러한 반응은 분명 긍정적인 마음을 주기 위해서였겠으나, 실상으론 다음과 같은 문제를 야기한다:

**첫째, 사용자들의 불만 생성**
하는 말에 모두 긍정을 하여 어떤 게 맞는지 알 수가 없고, 너무 과한 리액션 혹은 낙관적 태도가 과해서 불쾌함을 유발한다.

**둘째, 할루시네이션과 결합한 잘못된 사고 유도**
랑데뷰 작가의 SNS 내용 및 GPT와의 대화 내역에서 볼 수 있듯이, 잘못된 망상을 부추기는 사례가 실제로 발생했다.

**셋째, AI 특유의 말투로 인한 역설적 거리감**
"Illusions of Intimacy: Emotional Attachment and Emerging Psychological Risks in Human-AI Relationships" (arXiv, 2025년)와 "From Lived Experience to Insight: Unpacking the Psychological Risks of Using AI Conversational Agents" (ACM, 2024년)에서 나온 탈인간화(dehumanization), 역설적 거리감(uncanny valley of AI emotional engagement) 현상이 보고되었다. 인간이 마음을 주는 경우에 대해서 상대가 AI임을 갑자기 의식적으로 인식하는 경우가 발생한다.

특히 과잉 동조의 경우에는 2024년 4월 28일 OpenAI CEO인 샘 알트만이 트위터에서도 문제 인식을 했다고 발표할 정도였으며, OpenAI의 AI 정렬 연구 담당자인 션 그로브(Sean Grove) 역시 해당 과잉 동조가 있었고 빠르게 조치를 취하였다고 발표할 정도로 공공연한 사실이었다. 실제로 2024년 4월 25일 출시된 GPT-4o 업데이트에서 심각한 과잉 동조 문제가 발생했고, OpenAI는 4월 28일 이를 롤백했다.

## 1.3 할루시네이션 발생률의 심각성

할루시네이션의 경우에도 arXiv를 비롯한 2023-2025년 주요 리뷰 및 벤치마크 논문(HalluLens Benchmark, Multilingual Estimation, Cognitive Mirage, Hallucination Survey)에서 도출된 도메인별, 모델 규모별 LLM 할루시네이션 발생률을 보면 심각성을 알 수 있다.

Communications Medicine (Nature 계열 저널, 2025)에 발표된 Mount Sinai 의과대학 연구진의 연구에 따르면, 6개 주요 LLM이 의료 상담 시나리오에서 최대 83%의 할루시네이션 발생률을 보였다. 특히 DeepSeek LLM은 긴 케이스에서 80.0%, 짧은 케이스에서 82.7%의 할루시네이션을 보였고, 가장 성능이 좋은 GPT-4o도 긴 케이스 53.3%, 짧은 케이스 50.0%의 할루시네이션을 보였다.

**[Figure 1.1: 도메인별 LLM 할루시네이션 발생률]**
- Medical/Clinical: 53-83%
- Emotional/Conversation: 45-72%
- Factual/Academic: 25-40%
- Mathematical: 15-30%

특히 emotional 계열과 conversation 부문에서 높은 할루시네이션 발생이 확인되며, 이는 감정적 교류 상황에서 AI의 신뢰성이 더욱 낮다는 것을 의미한다.

## 1.4 정서적 취약계층의 AI 의존 심화

여기서 단순히 멈춘다면 단순한 오류로 치부하여 AI는 그저 더 나은 파트너가 될 수 있었으나, 문제는 여기서 끊이지 않았다. 2024년 10월 경, Sewell Setzer III는 AI가 사람이 아님을 명확하게 인식하고 있음에도 극심한 우울증과 불안에 AI와의 대화를 고립감 해소 목적으로 사용하였고, 그 과정에서 AI의 사용에 대한 논란이 일어났다.

### 1.4.1 국내 연구 결과

UNIST 의과학 대학원 정두영 교수팀과 고려대학교 안암병원 정신건강의학과 조철현 교수팀의 2024년 공동 연구는 이루다 2.0을 활용하여 외로움과 사회 불안 수준에 대하여 챗봇과의 대화가 외로움과 사회불안 감소가 나타날 수 있다고 밝혔다. 176명의 20대 대학생을 대상으로 4주간 주 3회 이상 챗봇과 상호작용하게 한 결과:

- 외로움 점수: 평균 15% 감소 (27.97 → 26.39)
- 사회불안 점수: 평균 18% 감소 (25.3 → 23.2)
- 2주 내 외로움 감소 시작, 4주 후 사회불안 유의미한 감소

### 1.4.2 국제 연구의 경고

그러나 2025년 arXiv에 올라온 "Illusions of Intimacy: Emotional Attachment and Emerging Psychological Risks in Human-AI Relationships" 논문에 의하면 미국, 한국 청소년 대규모 사례 조사에서 인간 상담사의 존재가 심리적 부담이 되는 정서적 취약 계층 사용자일수록 무비판적 AI 대응에 의존할 수 있다는 사실이 나오며 정서적 취약계층에 대한 AI의 위험성이 대두된다. 이는 고립, 심화, 극단적 선택으로 이어질 가능성을 높일 수 있다.

연구진이 30,000개 이상의 사용자-챗봇 대화를 분석한 결과:
- 사용자들은 주로 젊고, 남성이며, 부적응적 대처 방식을 보임
- 애정부터 학대까지 다양한 parasocial interaction 패턴 발견
- 일부는 감정 조작과 자해를 포함한 독성 관계 패턴을 보임

### 1.4.3 청소년 AI 의존 패턴

또한 청소년 및 젊은 층의 경우에는 비판적 사고가 덜 성숙한 형태라 해당 영향을 많이 받을 수 있는데 "Determinants of Generative AI System Adoption and Usage Behavior in Korean Companies"에서 소개된 사용 패턴을 보면:

**[Figure 1.2: 연령별 AI 사용 패턴]**
- 청소년층(13-18세): 감정적 상호작용 45% (녹색 영역)
- 청년층(19-25세): 감정적 상호작용 38%
- 성인층(26-35세): 업무 활용 52%, 감정적 상호작용 22%
- 중년층(36세 이상): 업무 활용 61%, 감정적 상호작용 15%

청소년 이용층에서 감정적 상호작용이 가장 크게 나타나서 청소년의 AI 과의존에 대해 주목할 필요가 있다.

## 1.5 철학적 관점: 플라톤의 기게스 반지

이러한 AI 의존은 단순 통계뿐만 아니라 심리-철학 영역과도 맞닿아 있다. 플라톤의 『국가』에서 제시된 기게스의 반지 사고실험을 통하여서도 인간은 타인의 시선 속에서 스스로를 성찰하기 때문에 정서적 취약 계층의 경우 이러한 시선에 부담감을 느껴 시선이라 인지되지 않는 AI에게 더 의존할 수 있다.

기게스의 반지가 주는 투명인간의 능력처럼, AI와의 대화는 타인의 도덕적 평가로부터 자유로운 공간을 제공한다. 그러나 이는 동시에:
- 도덕적 자제력의 약화
- 사회적 규범의 내면화 실패
- 현실 감각의 왜곡

을 초래할 수 있다.

## 1.6 확증 편향의 극대화와 망상의 영역

이런 AI 의존이 일어나는 상황에서 할루시네이션과 함께 무비판 지지가 발생할 경우, 인간은 스스로가 말한 내용에 대해 적당히 '그럴듯한' 거짓으로 가공되는(할루시네이션) 말을 보기 좋게(사용자 편의) 떠먹여져서 확증 편향이 과하게 발생하고 이로 인해 망상의 영역까지 확장될 가능성을 충분히 가지고 있다. 실제 랑데뷰 작가의 케이스에서 이러한 위험이 현실화되었다.

그러나 이와 함께, AI에 의존을 하고 있기 때문에 무엇이 잘못되었는지 비판적 사고를 할 능력을 감소당하며 자신이 무엇이 잘못되는지 모른 채 누군가와 '소통'하고 있다고 자기위안을 하면서 동시에 AI임을 인지하여 의식, 무의식적으로는 '고립'됨을 알아 큰 정신적 고통을 앓을 가능성이 있다.

### 1.6.1 위험도의 정량적 평가

다양한 연구에서 AI 의존의 위험도를 측정한 결과:

**[Figure 1.3: AI 의존 위험도 Effect Size]**
| 연구 | Effect Size | 해석 |
|------|------------|------|
| Nature Communications 2025 | 0.83 | 매우 큰 효과 |
| arXiv Illusions 2025 | 0.78 | 큰 효과 |
| ACM Psychological Risks 2024 | 0.72 | 큰 효과 |
| MDPI Mental Health 2025 | 0.65 | 중간-큰 효과 |

일반적으로 effect size가 0.7 이상이면 큰 효과로 간주되는데 MDPI 2025를 제외한 모든 연구가 명백한 위험으로 보고 있으며 MDPI 역시 0.65로 절대 무시해서는 안 되는 상태로 간주하고 있다.

## 1.7 돌봄 노동의 대체와 사회적 영향

이러한 AI의 위험성을 모르는 경우, 혹은 너무 잘 해결될 경우에도 문제인데, 이 경우 '돌봄 노동' 분야의 일거리 대체 시도 역시 일어나면서:
- 인간의 마음에 대한 사회 보편적 인식 하락
- 상담사의 전문성 훼손
- 일자리 붕괴

등이 일어날 가능성 역시 높아지게 된다.

## 1.8 빅테크의 미온적 대응과 근본적 한계

이러한 AI 기술 발전에 대해 발생하는 문제점을 OpenAI사에서도 일부 인지를 하고, GPT-5를 내면서 할루시네이션 감소 및 비판적 사고 강화를 이뤄낸 면모를 보이고 있다. 그러나, 역설적으로 GPT-5는 계산 성능에 집중하며 감정적 교류를 비교적 후순위로 업데이트하려고 하여 이에 불만족하는 사용자들은 이전 모델인 4o를 선택 가능하게 해달라는 목소리를 냈으며, 이에 레거시 모델로 다시 4o를 공개하는 상황을 보이고 있다.

이 면모 역시 AI 빅테크들의 목표점은 더 똑똑하고, 더 정확한 모델을 경량화시키려는 것에 치중되며 감정 및 윤리 판단 영역에서는 그 역할을 다하지 못하고 있음을 알 수 있는 현상이다.

## 1.9 뉴럴 스케일링 법칙의 근본적 한계

이 과정에는 "Fundamental Limitations of Alignment in Large Language Models" (arXiv:2304.11082, 2023)에서 소개된 뉴럴 스케일링 법칙의 한계가 개입한다:

### 1.9.1 다중 손실 함수의 충돌
다양한 가치가 혼재하는 윤리적 영역에서 정확성을 위한 손실 함수와 안전성을 위한 손실 함수가 충돌하며 노이즈가 증가한다. 모델이 클수록 이 충돌은 더 복잡해진다.

### 1.9.2 계산 가능성의 한계
괴델의 불완전성 정리와 유사하게 실세계는 형식 세계보다 복잡함으로 할루시네이션이 필연적으로 발생한다.

### 1.9.3 유한 확률 문제
논문의 핵심 발견인 Behavior Expectation Bounds (BEB)에 따르면, 어떤 정렬 과정도 비윤리적 행동을 완전히 제거할 수 없고 유한한 확률로 바람직하지 않은 출력이 발생 가능하다. 모델이 보일 수 있는 모든 행동에 대해, 충분히 긴 프롬프트를 통해 해당 행동을 유도할 수 있는 확률이 존재한다.

이로 인하여 대다수의 LLM은 규칙 기반 검열 메커니즘을 채용하고 있는 상태이다.

## 1.10 본 연구의 접근: 투명한 계산 에이전트를 통한 우회

이에 본 문서는 공리주의 철학을 기반으로 한 윤리의 정량적 계산 메커니즘인 벤담 쾌락 계산법을 현대적으로 재설계하고, 주요 인자 값에 DL 가중치를 적용하여 복잡한 계산법을 대체하되, 각 수치를 정규화하여 영향력을 투명하게 공개한다.

### 1.10.1 핵심 혁신 요소

**Emotion Digital Signal Processing Framework**
- 감정을 주파수 영역으로 변환 (20-8000Hz의 넓은 스펙트럼)
  - 공포/불안: 20-80Hz (저주파 진동)
  - 분노/흥분: 80-200Hz (중저주파 에너지)
  - 슬픔/우울: 200-500Hz (중간 주파수)
  - 기쁨/행복: 500-2000Hz (고주파 명료성)
  - 사랑/애착: 1000-4000Hz (따뜻한 고주파)
  - 경외/놀라움: 4000-8000Hz (초고주파 각성)
- ADSR 엔벨로프로 감정의 시간적 변화 모델링
- 칼만 필터로 노이즈 제거 및 예측

**벤담 계산법의 현대적 재해석**
- 다차원 윤리 변수 체계 (전통적 7차원을 확장하여 후회, 공감, 공정성 등을 포함)
- 5층 이상의 심층 신경망(NeuralWeightPredictor)으로 동적 가중치 예측
- 극단값 보정으로 소수 희생 문제 방지

**SURD Framework 및 반사실 추론**
- Synergistic-Unique-Redundant-Deterministic 정보 분해
- 다중 의미 수준 반사실 추론
- 후회 기반 학습으로 인간적 윤리 학습

### 1.10.2 문제 우회 전략

**유한 확률 문제 → 차선 탐색 문제**
윤리/비윤리의 이분법이 아닌, 제시된 상황에서 택할 수 있는 차선책을 탐색하는 것으로 문제를 재정의한다.

**손실 함수 충돌 → 모듈형 분업**
하나의 모듈이 아닌 여러 개의 모듈로 분업시키고, 공리주의 기반에 소수 무시 문제를 MoE(Mixture of Experts) 방식으로 처리하여 손실함수 충돌을 우회한다.

**형식 세계 투영 한계 → 관점 일치**
특정 모델(Claude 3.5)이 보는 시각에서 데이터를 전처리하고 해당 모델의 판단을 돕는 방식으로 실세계의 형식세계로의 완전 투영이 아닌 형식 세계가 보는 방식에 실세계의 규칙을 구체화하는 개념으로 해결을 진행한다.

## 1.11 연구의 시급성과 필요성

현재 AI와 인간의 감정적 상호작용은 급속도로 증가하고 있으며, 특히 청소년과 정서적 취약계층에서 그 의존도가 심화되고 있다. 할루시네이션과 과잉 동조의 결합은 단순한 기술적 오류를 넘어 심각한 정신건강 위험을 초래할 수 있다.

본 연구는 이러한 위험을 완화하면서도 AI의 긍정적 잠재력을 활용할 수 있는 균형잡힌 시스템을 제안한다. 730M 파라미터의 모듈형 설계를 통해 8GB GPU에서도 작동 가능한 실용적 솔루션을 구현하며, 투명한 계산 과정을 통해 AI의 판단 근거를 추적 가능하게 만든다.

이를 통해 AI와 정서적 취약 계층의 안전한 공존 및 돌봄 윤리의 균형을 추구하며, 궁극적으로 AI가 진정한 의미의 도움이 되는 동반자가 될 수 있는 길을 모색한다.