## 3. 연구 방법 및 내용

### 3.1 시스템 아키텍처와 설계 철학

#### 3.1.1 730M 파라미터 분산 설계의 필요성

본 연구의 핵심은 단일 거대 모델이 아닌 13개의 전문화된 모듈로 730M 파라미터를 정밀하게 분산시킨 것이다. 이러한 설계 결정은 단순한 기술적 선택이 아니라 LLM의 근본적 한계를 극복하기 위한 철학적 접근이다.

**왜 모듈형 설계인가?**

첫째, 투명성의 확보다. 거대 단일 모델은 내부 처리 과정이 블랙박스처럼 작동하여 어떤 근거로 윤리적 판단을 내렸는지 알 수 없다. 의료 AI가 "이 환자는 위험합니다"라고 판단했을 때, 의사는 그 근거를 알아야 신뢰할 수 있듯이, 윤리적 판단 역시 그 과정이 투명해야 한다. 우리의 13개 모듈은 각각 명확한 역할을 가지며, 감정 분석 → 윤리 계산 → 후회 예측 → 인과 분해의 과정을 추적할 수 있다.

둘째, 8GB VRAM이라는 현실적 제약의 극복이다. 대부분의 연구실과 개인이 보유한 GPU는 8GB 정도의 메모리를 갖는다. 730M 전체를 한 번에 로드하면 메모리 부족이 발생하지만, 모듈별로 순차 로드하면 안정적으로 작동한다. 이는 민주화된 AI를 위한 실용적 선택이다.

셋째, 모듈별 최적화의 가능성이다. 우리가 개발한 Sweet Spot Detection 기법은 각 모듈이 과적합되기 직전의 최적 시점을 자동으로 찾는다. 실제로 EmotionHead는 14 에폭에서, RegretHead는 12 에폭에서 최적 성능을 보였다. Parameter Crossover 기법으로 이러한 서로 다른 시점의 최적 파라미터를 조합할 수 있다.

#### 3.1.2 파라미터 분포의 전략적 설계

730M 파라미터를 어떻게 분배할 것인가는 시스템 성능을 좌우하는 핵심 결정이었다. 우리는 다음과 같은 원칙으로 분배했다:

**1. Unified Backbone (90.6M, 12.4%)**
백본은 모든 태스크가 공유하는 기본 표현을 학습한다. 너무 크면 메모리를 낭비하고, 너무 작으면 충분한 표현력을 갖지 못한다. 우리는 실험을 통해 90.6M이 최적임을 발견했다. 이는 BERT-base(110M)보다 작지만, 8층 트랜스포머와 14개 어텐션 헤드로 효율적인 문맥 이해를 달성한다.

**2. 전문 헤드 모듈 (153M, 21.0%)**
4개 헤드(Emotion, Bentham, Regret, SURD)가 각각 38.3M씩 균등하게 배분받았다. 이는 어떤 태스크도 우선시하지 않고 균형잡힌 성능을 추구하기 위함이다. 각 헤드는 백본의 출력을 받아 자신의 전문 영역으로 변환한다.

**3. Neural Analyzers (368M, 50.4%)**
전체의 절반을 차지하는 가장 큰 부분이다. 이는 심층 분석이 시스템의 핵심이기 때문이다:
- NeuralEmotionAnalyzer (68M): 3단계 위계(개인-타자-공동체)로 감정을 분석
- NeuralBenthamCalculator (61M): 10차원 윤리 변수를 6층 레이어로 재가중
- NeuralRegretAnalyzer (68M): 7개 분기점 × 3개 시나리오 = 21회 계산
- NeuralSURDAnalyzer (35M): 정보를 4가지 유형으로 분해

이러한 분배는 복잡한 윤리적 추론이 단순한 분류보다 훨씬 많은 파라미터를 필요로 한다는 인식에 기반한다.

**4. Advanced Wrappers (112M, 15.3%)**
Neural Analyzer를 실용적으로 만드는 래퍼 레이어다. GPU 가속, 배치 처리, 캐싱 등 공학적 최적화를 담당한다. 이는 학술적 완성도와 실용성 사이의 가교 역할을 한다.

**5. Phase Networks (4.3M, 0.6%)**
작지만 중요한 부분이다. 개인-타자-공동체라는 3단계 위계를 통합하는 역할을 한다. 크기는 작지만 전체 시스템의 일관성을 유지하는 핵심이다.

**6. EmotionDSP & Kalman (2.3M, 0.3%)**
가장 작은 모듈이지만 독창적인 접근법을 구현한다. 감정을 주파수 영역에서 처리하고 칼만 필터로 노이즈를 제거한다. 작은 크기로도 효과적인 이유는 수학적 모델링에 기반하기 때문이다.

#### 3.1.3 5단계 처리 워크플로우의 설계 근거

시스템은 입력부터 출력까지 5단계를 거친다. 각 단계는 명확한 목적을 가지며, 이전 단계의 출력이 다음 단계의 입력이 된다:

**1단계: 입력 처리 및 임베딩 생성**
왜 첫 단계에서 Claude API를 사용하는가? 이는 데이터 전처리의 일관성을 위해서다. 동일한 모델(Claude)로 전처리하면 학습과 추론 시 분포 차이가 줄어든다. Prompt Caching으로 90% 비용을 절감한 것은 실용성을 위한 공학적 선택이다.

**2단계: 백본 네트워크 인코딩**
백본이 왜 필요한가? 4개 헤드가 각자 처음부터 학습하면 중복이 발생한다. 백본은 공통 표현을 학습하여 효율성을 높인다. 896차원이라는 특이한 숫자는 7(감정 수) × 128(기본 단위)로, 감정 처리에 최적화된 설계다.

**3단계: 전문 헤드 모듈 병렬 처리**
왜 순차가 아닌 병렬인가? 감정, 윤리, 후회, 인과관계는 독립적으로 처리될 수 있고, 병렬 처리로 속도를 4배 향상시킬 수 있다. ModuleBridgeCoordinator가 비동기 처리를 관리하여 효율성을 극대화한다.

**4단계: Neural Analyzer 심층 분석**
헤드의 초기 분석을 왜 다시 심층 분석하는가? 헤드는 빠른 초기 판단을, Analyzer는 정밀한 재분석을 담당한다. 이는 인간의 System 1(빠른 직관)과 System 2(느린 숙고)를 모방한 설계다.

**5단계: Advanced Wrapper 통합 및 최종 출력**
왜 마지막에 Wrapper가 필요한가? 각 모듈의 출력은 형식이 다르다. Wrapper는 이를 통일된 형식으로 변환하고, 모순을 검사하며, 최종 신뢰도를 계산한다.

---

이제 3.1절을 이렇게 작성했습니다. 코드 없이 설명 중심으로, 각 설계 결정의 "왜"를 명확히 했습니다. 

이 방향으로 계속 진행할까요? 아니면 수정이 필요한 부분이 있나요?