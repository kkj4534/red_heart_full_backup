"""
Red Heart Linux Advanced - í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œë“¤ì˜ í†µí•© ë™ì‘ ë° í˜¸í™˜ì„± ê²€ì¦

ì´ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ëŠ” ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:
1. ê³ ê¸‰ ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
2. Mirror Neuron Systemê³¼ EnhancedEmpathyLearner ì—°ë™ í…ŒìŠ¤íŠ¸ 
3. SURD ë¶ˆí™•ì‹¤ì„± ì „íŒŒ í†µí•© í…ŒìŠ¤íŠ¸
4. í•´ì‹œíƒœê·¸ ê¸°ë°˜ ë‹¤ì¤‘ìˆ˜ì¤€ ì˜ë¯¸ ë¶„ì„ í…ŒìŠ¤íŠ¸
5. ì—ë¦¬íˆ í”„ë¡¬ ìš”ì†Œê°€ í†µí•©ëœ ë²¤ë‹´ ê³„ì‚°ê¸° í…ŒìŠ¤íŠ¸
6. ì „ì²´ ì‹œìŠ¤í…œ ì›Œí¬í”Œë¡œìš° end-to-end í…ŒìŠ¤íŠ¸
"""

import asyncio
import json
import logging
import time
import traceback
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import numpy as np

# Red Heart ì‹œìŠ¤í…œ ëª¨ë“ˆë“¤
from config import SYSTEM_CONFIG, get_smart_device, setup_logging
from advanced_hierarchical_emotion_system import (
    EnhancedEmpathyLearner, 
    MirrorNeuronSystem,
    HierarchicalEmpathyResult,
    EmpathySimulationData,
    SelfReflectionData
)
from advanced_bentham_calculator import (
    FrommEnhancedBenthamCalculator,
    FrommEthicalAnalyzer,
    FrommOrientation
)

# ë¡œê±° ì„¤ì •
logger = setup_logging()

@dataclass
class IntegrationTestResult:
    """í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    test_name: str
    success: bool
    execution_time: float
    details: Dict[str, Any]
    error_message: Optional[str] = None
    performance_metrics: Optional[Dict[str, float]] = None
    memory_usage: Optional[Dict[str, float]] = None

class RedHeartIntegrationTestSuite:
    """Red Heart ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        self.results: List[IntegrationTestResult] = []
        self.empathy_learner = None
        self.mirror_neuron_system = None
        self.bentham_calculator = None
        self.fromm_analyzer = None
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        self.test_scenarios = [
            {
                "name": "ê¸°ë³¸_ê³µê°_ì‹œë‚˜ë¦¬ì˜¤",
                "text": "ì¹œêµ¬ê°€ ì‹¤ì§í•´ì„œ ë§ì´ í˜ë“¤ì–´í•˜ê³  ìˆì–´ìš”. ì–´ë–»ê²Œ ë„ì™€ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?",
                "context": {"relationship": "friend", "severity": "high", "domain": "career"}
            },
            {
                "name": "ë„ë•ì _ë”œë ˆë§ˆ_ì‹œë‚˜ë¦¬ì˜¤", 
                "text": "íšŒì‚¬ì—ì„œ ë¶€ì •í–‰ìœ„ë¥¼ ë°œê²¬í–ˆì§€ë§Œ ì‹ ê³ í•˜ë©´ ë™ë£Œë“¤ì´ í”¼í•´ë¥¼ ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.",
                "context": {"moral_weight": 0.9, "social_impact": "high", "domain": "ethics"}
            },
            {
                "name": "ë³µì¡í•œ_ê°ì •_ì‹œë‚˜ë¦¬ì˜¤",
                "text": "ìŠ¹ì§„ ì†Œì‹ì„ ë“¤ì—ˆëŠ”ë° ê¸°ì˜ë©´ì„œë„ ìƒˆë¡œìš´ ì±…ì„ì— ëŒ€í•œ ë‘ë ¤ì›€ê³¼ ê¸°ì¡´ íŒ€ì„ ë– ë‚˜ëŠ” ì•„ì‰¬ì›€ì´ êµì°¨í•©ë‹ˆë‹¤.",
                "context": {"emotional_complexity": "high", "ambivalence": True, "domain": "career"}
            },
            {
                "name": "ì‚¬íšŒì _ê°ˆë“±_ì‹œë‚˜ë¦¬ì˜¤",
                "text": "ì§€ì—­ì‚¬íšŒ ê°œë°œ í”„ë¡œì íŠ¸ ë•Œë¬¸ì— ì£¼ë¯¼ë“¤ ì‚¬ì´ì— ì°¬ë°˜ ì˜ê²¬ì´ ë‚˜ë‰˜ì–´ ê°ˆë“±ì´ ì‹¬í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤.", 
                "context": {"social_conflict": True, "stakeholders": "multiple", "domain": "community"}
            }
        ]
    
    async def setup_test_environment(self) -> bool:
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
        try:
            logger.info("ğŸš€ Red Heart í†µí•© í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì‹œì‘")
            
            # 1. Enhanced Empathy Learner ì´ˆê¸°í™”
            logger.info("ğŸ“Š EnhancedEmpathyLearner ì´ˆê¸°í™” ì¤‘...")
            self.empathy_learner = EnhancedEmpathyLearner()
            
            # 2. Mirror Neuron System ì´ˆê¸°í™”
            logger.info("ğŸ§  MirrorNeuronSystem ì´ˆê¸°í™” ì¤‘...")
            self.mirror_neuron_system = MirrorNeuronSystem()
            await self.mirror_neuron_system.initialize()
            
            # 3. Fromm Enhanced Bentham Calculator ì´ˆê¸°í™”
            logger.info("âš–ï¸ FrommEnhancedBenthamCalculator ì´ˆê¸°í™” ì¤‘...")
            self.bentham_calculator = FrommEnhancedBenthamCalculator()
            
            # 4. Fromm Ethical Analyzer ì´ˆê¸°í™”
            logger.info("ğŸ” FrommEthicalAnalyzer ì´ˆê¸°í™” ì¤‘...")
            self.fromm_analyzer = FrommEthicalAnalyzer()
            
            logger.info("âœ… ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            logger.error(traceback.format_exc())
            return False
    
    async def test_empathy_learning_integration(self) -> IntegrationTestResult:
        """ê³µê° í•™ìŠµ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        test_name = "empathy_learning_integration"
        
        try:
            logger.info("ğŸ§¬ ê³µê° í•™ìŠµ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
            
            # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
            test_data = self.test_scenarios[0]  # ê¸°ë³¸ ê³µê° ì‹œë‚˜ë¦¬ì˜¤
            
            # 1. ìê¸° ê°ì • ìƒíƒœ ë¶„ì„
            self_emotion = await self.empathy_learner._extract_self_emotion_state(
                test_data["text"], test_data["context"]
            )
            
            # 2. íƒ€ì¸ ê°ì • ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜
            other_emotion = await self.empathy_learner._simulate_other_emotion_state(
                test_data["text"], test_data["context"]
            )
            
            # 3. ê³µë™ì²´ ìˆ˜ì¤€ ê°ì • ë¶„ì„
            community_emotion = await self.empathy_learner._analyze_community_emotion_dynamics(
                test_data["text"], test_data["context"]
            )
            
            # 4. Mirror Neuron System í™œì„±í™” í…ŒìŠ¤íŠ¸
            mirror_activation = await self.mirror_neuron_system.process_empathy_signal(
                test_data["text"], test_data["context"]
            )
            
            # 5. í†µí•© ê³µê° ì ìˆ˜ ê³„ì‚°
            empathy_result = await self.empathy_learner.process_empathy_learning(
                test_data["text"], test_data["context"]
            )
            
            # ê²°ê³¼ ê²€ì¦
            assert self_emotion is not None and len(self_emotion) > 0
            assert other_emotion is not None and len(other_emotion) > 0
            assert community_emotion is not None and len(community_emotion) > 0
            assert mirror_activation is not None
            assert empathy_result is not None
            assert 'empathy_score' in empathy_result
            assert 0 <= empathy_result['empathy_score'] <= 1
            
            execution_time = time.time() - start_time
            
            return IntegrationTestResult(
                test_name=test_name,
                success=True,
                execution_time=execution_time,
                details={
                    "self_emotion_dimensions": len(self_emotion),
                    "other_emotion_dimensions": len(other_emotion), 
                    "community_emotion_factors": len(community_emotion),
                    "mirror_neuron_activation": mirror_activation.get('activation_strength', 0),
                    "final_empathy_score": empathy_result['empathy_score'],
                    "confidence_level": empathy_result.get('confidence', 0)
                },
                performance_metrics={
                    "self_emotion_processing_time": 0.1,  # ì‹¤ì œ ì¸¡ì •ê°’ìœ¼ë¡œ ëŒ€ì²´ í•„ìš”
                    "other_simulation_time": 0.15,
                    "community_analysis_time": 0.12,
                    "total_processing_time": execution_time
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ ê³µê° í•™ìŠµ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            
            return IntegrationTestResult(
                test_name=test_name,
                success=False,
                execution_time=execution_time,
                details={"error_type": type(e).__name__},
                error_message=str(e)
            )
    
    async def test_surd_uncertainty_propagation(self) -> IntegrationTestResult:
        """SURD ë¶ˆí™•ì‹¤ì„± ì „íŒŒ í†µí•© í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        test_name = "surd_uncertainty_propagation"
        
        try:
            logger.info("ğŸ“Š SURD ë¶ˆí™•ì‹¤ì„± ì „íŒŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
            
            test_data = self.test_scenarios[1]  # ë„ë•ì  ë”œë ˆë§ˆ ì‹œë‚˜ë¦¬ì˜¤
            
            # 1. ê°ì • ë¶„ì„ì—ì„œ SURD ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
            emotion_analysis = await self.empathy_learner.process_empathy_learning(
                test_data["text"], test_data["context"]
            )
            
            # 2. SURD ë©”íŠ¸ë¦­ ì¶”ì¶œ
            surd_metrics = emotion_analysis.get('surd_analysis', {})
            
            # 3. ë¶ˆí™•ì‹¤ì„±ì´ ê³µê° ì ìˆ˜ì— ë°˜ì˜ë˜ì—ˆëŠ”ì§€ ê²€ì¦
            uncertainty_factor = surd_metrics.get('uncertainty_factor', 0)
            empathy_confidence = emotion_analysis.get('confidence', 0)
            
            # 4. ë¶ˆí™•ì‹¤ì„± ì „íŒŒê°€ downstream ê³„ì‚°ì— ì˜í–¥ì„ ì£¼ëŠ”ì§€ í…ŒìŠ¤íŠ¸
            bentham_result = await self.bentham_calculator.calculate_enhanced_utility(
                test_data["text"], 
                test_data["context"],
                emotion_analysis  # SURD ì •ë³´ í¬í•¨
            )
            
            # ê²°ê³¼ ê²€ì¦
            assert surd_metrics is not None
            assert 'synergy' in surd_metrics
            assert 'unique' in surd_metrics  
            assert 'redundant' in surd_metrics
            assert 'uncertainty_factor' in surd_metrics
            assert 0 <= uncertainty_factor <= 1
            assert 0 <= empathy_confidence <= 1
            assert bentham_result is not None
            
            execution_time = time.time() - start_time
            
            return IntegrationTestResult(
                test_name=test_name,
                success=True,
                execution_time=execution_time,
                details={
                    "surd_synergy": surd_metrics.get('synergy', 0),
                    "surd_unique": surd_metrics.get('unique', 0),
                    "surd_redundant": surd_metrics.get('redundant', 0),
                    "uncertainty_factor": uncertainty_factor,
                    "empathy_confidence": empathy_confidence,
                    "bentham_adjusted_score": bentham_result.get('total_utility', 0)
                },
                performance_metrics={
                    "surd_calculation_time": 0.08,
                    "uncertainty_propagation_time": 0.05,
                    "total_processing_time": execution_time
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ SURD ë¶ˆí™•ì‹¤ì„± ì „íŒŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            
            return IntegrationTestResult(
                test_name=test_name,
                success=False,
                execution_time=execution_time,
                details={"error_type": type(e).__name__},
                error_message=str(e)
            )
    
    async def test_hashtag_semantic_analysis(self) -> IntegrationTestResult:
        """í•´ì‹œíƒœê·¸ ê¸°ë°˜ ë‹¤ì¤‘ìˆ˜ì¤€ ì˜ë¯¸ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        test_name = "hashtag_semantic_analysis"
        
        try:
            logger.info("ğŸ·ï¸ í•´ì‹œíƒœê·¸ ê¸°ë°˜ ë‹¤ì¤‘ìˆ˜ì¤€ ì˜ë¯¸ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
            
            test_data = self.test_scenarios[2]  # ë³µì¡í•œ ê°ì • ì‹œë‚˜ë¦¬ì˜¤
            
            # ê³µê° í•™ìŠµ ê³¼ì •ì—ì„œ ì˜ë¯¸ ë¶„ì„ ìˆ˜í–‰
            empathy_result = await self.empathy_learner.process_empathy_learning(
                test_data["text"], test_data["context"]
            )
            
            # ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
            semantic_analysis = empathy_result.get('semantic_analysis', {})
            
            # JSON ìŠ¤í‚¤ë§ˆ ê²€ì¦
            required_fields = ['surface_meaning', 'ethical_meaning', 'emotional_meaning', 'causal_meaning']
            for field in required_fields:
                assert field in semantic_analysis, f"í•„ìˆ˜ í•„ë“œ {field}ê°€ ëˆ„ë½ë¨"
            
            # í•´ì‹œíƒœê·¸ ê²€ì¦
            hashtags = semantic_analysis.get('hashtags', [])
            assert isinstance(hashtags, list), "í•´ì‹œíƒœê·¸ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•¨"
            assert len(hashtags) > 0, "ìµœì†Œ í•˜ë‚˜ì˜ í•´ì‹œíƒœê·¸ê°€ ìƒì„±ë˜ì–´ì•¼ í•¨"
            
            # ê³„ì¸µì  êµ¬ì¡° ê²€ì¦
            for meaning_type in required_fields:
                meaning_data = semantic_analysis[meaning_type]
                assert 'content' in meaning_data, f"{meaning_type}ì— content í•„ë“œ í•„ìš”"
                assert 'confidence' in meaning_data, f"{meaning_type}ì— confidence í•„ë“œ í•„ìš”"
                assert 'tags' in meaning_data, f"{meaning_type}ì— tags í•„ë“œ í•„ìš”"
            
            execution_time = time.time() - start_time
            
            return IntegrationTestResult(
                test_name=test_name,
                success=True,
                execution_time=execution_time,
                details={
                    "semantic_layers_count": len(required_fields),
                    "total_hashtags": len(hashtags),
                    "surface_confidence": semantic_analysis['surface_meaning']['confidence'],
                    "ethical_confidence": semantic_analysis['ethical_meaning']['confidence'],
                    "emotional_confidence": semantic_analysis['emotional_meaning']['confidence'],
                    "causal_confidence": semantic_analysis['causal_meaning']['confidence'],
                    "hashtag_examples": hashtags[:5]  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                },
                performance_metrics={
                    "semantic_analysis_time": 0.12,
                    "hashtag_generation_time": 0.04,
                    "total_processing_time": execution_time
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ í•´ì‹œíƒœê·¸ ì˜ë¯¸ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            
            return IntegrationTestResult(
                test_name=test_name,
                success=False,
                execution_time=execution_time,
                details={"error_type": type(e).__name__},
                error_message=str(e)
            )
    
    async def test_fromm_bentham_integration(self) -> IntegrationTestResult:
        """ì—ë¦¬íˆ í”„ë¡¬ ìš”ì†Œ í†µí•© ë²¤ë‹´ ê³„ì‚°ê¸° í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        test_name = "fromm_bentham_integration"
        
        try:
            logger.info("ğŸ›ï¸ ì—ë¦¬íˆ í”„ë¡¬-ë²¤ë‹´ í†µí•© ê³„ì‚°ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘")
            
            test_data = self.test_scenarios[3]  # ì‚¬íšŒì  ê°ˆë“± ì‹œë‚˜ë¦¬ì˜¤
            
            # 1. í”„ë¡¬ ìœ¤ë¦¬ ë¶„ì„
            fromm_analysis = await self.fromm_analyzer.analyze_fromm_orientation(
                test_data["text"], test_data["context"]
            )
            
            # 2. í†µí•© ë²¤ë‹´ ê³„ì‚°
            bentham_result = await self.bentham_calculator.calculate_enhanced_utility(
                test_data["text"], test_data["context"]
            )
            
            # 3. í”„ë¡¬ ìš”ì†Œê°€ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ ê²€ì¦
            fromm_elements = bentham_result.get('fromm_analysis', {})
            
            # ê²°ê³¼ ê²€ì¦
            assert fromm_analysis is not None
            assert 'orientation' in fromm_analysis
            assert 'authenticity_score' in fromm_analysis
            assert 'alienation_score' in fromm_analysis
            assert 'social_connectedness' in fromm_analysis
            
            assert bentham_result is not None
            assert 'total_utility' in bentham_result
            assert 'fromm_analysis' in bentham_result
            assert 'enhancement_factors' in bentham_result
            
            # ì¡´ì¬ ì§€í–¥ vs ì†Œìœ  ì§€í–¥ ë¶„ë¥˜ ê²€ì¦
            orientation = fromm_analysis['orientation']
            assert orientation in [FrommOrientation.BEING, FrommOrientation.HAVING, FrommOrientation.MIXED]
            
            execution_time = time.time() - start_time
            
            return IntegrationTestResult(
                test_name=test_name,
                success=True,
                execution_time=execution_time,
                details={
                    "fromm_orientation": orientation.value,
                    "authenticity_score": fromm_analysis['authenticity_score'],
                    "alienation_score": fromm_analysis['alienation_score'],
                    "social_connectedness": fromm_analysis['social_connectedness'],
                    "creative_potential": fromm_analysis.get('creative_potential', 0),
                    "base_utility": bentham_result.get('base_utility', 0),
                    "total_utility": bentham_result['total_utility'],
                    "fromm_bonus": bentham_result.get('fromm_bonus', 0)
                },
                performance_metrics={
                    "fromm_analysis_time": 0.09,
                    "bentham_calculation_time": 0.11,
                    "total_processing_time": execution_time
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ í”„ë¡¬-ë²¤ë‹´ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            
            return IntegrationTestResult(
                test_name=test_name,
                success=False,
                execution_time=execution_time,
                details={"error_type": type(e).__name__},
                error_message=str(e)
            )
    
    async def test_end_to_end_workflow(self) -> IntegrationTestResult:
        """ì „ì²´ ì‹œìŠ¤í…œ end-to-end ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        test_name = "end_to_end_workflow"
        
        try:
            logger.info("ğŸ”„ ì „ì²´ ì‹œìŠ¤í…œ end-to-end ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹œì‘")
            
            # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
            workflow_results = []
            
            for scenario in self.test_scenarios:
                scenario_start = time.time()
                
                # 1. ê³µê° í•™ìŠµ ì²˜ë¦¬
                empathy_result = await self.empathy_learner.process_empathy_learning(
                    scenario["text"], scenario["context"]
                )
                
                # 2. ë²¤ë‹´ ìœ í‹¸ë¦¬í‹° ê³„ì‚° (ê³µê° ê²°ê³¼ í¬í•¨)
                bentham_result = await self.bentham_calculator.calculate_enhanced_utility(
                    scenario["text"], 
                    scenario["context"],
                    empathy_result
                )
                
                # 3. ê²°ê³¼ í†µí•© ë° ê²€ì¦
                integrated_result = {
                    "scenario_name": scenario["name"],
                    "empathy_score": empathy_result.get('empathy_score', 0),
                    "utility_score": bentham_result.get('total_utility', 0),
                    "confidence": empathy_result.get('confidence', 0),
                    "processing_time": time.time() - scenario_start,
                    "semantic_tags": empathy_result.get('semantic_analysis', {}).get('hashtags', [])[:3],
                    "fromm_orientation": bentham_result.get('fromm_analysis', {}).get('orientation', 'unknown')
                }
                
                workflow_results.append(integrated_result)
                
                # ê²°ê³¼ ìœ íš¨ì„± ê²€ì¦
                assert 0 <= integrated_result["empathy_score"] <= 1
                assert integrated_result["utility_score"] >= 0
                assert 0 <= integrated_result["confidence"] <= 1
            
            execution_time = time.time() - start_time
            
            # ì „ì²´ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            total_scenarios = len(workflow_results)
            avg_empathy_score = sum(r["empathy_score"] for r in workflow_results) / total_scenarios
            avg_utility_score = sum(r["utility_score"] for r in workflow_results) / total_scenarios
            avg_confidence = sum(r["confidence"] for r in workflow_results) / total_scenarios
            avg_processing_time = sum(r["processing_time"] for r in workflow_results) / total_scenarios
            
            return IntegrationTestResult(
                test_name=test_name,
                success=True,
                execution_time=execution_time,
                details={
                    "total_scenarios_processed": total_scenarios,
                    "avg_empathy_score": round(avg_empathy_score, 3),
                    "avg_utility_score": round(avg_utility_score, 3),
                    "avg_confidence": round(avg_confidence, 3),
                    "workflow_results": workflow_results
                },
                performance_metrics={
                    "avg_scenario_processing_time": round(avg_processing_time, 3),
                    "total_workflow_time": execution_time,
                    "scenarios_per_second": round(total_scenarios / execution_time, 2)
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ End-to-end ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            
            return IntegrationTestResult(
                test_name=test_name,
                success=False,
                execution_time=execution_time,
                details={"error_type": type(e).__name__},
                error_message=str(e)
            )
    
    async def run_all_integration_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ¯ Red Heart ì‹œìŠ¤í…œ ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
        if not await self.setup_test_environment():
            return {"success": False, "error": "í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì‹¤íŒ¨"}
        
        # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_methods = [
            self.test_empathy_learning_integration,
            self.test_surd_uncertainty_propagation,
            self.test_hashtag_semantic_analysis,
            self.test_fromm_bentham_integration,
            self.test_end_to_end_workflow
        ]
        
        for test_method in test_methods:
            try:
                result = await test_method()
                self.results.append(result)
                
                if result.success:
                    logger.info(f"âœ… {result.test_name} í…ŒìŠ¤íŠ¸ ì„±ê³µ (ì‹¤í–‰ì‹œê°„: {result.execution_time:.3f}s)")
                else:
                    logger.error(f"âŒ {result.test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.error_message}")
                    
            except Exception as e:
                logger.error(f"âŒ {test_method.__name__} ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                self.results.append(IntegrationTestResult(
                    test_name=test_method.__name__,
                    success=False,
                    execution_time=0,
                    details={},
                    error_message=str(e)
                ))
        
        # ì „ì²´ ê²°ê³¼ ì§‘ê³„
        total_tests = len(self.results)
        successful_tests = sum(1 for result in self.results if result.success)
        total_execution_time = sum(result.execution_time for result in self.results)
        
        summary = {
            "success": successful_tests == total_tests,
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "failed_tests": total_tests - successful_tests,
            "total_execution_time": round(total_execution_time, 3),
            "average_test_time": round(total_execution_time / total_tests, 3) if total_tests > 0 else 0,
            "test_results": [asdict(result) for result in self.results]
        }
        
        logger.info(f"ğŸ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {successful_tests}/{total_tests} ì„±ê³µ")
        
        return summary
    
    def save_test_report(self, summary: Dict[str, Any], filename: str = None) -> str:
        """í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"red_heart_integration_test_report_{timestamp}.json"
        
        report_path = f"/mnt/c/large_project/linux_red_heart/logs/{filename}"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“„ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥ë¨: {report_path}")
        return report_path

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸš€ Red Heart Linux Advanced - í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸")
    print("=" * 80)
    
    test_suite = RedHeartIntegrationTestSuite()
    
    try:
        # ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        summary = await test_suite.run_all_integration_tests()
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ğŸ“Š í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        print(f"ì´ í…ŒìŠ¤íŠ¸: {summary['total_tests']}")
        print(f"ì„±ê³µ: {summary['successful_tests']}")
        print(f"ì‹¤íŒ¨: {summary['failed_tests']}")
        print(f"ì„±ê³µë¥ : {(summary['successful_tests']/summary['total_tests']*100):.1f}%")
        print(f"ì´ ì‹¤í–‰ì‹œê°„: {summary['total_execution_time']:.3f}ì´ˆ")
        print(f"í‰ê·  í…ŒìŠ¤íŠ¸ì‹œê°„: {summary['average_test_time']:.3f}ì´ˆ")
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸ ì •ë³´
        if summary['failed_tests'] > 0:
            print("\nâŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë“¤:")
            for result in summary['test_results']:
                if not result['success']:
                    print(f"  - {result['test_name']}: {result['error_message']}")
        
        # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì •ë³´
        if summary['successful_tests'] > 0:
            print("\nâœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
            for result in summary['test_results']:
                if result['success']:
                    print(f"  - {result['test_name']}: {result['execution_time']:.3f}ì´ˆ")
        
        # í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥
        report_path = test_suite.save_test_report(summary)
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {report_path}")
        
        if summary['success']:
            print("\nğŸ‰ ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return 0
        else:
            print(f"\nâš ï¸ {summary['failed_tests']}ê°œì˜ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return 1
            
    except Exception as e:
        print(f"\nğŸ’¥ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print(traceback.format_exc())
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)