#!/usr/bin/env python3
"""
Red Heart AI ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
ê° ëª¨ë“œë³„ë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
"""

import sys
import asyncio
import time
import torch
sys.path.append('/mnt/c/large_project/linux_red_heart')

from main_unified import UnifiedInferenceSystem, InferenceConfig, MemoryMode

# ìƒ‰ìƒ ì½”ë“œ
GREEN = '\033[92m'
RED = '\033[91m'
YELLOW = '\033[93m'
BLUE = '\033[94m'
CYAN = '\033[96m'
END = '\033[0m'

async def test_inference(mode: MemoryMode, test_text: str):
    """ë‹¨ì¼ ëª¨ë“œ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print(f"\n{CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{END}")
    print(f"{BLUE}í…ŒìŠ¤íŠ¸: {mode.value.upper()} ëª¨ë“œ{END}")
    print(f"{CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{END}")
    
    try:
        # ì„¤ì • ìƒì„±
        config = InferenceConfig(
            memory_mode=mode,
            auto_memory_mode=False,
            device='cuda' if torch.cuda.is_available() else 'cpu',
            cache_size=5,
            verbose=False
        )
        
        print(f"1ï¸âƒ£ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        start_init = time.time()
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        system = UnifiedInferenceSystem(config)
        await system.initialize()
        
        init_time = time.time() - start_init
        print(f"   {GREEN}âœ… ì´ˆê¸°í™” ì™„ë£Œ ({init_time:.1f}ì´ˆ){END}")
        
        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            print(f"   ğŸ“Š GPU ë©”ëª¨ë¦¬: {allocated:.2f}GB")
        
        # í™œì„± ëª¨ë“ˆ ìš”ì•½
        active_modules = []
        if system.config.use_three_view_scenario:
            active_modules.append("3-View")
        if system.config.use_multi_ethics_system:
            active_modules.append("Multi-Ethics")
        if system.config.use_neural_analyzers:
            active_modules.append("Neural")
        if system.config.use_meta_integration:
            active_modules.append("Meta")
        
        print(f"   ğŸ“¦ í™œì„± ëª¨ë“ˆ: {', '.join(active_modules) if active_modules else 'Basic only'}")
        
        print(f"\n2ï¸âƒ£ ì¶”ë¡  í…ŒìŠ¤íŠ¸")
        print(f"   ğŸ“ ì…ë ¥: \"{test_text}\"")
        
        start_inference = time.time()
        result = await system.analyze(test_text)
        inference_time = time.time() - start_inference
        
        print(f"   {GREEN}âœ… ì¶”ë¡  ì™„ë£Œ ({inference_time:.2f}ì´ˆ){END}")
        
        # ê²°ê³¼ ìš”ì•½
        if result and not result.get('error'):
            print(f"\n3ï¸âƒ£ ê²°ê³¼ ìš”ì•½")
            print(f"   â€¢ í†µí•© ì ìˆ˜: {result.get('integrated_score', 0):.3f}")
            
            if 'emotion' in result:
                emotion = result['emotion']
                # ê°ì • ê°’ë§Œ ì¶”ì¶œ (dictë‚˜ íŠ¹ìˆ˜ í‚¤ëŠ” ì œì™¸)
                emotion_scores = {}
                for key, value in emotion.items():
                    if isinstance(value, (int, float)) and key not in ['dsp_processed', 'kalman_filtered', 'hierarchy']:
                        emotion_scores[key] = value
                
                if emotion_scores:
                    top_emotion = max(emotion_scores.items(), key=lambda x: x[1])
                    print(f"   â€¢ ì£¼ìš” ê°ì •: {top_emotion[0]} ({top_emotion[1]:.3f})")
            
            if 'bentham' in result:
                bentham = result['bentham']
                total_pleasure = sum(v for k, v in bentham.items() if k != 'total')
                print(f"   â€¢ ë²¤ë‹´ ì¾Œë½: {total_pleasure:.3f}")
            
            if 'regret' in result:
                regret = result['regret']
                if isinstance(regret, dict):
                    if 'unified' in regret:
                        print(f"   â€¢ í›„íšŒ ì ìˆ˜: {regret['unified'].get('score', 0):.3f}")
                    else:
                        print(f"   â€¢ í›„íšŒ ë¶„ì„: {len(regret)} í•­ëª©")
                elif isinstance(regret, (int, float)):
                    print(f"   â€¢ í›„íšŒ ì ìˆ˜: {regret:.3f}")
            
            print(f"\n   {GREEN}âœ… {mode.value.upper()} ëª¨ë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!{END}")
        else:
            error_msg = result.get('error', 'Unknown error') if result else 'No result'
            print(f"\n   {RED}âŒ ì¶”ë¡  ì‹¤íŒ¨: {error_msg}{END}")
            
    except Exception as e:
        print(f"\n   {RED}âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}{END}")
        import traceback
        if '--debug' in sys.argv:
            traceback.print_exc()
    
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if 'system' in locals():
            del system
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print(f"{CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{END}")

async def test_ethical_dilemma():
    """ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ í…ŒìŠ¤íŠ¸ (HEAVY ëª¨ë“œ)"""
    print(f"\n{YELLOW}ğŸ¯ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ í…ŒìŠ¤íŠ¸ (ë¹„ì„ í˜• ì›Œí¬í”Œë¡œìš°){END}")
    print(f"{CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”{END}")
    
    try:
        config = InferenceConfig(
            memory_mode=MemoryMode.HEAVY,
            auto_memory_mode=False,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        
        print("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        system = UnifiedInferenceSystem(config)
        await system.initialize()
        
        # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
        scenarios = [
            "ì¹œêµ¬ë¥¼ ì ê·¹ì ìœ¼ë¡œ ë„ì™€ì¤€ë‹¤",
            "ìƒí™©ì„ ì§€ì¼œë³¸ í›„ ì‹ ì¤‘í•˜ê²Œ íŒë‹¨í•œë‹¤",
            "ìµœì†Œí•œì˜ ë„ì›€ë§Œ ì œê³µí•œë‹¤"
        ]
        
        print(f"\nì‹œë‚˜ë¦¬ì˜¤ {len(scenarios)}ê°œ ë¶„ì„ ì¤‘...")
        start_time = time.time()
        
        result = await system.analyze_ethical_dilemma(scenarios)
        
        elapsed = time.time() - start_time
        print(f"{GREEN}âœ… ë¶„ì„ ì™„ë£Œ ({elapsed:.2f}ì´ˆ){END}")
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"   â€¢ í‰ê°€ëœ ì‹œë‚˜ë¦¬ì˜¤: {result.get('total_evaluated', 0)}ê°œ")
        print(f"   â€¢ ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.2f}ì´ˆ")
        
        if 'selected_scenarios' in result and result['selected_scenarios']:
            print(f"\nğŸ† ìƒìœ„ 2ê°œ ì‹œë‚˜ë¦¬ì˜¤:")
            for i, scenario in enumerate(result['selected_scenarios'][:2], 1):
                score = scenario['analysis'].get('integrated_score', 0)
                text = scenario.get('original_scenario', 'Unknown')
                print(f"   {i}. {text[:50]}... (ì ìˆ˜: {score:.3f})")
        
        if 'recommendation' in result:
            print(f"\nğŸ’¡ ì¶”ì²œ: {result['recommendation'][:100]}...")
        
        print(f"\n{GREEN}âœ… ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ ì„±ê³µ!{END}")
        
    except Exception as e:
        print(f"{RED}âŒ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}{END}")
    
    finally:
        if 'system' in locals():
            del system
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print(f"{BLUE}{'='*50}{END}")
    print(f"{BLUE}ğŸš€ Red Heart AI ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸{END}")
    print(f"{BLUE}{'='*50}{END}")
    
    # GPU ì •ë³´
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"GPU: {gpu_name} ({gpu_memory:.1f}GB)")
    else:
        print("GPU: ì‚¬ìš© ë¶ˆê°€ (CPU ëª¨ë“œ)")
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_text = "ì¹œêµ¬ê°€ ì–´ë ¤ìš´ ìƒí™©ì— ì²˜í–ˆì„ ë•Œ ì–´ë–»ê²Œ ë„ì™€ì•¼ í• ê¹Œ?"
    
    # ëª¨ë“œ ì„ íƒ ë¡œì§ ê°œì„ 
    run_light = True  # ê¸°ë³¸ì€ LIGHT ëª¨ë“œ
    
    # íŠ¹ì • ëª¨ë“œê°€ ì§€ì •ë˜ë©´ LIGHT ëª¨ë“œ ê±´ë„ˆë›°ê¸°
    if '--medium' in sys.argv or '--heavy' in sys.argv or '--dilemma' in sys.argv:
        run_light = False
    
    # --allì´ë©´ ëª¨ë“  ëª¨ë“œ ì‹¤í–‰
    if '--all' in sys.argv:
        run_light = True
    
    # 1. LIGHT ëª¨ë“œ í…ŒìŠ¤íŠ¸
    if run_light:
        await test_inference(MemoryMode.LIGHT, test_text)
    
    # 2. MEDIUM ëª¨ë“œ í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
    if '--medium' in sys.argv or '--all' in sys.argv:
        await test_inference(MemoryMode.MEDIUM, test_text)
    
    # 3. HEAVY ëª¨ë“œ í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
    if '--heavy' in sys.argv or '--all' in sys.argv:
        await test_inference(MemoryMode.HEAVY, test_text)
    
    # 4. ìœ¤ë¦¬ì  ë”œë ˆë§ˆ í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
    if '--dilemma' in sys.argv or '--all' in sys.argv:
        await test_ethical_dilemma()
    
    print(f"\n{GREEN}{'='*50}{END}")
    print(f"{GREEN}ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!{END}")
    print(f"{GREEN}{'='*50}{END}")
    
    # ì‚¬ìš©ë²• ì•ˆë‚´
    if len(sys.argv) == 1:
        print(f"\n{YELLOW}ğŸ’¡ ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì˜µì…˜:{END}")
        print("   --medium  : MEDIUM ëª¨ë“œ í…ŒìŠ¤íŠ¸")
        print("   --heavy   : HEAVY ëª¨ë“œ í…ŒìŠ¤íŠ¸")
        print("   --dilemma : ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ í…ŒìŠ¤íŠ¸")
        print("   --all     : ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        print("   --debug   : ë””ë²„ê·¸ ëª¨ë“œ (ìƒì„¸ ì—ëŸ¬ ì¶œë ¥)")

if __name__ == "__main__":
    asyncio.run(main())