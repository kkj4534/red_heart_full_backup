"""
SentenceTransformer Singleton Manager - Persistent Subprocess Server Architecture

ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜:
- Persistent subprocess server (RAM ìƒì£¼ ëª¨ë¸)
- ì™„ì „í•œ í”„ë¡œì„¸ìŠ¤ ê²©ë¦¬ (signal handler inheritance ì™„ì „ ì°¨ë‹¨)
- JSON IPC í†µì‹ 
- ì„±ëŠ¥ ìµœì í™” (í•œë²ˆ ë¡œë”© í›„ ì§€ì†ì  ì„œë¹„ìŠ¤)
- WSL í˜¸í™˜ì„± ë° ì•ˆì •ì„± ë³´ì¥
"""

import os
import logging
import asyncio
import threading
import time
from typing import Dict, Optional, Any, List
import torch
from sentence_transformer_client import SentenceTransformerClient
from config import MODELS_DIR, get_device

logger = logging.getLogger(__name__)

class SentenceTransformerManager:
    """
    SentenceTransformer ì‹±ê¸€í†¤ ê´€ë¦¬ì - Persistent Subprocess Server ê¸°ë°˜
    
    ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜:
    - Persistent subprocess server (ëª¨ë¸ RAM ìƒì£¼)
    - ì™„ì „í•œ í”„ë¡œì„¸ìŠ¤ ê²©ë¦¬ (signal handler inheritance ì°¨ë‹¨)
    - JSON IPC í†µì‹ 
    - ìë™ ì¬ì—°ê²° ë° ë³µêµ¬
    - ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
        
        self._initialized = True
        self._clients: Dict[str, SentenceTransformerClient] = {}  # ëª¨ë¸ë³„ í´ë¼ì´ì–¸íŠ¸
        self._model_locks: Dict[str, threading.Lock] = {}
        self._gpu_semaphore = asyncio.Semaphore(2)  # 2ê°œ ë™ì‹œ GPU ì—°ì‚° í—ˆìš©
        
        # ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ ì„¤ì •
        self._server_script_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), 
            "sentence_transformer_server.py"
        )
        
        logger.info("SentenceTransformerManager ì´ˆê¸°í™” ì™„ë£Œ (Subprocess Server ê¸°ë°˜)")
    
    def get_model(self, model_name: str, device: str = None, cache_folder: str = None):
        """
        ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (Subprocess Server ê¸°ë°˜)
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            device: ë””ë°”ì´ìŠ¤ ì„¤ì • (ì—†ìœ¼ë©´ ìë™ ê²°ì •)
            cache_folder: ìºì‹œ í´ë” ê²½ë¡œ
            
        Returns:
            SentenceTransformerProxy ê°ì²´ (SentenceTransformer í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤)
        """
        # ëª¨ë¸ë³„ ê³ ìœ  í‚¤ ìƒì„±
        if device is None:
            # FORCE_CPU_INIT í™˜ê²½ë³€ìˆ˜ ì²´í¬ ì¶”ê°€
            import os
            if os.environ.get('FORCE_CPU_INIT', '0') == '1':
                device = 'cpu'
                logger.info("ğŸ“Œ FORCE_CPU_INIT ê°ì§€: CPU ë””ë°”ì´ìŠ¤ ê°•ì œ ì„¤ì •")
            else:
                device = str(get_device())
        
        # CPU ëª¨ë“œì—ì„œëŠ” ì§ì ‘ ë¡œë“œ (subprocess ìš°íšŒ)
        if device == 'cpu' or device == 'cpu:0':
            logger.info(f"ğŸ“Œ CPU ëª¨ë“œ: {model_name} ì§ì ‘ ë¡œë“œ (subprocess ìš°íšŒ)")
            from sentence_transformers import SentenceTransformer
            import os  # CPU ë¸”ë¡ ë‚´ì—ì„œë„ os í•„ìš”
            if cache_folder is None:
                cache_folder = os.path.join(MODELS_DIR, 'sentence_transformers')
            os.makedirs(cache_folder, exist_ok=True)
            
            # ì§ì ‘ CPUì—ì„œ ë¡œë“œ
            model = SentenceTransformer(model_name, device='cpu', cache_folder=cache_folder)
            
            # SimpleCPUProxyë¡œ ë˜í•‘í•˜ì—¬ í˜¸í™˜ì„± ìœ ì§€
            class SimpleCPUProxy:
                def __init__(self, model):
                    self.model = model
                    
                def encode(self, sentences, **kwargs):
                    return self.model.encode(sentences, **kwargs)
                
                def get_sentence_embedding_dimension(self):
                    return self.model.get_sentence_embedding_dimension()
                    
                @property
                def device(self):
                    return torch.device('cpu')
                    
                @property
                def max_seq_length(self):
                    return self.model.max_seq_length
            
            return SimpleCPUProxy(model)
        
        model_key = f"{model_name}_{device}"
        
        # í´ë¼ì´ì–¸íŠ¸ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë°˜í™˜
        if model_key in self._clients:
            # Health checkë¡œ ì—°ê²° ìƒíƒœ í™•ì¸
            try:
                health_result = self._clients[model_key].health_check(force=False)
                if health_result.get("status") == "success":
                    logger.info(f"ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì¬ì‚¬ìš©: {model_key}")
                    return SentenceTransformerProxy(self._clients[model_key], model_key)
                else:
                    logger.warning(f"ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ë¶ˆì•ˆì • - ì¬ìƒì„±: {model_key}")
                    # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
                    self._clients[model_key].stop_server()
                    del self._clients[model_key]
            except Exception as e:
                logger.warning(f"Health check ì‹¤íŒ¨ - í´ë¼ì´ì–¸íŠ¸ ì¬ìƒì„±: {model_key}, ì˜¤ë¥˜: {e}")
                # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
                try:
                    self._clients[model_key].stop_server()
                except Exception:
                    pass
                del self._clients[model_key]
        
        # ëª¨ë¸ë³„ ë½ ìƒì„±
        if model_key not in self._model_locks:
            self._model_locks[model_key] = threading.Lock()
        
        # ìŠ¤ë ˆë“œ ì•ˆì „í•˜ê²Œ í´ë¼ì´ì–¸íŠ¸ ìƒì„± (60ì´ˆ íƒ€ì„ì•„ì›ƒ)
        lock_acquired = self._model_locks[model_key].acquire(timeout=60.0)
        if not lock_acquired:
            logger.error(f"ëª¨ë¸ ë½ íšë“ ì‹¤íŒ¨ (60ì´ˆ íƒ€ì„ì•„ì›ƒ): {model_key}")
            raise RuntimeError(f"SentenceTransformer ëª¨ë¸ ë½ íƒ€ì„ì•„ì›ƒ: {model_name}")
        
        try:
            # ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸ (ë‹¤ë¥¸ ìŠ¤ë ˆë“œì—ì„œ ìƒì„±í–ˆì„ ìˆ˜ ìˆìŒ)
            if model_key in self._clients:
                logger.info(f"ëŒ€ê¸° ì¤‘ ë‹¤ë¥¸ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ì™„ë£Œ: {model_key}")
                return SentenceTransformerProxy(self._clients[model_key], model_key)
            
            try:
                logger.info(f"ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹œì‘: {model_key}")
                
                # ìºì‹œ í´ë” ì„¤ì •
                if cache_folder is None:
                    cache_folder = os.path.join(MODELS_DIR, 'sentence_transformers')
                os.makedirs(cache_folder, exist_ok=True)
                
                # ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                client = SentenceTransformerClient(
                    server_script_path=self._server_script_path,
                    startup_timeout=60.0,  # ì¶©ë¶„í•œ ì‹œê°„
                    request_timeout=180.0  # 3ë¶„ (ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¶©ë¶„í•œ ì‹œê°„)
                )
                
                # ì„œë²„ ì‹œì‘ ë° ëª¨ë¸ ë¡œë”©
                logger.info(f"ì„œë²„ ì‹œì‘ ë° ëª¨ë¸ ë¡œë”©: {model_name} (device: {device})")
                
                # ì„œë²„ ì‹œì‘
                if not client.start_server():
                    raise RuntimeError(f"ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {model_name}")
                
                # ëª¨ë¸ ë¡œë”©
                load_result = client.load_model(
                    model_name=model_name,
                    device=device,
                    cache_folder=cache_folder
                )
                
                if load_result.get("status") != "success":
                    error_msg = load_result.get("error", "Unknown error")
                    raise RuntimeError(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {error_msg}")
                
                # í´ë¼ì´ì–¸íŠ¸ ì €ì¥
                self._clients[model_key] = client
                logger.info(f"í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_key}")
                
                return SentenceTransformerProxy(client, model_key)
                
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_key}, ì˜¤ë¥˜: {e}")
                logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                import traceback
                logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
                # fallback ì—†ìŒ - ë°”ë¡œ ì˜ˆì™¸ ë°œìƒ
                raise RuntimeError(f"SentenceTransformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}") from e
        finally:
            # ë½ í•­ìƒ í•´ì œ
            self._model_locks[model_key].release()
    
    async def get_gpu_semaphore(self):
        """GPU ì„¸ë§ˆí¬ì–´ ë°˜í™˜"""
        return self._gpu_semaphore
    
    def get_loaded_models(self) -> Dict[str, str]:
        """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        loaded_models = {}
        for key, client in self._clients.items():
            try:
                health_result = client.health_check(force=False)
                if health_result.get("status") == "success":
                    model_info = health_result.get("result", {})
                    model_name = model_info.get("model_name", "Unknown")
                    loaded_models[key] = f"SentenceTransformerProxy({model_name})"
                else:
                    loaded_models[key] = "Disconnected"
            except Exception:
                loaded_models[key] = "Error"
        return loaded_models
    
    def get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB)"""
        import psutil
        import torch
        
        memory_info = {
            'system_used': psutil.virtual_memory().percent,
            'process_memory': psutil.Process().memory_info().rss / (1024 * 1024),
            'loaded_clients': len(self._clients)
        }
        
        # ê° í´ë¼ì´ì–¸íŠ¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìˆ˜ì§‘
        total_server_memory = 0
        for key, client in self._clients.items():
            try:
                health_result = client.health_check(force=False)
                if health_result.get("status") == "success":
                    server_info = health_result.get("result", {})
                    server_memory = server_info.get("memory_usage", 0)
                    total_server_memory += server_memory
            except Exception:
                pass
        
        memory_info['server_processes_memory'] = total_server_memory
        
        if torch.cuda.is_available():
            memory_info['gpu_allocated'] = torch.cuda.memory_allocated() / (1024 * 1024)
            memory_info['gpu_cached'] = torch.cuda.memory_reserved() / (1024 * 1024)
        
        return memory_info
    
    def restart_server(self, model_name: str, device: str = None):
        """íŠ¹ì • ëª¨ë¸ì˜ ì„œë²„ ì¬ì‹œì‘"""
        if device is None:
            device = str(get_device())
        
        model_key = f"{model_name}_{device}"
        
        # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ
        if model_key in self._clients:
            try:
                logger.info(f"ì„œë²„ ì¬ì‹œì‘ ì¤‘: {model_key}")
                self._clients[model_key].stop_server()
                del self._clients[model_key]
            except Exception as e:
                logger.warning(f"ì„œë²„ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info(f"ì„œë²„ ì¬ì‹œì‘ ì™„ë£Œ: {model_key}")
    
    def unload_model_from_gpu(self, model_key: str):
        """íŠ¹ì • ëª¨ë¸ì„ GPUì—ì„œ ì–¸ë¡œë“œ (ì„œë²„ëŠ” ìœ ì§€, GPU ë©”ëª¨ë¦¬ë§Œ í•´ì œ)"""
        if model_key not in self._clients:
            return
        
        try:
            # GPU ëª¨ë¸ì¸ ê²½ìš°ì—ë§Œ ì–¸ë¡œë“œ
            if 'cuda' in model_key or 'gpu' in model_key:
                logger.info(f"GPU ë©”ëª¨ë¦¬ í•´ì œ ì¤‘: {model_key}")
                # ì„œë²„ì— GPU ë©”ëª¨ë¦¬ í•´ì œ ìš”ì²­
                # ì—¬ê¸°ì„œëŠ” ì„œë²„ë¥¼ ì¢…ë£Œí•˜ê³  ì¬ì‹œì‘í•˜ëŠ” ë°©ì‹ ì‚¬ìš©
                client = self._clients[model_key]
                
                # ì„œë²„ ì¢…ë£Œ (GPU ë©”ëª¨ë¦¬ í•´ì œ)
                client.stop_server()
                
                # í´ë¼ì´ì–¸íŠ¸ ì œê±° (ì¬ì‚¬ìš©ì‹œ ë‹¤ì‹œ ìƒì„±ë¨)
                del self._clients[model_key]
                
                # GPU ìºì‹œ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info(f"GPU ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ: {model_key}")
        except Exception as e:
            logger.warning(f"GPU ì–¸ë¡œë“œ ì‹¤íŒ¨: {model_key}, ì˜¤ë¥˜: {e}")
    
    def clear_cache(self):
        """ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
        logger.info("í´ë¼ì´ì–¸íŠ¸ ìºì‹œ ì •ë¦¬ ì‹œì‘")
        
        # ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ ì„œë²„ ì¢…ë£Œ
        for key, client in self._clients.items():
            try:
                logger.info(f"í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì¤‘: {key}")
                client.stop_server()
            except Exception as e:
                logger.warning(f"í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì‹¤íŒ¨: {key}, ì˜¤ë¥˜: {e}")
        
        self._clients.clear()
        self._model_locks.clear()
        logger.info("í´ë¼ì´ì–¸íŠ¸ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

class SentenceTransformerProxy:
    """
    SentenceTransformer í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ - Subprocess Client Wrapper
    
    ê¸°ì¡´ SentenceTransformer APIì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
    ë‚´ë¶€ì ìœ¼ë¡œëŠ” subprocess serverì™€ í†µì‹ 
    """
    
    def __init__(self, client: SentenceTransformerClient, model_key: str = None):
        self.client = client
        self._model_info = None
        self._model_key = model_key  # GPU ì–¸ë¡œë“œë¥¼ ìœ„í•œ í‚¤ ì €ì¥
        
        # ëª¨ë¸ ì •ë³´ ìºì‹œ
        try:
            health_result = client.health_check(force=True)
            if health_result.get("status") == "success":
                self._model_info = health_result.get("result", {})
        except Exception as e:
            logger.warning(f"ëª¨ë¸ ì •ë³´ ìºì‹œ ì‹¤íŒ¨: {e}")
    
    def encode(self, sentences: List[str], **kwargs) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (SentenceTransformer í˜¸í™˜)
        GPU ì‚¬ìš© í›„ ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ í•´ì œ
        
        Args:
            sentences: ì¸ì½”ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            **kwargs: ì¶”ê°€ ì¸ì (auto_unload=Falseë¡œ ì–¸ë¡œë“œ ë¹„í™œì„±í™” ê°€ëŠ¥)
            
        Returns:
            ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
        """
        # auto_unload ì˜µì…˜ í™•ì¸ (ê¸°ë³¸ê°’: GPUëŠ” True, CPUëŠ” False)
        if 'cuda' in str(self.device) or 'gpu' in str(self.device):
            auto_unload = kwargs.pop('auto_unload', True)  # GPUëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì–¸ë¡œë“œ
        else:
            auto_unload = kwargs.pop('auto_unload', False)  # CPUëŠ” ìœ ì§€
        
        # ë‹¨ì¼ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(sentences, str):
            sentences = [sentences]
            return_single = True
        else:
            return_single = False
        
        try:
            # ì„œë²„ì— ì¸ì½”ë”© ìš”ì²­
            response = self.client.encode_texts(sentences, **kwargs)
            
            if response.get("status") != "success":
                error_msg = response.get("error", "Unknown error")
                raise RuntimeError(f"í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì‹¤íŒ¨: {error_msg}")
            
            embeddings = response.get("result", {}).get("embeddings", [])
            
            # ë‹¨ì¼ ë¬¸ìì—´ ì…ë ¥ì´ì—ˆë‹¤ë©´ ë‹¨ì¼ ê²°ê³¼ ë°˜í™˜
            if return_single and embeddings:
                return embeddings[0]
            
            return embeddings
            
        finally:
            # GPU ì‚¬ìš© í›„ ìë™ ì–¸ë¡œë“œ (GPU ëª¨ë¸ì¸ ê²½ìš°ì—ë§Œ)
            if auto_unload and self._model_key and ('cuda' in str(self.device) or 'gpu' in str(self.device)):
                try:
                    logger.debug(f"ì„ë² ë”© ì™„ë£Œ, GPU ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ: {self._model_key}")
                    # ì „ì—­ ë§¤ë‹ˆì €ë¥¼ í†µí•´ GPU ì–¸ë¡œë“œ
                    _manager.unload_model_from_gpu(self._model_key)
                    # GPU ìºì‹œë„ ì¦‰ì‹œ ì •ë¦¬
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()  # GPU ì—°ì‚° ì™„ë£Œ ëŒ€ê¸°
                except Exception as e:
                    logger.warning(f"GPU ìë™ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    @property
    def max_seq_length(self) -> Optional[int]:
        """ëª¨ë¸ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"""
        if self._model_info:
            return self._model_info.get("model_max_seq_length")
        return None
    
    @property
    def device(self) -> str:
        """ëª¨ë¸ì´ ë¡œë“œëœ ë””ë°”ì´ìŠ¤"""
        if self._model_info:
            return self._model_info.get("device", "unknown")
        return "unknown"
    
    def get_sentence_embedding_dimension(self) -> int:
        """ëª¨ë¸ì˜ ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        if self._model_info:
            # embedding_dimension ë˜ëŠ” hidden_size í‚¤ ì‚¬ìš©
            dim = self._model_info.get("embedding_dimension")
            if dim is not None:
                return dim
            # fallbackìœ¼ë¡œ hidden_size í™•ì¸
            dim = self._model_info.get("hidden_size")
            if dim is not None:
                return dim
        # ê¸°ë³¸ê°’ ë°˜í™˜ (ì¼ë°˜ì ì¸ ì„ë² ë”© ì°¨ì›)
        return 768
    
    def __repr__(self) -> str:
        model_name = self._model_info.get("model_name", "Unknown") if self._model_info else "Unknown"
        device = self.device
        return f"SentenceTransformerProxy(model='{model_name}', device='{device}')"

# ì „ì—­ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_manager = SentenceTransformerManager()

def get_sentence_transformer(model_name: str, device: str = None, cache_folder: str = None):
    """
    ê³µìœ  SentenceTransformer ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Subprocess Server ê¸°ë°˜)
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
        cache_folder: ìºì‹œ í´ë”
        
    Returns:
        SentenceTransformerProxy ì¸ìŠ¤í„´ìŠ¤ (SentenceTransformer í˜¸í™˜)
    """
    return _manager.get_model(model_name, device, cache_folder)

async def get_gpu_semaphore():
    """GPU ì„¸ë§ˆí¬ì–´ ë°˜í™˜"""
    return await _manager.get_gpu_semaphore()

def get_model_info():
    """ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
    return _manager.get_loaded_models()

def get_memory_info():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
    return _manager.get_memory_usage()

def clear_model_cache():
    """ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
    _manager.clear_cache()

def get_client_info():
    """í™œì„± í´ë¼ì´ì–¸íŠ¸ ì •ë³´ ë°˜í™˜"""
    client_info = {}
    for key, client in _manager._clients.items():
        try:
            health_result = client.health_check(force=False)
            client_info[key] = health_result
        except Exception as e:
            client_info[key] = {"status": "error", "error": str(e)}
    return client_info