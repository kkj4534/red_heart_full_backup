"""
SentenceTransformer Singleton Manager - Persistent Subprocess Server Architecture

ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜:
- Persistent subprocess server (RAM ìƒì£¼ ëª¨ë¸)
- ì™„ì „í•œ í”„ë¡œì„¸ìŠ¤ ê²©ë¦¬ (signal handler inheritance ì™„ì „ ì°¨ë‹¨)
- JSON IPC í†µì‹ 
- ì„±ëŠ¥ ìµœì í™” (í•œë²ˆ ë¡œë”© í›„ ì§€ì†ì  ì„œë¹„ìŠ¤)
- WSL í˜¸í™˜ì„± ë° ì•ˆì •ì„± ë³´ì¥
"""

import os
import logging
import asyncio
import threading
import time
from typing import Dict, Optional, Any, List
import torch
from sentence_transformer_client import SentenceTransformerClient
from config import MODELS_DIR, get_device

logger = logging.getLogger(__name__)

class SentenceTransformerManager:
    """
    SentenceTransformer ì‹±ê¸€í†¤ ê´€ë¦¬ì - Persistent Subprocess Server ê¸°ë°˜
    
    ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜:
    - Persistent subprocess server (ëª¨ë¸ RAM ìƒì£¼)
    - ì™„ì „í•œ í”„ë¡œì„¸ìŠ¤ ê²©ë¦¬ (signal handler inheritance ì°¨ë‹¨)
    - JSON IPC í†µì‹ 
    - ìë™ ì¬ì—°ê²° ë° ë³µêµ¬
    - ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
        
        self._initialized = True
        self._clients: Dict[str, SentenceTransformerClient] = {}  # ëª¨ë¸ë³„ í´ë¼ì´ì–¸íŠ¸
        self._model_locks: Dict[str, threading.Lock] = {}
        self._gpu_semaphore = asyncio.Semaphore(2)  # 2ê°œ ë™ì‹œ GPU ì—°ì‚° í—ˆìš©
        
        # ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ ì„¤ì •
        self._server_script_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), 
            "sentence_transformer_server.py"
        )
        
        logger.info("SentenceTransformerManager ì´ˆê¸°í™” ì™„ë£Œ (Subprocess Server ê¸°ë°˜)")
    
    def get_model(self, model_name: str, device: str = None, cache_folder: str = None):
        """
        ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (Subprocess Server ê¸°ë°˜)
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            device: ë””ë°”ì´ìŠ¤ ì„¤ì • (ì—†ìœ¼ë©´ ìë™ ê²°ì •)
            cache_folder: ìºì‹œ í´ë” ê²½ë¡œ
            
        Returns:
            SentenceTransformerProxy ê°ì²´ (SentenceTransformer í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤)
        """
        # ëª¨ë¸ë³„ ê³ ìœ  í‚¤ ìƒì„±
        if device is None:
            # FORCE_CPU_INIT í™˜ê²½ë³€ìˆ˜ ì²´í¬ ì¶”ê°€
            if os.environ.get('FORCE_CPU_INIT', '0') == '1':
                device = 'cpu'
                logger.info("ğŸ“Œ FORCE_CPU_INIT ê°ì§€: CPU ë””ë°”ì´ìŠ¤ ê°•ì œ ì„¤ì •")
            else:
                device = str(get_device())
        
        # CPU ëª¨ë“œì—ì„œëŠ” ì§ì ‘ ë¡œë“œ (subprocess ìš°íšŒ)
        if device == 'cpu' or device == 'cpu:0':
            logger.info(f"ğŸ“Œ CPU ëª¨ë“œ: {model_name} ì§ì ‘ ë¡œë“œ (subprocess ìš°íšŒ)")
            from sentence_transformers import SentenceTransformer
            if cache_folder is None:
                cache_folder = os.path.join(MODELS_DIR, 'sentence_transformers')
            os.makedirs(cache_folder, exist_ok=True)
            
            # ì§ì ‘ CPUì—ì„œ ë¡œë“œ
            model = SentenceTransformer(model_name, device='cpu', cache_folder=cache_folder)
            
            # SimpleCPUProxyë¡œ ë˜í•‘í•˜ì—¬ í˜¸í™˜ì„± ìœ ì§€
            class SimpleCPUProxy:
                def __init__(self, model):
                    self.model = model
                    
                def encode(self, sentences, **kwargs):
                    # convert_to_tensor ì²˜ë¦¬
                    convert_to_tensor = kwargs.get('convert_to_tensor', False)
                    result = self.model.encode(sentences, **kwargs)
                    
                    # í…ì„œë¡œ ë³€í™˜ ìš”ì²­ì´ì§€ë§Œ ì´ë¯¸ í…ì„œì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
                    if convert_to_tensor and torch.is_tensor(result):
                        return result
                    # í…ì„œê°€ ì•„ë‹Œë° í…ì„œë¡œ ë³€í™˜ ìš”ì²­ì¸ ê²½ìš°
                    elif convert_to_tensor and not torch.is_tensor(result):
                        import numpy as np
                        if isinstance(result, (list, np.ndarray)):
                            return torch.tensor(result)
                    # í…ì„œì¸ë° í…ì„œ ë³€í™˜ ìš”ì²­ì´ ì•„ë‹Œ ê²½ìš°
                    elif not convert_to_tensor and torch.is_tensor(result):
                        return result.cpu().numpy() if result.is_cuda else result.numpy()
                    
                    return result
                
                def get_sentence_embedding_dimension(self):
                    return self.model.get_sentence_embedding_dimension()
                    
                @property
                def device(self):
                    return torch.device('cpu')
                    
                @property
                def max_seq_length(self):
                    return self.model.max_seq_length
            
            return SimpleCPUProxy(model)
        
        model_key = f"{model_name}_{device}"
        
        # í´ë¼ì´ì–¸íŠ¸ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë°˜í™˜
        if model_key in self._clients:
            # Health checkë¡œ ì—°ê²° ìƒíƒœ í™•ì¸
            try:
                health_result = self._clients[model_key].health_check(force=False)
                if health_result.get("status") == "success":
                    # ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸
                    model_info = health_result.get("result", {})
                    if model_info.get("model_loaded", False):
                        logger.info(f"ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì¬ì‚¬ìš© (ëª¨ë¸ ë¡œë“œë¨): {model_key}")
                        return SentenceTransformerProxy(self._clients[model_key], model_key, model_name, device, cache_folder)
                    else:
                        # ì„œë²„ëŠ” ì‚´ì•„ìˆì§€ë§Œ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ - ëª¨ë¸ ë¡œë“œ ì‹œë„
                        logger.info(f"ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì„œë²„ëŠ” ì‚´ì•„ìˆì§€ë§Œ ëª¨ë¸ ë¯¸ë¡œë“œ - ëª¨ë¸ ë¡œë“œ ì‹œë„: {model_key}")
                        load_result = self._clients[model_key].load_model(
                            model_name=model_name,
                            device=device,
                            cache_folder=cache_folder
                        )
                        if load_result.get("status") == "success":
                            logger.info(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_key}")
                            return SentenceTransformerProxy(self._clients[model_key], model_key, model_name, device, cache_folder)
                        else:
                            logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - í´ë¼ì´ì–¸íŠ¸ ì¬ìƒì„±: {model_key}")
                            self._clients[model_key].stop_server()
                            del self._clients[model_key]
                else:
                    logger.warning(f"ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ë¶ˆì•ˆì • - ì¬ìƒì„±: {model_key}")
                    # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
                    self._clients[model_key].stop_server()
                    del self._clients[model_key]
            except Exception as e:
                logger.warning(f"Health check ì‹¤íŒ¨ - í´ë¼ì´ì–¸íŠ¸ ì¬ìƒì„±: {model_key}, ì˜¤ë¥˜: {e}")
                # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
                try:
                    self._clients[model_key].stop_server()
                except Exception:
                    pass
                del self._clients[model_key]
        
        # ëª¨ë¸ë³„ ë½ ìƒì„±
        if model_key not in self._model_locks:
            self._model_locks[model_key] = threading.Lock()
        
        # ìŠ¤ë ˆë“œ ì•ˆì „í•˜ê²Œ í´ë¼ì´ì–¸íŠ¸ ìƒì„± (60ì´ˆ íƒ€ì„ì•„ì›ƒ)
        lock_acquired = self._model_locks[model_key].acquire(timeout=60.0)
        if not lock_acquired:
            logger.error(f"ëª¨ë¸ ë½ íšë“ ì‹¤íŒ¨ (60ì´ˆ íƒ€ì„ì•„ì›ƒ): {model_key}")
            raise RuntimeError(f"SentenceTransformer ëª¨ë¸ ë½ íƒ€ì„ì•„ì›ƒ: {model_name}")
        
        try:
            # ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸ (ë‹¤ë¥¸ ìŠ¤ë ˆë“œì—ì„œ ìƒì„±í–ˆì„ ìˆ˜ ìˆìŒ)
            if model_key in self._clients:
                logger.info(f"ëŒ€ê¸° ì¤‘ ë‹¤ë¥¸ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ì™„ë£Œ: {model_key}")
                return SentenceTransformerProxy(self._clients[model_key], model_key, model_name, device, cache_folder)
            
            try:
                logger.info(f"ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹œì‘: {model_key}")
                
                # ìºì‹œ í´ë” ì„¤ì •
                if cache_folder is None:
                    cache_folder = os.path.join(MODELS_DIR, 'sentence_transformers')
                os.makedirs(cache_folder, exist_ok=True)
                
                # ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                client = SentenceTransformerClient(
                    server_script_path=self._server_script_path,
                    startup_timeout=60.0,  # ì¶©ë¶„í•œ ì‹œê°„
                    request_timeout=180.0  # 3ë¶„ (ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ì¶©ë¶„í•œ ì‹œê°„)
                )
                
                # ì„œë²„ ì‹œì‘ ë° ëª¨ë¸ ë¡œë”©
                logger.info(f"ì„œë²„ ì‹œì‘ ë° ëª¨ë¸ ë¡œë”©: {model_name} (device: {device})")
                
                # ì„œë²„ ì‹œì‘
                if not client.start_server():
                    raise RuntimeError(f"ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {model_name}")
                
                # ëª¨ë¸ ë¡œë”©
                load_result = client.load_model(
                    model_name=model_name,
                    device=device,
                    cache_folder=cache_folder
                )
                
                if load_result.get("status") != "success":
                    error_msg = load_result.get("error", "Unknown error")
                    raise RuntimeError(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {error_msg}")
                
                # í´ë¼ì´ì–¸íŠ¸ ì €ì¥
                self._clients[model_key] = client
                logger.info(f"í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_key}")
                
                return SentenceTransformerProxy(client, model_key, model_name, device, cache_folder)
                
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_key}, ì˜¤ë¥˜: {e}")
                logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                import traceback
                logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
                # fallback ì—†ìŒ - ë°”ë¡œ ì˜ˆì™¸ ë°œìƒ
                raise RuntimeError(f"SentenceTransformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name}") from e
        finally:
            # ë½ í•­ìƒ í•´ì œ
            self._model_locks[model_key].release()
    
    async def get_gpu_semaphore(self):
        """GPU ì„¸ë§ˆí¬ì–´ ë°˜í™˜"""
        return self._gpu_semaphore
    
    def get_loaded_models(self) -> Dict[str, str]:
        """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        loaded_models = {}
        for key, client in self._clients.items():
            try:
                health_result = client.health_check(force=False)
                if health_result.get("status") == "success":
                    model_info = health_result.get("result", {})
                    model_name = model_info.get("model_name", "Unknown")
                    loaded_models[key] = f"SentenceTransformerProxy({model_name})"
                else:
                    loaded_models[key] = "Disconnected"
            except Exception:
                loaded_models[key] = "Error"
        return loaded_models
    
    def get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB)"""
        import psutil
        import torch
        
        memory_info = {
            'system_used': psutil.virtual_memory().percent,
            'process_memory': psutil.Process().memory_info().rss / (1024 * 1024),
            'loaded_clients': len(self._clients)
        }
        
        # ê° í´ë¼ì´ì–¸íŠ¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìˆ˜ì§‘
        total_server_memory = 0
        for key, client in self._clients.items():
            try:
                health_result = client.health_check(force=False)
                if health_result.get("status") == "success":
                    server_info = health_result.get("result", {})
                    server_memory = server_info.get("memory_usage", 0)
                    total_server_memory += server_memory
            except Exception:
                pass
        
        memory_info['server_processes_memory'] = total_server_memory
        
        if torch.cuda.is_available():
            memory_info['gpu_allocated'] = torch.cuda.memory_allocated() / (1024 * 1024)
            memory_info['gpu_cached'] = torch.cuda.memory_reserved() / (1024 * 1024)
        
        return memory_info
    
    def restart_server(self, model_name: str, device: str = None):
        """íŠ¹ì • ëª¨ë¸ì˜ ì„œë²„ ì¬ì‹œì‘"""
        if device is None:
            device = str(get_device())
        
        model_key = f"{model_name}_{device}"
        
        # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ
        if model_key in self._clients:
            try:
                logger.info(f"ì„œë²„ ì¬ì‹œì‘ ì¤‘: {model_key}")
                self._clients[model_key].stop_server()
                del self._clients[model_key]
            except Exception as e:
                logger.warning(f"ì„œë²„ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info(f"ì„œë²„ ì¬ì‹œì‘ ì™„ë£Œ: {model_key}")
    
    def unload_model_from_gpu(self, model_key: str):
        """íŠ¹ì • ëª¨ë¸ì„ GPUì—ì„œ RAMìœ¼ë¡œ ìŠ¤ì™‘ (ì„œë²„ëŠ” ìœ ì§€)"""
        if model_key not in self._clients:
            return
        
        try:
            # GPU ëª¨ë¸ì¸ ê²½ìš°ì—ë§Œ ìŠ¤ì™‘
            if 'cuda' in model_key or 'gpu' in model_key:
                logger.info(f"ëª¨ë¸ì„ GPUì—ì„œ RAMìœ¼ë¡œ ìŠ¤ì™‘: {model_key}")
                client = self._clients[model_key]
                
                # GPUâ†’CPU ìŠ¤ì™‘ (ì„œë²„ëŠ” ìœ ì§€, ëª¨ë¸ë§Œ ì´ë™)
                swap_result = client.swap_to_cpu()
                
                if swap_result.get("status") == "success":
                    logger.info(f"GPUâ†’RAM ìŠ¤ì™‘ ì™„ë£Œ: {model_key}")
                    # GPU ìºì‹œ ì •ë¦¬
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                else:
                    logger.warning(f"GPUâ†’RAM ìŠ¤ì™‘ ì‹¤íŒ¨: {swap_result.get('error')}")
        except Exception as e:
            logger.warning(f"GPU ìŠ¤ì™‘ ì‹¤íŒ¨: {model_key}, ì˜¤ë¥˜: {e}")
    
    def clear_cache(self):
        """ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
        logger.info("í´ë¼ì´ì–¸íŠ¸ ìºì‹œ ì •ë¦¬ ì‹œì‘")
        
        # ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ ì„œë²„ ì¢…ë£Œ
        for key, client in self._clients.items():
            try:
                logger.info(f"í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì¤‘: {key}")
                client.stop_server()
            except Exception as e:
                logger.warning(f"í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì‹¤íŒ¨: {key}, ì˜¤ë¥˜: {e}")
        
        self._clients.clear()
        self._model_locks.clear()
        logger.info("í´ë¼ì´ì–¸íŠ¸ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

class SentenceTransformerProxy:
    """
    SentenceTransformer í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ - Subprocess Client Wrapper
    
    ê¸°ì¡´ SentenceTransformer APIì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
    ë‚´ë¶€ì ìœ¼ë¡œëŠ” subprocess serverì™€ í†µì‹ 
    """
    
    def __init__(self, client: SentenceTransformerClient, model_key: str = None, 
                 model_name: str = None, device: str = None, cache_folder: str = None):
        self.client = client
        self._model_info = None
        self._model_key = model_key  # GPU ì–¸ë¡œë“œë¥¼ ìœ„í•œ í‚¤ ì €ì¥
        self._model_name = model_name  # ì¬ì—°ê²°ì„ ìœ„í•œ ëª¨ë¸ ì´ë¦„ ì €ì¥
        self._device = device  # ì¬ì—°ê²°ì„ ìœ„í•œ ë””ë°”ì´ìŠ¤ ì €ì¥
        self._cache_folder = cache_folder  # ì¬ì—°ê²°ì„ ìœ„í•œ ìºì‹œ í´ë” ì €ì¥
        
        # ëª¨ë¸ ì •ë³´ ìºì‹œ
        try:
            health_result = client.health_check(force=True)
            if health_result.get("status") == "success":
                self._model_info = health_result.get("result", {})
        except Exception as e:
            logger.warning(f"ëª¨ë¸ ì •ë³´ ìºì‹œ ì‹¤íŒ¨: {e}")
    
    def encode(self, sentences: List[str], **kwargs) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (SentenceTransformer í˜¸í™˜)
        GPU ì‚¬ìš© í›„ ìë™ìœ¼ë¡œ RAMìœ¼ë¡œ ìŠ¤ì™‘ (ì„œë²„ëŠ” ìœ ì§€)
        
        Args:
            sentences: ì¸ì½”ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            **kwargs: ì¶”ê°€ ì¸ì (auto_swap=Falseë¡œ ìŠ¤ì™‘ ë¹„í™œì„±í™” ê°€ëŠ¥)
            
        Returns:
            ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
        """
        # auto_swap ì˜µì…˜ í™•ì¸ (ê¸°ë³¸ê°’: GPUëŠ” True, CPUëŠ” False)
        if 'cuda' in str(self.device) or 'gpu' in str(self.device):
            auto_swap = kwargs.pop('auto_swap', True)  # GPUëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ìŠ¤ì™‘
            # ë ˆê±°ì‹œ í˜¸í™˜ì„±: auto_unloadê°€ ìˆìœ¼ë©´ auto_swapìœ¼ë¡œ ë§¤í•‘
            if 'auto_unload' in kwargs:
                auto_swap = kwargs.pop('auto_unload')
        else:
            auto_swap = kwargs.pop('auto_swap', False)  # CPUëŠ” ìŠ¤ì™‘ ì•ˆí•¨
            kwargs.pop('auto_unload', None)  # ë ˆê±°ì‹œ íŒŒë¼ë¯¸í„° ì œê±°
        
        # ë‹¨ì¼ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(sentences, str):
            sentences = [sentences]
            return_single = True
        else:
            return_single = False
        
        try:
            # GPU ëª¨ë¸ì¸ ê²½ìš° í•„ìš”ì‹œ GPUë¡œ ìŠ¤ì™‘
            if auto_swap and ('cuda' in str(self.device) or 'gpu' in str(self.device)):
                # í˜„ì¬ ëª¨ë¸ì´ CPUì— ìˆìœ¼ë©´ GPUë¡œ ì´ë™
                try:
                    # Health checkë¡œ í˜„ì¬ device í™•ì¸
                    health_result = self.client.health_check(force=True)
                    if health_result.get("status") == "success":
                        current_device = health_result.get("result", {}).get("device", "unknown")
                        if current_device == "cpu":
                            logger.info(f"ëª¨ë¸ì´ CPUì— ìˆìŒ, GPUë¡œ ìŠ¤ì™‘: {self._model_key}")
                            swap_result = self.client.swap_to_gpu(self._device)
                            if swap_result.get("status") != "success":
                                logger.warning(f"GPU ìŠ¤ì™‘ ì‹¤íŒ¨, CPUì—ì„œ ê³„ì† ì§„í–‰: {swap_result.get('error')}")
                except Exception as e:
                    logger.warning(f"GPU ìŠ¤ì™‘ ì²´í¬ ì¤‘ ì˜¤ë¥˜, ê³„ì† ì§„í–‰: {e}")
            
            # ìµœëŒ€ 2ë²ˆ ì‹œë„ (ì²« ì‹œë„ ì‹¤íŒ¨ ì‹œ ì¬ì—°ê²° í›„ ì¬ì‹œë„)
            max_attempts = 2
            
            for attempt in range(max_attempts):
                try:
                    # ì„œë²„ì— ì¸ì½”ë”© ìš”ì²­
                    response = self.client.encode_texts(sentences, **kwargs)
                    
                    if response.get("status") == "success":
                        # ì„±ê³µí•˜ë©´ ë°”ë¡œ ë°˜í™˜
                        break
                    else:
                        error_msg = response.get("error", "Unknown error")
                        
                        # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ì¬ì—°ê²° ì‹œë„
                        if "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ" in error_msg or "not loaded" in error_msg.lower():
                            if attempt < max_attempts - 1 and self._model_name and self._model_key:
                                logger.warning(f"ëª¨ë¸ ë¯¸ë¡œë“œ ê°ì§€, ì¬ì—°ê²° ì‹œë„ ({attempt+1}/{max_attempts}): {self._model_key}")
                                # ë§¤ë‹ˆì €ë¥¼ í†µí•´ ìƒˆ í´ë¼ì´ì–¸íŠ¸ íšë“ (ëª¨ë¸ ë¡œë”© í¬í•¨)
                                new_proxy = _manager.get_model(self._model_name, self._device, self._cache_folder)
                                # ìƒˆ í´ë¼ì´ì–¸íŠ¸ë¡œ êµì²´
                                self.client = new_proxy.client
                                self._model_info = new_proxy._model_info
                                logger.info(f"í´ë¼ì´ì–¸íŠ¸ ì¬ì—°ê²° ë° ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self._model_key}")
                                continue  # ë‹¤ìŒ ì‹œë„ë¡œ
                        
                        # ì¬ì—°ê²°í•´ë„ ì•ˆ ë˜ê±°ë‚˜ ë‹¤ë¥¸ ì—ëŸ¬ë©´ ì˜ˆì™¸ ë°œìƒ
                        raise RuntimeError(f"í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì‹¤íŒ¨: {error_msg}")
                        
                except Exception as e:
                    # ì—°ê²° ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
                    if attempt < max_attempts - 1 and self._model_name and self._model_key:
                        logger.warning(f"ì¸ì½”ë”© ì‹¤íŒ¨, ì¬ì—°ê²° ì‹œë„ ({attempt+1}/{max_attempts}): {e}")
                        try:
                            # ë§¤ë‹ˆì €ë¥¼ í†µí•´ ìƒˆ í´ë¼ì´ì–¸íŠ¸ íšë“ (ëª¨ë¸ ë¡œë”© í¬í•¨)
                            new_proxy = _manager.get_model(self._model_name, self._device, self._cache_folder)
                            # ìƒˆ í´ë¼ì´ì–¸íŠ¸ë¡œ êµì²´
                            self.client = new_proxy.client
                            self._model_info = new_proxy._model_info
                            logger.info(f"í´ë¼ì´ì–¸íŠ¸ ì¬ì—°ê²° ë° ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self._model_key}")
                            continue  # ë‹¤ìŒ ì‹œë„ë¡œ
                        except Exception as reconnect_error:
                            logger.error(f"ì¬ì—°ê²° ì‹¤íŒ¨: {reconnect_error}")
                            raise RuntimeError(f"í´ë¼ì´ì–¸íŠ¸ ì¬ì—°ê²° ì‹¤íŒ¨: {reconnect_error}") from e
                    else:
                        # ë§ˆì§€ë§‰ ì‹œë„ì´ê±°ë‚˜ ëª¨ë¸ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°œìƒ
                        raise RuntimeError(f"í…ìŠ¤íŠ¸ ì¸ì½”ë”© ìµœì¢… ì‹¤íŒ¨: {e}") from e
            
            embeddings = response.get("result", {}).get("embeddings", [])
            
            # ë‹¨ì¼ ë¬¸ìì—´ ì…ë ¥ì´ì—ˆë‹¤ë©´ ë‹¨ì¼ ê²°ê³¼ ë°˜í™˜
            # í…ì„œì™€ ë¦¬ìŠ¤íŠ¸ ëª¨ë‘ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •
            if return_single:
                # í…ì„œì¸ ê²½ìš°
                if torch.is_tensor(embeddings):
                    if embeddings.numel() > 0:  # í…ì„œê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°
                        return embeddings[0] if embeddings.dim() > 1 else embeddings
                # ë¦¬ìŠ¤íŠ¸ë‚˜ ë°°ì—´ì¸ ê²½ìš°
                elif embeddings and len(embeddings) > 0:
                    return embeddings[0]
            
            return embeddings
            
        finally:
            # GPU ì‚¬ìš© í›„ ìë™ ìŠ¤ì™‘ (GPU ëª¨ë¸ì¸ ê²½ìš°ì—ë§Œ RAMìœ¼ë¡œ ì´ë™)
            if auto_swap and self._model_key and ('cuda' in str(self.device) or 'gpu' in str(self.device)):
                try:
                    logger.debug(f"ì„ë² ë”© ì™„ë£Œ, ëª¨ë¸ì„ RAMìœ¼ë¡œ ìŠ¤ì™‘: {self._model_key}")
                    # í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ GPUâ†’CPU ìŠ¤ì™‘ (ì„œë²„ëŠ” ìœ ì§€)
                    swap_result = self.client.swap_to_cpu()
                    if swap_result.get("status") == "success":
                        logger.debug(f"GPUâ†’RAM ìŠ¤ì™‘ ì„±ê³µ: {self._model_key}")
                        # GPU ìºì‹œë„ ì •ë¦¬
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()  # GPU ì—°ì‚° ì™„ë£Œ ëŒ€ê¸°
                    else:
                        logger.warning(f"GPUâ†’RAM ìŠ¤ì™‘ ì‹¤íŒ¨: {swap_result.get('error')}")
                except Exception as e:
                    logger.warning(f"GPU ìë™ ìŠ¤ì™‘ ì‹¤íŒ¨: {e}")
    
    @property
    def max_seq_length(self) -> Optional[int]:
        """ëª¨ë¸ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"""
        if self._model_info:
            return self._model_info.get("model_max_seq_length")
        return None
    
    @property
    def device(self) -> str:
        """ëª¨ë¸ì´ ë¡œë“œëœ ë””ë°”ì´ìŠ¤"""
        if self._model_info:
            return self._model_info.get("device", "unknown")
        return "unknown"
    
    def get_sentence_embedding_dimension(self) -> int:
        """ëª¨ë¸ì˜ ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        if self._model_info:
            # embedding_dimension ë˜ëŠ” hidden_size í‚¤ ì‚¬ìš©
            dim = self._model_info.get("embedding_dimension")
            if dim is not None:
                return dim
            # fallbackìœ¼ë¡œ hidden_size í™•ì¸
            dim = self._model_info.get("hidden_size")
            if dim is not None:
                return dim
        # ê¸°ë³¸ê°’ ë°˜í™˜ (ì¼ë°˜ì ì¸ ì„ë² ë”© ì°¨ì›)
        return 768
    
    def __repr__(self) -> str:
        model_name = self._model_info.get("model_name", "Unknown") if self._model_info else "Unknown"
        device = self.device
        return f"SentenceTransformerProxy(model='{model_name}', device='{device}')"

# ì „ì—­ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_manager = SentenceTransformerManager()

def get_sentence_transformer(model_name: str, device: str = None, cache_folder: str = None):
    """
    ê³µìœ  SentenceTransformer ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Subprocess Server ê¸°ë°˜)
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        device: ë””ë°”ì´ìŠ¤ ì„¤ì •
        cache_folder: ìºì‹œ í´ë”
        
    Returns:
        SentenceTransformerProxy ì¸ìŠ¤í„´ìŠ¤ (SentenceTransformer í˜¸í™˜)
    """
    return _manager.get_model(model_name, device, cache_folder)

async def get_gpu_semaphore():
    """GPU ì„¸ë§ˆí¬ì–´ ë°˜í™˜"""
    return await _manager.get_gpu_semaphore()

def get_model_info():
    """ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
    return _manager.get_loaded_models()

def get_memory_info():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
    return _manager.get_memory_usage()

def clear_model_cache():
    """ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
    _manager.clear_cache()

def get_client_info():
    """í™œì„± í´ë¼ì´ì–¸íŠ¸ ì •ë³´ ë°˜í™˜"""
    client_info = {}
    for key, client in _manager._clients.items():
        try:
            health_result = client.health_check(force=False)
            client_info[key] = health_result
        except Exception as e:
            client_info[key] = {"status": "error", "error": str(e)}
    return client_info