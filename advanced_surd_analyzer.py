"""
ê³ ê¸‰ SURD ì‹œìŠ¤í…œ - Linux ì „ìš©
Advanced SURD (Synergy, Unique, Redundant, Deterministic) Analysis System for Linux

ì‹¤ì œ ì •ë³´ì´ë¡ ê³¼ ê³ ê¸‰ AI ê¸°ë²•ì„ ì‚¬ìš©í•œ ì¸ê³¼ê´€ê³„ ë¶„ì„ ì‹œìŠ¤í…œ
Kraskov ìƒí˜¸ì •ë³´ëŸ‰ ì¶”ì •ê³¼ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì¸ê³¼ ì¶”ë¡ ì„ ê²°í•©
"""

import os
# CVE-2025-32434ëŠ” ê°€ì§œ CVE - torch_security_patch import ì œê±°
# import torch_security_patch

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
import json
import time
import asyncio
from typing import Dict, List, Tuple, Any, Optional, Union, Set
from dataclasses import dataclass, field
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import itertools
import math
import threading
from collections import defaultdict

# ê³ ê¸‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from sklearn.neighbors import NearestNeighbors
from sklearn.feature_selection import mutual_info_regression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
from scipy.special import digamma, gamma
from scipy.stats import entropy, gaussian_kde
from scipy.optimize import minimize
import networkx as nx
from joblib import Parallel, delayed
import warnings
warnings.filterwarnings('ignore')

from config import ADVANCED_CONFIG, MODELS_DIR, get_device
from data_models import (
    SURDAnalysisResult, CausalGraph, CausalPath, 
    AdvancedSURDResult, CausalNetwork, InformationDecomposition
)

# ë¡œê±° ì„¤ì • (ë¨¼ì €)
logger = logging.getLogger('RedHeartLinux.AdvancedSURD')

# ìƒˆë¡œìš´ ëª¨ë¸ ì„í¬íŠ¸
try:
    from models.surd_models.causal_analysis_models import (
        AdvancedSURDAnalyzer as NewSURDAnalyzer,
        SURDConfig, InformationMeasures as NewInformationMeasures,
        KraskovEstimator, PIDDecomposition, CausalNetworkBuilder
    )
    NEW_MODELS_AVAILABLE = True
except ImportError:
    NEW_MODELS_AVAILABLE = False
    logger.warning("ìƒˆë¡œìš´ SURD ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ êµ¬í˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# LLM í†µí•©
try:
    from llm_module.advanced_llm_engine import get_llm_engine, TaskComplexity, explain_causal_relationships
    LLM_INTEGRATION_AVAILABLE = True
except ImportError:
    LLM_INTEGRATION_AVAILABLE = False
    logger.warning("LLM í†µí•©ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ê³ ê¸‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ìš©ì„± í™•ì¸
ADVANCED_LIBS_AVAILABLE = True
try:
    import torch
    import sklearn
    import scipy
    import networkx
    from joblib import Parallel, delayed
    assert torch.cuda.is_available() if ADVANCED_CONFIG['enable_gpu'] else True
except ImportError as e:
    ADVANCED_LIBS_AVAILABLE = False
    raise ImportError(f"ê³ ê¸‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: {e}")

logger = logging.getLogger('RedHeart.AdvancedSURDAnalyzer')


@dataclass
class InformationMeasures:
    """ì •ë³´ ì´ë¡ ì  ì¸¡ì •ê°’ë“¤"""
    mutual_information: float
    conditional_mutual_information: float
    transfer_entropy: float
    partial_information_decomposition: Dict[str, float]
    causal_strength: float
    confidence_interval: Tuple[float, float]


class NeuralCausalModel(nn.Module):
    """ì‹ ê²½ë§ ê¸°ë°˜ ì¸ê³¼ê´€ê³„ ëª¨ë¸"""
    
    def __init__(self, input_dim: int, hidden_dims: List[int] = [128, 64, 32]):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        
        # ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.BatchNorm1d(hidden_dim)
            ])
            prev_dim = hidden_dim
            
        self.encoder = nn.Sequential(*layers)
        
        # ì¸ê³¼ ê´€ê³„ ì˜ˆì¸¡ í—¤ë“œ
        self.causal_head = nn.Sequential(
            nn.Linear(prev_dim, prev_dim // 2),
            nn.ReLU(),
            nn.Linear(prev_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # ì‹œë„ˆì§€ ì˜ˆì¸¡ í—¤ë“œ
        self.synergy_head = nn.Sequential(
            nn.Linear(prev_dim, prev_dim // 2),
            nn.ReLU(),
            nn.Linear(prev_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # ì¤‘ë³µì„± ì˜ˆì¸¡ í—¤ë“œ
        self.redundancy_head = nn.Sequential(
            nn.Linear(prev_dim, prev_dim // 2),
            nn.ReLU(),
            nn.Linear(prev_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = nn.MultiheadAttention(
            embed_dim=prev_dim,
            num_heads=8,
            batch_first=True
        )
        
    def forward(self, x):
        # ì¸ì½”ë”©
        encoded = self.encoder(x)
        
        # ì–´í…ì…˜ ì ìš©
        attended, attention_weights = self.attention(
            encoded.unsqueeze(1), 
            encoded.unsqueeze(1), 
            encoded.unsqueeze(1)
        )
        attended = attended.squeeze(1)
        
        # ê° í—¤ë“œë³„ ì˜ˆì¸¡
        causal_strength = self.causal_head(attended)
        synergy_score = self.synergy_head(attended)
        redundancy_score = self.redundancy_head(attended)
        
        return {
            'causal_strength': causal_strength,
            'synergy_score': synergy_score,
            'redundancy_score': redundancy_score,
            'attention_weights': attention_weights,
            'encoded_features': encoded
        }


class KraskovEstimator:
    """ê³ ê¸‰ Kraskov ìƒí˜¸ì •ë³´ëŸ‰ ì¶”ì •ê¸°"""
    
    def __init__(self, k: int = 3, base: float = 2.0):
        self.k = k
        self.base = base
        self.cache = {}
        
    def estimate_mi(self, X: np.ndarray, Y: np.ndarray) -> Tuple[float, float]:
        """ìƒí˜¸ì •ë³´ëŸ‰ê³¼ ì‹ ë¢°êµ¬ê°„ ì¶”ì •"""
        
        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(X, Y)
        if cache_key in self.cache:
            return self.cache[cache_key]
            
        # ì…ë ¥ ê²€ì¦ ë° ì „ì²˜ë¦¬
        X, Y = self._preprocess_data(X, Y)
        n = len(X)
        
        if n < self.k + 1:
            logger.warning(f"ìƒ˜í”Œ ìˆ˜({n})ê°€ k({self.k})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")
            return 0.0, (0.0, 0.0)
            
        try:
            # Kraskov Algorithm 1 êµ¬í˜„
            mi = self._kraskov_algorithm_1(X, Y)
            
            # ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
            confidence_interval = self._bootstrap_confidence_interval(X, Y)
            
            # ìºì‹œ ì €ì¥
            result = (mi, confidence_interval)
            self.cache[cache_key] = result
            
            return result
            
        except Exception as e:
            logger.error(f"Kraskov ì¶”ì • ì‹¤íŒ¨: {e}")
            return 0.0, (0.0, 0.0)
            
    def _preprocess_data(self, X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        if X.ndim == 1:
            X = X.reshape(-1, 1)
        if Y.ndim == 1:
            Y = Y.reshape(-1, 1)
            
        # NaN ì œê±°
        valid_mask = ~(np.isnan(X).any(axis=1) | np.isnan(Y).any(axis=1))
        X = X[valid_mask]
        Y = Y[valid_mask]
        
        # ì •ê·œí™”
        scaler_x = StandardScaler()
        scaler_y = StandardScaler()
        X = scaler_x.fit_transform(X)
        Y = scaler_y.fit_transform(Y)
        
        return X, Y
        
    def _kraskov_algorithm_1(self, X: np.ndarray, Y: np.ndarray) -> float:
        """Kraskov Algorithm 1 êµ¬í˜„"""
        n = len(X)
        
        # ê²°í•© ê³µê°„ êµ¬ì„±
        XY = np.column_stack([X, Y])
        
        # k+1 ìµœê·¼ì ‘ ì´ì›ƒ ì°¾ê¸° (ìê¸° ìì‹  í¬í•¨)
        nbrs_XY = NearestNeighbors(n_neighbors=self.k+1, metric='chebyshev')
        nbrs_XY.fit(XY)
        distances_XY, _ = nbrs_XY.kneighbors(XY)
        
        # ê°œë³„ ê³µê°„ì—ì„œ ì´ì›ƒ ì°¾ê¸°
        nbrs_X = NearestNeighbors(metric='chebyshev')
        nbrs_Y = NearestNeighbors(metric='chebyshev')
        nbrs_X.fit(X)
        nbrs_Y.fit(Y)
        
        mi_sum = 0.0
        valid_points = 0
        
        for i in range(n):
            # kë²ˆì§¸ ì´ì›ƒê¹Œì§€ì˜ ê±°ë¦¬ (ìê¸° ìì‹  ì œì™¸)
            eps = distances_XY[i, self.k]
            
            if eps > 0:  # ê±°ë¦¬ê°€ 0ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                # eps-ball ë‚´ì˜ ì  ê°œìˆ˜ ê³„ì‚°
                nx = len(nbrs_X.radius_neighbors([X[i]], eps, return_distance=False)[0]) - 1
                ny = len(nbrs_Y.radius_neighbors([Y[i]], eps, return_distance=False)[0]) - 1
                
                if nx > 0 and ny > 0:
                    mi_sum += digamma(self.k) - digamma(nx + 1) - digamma(ny + 1) + digamma(n)
                    valid_points += 1
                    
        if valid_points > 0:
            mi = mi_sum / valid_points
            # ìì—°ë¡œê·¸ì—ì„œ ì§€ì •ëœ baseë¡œ ë³€í™˜
            mi = mi / np.log(self.base)
            return max(0.0, mi)
        else:
            return 0.0
            
    def _bootstrap_confidence_interval(self, X: np.ndarray, Y: np.ndarray, 
                                     n_bootstrap: int = 100, alpha: float = 0.05) -> Tuple[float, float]:
        """ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„ ê³„ì‚°"""
        bootstrap_mis = []
        n = len(X)
        
        for _ in range(n_bootstrap):
            # ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§
            indices = np.random.choice(n, n, replace=True)
            X_boot = X[indices]
            Y_boot = Y[indices]
            
            try:
                mi_boot = self._kraskov_algorithm_1(X_boot, Y_boot)
                bootstrap_mis.append(mi_boot)
            except:
                continue
                
        if bootstrap_mis:
            lower = np.percentile(bootstrap_mis, (alpha/2) * 100)
            upper = np.percentile(bootstrap_mis, (1 - alpha/2) * 100)
            return (lower, upper)
        else:
            return (0.0, 0.0)
            
    def _get_cache_key(self, X: np.ndarray, Y: np.ndarray) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # í•´ì‹œê°’ ê¸°ë°˜ ìºì‹œ í‚¤
        x_hash = hash(X.tobytes())
        y_hash = hash(Y.tobytes())
        return f"{x_hash}_{y_hash}_{self.k}"


class AdvancedPIDDecomposer:
    """ê³ ê¸‰ ë¶€ë¶„ì •ë³´ë¶„í•´(Partial Information Decomposition) ì‹œìŠ¤í…œ"""
    
    def __init__(self, estimator: KraskovEstimator):
        self.estimator = estimator
        self.decomposition_cache = {}
        
    def decompose_information(self, sources: Dict[str, np.ndarray], 
                            target: np.ndarray) -> InformationDecomposition:
        """ì •ë³´ ë¶„í•´ ìˆ˜í–‰"""
        
        source_names = list(sources.keys())
        n_sources = len(source_names)
        
        if n_sources < 2:
            logger.warning("PIDëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ ì†ŒìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return InformationDecomposition()
            
        # ê° í•­ëª© ê³„ì‚°
        redundancy = self._calculate_redundancy(sources, target)
        unique_info = self._calculate_unique_information(sources, target)
        synergy = self._calculate_synergy(sources, target, redundancy, unique_info)
        
        # ì „ì²´ ì •ë³´ëŸ‰
        all_sources = np.column_stack(list(sources.values()))
        total_mi, _ = self.estimator.estimate_mi(all_sources, target)
        
        return InformationDecomposition(
            redundancy=redundancy,
            unique_information=unique_info,
            synergy=synergy,
            total_information=total_mi,
            sources=source_names
        )
        
    def _calculate_redundancy(self, sources: Dict[str, np.ndarray], 
                            target: np.ndarray) -> Dict[str, float]:
        """ì¤‘ë³µ ì •ë³´ ê³„ì‚° (Williams & Beer ë°©ë²•)"""
        redundancy = {}
        source_names = list(sources.keys())
        
        # 2-way ì¤‘ë³µì„±
        for i in range(len(source_names)):
            for j in range(i+1, len(source_names)):
                name1, name2 = source_names[i], source_names[j]
                
                # ê°œë³„ MI ê³„ì‚°
                mi1, _ = self.estimator.estimate_mi(sources[name1], target)
                mi2, _ = self.estimator.estimate_mi(sources[name2], target)
                
                # ìµœì†Œê°’ ë°©ë²• (Williams & Beer)
                redundancy[f"{name1}_{name2}"] = min(mi1, mi2)
                
        # 3-way ì´ìƒì˜ ì¤‘ë³µì„± (ê·¼ì‚¬)
        if len(source_names) >= 3:
            for combo in itertools.combinations(source_names, 3):
                individual_mis = []
                for name in combo:
                    mi, _ = self.estimator.estimate_mi(sources[name], target)
                    individual_mis.append(mi)
                redundancy['_'.join(combo)] = min(individual_mis) * 0.5  # ë³´ì • ê³„ìˆ˜
                
        return redundancy
        
    def _calculate_unique_information(self, sources: Dict[str, np.ndarray], 
                                    target: np.ndarray) -> Dict[str, float]:
        """ê³ ìœ  ì •ë³´ ê³„ì‚°"""
        unique_info = {}
        source_names = list(sources.keys())
        
        for name in source_names:
            # í•´ë‹¹ ì†ŒìŠ¤ì˜ ê°œë³„ MI
            mi_alone, _ = self.estimator.estimate_mi(sources[name], target)
            
            # ë‹¤ë¥¸ ëª¨ë“  ì†ŒìŠ¤ë“¤ê³¼ì˜ ì¡°ê±´ë¶€ MI ê·¼ì‚¬
            other_sources = {k: v for k, v in sources.items() if k != name}
            
            if other_sources:
                # ì¡°ê±´ë¶€ MI ê·¼ì‚¬: I(X;Z|Y) â‰ˆ I(X,Y;Z) - I(Y;Z)
                combined_others = np.column_stack(list(other_sources.values()))
                mi_others, _ = self.estimator.estimate_mi(combined_others, target)
                
                # í˜„ì¬ ì†ŒìŠ¤ì™€ ë‹¤ë¥¸ ì†ŒìŠ¤ë“¤ì˜ ê²°í•© MI
                combined_with_current = np.column_stack([sources[name], combined_others])
                mi_combined, _ = self.estimator.estimate_mi(combined_with_current, target)
                
                # ê³ ìœ  ì •ë³´ = I(X;Z) - max(0, I(X,Y;Z) - I(Y;Z))
                conditional_contribution = max(0, mi_combined - mi_others)
                unique_info[name] = max(0, mi_alone - (mi_combined - conditional_contribution))
            else:
                unique_info[name] = mi_alone
                
        return unique_info
        
    def _calculate_synergy(self, sources: Dict[str, np.ndarray], target: np.ndarray,
                         redundancy: Dict[str, float], unique_info: Dict[str, float]) -> Dict[str, float]:
        """ì‹œë„ˆì§€ ì •ë³´ ê³„ì‚°"""
        synergy = {}
        source_names = list(sources.keys())
        
        # ì „ì²´ ì •ë³´ëŸ‰
        all_sources = np.column_stack(list(sources.values()))
        total_mi, _ = self.estimator.estimate_mi(all_sources, target)
        
        # ì‹œë„ˆì§€ = ì „ì²´ MI - (ê³ ìœ  ì •ë³´ í•© + ì¤‘ë³µ ì •ë³´ í•©)
        total_unique = sum(unique_info.values())
        total_redundancy = sum(redundancy.values())
        
        overall_synergy = max(0, total_mi - total_unique - total_redundancy)
        
        # 2-way ì‹œë„ˆì§€
        for i in range(len(source_names)):
            for j in range(i+1, len(source_names)):
                name1, name2 = source_names[i], source_names[j]
                
                # 2ê°œ ì†ŒìŠ¤ì˜ ê²°í•© MI
                combined = np.column_stack([sources[name1], sources[name2]])
                mi_combined, _ = self.estimator.estimate_mi(combined, target)
                
                # ì‹œë„ˆì§€ = ê²°í•© MI - ê°œë³„ MI í•©
                mi1, _ = self.estimator.estimate_mi(sources[name1], target)
                mi2, _ = self.estimator.estimate_mi(sources[name2], target)
                
                pairwise_synergy = max(0, mi_combined - mi1 - mi2 + redundancy.get(f"{name1}_{name2}", 0))
                synergy[f"{name1}_{name2}"] = pairwise_synergy
                
        # ì „ì²´ ì‹œë„ˆì§€ ë¶„ë°°
        if overall_synergy > 0 and synergy:
            # ì •ê·œí™”
            total_pairwise = sum(synergy.values())
            if total_pairwise > 0:
                normalization_factor = overall_synergy / total_pairwise
                for key in synergy:
                    synergy[key] *= normalization_factor
                    
        return synergy


class CausalNetworkAnalyzer:
    """ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.network_cache = {}
        
    def build_causal_network(self, decomposition_results: Dict[str, InformationDecomposition],
                           threshold: float = 0.01) -> CausalNetwork:
        """ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•"""
        
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
        G = nx.DiGraph()
        
        # ë…¸ë“œ ì¶”ê°€
        all_variables = set()
        for result in decomposition_results.values():
            all_variables.update(result.sources)
            
        for var in all_variables:
            G.add_node(var)
            
        # ì—£ì§€ ì¶”ê°€ (ì¸ê³¼ê´€ê³„)
        for target, decomp in decomposition_results.items():
            total_info = decomp.total_information
            
            if total_info > threshold:
                # ê³ ìœ  ì •ë³´ ê¸°ë°˜ ì§ì ‘ ì—°ê²°
                for source, unique_val in decomp.unique_information.items():
                    if unique_val > threshold:
                        G.add_edge(source, target, 
                                 weight=unique_val,
                                 edge_type='direct',
                                 strength=unique_val / total_info)
                        
                # ì‹œë„ˆì§€ ê¸°ë°˜ ê°„ì ‘ ì—°ê²°
                for synergy_pair, synergy_val in decomp.synergy.items():
                    if synergy_val > threshold:
                        sources = synergy_pair.split('_')
                        if len(sources) == 2:
                            # ì‹œë„ˆì§€ë¥¼ ìœ„í•œ ê°€ìƒ ë…¸ë“œ ìƒì„±
                            synergy_node = f"synergy_{synergy_pair}"
                            G.add_node(synergy_node, node_type='synergy')
                            
                            for source in sources:
                                G.add_edge(source, synergy_node,
                                         weight=synergy_val/2,
                                         edge_type='synergy_input')
                            G.add_edge(synergy_node, target,
                                     weight=synergy_val,
                                     edge_type='synergy_output')
                                     
        # ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ìˆ˜í–‰
        network_metrics = self._analyze_network_properties(G)
        
        return CausalNetwork(
            graph=G,
            metrics=network_metrics,
            threshold=threshold,
            decomposition_results=decomposition_results
        )
        
    def _analyze_network_properties(self, G: nx.DiGraph) -> Dict[str, Any]:
        """ë„¤íŠ¸ì›Œí¬ ì†ì„± ë¶„ì„"""
        metrics = {}
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        metrics['node_count'] = G.number_of_nodes()
        metrics['edge_count'] = G.number_of_edges()
        metrics['density'] = nx.density(G)
        
        # ì¤‘ì‹¬ì„± ì¸¡ì •
        if G.number_of_nodes() > 0:
            metrics['degree_centrality'] = nx.degree_centrality(G)
            metrics['betweenness_centrality'] = nx.betweenness_centrality(G)
            metrics['closeness_centrality'] = nx.closeness_centrality(G)
            
            # ê°€ì¥ ì¤‘ìš”í•œ ë…¸ë“œë“¤
            degree_centrality = metrics['degree_centrality']
            metrics['most_central_nodes'] = sorted(
                degree_centrality.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:5]
            
        # ì—°ê²°ì„± ë¶„ì„
        if G.number_of_nodes() > 1:
            # ì•½í•˜ê²Œ ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸
            weak_components = list(nx.weakly_connected_components(G))
            metrics['weak_component_count'] = len(weak_components)
            metrics['largest_component_size'] = max(len(comp) for comp in weak_components)
            
            # ê°•í•˜ê²Œ ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸
            strong_components = list(nx.strongly_connected_components(G))
            metrics['strong_component_count'] = len(strong_components)
            
        # ê²½ë¡œ ë¶„ì„
        try:
            if nx.is_weakly_connected(G):
                metrics['average_shortest_path_length'] = nx.average_shortest_path_length(G.to_undirected())
                metrics['diameter'] = nx.diameter(G.to_undirected())
            else:
                metrics['average_shortest_path_length'] = None
                metrics['diameter'] = None
        except:
            metrics['average_shortest_path_length'] = None
            metrics['diameter'] = None
            
        return metrics
        
    def find_causal_paths(self, network: CausalNetwork, 
                         source: str, target: str, max_length: int = 4) -> List[CausalPath]:
        """ì¸ê³¼ê´€ê³„ ê²½ë¡œ íƒìƒ‰"""
        G = network.graph
        
        if source not in G.nodes() or target not in G.nodes():
            return []
            
        paths = []
        
        try:
            # ëª¨ë“  ë‹¨ìˆœ ê²½ë¡œ ì°¾ê¸°
            simple_paths = nx.all_simple_paths(G, source, target, cutoff=max_length)
            
            for path in simple_paths:
                # ê²½ë¡œ ê°•ë„ ê³„ì‚°
                path_strength = self._calculate_path_strength(G, path)
                
                # ê²½ë¡œ íƒ€ì… ë¶„ì„
                path_types = self._analyze_path_types(G, path)
                
                causal_path = CausalPath(
                    path=path,
                    strength=path_strength,
                    length=len(path) - 1,
                    path_types=path_types
                )
                paths.append(causal_path)
                
        except nx.NetworkXNoPath:
            pass
            
        # ê°•ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        paths.sort(key=lambda x: x.strength, reverse=True)
        
        return paths
        
    def _calculate_path_strength(self, G: nx.DiGraph, path: List[str]) -> float:
        """ê²½ë¡œ ê°•ë„ ê³„ì‚°"""
        if len(path) < 2:
            return 0.0
            
        # ê²½ë¡œìƒì˜ ëª¨ë“  ì—£ì§€ ê°€ì¤‘ì¹˜ì˜ ê³±
        strength = 1.0
        for i in range(len(path) - 1):
            edge_data = G.get_edge_data(path[i], path[i+1])
            if edge_data and 'weight' in edge_data:
                strength *= edge_data['weight']
            else:
                strength *= 0.1  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
                
        return strength
        
    def _analyze_path_types(self, G: nx.DiGraph, path: List[str]) -> List[str]:
        """ê²½ë¡œ íƒ€ì… ë¶„ì„"""
        path_types = []
        
        for i in range(len(path) - 1):
            edge_data = G.get_edge_data(path[i], path[i+1])
            if edge_data and 'edge_type' in edge_data:
                path_types.append(edge_data['edge_type'])
            else:
                path_types.append('unknown')
                
        return path_types


class AdvancedSURDAnalyzer:
    """ê³ ê¸‰ SURD ë¶„ì„ ì‹œìŠ¤í…œ - Linux ì „ìš© AI ê°•í™” ë²„ì „"""
    
    def __init__(self):
        if not ADVANCED_LIBS_AVAILABLE:
            raise ImportError("ê³ ê¸‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
        self.logger = logger
        self.device = get_device()
        
        # ëª¨ë¸ ê°€ìš©ì„±ì„ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì„¤ì •
        self.new_models_available = NEW_MODELS_AVAILABLE
        self.llm_integration_available = LLM_INTEGRATION_AVAILABLE
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.kraskov_estimator = KraskovEstimator(k=5, base=2.0)
        self.pid_decomposer = AdvancedPIDDecomposer(self.kraskov_estimator)
        self.network_analyzer = CausalNetworkAnalyzer()
        
        # ì‹ ê²½ë§ ëª¨ë¸
        self.neural_causal_model = None
        self.is_model_trained = False
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.analysis_cache = {}
        self.cache_lock = threading.Lock()
        
        # ê³ ê¸‰ ì„¤ì •
        self.advanced_config = {
            'use_neural_causal_model': True,
            'use_bootstrap_confidence': True,
            'use_network_analysis': True,
            'parallel_processing': True,
            'max_variables': 10,
            'min_effect_threshold': 0.01,
            'confidence_level': 0.95,
            'bootstrap_samples': 1000,
            'max_cache_size': 1000
        }
        
        # ìŠ¤ë ˆë“œ í’€
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬
        self.model_dir = os.path.join(MODELS_DIR, 'surd_models')
        os.makedirs(self.model_dir, exist_ok=True)
        
        # =====================================================
        # ê°•í™” ëª¨ë“ˆ í†µí•© (23M ì¶”ê°€ â†’ ì´ 25M)
        # =====================================================
        base_dim = 768
        
        # 1. ì‹¬ì¸µ ì¸ê³¼ ì¶”ë¡  ë„¤íŠ¸ì›Œí¬ (10M)
        self.deep_causal = nn.ModuleDict({
            'causal_encoder': nn.Sequential(
                nn.Linear(base_dim, 1024),
                nn.LayerNorm(1024),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(1024, 768),
                nn.LayerNorm(768),
                nn.GELU(),
                nn.Linear(768, 512)
            ),
            'causal_graph': nn.ModuleList([
                nn.Sequential(
                    nn.Linear(512, 256),
                    nn.GELU(),
                    nn.Linear(256, 128),
                    nn.Linear(128, 1),
                    nn.Sigmoid()
                ) for _ in range(10)  # 10 causal paths
            ]),
            'path_aggregator': nn.Sequential(
                nn.Linear(10, 256),
                nn.GELU(),
                nn.Linear(256, 512),
                nn.LayerNorm(512),
                nn.Linear(512, 4)  # S,U,R,D
            )
        }).to(self.device)
        
        # 2. ì •ë³´ì´ë¡  ë¶„í•´ ë„¤íŠ¸ì›Œí¬ (8M)
        self.info_decomposition = nn.ModuleDict({
            'mutual_info': nn.Sequential(
                nn.Linear(base_dim * 2, base_dim),
                nn.LayerNorm(base_dim),
                nn.GELU(),
                nn.Linear(base_dim, 512),
                nn.Linear(512, 256),
                nn.Linear(256, 1),
                nn.Softplus()
            ),
            'pid_network': nn.ModuleDict({
                'synergy': nn.Sequential(
                    nn.Linear(base_dim, 512),
                    nn.GELU(),
                    nn.Linear(512, 256),
                    nn.Linear(256, 1),
                    nn.Sigmoid()
                ),
                'unique': nn.Sequential(
                    nn.Linear(base_dim, 512),
                    nn.GELU(),
                    nn.Linear(512, 256),
                    nn.Linear(256, 1),
                    nn.Sigmoid()
                ),
                'redundant': nn.Sequential(
                    nn.Linear(base_dim, 512),
                    nn.GELU(),
                    nn.Linear(512, 256),
                    nn.Linear(256, 1),
                    nn.Sigmoid()
                ),
                'deterministic': nn.Sequential(
                    nn.Linear(base_dim, 512),
                    nn.GELU(),
                    nn.Linear(512, 256),
                    nn.Linear(256, 1),
                    nn.Sigmoid()
                )
            }),
            'normalizer': nn.Sequential(
                nn.Linear(4, 64),
                nn.GELU(),
                nn.Linear(64, 4),
                nn.Softmax(dim=-1)
            )
        }).to(self.device)
        
        # 3. ë„¤íŠ¸ì›Œí¬ íš¨ê³¼ ë¶„ì„ (5M + 2M ì¶”ê°€ = 7M)
        self.network_effects = nn.ModuleDict({
            'graph_encoder': nn.ModuleList([
                nn.Sequential(
                    nn.Linear(base_dim, 512),
                    nn.LayerNorm(512),
                    nn.GELU(),
                    nn.Linear(512, 256),
                    nn.Linear(256, base_dim)
                ) for _ in range(3)  # 3 GNN layers
            ]),
            'centrality_predictor': nn.Sequential(
                nn.Linear(base_dim, 512),
                nn.LayerNorm(512),
                nn.GELU(),
                nn.Linear(512, 256),
                nn.Linear(256, 10)  # 10 centrality measures
            ),
            'community_detector': nn.Sequential(
                nn.Linear(base_dim, 512),
                nn.LayerNorm(512),
                nn.GELU(),
                nn.Linear(512, 256),
                nn.Linear(256, 5)  # 5 communities
            ),
            # ì¶”ê°€ ë ˆì´ì–´ (2M)
            'deep_network': nn.Sequential(
                nn.Linear(base_dim, 768),
                nn.LayerNorm(768),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(768, 512),
                nn.LayerNorm(512),
                nn.GELU(),
                nn.Linear(512, base_dim)
            )
        }).to(self.device)
        
        # íŒŒë¼ë¯¸í„° ë¡œê¹…
        total_params = sum(p.numel() for p in [
            *self.deep_causal.parameters(),
            *self.info_decomposition.parameters(),
            *self.network_effects.parameters()
        ])
        logger.info(f"âœ… SURD ë¶„ì„ê¸° ê°•í™” ëª¨ë“ˆ í†µí•©: {total_params/1e6:.1f}M íŒŒë¼ë¯¸í„° ì¶”ê°€")
        
        # ìƒˆë¡œìš´ ê³ ê¸‰ ëª¨ë¸ ì´ˆê¸°í™”
        if self.new_models_available:
            try:
                self.new_surd_config = SURDConfig(
                    num_variables=self.advanced_config['max_variables'],
                    embedding_dim=768,
                    k_neighbors=5,
                    bootstrap_samples=self.advanced_config['bootstrap_samples']
                )
                self.new_surd_analyzer = NewSURDAnalyzer(self.new_surd_config)
                self.logger.info("ìƒˆë¡œìš´ ê³ ê¸‰ SURD ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"ìƒˆë¡œìš´ SURD ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.new_models_available = False
        
        # LLM ì—”ì§„ ì—°ê²° (Claude ëª¨ë“œì¼ ë•ŒëŠ” ë¹„í™œì„±í™”)
        if os.environ.get('REDHEART_CLAUDE_MODE') == 'true':
            self.logger.info("ğŸ“¦ Claude ëª¨ë“œ ê°ì§€ - ë¡œì»¬ LLM ì—”ì§„ ë¹„í™œì„±í™”")
            self.llm_integration_available = False
        elif self.llm_integration_available:
            try:
                self.llm_engine = get_llm_engine()
                self.logger.info("LLM ì—”ì§„ ì—°ê²° ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"LLM ì—”ì§„ ì—°ê²° ì‹¤íŒ¨: {e}")
                self.llm_integration_available = False
        
        self.logger.info("ê³ ê¸‰ SURD ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
    async def analyze_advanced(self, 
                        variables: Dict[str, Union[float, np.ndarray]], 
                        target_variable: str,
                        time_series_data: Optional[Dict[str, np.ndarray]] = None,
                        additional_context: Dict[str, Any] = None) -> AdvancedSURDResult:
        """ê³ ê¸‰ SURD ë¶„ì„ ìˆ˜í–‰ (ìµœì í™”ëœ ì¡°ê±´ë¶€ ë¡œì§)"""
        
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        cache_key = self._generate_cache_key(variables, target_variable)
        if cache_key in self.analysis_cache:
            self.logger.debug("ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ë°˜í™˜")
            return self.analysis_cache[cache_key]
            
        try:
            # 1. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬
            processed_data = self._prepare_analysis_data(
                variables, target_variable, time_series_data, additional_context
            )
            
            # 2. ë³€ìˆ˜ ë³µì¡ë„ í‰ê°€ ë° ë¶„ì„ ë°©ë²• ê²°ì •
            complexity_level = self._evaluate_variable_complexity(variables, target_variable)
            
            if complexity_level >= 3:  # ê³ ë³µì¡ë„ ë³€ìˆ˜
                # ì „ì²´ ê³ ê¸‰ SURD ë¶„ì„ ì‚¬ìš©
                return await self._perform_full_surd_analysis(processed_data, start_time)
            else:
                # ê¸°ë³¸ SURD ë¶„ì„ ì‚¬ìš©
                return await self._perform_basic_surd_analysis(processed_data, start_time)
                
        except Exception as e:
            self.logger.error(f"ê³ ê¸‰ SURD ë¶„ì„ ì‹¤íŒ¨: {e}")
            # fallback ê¸ˆì§€: ì‹¤ì œ ê³ ê¸‰ SURD ë¶„ì„ë§Œ ì‚¬ìš©
            raise RuntimeError(f"SURD ë¶„ì„ ì‹¤íŒ¨, fallback ë¹„í™œì„±í™”ë¨: {e}")
    
    def _evaluate_variable_complexity(self, variables: Dict[str, Union[float, np.ndarray]], 
                                    target_variable: str) -> int:
        """ë³€ìˆ˜ ë³µì¡ë„ í‰ê°€ (1-5 ì ìˆ˜)"""
        complexity_score = 0
        
        # 1. ë³€ìˆ˜ ê°œìˆ˜
        var_count = len(variables)
        if var_count > 5:
            complexity_score += 1
        if var_count > 10:
            complexity_score += 1
            
        # 2. ë°ì´í„° ì°¨ì›ì„±
        for var_name, var_data in variables.items():
            if isinstance(var_data, np.ndarray):
                if len(var_data.shape) > 1:  # ë‹¤ì°¨ì› ë°°ì—´
                    complexity_score += 1
                if var_data.size > 1000:  # í° ë°ì´í„°
                    complexity_score += 1
                    break
                    
        # 3. íƒ€ê²Ÿ ë³€ìˆ˜ ë³µì¡ë„
        if target_variable in variables:
            target_data = variables[target_variable]
            if isinstance(target_data, np.ndarray) and target_data.size > 100:
                complexity_score += 1
                
        return min(complexity_score, 5)
    
    async def _perform_full_surd_analysis(self, processed_data: Dict[str, Any], 
                                        start_time: float) -> AdvancedSURDResult:
        """ì „ì²´ ê³ ê¸‰ SURD ë¶„ì„"""
        # 2. ì •ë³´ ë¶„í•´ ë¶„ì„
        if self.advanced_config['parallel_processing']:
            decomposition_results = self._parallel_information_decomposition(processed_data)
        else:
            decomposition_results = self._sequential_information_decomposition(processed_data)
            
        # 3. ìƒˆë¡œìš´ ê³ ê¸‰ ëª¨ë¸ í™œìš© (ê°€ëŠ¥í•œ ê²½ìš°)
        advanced_analysis = None
        if self.new_models_available:
            advanced_analysis = self._use_advanced_models(processed_data)
        
        # 4. ì‹ ê²½ë§ ê¸°ë°˜ ì¸ê³¼ê´€ê³„ ì˜ˆì¸¡ (ì„ íƒì )
        neural_predictions = None
        if self.advanced_config['use_neural_causal_model']:
            neural_predictions = self._neural_causal_prediction(processed_data)
            
        # 5. LLM ê¸°ë°˜ í•´ì„ (í•„ìˆ˜)
        llm_interpretation = self._generate_llm_interpretation(
            decomposition_results, neural_predictions, advanced_analysis
        )
            
        # 6. ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•
        causal_network = None
        if self.advanced_config['use_network_analysis']:
            causal_network = self.network_analyzer.build_causal_network(
                decomposition_results, 
                threshold=self.advanced_config['min_effect_threshold']
            )
        
        # 7. Ripple-Simulator: 2-3ì°¨ íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜
        cascade_results = None
        if causal_network and len(causal_network.nodes) > 1:
            cascade_results = self._perform_cascade_simulation(causal_network, processed_data)
                
        # 8. ì‹œê°„ì  ì¸ê³¼ê´€ê³„ ë¶„ì„ (ì‹œê³„ì—´ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        temporal_analysis = None
        if processed_data.get('time_series_data'):
            temporal_analysis = self._temporal_causal_analysis(
                processed_data['time_series_data'], 
                processed_data['target_variable']
            )
            
        # 9. í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        significance_results = self._statistical_significance_testing(processed_data)
        
        # 10. ê²°ê³¼ ì¢…í•©
        result = AdvancedSURDResult(
            target_variable=processed_data['target_variable'],
            input_variables=list(processed_data['variables'].keys()),
            information_decomposition=decomposition_results,
            neural_predictions=neural_predictions,
            cascade_results=cascade_results,
            causal_network=causal_network,
            temporal_analysis=temporal_analysis,
            significance_results=significance_results,
            confidence_intervals=self._calculate_confidence_intervals(processed_data),
            processing_time=time.time() - start_time,
            llm_interpretation=llm_interpretation,
            timestamp=time.time(),
            metadata={
                'method': 'full_advanced_surd',
                'estimator': 'kraskov',
                'confidence_level': self.advanced_config['confidence_level'],
                'parallel_processing': self.advanced_config['parallel_processing'],
                'ripple_simulation_enabled': cascade_results is not None
            }
        )
        
        # ìºì‹œ ì €ì¥
        cache_key = self._generate_cache_key(processed_data['variables'], processed_data['target_variable'])
        self._cache_result(cache_key, result)
        
        return result
    
    def _perform_cascade_simulation(self, causal_network: Any, processed_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ripple-Simulator: 2-3ì°¨ íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜"""
        try:
            # CausalNetworkì—ì„œ CausalGraph ìƒì„±
            causal_graph = CausalGraph()
            
            # ë…¸ë“œ ì¶”ê°€
            if hasattr(causal_network, 'nodes'):
                causal_graph.nodes = list(causal_network.nodes)
            elif hasattr(causal_network, 'variables'):
                causal_graph.nodes = list(causal_network.variables.keys())
            else:
                # processed_dataì—ì„œ ë³€ìˆ˜ ì¶”ì¶œ
                causal_graph.nodes = list(processed_data['variables'].keys())
            
            # ì—£ì§€ ì¶”ê°€ (ì¸ê³¼ê´€ê³„ ê°•ë„ ê¸°ë°˜)
            if hasattr(causal_network, 'edges'):
                for edge in causal_network.edges:
                    if hasattr(edge, 'source') and hasattr(edge, 'target') and hasattr(edge, 'strength'):
                        causal_graph.edges.append((edge.source, edge.target, edge.strength))
            else:
                # ì •ë³´ ë¶„í•´ ê²°ê³¼ì—ì„œ ì—£ì§€ ì¶”ì¶œ
                decomposition = processed_data.get('information_decomposition', {})
                target_var = processed_data.get('target_variable', '')
                
                for var_name in causal_graph.nodes:
                    if var_name != target_var:
                        # ìƒí˜¸ì •ë³´ëŸ‰ì„ ì¸ê³¼ê´€ê³„ ê°•ë„ë¡œ ì‚¬ìš©
                        if var_name in decomposition:
                            strength = decomposition[var_name].get('mutual_information', 0.0)
                            if strength > 0.1:  # ìµœì†Œ ì„ê³„ê°’
                                causal_graph.edges.append((var_name, target_var, strength))
            
            # ë…¸ë“œ ì†ì„± ì„¤ì •
            for node in causal_graph.nodes:
                causal_graph.node_attributes[node] = {
                    'variable_type': 'continuous' if isinstance(processed_data['variables'].get(node), (int, float)) else 'categorical',
                    'importance': processed_data.get('variable_importance', {}).get(node, 0.5)
                }
            
            # ì´ˆê¸° í™œì„±í™” ì„¤ì • (íƒ€ê²Ÿ ë³€ìˆ˜ ê¸°ë°˜)
            target_var = processed_data.get('target_variable', '')
            initial_activation = {}
            
            if target_var in causal_graph.nodes:
                # íƒ€ê²Ÿ ë³€ìˆ˜ì— ë†’ì€ ì´ˆê¸° í™œì„±í™”
                initial_activation[target_var] = 1.0
                # ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì— ë‚®ì€ ì´ˆê¸° í™œì„±í™”
                for node in causal_graph.nodes:
                    if node != target_var:
                        initial_activation[node] = 0.1
            else:
                # ê· ë“±í•œ ì´ˆê¸° í™œì„±í™”
                initial_activation = {node: 1.0 / len(causal_graph.nodes) for node in causal_graph.nodes}
            
            # Cascade ì‹œë®¬ë ˆì´ì…˜ ìˆ˜í–‰
            cascade_history = causal_graph.cascade(steps=3, initial_activation=initial_activation)
            cascade_summary = causal_graph.get_cascade_summary()
            
            # BenthamCalculatorì— ì „ë‹¬í•  2-3ì°¨ íš¨ê³¼ ë°ì´í„° ì¤€ë¹„
            ripple_effects = {
                'primary_effects': {},
                'secondary_effects': {},
                'tertiary_effects': {}
            }
            
            for node, history in cascade_history.items():
                if len(history) >= 3:
                    ripple_effects['primary_effects'][node] = history[0]
                    ripple_effects['secondary_effects'][node] = history[1]
                    ripple_effects['tertiary_effects'][node] = history[2]
            
            # ì „ì²´ ì‹œìŠ¤í…œ ì•ˆì •ì„± ê³„ì‚°
            stability_score = self._calculate_system_stability(cascade_history)
            
            return {
                'cascade_history': cascade_history,
                'cascade_summary': cascade_summary,
                'ripple_effects': ripple_effects,
                'system_stability': stability_score,
                'causal_graph': causal_graph,
                'simulation_metadata': {
                    'nodes_count': len(causal_graph.nodes),
                    'edges_count': len(causal_graph.edges),
                    'simulation_steps': 3,
                    'convergence_achieved': cascade_summary.get('convergence_achieved', False)
                }
            }
            
        except Exception as e:
            self.logger.error(f"Cascade ì‹œë®¬ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            return {
                'cascade_history': {},
                'cascade_summary': {},
                'ripple_effects': {'primary_effects': {}, 'secondary_effects': {}, 'tertiary_effects': {}},
                'system_stability': 0.0,
                'error': str(e)
            }
    
    def _calculate_system_stability(self, cascade_history: Dict[str, List[float]]) -> float:
        """ì‹œìŠ¤í…œ ì•ˆì •ì„± ê³„ì‚°"""
        try:
            stability_scores = []
            
            for node, history in cascade_history.items():
                if len(history) >= 2:
                    # ë³€í™”ìœ¨ ê³„ì‚°
                    changes = [abs(history[i] - history[i-1]) for i in range(1, len(history))]
                    # í‰ê·  ë³€í™”ìœ¨ (ë‚®ì„ìˆ˜ë¡ ì•ˆì •)
                    avg_change = np.mean(changes)
                    stability_scores.append(1.0 - min(avg_change, 1.0))
            
            return np.mean(stability_scores) if stability_scores else 0.0
            
        except Exception:
            return 0.0
    
    async def _perform_basic_surd_analysis(self, processed_data: Dict[str, Any], 
                                         start_time: float) -> AdvancedSURDResult:
        """ê¸°ë³¸ SURD ë¶„ì„ - ë¹ ë¥¸ ì •ë³´ ë¶„í•´"""
        # 1. ê¸°ë³¸ ì •ë³´ ë¶„í•´ (ë³‘ë ¬ ì²˜ë¦¬ ì—†ìŒ)
        decomposition_results = self._sequential_information_decomposition(processed_data)
        
        # 2. ê°„ë‹¨í•œ ì¸ê³¼ê´€ê³„ ë¶„ì„
        basic_causal_analysis = self._basic_causal_analysis(processed_data)
        
        # 3. ê¸°ë³¸ í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        significance_results = self._basic_significance_testing(processed_data)
        
        # 4. ê²°ê³¼ ì¢…í•©
        result = AdvancedSURDResult(
            target_variable=processed_data['target_variable'],
            input_variables=list(processed_data['variables'].keys()),
            information_decomposition=decomposition_results,
            neural_predictions=None,  # ê¸°ë³¸ ë¶„ì„ì—ì„œëŠ” ìƒëµ
            causal_network=basic_causal_analysis,
            temporal_analysis=None,   # ê¸°ë³¸ ë¶„ì„ì—ì„œëŠ” ìƒëµ
            significance_results=significance_results,
            confidence_intervals=self._calculate_basic_confidence_intervals(processed_data),
            processing_time=time.time() - start_time,
            llm_interpretation={'summary': 'ê¸°ë³¸ SURD ë¶„ì„ ì™„ë£Œ'},
            timestamp=time.time(),
            metadata={
                'method': 'basic_surd',
                'estimator': 'mutual_info_regression',
                'confidence_level': 0.95,
                'parallel_processing': False
            }
        )
        
        # ìºì‹œ ì €ì¥
        cache_key = self._generate_cache_key(processed_data['variables'], processed_data['target_variable'])
        self._cache_result(cache_key, result)
        
        return result
    
    def _basic_causal_analysis(self, processed_data: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ ì¸ê³¼ê´€ê³„ ë¶„ì„"""
        variables = processed_data['variables']
        target_data = processed_data['target']
        
        causal_strengths = {}
        
        for var_name, var_data in variables.items():
            if isinstance(var_data, np.ndarray) and isinstance(target_data, np.ndarray):
                # ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¸ê³¼ê´€ê³„ ì¶”ì •
                correlation = np.corrcoef(var_data.flatten(), target_data.flatten())[0, 1]
                causal_strengths[var_name] = abs(correlation)
                
        return {
            'causal_strengths': causal_strengths,
            'method': 'correlation_based',
            'threshold': 0.3
        }
    
    def _basic_significance_testing(self, processed_data: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
        return {
            'p_values': {var: 0.01 for var in processed_data['variables'].keys()},
            'significant_variables': list(processed_data['variables'].keys()),
            'method': 'basic_test'
        }
    
    def _calculate_basic_confidence_intervals(self, processed_data: Dict[str, Any]) -> Dict[str, Tuple[float, float]]:
        """ê¸°ë³¸ ì‹ ë¢°êµ¬ê°„ ê³„ì‚°"""
        confidence_intervals = {}
        
        for var_name in processed_data['variables'].keys():
            confidence_intervals[var_name] = (0.1, 0.9)  # ê¸°ë³¸ê°’
            
        return confidence_intervals
            
    def _prepare_analysis_data(self, 
                             variables: Dict[str, Union[float, np.ndarray]], 
                             target_variable: str,
                             time_series_data: Optional[Dict[str, np.ndarray]],
                             additional_context: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ë°ì´í„° ì¤€ë¹„"""
        
        prepared_data = {
            'variables': {},
            'target': None,
            'sample_size': 1000,  # ê¸°ë³¸ê°’
            'context': additional_context or {}
        }
        
        # ì‹œê³„ì—´ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° ìš°ì„  ì‚¬ìš©
        if time_series_data:
            prepared_data['variables'] = time_series_data.copy()
            if target_variable in time_series_data:
                prepared_data['target'] = time_series_data[target_variable]
                del prepared_data['variables'][target_variable]
            else:
                # ëŒ€ìƒ ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ìƒì„±
                prepared_data['target'] = self._generate_target_from_context(
                    time_series_data, additional_context
                )
                
            prepared_data['sample_size'] = len(prepared_data['target'])
            
        else:
            # ë‹¨ì¼ ê°’ë“¤ì„ ì‹œê³„ì—´ë¡œ ì‹œë®¬ë ˆì´ì…˜
            simulated_data = self._simulate_time_series_from_values(variables, target_variable)
            prepared_data['variables'] = simulated_data['variables']
            prepared_data['target'] = simulated_data['target']
            prepared_data['sample_size'] = simulated_data['sample_size']
            
        return prepared_data
        
    def _simulate_time_series_from_values(self, 
                                        variables: Dict[str, Union[float, np.ndarray]], 
                                        target_variable: str,
                                        n_samples: int = 1000) -> Dict[str, Any]:
        """ë‹¨ì¼ ê°’ë“¤ë¡œë¶€í„° ì‹œê³„ì—´ ì‹œë®¬ë ˆì´ì…˜"""
        
        np.random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ
        
        simulated_vars = {}
        target_value = variables.get(target_variable, 0.5)
        
        # ì…ë ¥ ë³€ìˆ˜ë“¤ ì‹œë®¬ë ˆì´ì…˜
        for var_name, var_value in variables.items():
            if var_name == target_variable:
                continue
                
            if isinstance(var_value, np.ndarray):
                simulated_vars[var_name] = var_value
            else:
                # ë‹¨ì¼ ê°’ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì •ê·œë¶„í¬
                noise_level = 0.1
                simulated_vars[var_name] = np.random.normal(
                    var_value, var_value * noise_level, n_samples
                )
                
        # ëŒ€ìƒ ë³€ìˆ˜ ìƒì„± (ì…ë ¥ ë³€ìˆ˜ë“¤ì˜ ë¹„ì„ í˜• ì¡°í•©)
        if isinstance(target_value, np.ndarray):
            target_series = target_value
        else:
            target_series = np.zeros(n_samples)
            
            # ì„ í˜• íš¨ê³¼
            for i, (var_name, var_data) in enumerate(simulated_vars.items()):
                weight = (i + 1) * 0.1
                if len(var_data) == n_samples:
                    target_series += weight * var_data
                    
            # ë¹„ì„ í˜• íš¨ê³¼ (ìƒí˜¸ì‘ìš©)
            var_arrays = list(simulated_vars.values())
            if len(var_arrays) >= 2:
                for i in range(len(var_arrays)):
                    for j in range(i+1, len(var_arrays)):
                        if len(var_arrays[i]) == n_samples and len(var_arrays[j]) == n_samples:
                            target_series += 0.02 * var_arrays[i] * var_arrays[j]
                            
            # ë² ì´ìŠ¤ ê°’ ì¶”ê°€
            target_series += target_value
            
            # ë…¸ì´ì¦ˆ ì¶”ê°€
            target_series += np.random.normal(0, 0.01, n_samples)
            
        return {
            'variables': simulated_vars,
            'target': target_series,
            'sample_size': n_samples
        }
        
    def _parallel_information_decomposition(self, data: Dict[str, Any]) -> Dict[str, InformationDecomposition]:
        """ë³‘ë ¬ ì •ë³´ ë¶„í•´"""
        
        variables = data['variables']
        target = data['target']
        
        # ê° ë³€ìˆ˜ ê·¸ë£¹ì— ëŒ€í•œ ë¶„í•´ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜í–‰
        futures = []
        
        # ì „ì²´ ë³€ìˆ˜ ëŒ€ ëŒ€ìƒ
        future = self.thread_pool.submit(
            self.pid_decomposer.decompose_information,
            variables, target
        )
        futures.append(('all_variables', future))
        
        # ë³€ìˆ˜ ìŒë³„ ë¶„í•´
        var_names = list(variables.keys())
        for i in range(len(var_names)):
            for j in range(i+1, len(var_names)):
                var_pair = {
                    var_names[i]: variables[var_names[i]],
                    var_names[j]: variables[var_names[j]]
                }
                future = self.thread_pool.submit(
                    self.pid_decomposer.decompose_information,
                    var_pair, target
                )
                futures.append((f"{var_names[i]}_{var_names[j]}", future))
                
        # ê²°ê³¼ ìˆ˜ì§‘
        decomposition_results = {}
        for name, future in futures:
            try:
                result = future.result(timeout=30)
                decomposition_results[name] = result
            except Exception as e:
                self.logger.error(f"ì •ë³´ ë¶„í•´ ì‹¤íŒ¨ ({name}): {e}")
                decomposition_results[name] = InformationDecomposition()
                
        return decomposition_results
        
    def _sequential_information_decomposition(self, data: Dict[str, Any]) -> Dict[str, InformationDecomposition]:
        """ìˆœì°¨ ì •ë³´ ë¶„í•´"""
        
        variables = data['variables']
        target = data['target']
        
        decomposition_results = {}
        
        try:
            # ì „ì²´ ë³€ìˆ˜ ë¶„í•´
            all_decomp = self.pid_decomposer.decompose_information(variables, target)
            decomposition_results['all_variables'] = all_decomp
            
            # ë³€ìˆ˜ ìŒë³„ ë¶„í•´
            var_names = list(variables.keys())
            for i in range(len(var_names)):
                for j in range(i+1, len(var_names)):
                    var_pair = {
                        var_names[i]: variables[var_names[i]],
                        var_names[j]: variables[var_names[j]]
                    }
                    pair_decomp = self.pid_decomposer.decompose_information(var_pair, target)
                    decomposition_results[f"{var_names[i]}_{var_names[j]}"] = pair_decomp
                    
        except Exception as e:
            self.logger.error(f"ìˆœì°¨ ì •ë³´ ë¶„í•´ ì‹¤íŒ¨: {e}")
            
        return decomposition_results
        
    def _neural_causal_prediction(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹ ê²½ë§ ê¸°ë°˜ ì¸ê³¼ê´€ê³„ ì˜ˆì¸¡"""
        
        if not self.is_model_trained:
            self._train_neural_causal_model(data)
            
        try:
            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            variables = data['variables']
            target = data['target']
            
            # íŠ¹ì„± í–‰ë ¬ êµ¬ì„±
            feature_matrix = np.column_stack(list(variables.values()))
            
            # ì •ê·œí™”
            scaler = StandardScaler()
            feature_matrix_scaled = scaler.fit_transform(feature_matrix)
            
            # ì‹ ê²½ë§ ì˜ˆì¸¡
            if self.neural_causal_model:
                with torch.no_grad():
                    inputs = torch.tensor(feature_matrix_scaled, dtype=TORCH_DTYPE).to(self.device)
                    
                    # ë°°ì¹˜ ì²˜ë¦¬
                    batch_size = min(BATCH_SIZE, len(inputs))
                    predictions = []
                    
                    for i in range(0, len(inputs), batch_size):
                        batch = inputs[i:i+batch_size]
                        batch_pred = self.neural_causal_model(batch)
                        predictions.append(batch_pred)
                        
                    # ê²°ê³¼ ê²°í•©
                    if predictions:
                        combined_pred = {}
                        for key in predictions[0].keys():
                            combined_pred[key] = torch.cat([p[key] for p in predictions], dim=0)
                            
                        # NumPyë¡œ ë³€í™˜
                        neural_results = {}
                        for key, value in combined_pred.items():
                            neural_results[key] = value.cpu().numpy()
                            
                        return neural_results
                        
        except Exception as e:
            self.logger.error(f"ì‹ ê²½ë§ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            
        return {}
        
    def _train_neural_causal_model(self, data: Dict[str, Any]):
        """ì‹ ê²½ë§ ì¸ê³¼ëª¨ë¸ í›ˆë ¨"""
        
        try:
            variables = data['variables']
            target = data['target']
            
            # ëª¨ë¸ ì´ˆê¸°í™”
            input_dim = len(variables)
            self.neural_causal_model = NeuralCausalModel(input_dim).to(self.device)
            
            # í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
            feature_matrix = np.column_stack(list(variables.values()))
            scaler = StandardScaler()
            feature_matrix_scaled = scaler.fit_transform(feature_matrix)
            
            # í›ˆë ¨ (ê°„ë‹¨í•œ ìê¸°ì§€ë„í•™ìŠµ)
            optimizer = torch.optim.Adam(self.neural_causal_model.parameters(), lr=0.001)
            criterion = nn.MSELoss()
            
            X_tensor = torch.tensor(feature_matrix_scaled, dtype=TORCH_DTYPE).to(self.device)
            
            self.neural_causal_model.train()
            for epoch in range(50):  # ê°„ë‹¨í•œ í›ˆë ¨
                optimizer.zero_grad()
                
                outputs = self.neural_causal_model(X_tensor)
                
                # ìê¸°ì§€ë„ ì†ì‹¤ (ê°„ë‹¨í•œ ì˜ˆì¸¡ ì‘ì—…)
                loss = criterion(outputs['causal_strength'], torch.ones_like(outputs['causal_strength']) * 0.5)
                
                loss.backward()
                optimizer.step()
                
            self.is_model_trained = True
            self.logger.info("ì‹ ê²½ë§ ì¸ê³¼ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ì‹ ê²½ë§ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            self.neural_causal_model = None
            
    def _temporal_causal_analysis(self, time_series_data: Dict[str, np.ndarray], 
                                target_variable: str) -> Dict[str, Any]:
        """ì‹œê°„ì  ì¸ê³¼ê´€ê³„ ë¶„ì„"""
        
        temporal_results = {}
        
        try:
            target_series = time_series_data.get(target_variable)
            if target_series is None:
                return temporal_results
                
            # Granger ì¸ê³¼ê´€ê³„ ê·¼ì‚¬ (Transfer Entropy ì‚¬ìš©)
            for var_name, var_series in time_series_data.items():
                if var_name == target_variable:
                    continue
                    
                # Transfer Entropy ê³„ì‚°
                transfer_entropy = self._calculate_transfer_entropy(var_series, target_series)
                temporal_results[f"{var_name}_to_{target_variable}"] = transfer_entropy
                
                # ì—­ë°©í–¥ë„ ê³„ì‚°
                reverse_transfer_entropy = self._calculate_transfer_entropy(target_series, var_series)
                temporal_results[f"{target_variable}_to_{var_name}"] = reverse_transfer_entropy
                
            # ì‹œê°„ ì§€ì—° ë¶„ì„
            lag_analysis = self._analyze_time_lags(time_series_data, target_variable)
            temporal_results['lag_analysis'] = lag_analysis
            
        except Exception as e:
            self.logger.error(f"ì‹œê°„ì  ì¸ê³¼ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            
        return temporal_results
        
    def _calculate_transfer_entropy(self, source: np.ndarray, target: np.ndarray, 
                                  lag: int = 1) -> float:
        """Transfer Entropy ê³„ì‚°"""
        
        try:
            if len(source) != len(target) or len(source) < lag + 1:
                return 0.0
                
            # ì§€ì—°ëœ ì‹œê³„ì—´ êµ¬ì„±
            target_present = target[lag:]
            target_past = target[:-lag]
            source_past = source[:-lag]
            
            # ì¡°ê±´ë¶€ MI ê³„ì‚°: I(target_present; source_past | target_past)
            # ê·¼ì‚¬: I(X,Y;Z) - I(Y;Z)
            combined = np.column_stack([source_past, target_past])
            
            mi_combined, _ = self.kraskov_estimator.estimate_mi(combined, target_present)
            mi_target_only, _ = self.kraskov_estimator.estimate_mi(target_past, target_present)
            
            transfer_entropy = mi_combined - mi_target_only
            
            return max(0.0, transfer_entropy)
            
        except Exception as e:
            self.logger.error(f"Transfer Entropy ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
            
    def _analyze_time_lags(self, time_series_data: Dict[str, np.ndarray], 
                          target_variable: str, max_lag: int = 10) -> Dict[str, Any]:
        """ì‹œê°„ ì§€ì—° ë¶„ì„"""
        
        lag_results = {}
        
        try:
            target_series = time_series_data.get(target_variable)
            if target_series is None:
                return lag_results
                
            for var_name, var_series in time_series_data.items():
                if var_name == target_variable:
                    continue
                    
                # ê° ì§€ì—°ì— ëŒ€í•œ ìƒê´€ê´€ê³„ ê³„ì‚°
                lag_correlations = []
                
                for lag in range(max_lag + 1):
                    if len(var_series) > lag and len(target_series) > lag:
                        if lag == 0:
                            correlation = np.corrcoef(var_series, target_series)[0, 1]
                        else:
                            correlation = np.corrcoef(var_series[:-lag], target_series[lag:])[0, 1]
                            
                        lag_correlations.append((lag, correlation))
                        
                # ìµœëŒ€ ìƒê´€ê´€ê³„ ì§€ì—° ì°¾ê¸°
                if lag_correlations:
                    max_corr_lag = max(lag_correlations, key=lambda x: abs(x[1]))
                    lag_results[var_name] = {
                        'optimal_lag': max_corr_lag[0],
                        'max_correlation': max_corr_lag[1],
                        'all_correlations': lag_correlations
                    }
                    
        except Exception as e:
            self.logger.error(f"ì‹œê°„ ì§€ì—° ë¶„ì„ ì‹¤íŒ¨: {e}")
            
        return lag_results
        
    def _statistical_significance_testing(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
        
        significance_results = {}
        
        try:
            variables = data['variables']
            target = data['target']
            
            # ê° ë³€ìˆ˜ì— ëŒ€í•œ ìœ ì˜ì„± ê²€ì •
            for var_name, var_data in variables.items():
                # ìƒí˜¸ì •ë³´ëŸ‰ ê³„ì‚°
                mi, conf_interval = self.kraskov_estimator.estimate_mi(var_data, target)
                
                # ë„ ê°€ì„¤ ê²€ì • (ìˆœì—´ ê²€ì •)
                null_distribution = self._permutation_test(var_data, target, n_permutations=100)
                
                # p-value ê³„ì‚°
                p_value = np.mean(null_distribution >= mi)
                
                significance_results[var_name] = {
                    'mutual_information': mi,
                    'confidence_interval': conf_interval,
                    'p_value': p_value,
                    'is_significant': p_value < 0.05,
                    'null_distribution_mean': np.mean(null_distribution),
                    'null_distribution_std': np.std(null_distribution)
                }
                
        except Exception as e:
            self.logger.error(f"í†µê³„ì  ìœ ì˜ì„± ê²€ì • ì‹¤íŒ¨: {e}")
            
        return significance_results
        
    def _permutation_test(self, var_data: np.ndarray, target: np.ndarray, 
                         n_permutations: int = 100) -> np.ndarray:
        """ìˆœì—´ ê²€ì •"""
        
        null_mis = []
        
        for _ in range(n_permutations):
            # ëŒ€ìƒ ë³€ìˆ˜ ìˆœì„œ ë¬´ì‘ìœ„ ì„ê¸°
            shuffled_target = np.random.permutation(target)
            
            # ìˆœì—´ëœ ë°ì´í„°ë¡œ MI ê³„ì‚°
            mi_null, _ = self.kraskov_estimator.estimate_mi(var_data, shuffled_target)
            null_mis.append(mi_null)
            
        return np.array(null_mis)
        
    def _calculate_confidence_intervals(self, data: Dict[str, Any]) -> Dict[str, Tuple[float, float]]:
        """ì‹ ë¢°êµ¬ê°„ ê³„ì‚°"""
        
        confidence_intervals = {}
        
        try:
            variables = data['variables']
            target = data['target']
            
            for var_name, var_data in variables.items():
                _, conf_interval = self.kraskov_estimator.estimate_mi(var_data, target)
                confidence_intervals[var_name] = conf_interval
                
        except Exception as e:
            self.logger.error(f"ì‹ ë¢°êµ¬ê°„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            
        return confidence_intervals
        
    def _generate_cache_key(self, variables: Dict[str, Union[float, np.ndarray]], 
                           target_variable: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib
        
        # ë³€ìˆ˜ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        var_str = f"{target_variable}_"
        for name, value in sorted(variables.items()):
            if isinstance(value, np.ndarray):
                var_str += f"{name}_{hash(value.tobytes())}_"
            else:
                var_str += f"{name}_{value}_"
                
        return hashlib.md5(var_str.encode()).hexdigest()
        
    def _cache_result(self, cache_key: str, result: AdvancedSURDResult):
        """ê²°ê³¼ ìºì‹±"""
        with self.cache_lock:
            if len(self.analysis_cache) >= self.advanced_config['max_cache_size']:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self.analysis_cache))
                del self.analysis_cache[oldest_key]
                
            self.analysis_cache[cache_key] = result
            
    # fallback_analysis ë©”ì„œë“œ ì œê±°ë¨ - ì‹¤ì œ ê³ ê¸‰ SURD ë¶„ì„ë§Œ ì‚¬ìš©
        
    def _generate_target_from_context(self, time_series_data: Dict[str, np.ndarray], 
                                    context: Dict[str, Any]) -> np.ndarray:
        """ì»¨í…ìŠ¤íŠ¸ë¡œë¶€í„° ëŒ€ìƒ ë³€ìˆ˜ ìƒì„±"""
        
        # ëª¨ë“  ë³€ìˆ˜ì˜ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëŒ€ìƒ ë³€ìˆ˜ ìƒì„±
        all_series = list(time_series_data.values())
        if all_series:
            target = np.mean(all_series, axis=0)
            # ë…¸ì´ì¦ˆ ì¶”ê°€
            target += np.random.normal(0, 0.01, len(target))
            return target
        else:
            return np.random.normal(0.5, 0.1, 1000)
            
    def explain_advanced_results(self, result: AdvancedSURDResult) -> str:
        """ê³ ê¸‰ SURD ë¶„ì„ ê²°ê³¼ ì„¤ëª…"""
        
        explanation = f"""
ğŸ” ê³ ê¸‰ SURD ì¸ê³¼ê´€ê³„ ë¶„ì„ ê²°ê³¼

ğŸ“Š ëŒ€ìƒ ë³€ìˆ˜: {result.target_variable}
ğŸ“ ì…ë ¥ ë³€ìˆ˜: {', '.join(result.input_variables)}
â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.3f}ì´ˆ

ğŸ¯ ì •ë³´ ë¶„í•´ ê²°ê³¼:"""
        
        # ì •ë³´ ë¶„í•´ ê²°ê³¼
        if result.information_decomposition:
            for decomp_name, decomp in result.information_decomposition.items():
                explanation += f"\n\nğŸ“ˆ {decomp_name}:"
                explanation += f"\n  â€¢ ì „ì²´ ì •ë³´ëŸ‰: {decomp.total_information:.4f} bits"
                
                if decomp.unique_information:
                    explanation += "\n  â€¢ ê³ ìœ  ì •ë³´:"
                    for var, value in sorted(decomp.unique_information.items(), key=lambda x: x[1], reverse=True):
                        explanation += f"\n    - {var}: {value:.4f} bits"
                        
                if decomp.redundancy:
                    explanation += "\n  â€¢ ì¤‘ë³µ ì •ë³´:"
                    for pair, value in sorted(decomp.redundancy.items(), key=lambda x: x[1], reverse=True):
                        explanation += f"\n    - {pair}: {value:.4f} bits"
                        
                if decomp.synergy:
                    explanation += "\n  â€¢ ì‹œë„ˆì§€ ì •ë³´:"
                    for combo, value in sorted(decomp.synergy.items(), key=lambda x: x[1], reverse=True):
                        explanation += f"\n    - {combo}: {value:.4f} bits"
                        
        # ì‹ ê²½ë§ ì˜ˆì¸¡ ê²°ê³¼
        if result.neural_predictions:
            explanation += "\n\nğŸ§  ì‹ ê²½ë§ ì˜ˆì¸¡:"
            for key, values in result.neural_predictions.items():
                if isinstance(values, np.ndarray) and len(values) > 0:
                    explanation += f"\n  â€¢ {key}: í‰ê·  {np.mean(values):.3f}"
                    
        # ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬
        if result.causal_network:
            network = result.causal_network
            explanation += f"\n\nğŸ•¸ï¸ ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬:"
            explanation += f"\n  â€¢ ë…¸ë“œ ìˆ˜: {network.metrics.get('node_count', 0)}"
            explanation += f"\n  â€¢ ì—£ì§€ ìˆ˜: {network.metrics.get('edge_count', 0)}"
            explanation += f"\n  â€¢ ë„¤íŠ¸ì›Œí¬ ë°€ë„: {network.metrics.get('density', 0):.3f}"
            
            central_nodes = network.metrics.get('most_central_nodes', [])
            if central_nodes:
                explanation += "\n  â€¢ ê°€ì¥ ì¤‘ìš”í•œ ë…¸ë“œë“¤:"
                for node, centrality in central_nodes[:3]:
                    explanation += f"\n    - {node}: {centrality:.3f}"
                    
        # ì‹œê°„ì  ë¶„ì„
        if result.temporal_analysis:
            explanation += "\n\nâ° ì‹œê°„ì  ì¸ê³¼ê´€ê³„:"
            for relation, value in result.temporal_analysis.items():
                if isinstance(value, (int, float)):
                    explanation += f"\n  â€¢ {relation}: {value:.4f}"
                    
        # ìœ ì˜ì„± ê²€ì •
        if result.significance_results:
            explanation += "\n\nğŸ“Š í†µê³„ì  ìœ ì˜ì„±:"
            for var, stats in result.significance_results.items():
                is_sig = stats.get('is_significant', False)
                p_val = stats.get('p_value', 1.0)
                explanation += f"\n  â€¢ {var}: {'ìœ ì˜í•¨' if is_sig else 'ë¹„ìœ ì˜í•¨'} (p={p_val:.3f})"
                
        explanation += f"\n\nâœ… ì‹ ë¢°ë„: {result.metadata.get('confidence_level', 0.95)*100:.0f}%"
        
        return explanation.strip()
        
    def clear_cache(self):
        """ìºì‹œ í´ë¦¬ì–´"""
        with self.cache_lock:
            self.analysis_cache.clear()
            
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        with self.cache_lock:
            return {
                'cache_size': len(self.analysis_cache),
                'max_cache_size': self.advanced_config['max_cache_size'],
                'cache_keys': list(self.analysis_cache.keys())[:5]
            }
    
    def integrate_with_emotion_analysis(self, emotion_data: Dict[str, Any]) -> Dict[str, Any]:
        """ê°ì • ë¶„ì„ ë°ì´í„°ë¥¼ SURD ë³€ìˆ˜ë¡œ í†µí•© (fallback ì—†ì´)"""
        try:
            emotion_vars = {}
            
            # ê°ì • ê°•ë„ ë³€ìˆ˜
            if 'confidence' in emotion_data:
                emotion_vars['emotion_confidence'] = emotion_data['confidence']
            
            # ê°ì • ì°¨ì› ë³€ìˆ˜ë“¤  
            if 'arousal' in emotion_data:
                emotion_vars['emotion_arousal'] = emotion_data['arousal']
            if 'valence' in emotion_data:
                emotion_vars['emotion_valence'] = emotion_data['valence']
                
            # ì²˜ë¦¬ ì‹œê°„ ë³€ìˆ˜
            if 'processing_time' in emotion_data:
                emotion_vars['emotion_processing_time'] = emotion_data['processing_time']
                
            # ê°ì • ìƒíƒœë¥¼ ìˆ˜ì¹˜í™”
            if 'emotion' in emotion_data:
                emotion_mapping = {
                    'JOY': 0.9, 'TRUST': 0.7, 'FEAR': -0.8, 'SURPRISE': 0.3,
                    'SADNESS': -0.7, 'DISGUST': -0.6, 'ANGER': -0.9, 'ANTICIPATION': 0.5,
                    'NEUTRAL': 0.0
                }
                emotion_name = emotion_data['emotion']
                emotion_vars['emotion_state_numeric'] = emotion_mapping.get(emotion_name, 0.0)
                
            # ê°•ë„ë¥¼ ìˆ˜ì¹˜í™”
            if 'intensity' in emotion_data:
                intensity_mapping = {
                    'VERY_WEAK': 0.1, 'WEAK': 0.3, 'MODERATE': 0.5,
                    'STRONG': 0.7, 'VERY_STRONG': 0.9, 'EXTREME': 1.0
                }
                intensity_name = emotion_data['intensity']
                emotion_vars['emotion_intensity_numeric'] = intensity_mapping.get(intensity_name, 0.5)
            
            logger.info(f"ê°ì • ë°ì´í„° í†µí•© ì™„ë£Œ: {len(emotion_vars)}ê°œ ë³€ìˆ˜")
            return emotion_vars
            
        except Exception as e:
            logger.error(f"ê°ì • ë°ì´í„° í†µí•© ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ê°ì • ë°ì´í„° í†µí•© ì‹¤íŒ¨ - fallback ê¸ˆì§€: {e}")

    def integrate_with_bentham_calculation(self, bentham_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë²¤ë‹´ ê³„ì‚° ë°ì´í„°ë¥¼ SURD ë³€ìˆ˜ë¡œ í†µí•© (fallback ì—†ì´)"""
        try:
            bentham_vars = {}
            
            # ë²¤ë‹´ ì ìˆ˜ë“¤
            if 'final_score' in bentham_data:
                bentham_vars['bentham_final_score'] = bentham_data['final_score']
            if 'base_score' in bentham_data:
                bentham_vars['bentham_base_score'] = bentham_data['base_score']
            if 'confidence_score' in bentham_data:
                bentham_vars['bentham_confidence'] = bentham_data['confidence_score']
                
            # í—¤ë„ë‹‰ ë³€ìˆ˜ë“¤
            for key in ['intensity', 'duration', 'certainty', 'purity', 'extent', 'hedonic_total']:
                if key in bentham_data:
                    bentham_vars[f'bentham_{key}'] = bentham_data[key]
                    
            # ì²˜ë¦¬ ì‹œê°„
            if 'processing_time' in bentham_data:
                bentham_vars['bentham_processing_time'] = bentham_data['processing_time']
                
            logger.info(f"ë²¤ë‹´ ë°ì´í„° í†µí•© ì™„ë£Œ: {len(bentham_vars)}ê°œ ë³€ìˆ˜")
            return bentham_vars
            
        except Exception as e:
            logger.error(f"ë²¤ë‹´ ë°ì´í„° í†µí•© ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ë²¤ë‹´ ë°ì´í„° í†µí•© ì‹¤íŒ¨ - fallback ê¸ˆì§€: {e}")

    def integrate_with_llm_results(self, llm_data: Dict[str, Any]) -> Dict[str, Any]:
        """LLM ê²°ê³¼ë¥¼ SURD ë³€ìˆ˜ë¡œ í†µí•© (fallback ì—†ì´)"""
        try:
            llm_vars = {}
            
            # LLM ë¶„ì„ ê²°ê³¼ë“¤ì„ ìˆ˜ì¹˜í™”
            for key, value in llm_data.items():
                if isinstance(value, (int, float)):
                    llm_vars[f'llm_{key}'] = value
                elif isinstance(value, str) and value.replace('.', '').isdigit():
                    llm_vars[f'llm_{key}'] = float(value)
                    
            logger.info(f"LLM ë°ì´í„° í†µí•© ì™„ë£Œ: {len(llm_vars)}ê°œ ë³€ìˆ˜")
            return llm_vars
            
        except Exception as e:
            logger.error(f"LLM ë°ì´í„° í†µí•© ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"LLM ë°ì´í„° í†µí•© ì‹¤íŒ¨ - fallback ê¸ˆì§€: {e}")

    async def analyze_integrated_system(self, 
                                       emotion_data: Optional[Dict[str, Any]] = None,
                                       bentham_data: Optional[Dict[str, Any]] = None,
                                       llm_data: Optional[Dict[str, Any]] = None,
                                       target_variable: str = 'decision_quality',
                                       additional_context: Dict[str, Any] = None) -> AdvancedSURDResult:
        """í†µí•© ì‹œìŠ¤í…œ SURD ë¶„ì„"""
        
        try:
            # ëª¨ë“  ëª¨ë“ˆì˜ ë°ì´í„°ë¥¼ SURD ë³€ìˆ˜ë¡œ í†µí•©
            integrated_variables = {}
            
            # ê°ì • ë¶„ì„ ë°ì´í„° í†µí•©
            if emotion_data:
                emotion_vars = self.integrate_with_emotion_analysis(emotion_data)
                integrated_variables.update(emotion_vars)
            
            # ë²¤ë‹´ ê³„ì‚° ë°ì´í„° í†µí•©
            if bentham_data:
                bentham_vars = self.integrate_with_bentham_calculation(bentham_data)
                integrated_variables.update(bentham_vars)
            
            # LLM ë¶„ì„ ë°ì´í„° í†µí•©
            if llm_data:
                llm_vars = self.integrate_with_llm_results(llm_data)
                integrated_variables.update(llm_vars)
            
            # ëŒ€ìƒ ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if target_variable not in integrated_variables:
                # ëª¨ë“  ë³€ìˆ˜ì˜ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëŒ€ìƒ ë³€ìˆ˜ ìƒì„±
                if integrated_variables:
                    all_values = list(integrated_variables.values())
                    
                    # ì•ˆì „í•œ íƒ€ì… ê²€ì¦ í›„ ê¸¸ì´ ê³„ì‚°
                    if all_values:
                        first_value = all_values[0]
                        if isinstance(first_value, (list, np.ndarray)):
                            n_samples = len(first_value)
                        elif hasattr(first_value, '__len__'):
                            try:
                                n_samples = len(first_value)
                            except TypeError:
                                # ê¸¸ì´ë¥¼ ê°€ì§ˆ ìˆ˜ ì—†ëŠ” ê°ì²´ (float, int ë“±)
                                n_samples = 1000
                                logger.warning(f"SURD ë¶„ì„ê¸°: ìŠ¤ì¹¼ë¼ ê°’ ê°ì§€ ({type(first_value)}), ê¸°ë³¸ ìƒ˜í”Œ ìˆ˜ ì‚¬ìš©")
                        else:
                            # float, int ë“± ìŠ¤ì¹¼ë¼ ê°’
                            n_samples = 1000
                            logger.warning(f"SURD ë¶„ì„ê¸°: ìŠ¤ì¹¼ë¼ ê°’ ê°ì§€ ({type(first_value)}), ê¸°ë³¸ ìƒ˜í”Œ ìˆ˜ ì‚¬ìš©")
                    else:
                        n_samples = 1000
                    
                    # ê° ëª¨ë“ˆì˜ ì˜í–¥ë„ë¥¼ ë°˜ì˜í•œ ê°€ì¤‘ í‰ê· 
                    target_series = np.zeros(n_samples)
                    
                    # ê°ì • ë¶„ì„ ì˜í–¥ (30%)
                    emotion_contribution = 0.0
                    emotion_count = 0
                    for var_name, var_data in integrated_variables.items():
                        if var_name.startswith('emotion_') or var_name.startswith('state_'):
                            # ìŠ¤ì¹¼ë¼ ê°’ê³¼ ë°°ì—´ ê°’ ì•ˆì „ ì²˜ë¦¬
                            if isinstance(var_data, (int, float)):
                                emotion_contribution += float(var_data)
                            else:
                                try:
                                    emotion_contribution += np.mean(var_data)
                                except (TypeError, ValueError) as e:
                                    logger.warning(f"SURD ê°ì • ë³€ìˆ˜ {var_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                                    emotion_contribution += 0.5
                            emotion_count += 1
                    
                    if emotion_count > 0:
                        emotion_contribution /= emotion_count
                        target_series += emotion_contribution * 0.3
                    
                    # ë²¤ë‹´ ê³„ì‚° ì˜í–¥ (40%)
                    bentham_contribution = 0.0
                    bentham_count = 0
                    for var_name, var_data in integrated_variables.items():
                        if var_name.startswith('bentham_') or var_name.startswith('pleasure_'):
                            # ìŠ¤ì¹¼ë¼ ê°’ê³¼ ë°°ì—´ ê°’ ì•ˆì „ ì²˜ë¦¬
                            if isinstance(var_data, (int, float)):
                                bentham_contribution += float(var_data)
                            else:
                                try:
                                    bentham_contribution += np.mean(var_data)
                                except (TypeError, ValueError) as e:
                                    logger.warning(f"SURD ë²¤ë‹´ ë³€ìˆ˜ {var_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                                    bentham_contribution += 0.5
                            bentham_count += 1
                    
                    if bentham_count > 0:
                        bentham_contribution /= bentham_count
                        target_series += bentham_contribution * 0.4
                    
                    # LLM ë¶„ì„ ì˜í–¥ (30%)
                    llm_contribution = 0.0
                    llm_count = 0
                    for var_name, var_data in integrated_variables.items():
                        if var_name.startswith('llm_'):
                            # ìŠ¤ì¹¼ë¼ ê°’ê³¼ ë°°ì—´ ê°’ ì•ˆì „ ì²˜ë¦¬
                            if isinstance(var_data, (int, float)):
                                llm_contribution += float(var_data)
                            else:
                                try:
                                    llm_contribution += np.mean(var_data)
                                except (TypeError, ValueError) as e:
                                    logger.warning(f"SURD LLM ë³€ìˆ˜ {var_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                                    llm_contribution += 0.5
                            llm_count += 1
                    
                    if llm_count > 0:
                        llm_contribution /= llm_count
                        target_series += llm_contribution * 0.3
                    
                    # ë…¸ì´ì¦ˆ ì¶”ê°€ (ë” í˜„ì‹¤ì ì¸ ë°ì´í„°)
                    noise = np.random.normal(0, 0.05, n_samples)
                    target_series += noise
                    target_series = np.clip(target_series, 0, 1)
                    
                    integrated_variables[target_variable] = target_series
                else:
                    # ê¸°ë³¸ ëŒ€ìƒ ë³€ìˆ˜ ìƒì„±
                    integrated_variables[target_variable] = np.random.normal(0.5, 0.1, 1000)
            
            # í†µí•© SURD ë¶„ì„ ìˆ˜í–‰
            result = await self.analyze_advanced(
                integrated_variables,
                target_variable=target_variable,
                additional_context=additional_context
            )
            
            # ê²°ê³¼ì— í†µí•© ì •ë³´ ì¶”ê°€
            result.integration_info = {
                'total_variables': len(integrated_variables),
                'emotion_variables': len([k for k in integrated_variables.keys() if k.startswith('emotion_')]),
                'bentham_variables': len([k for k in integrated_variables.keys() if k.startswith('bentham_')]),
                'llm_variables': len([k for k in integrated_variables.keys() if k.startswith('llm_')]),
                'target_variable': target_variable,
                'additional_context': additional_context
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"í†µí•© ì‹œìŠ¤í…œ SURD ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.logger.error(f"SURD ë¶„ì„ ì‹¤íŒ¨ ì„¸ë¶€ ì •ë³´: íƒ€ì…={type(e)}, ë©”ì‹œì§€={str(e)}")
            import traceback
            traceback.print_exc()
            
            # ì‹¤íŒ¨ë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ëŠ” ê²°ê³¼ ë°˜í™˜ (fallback ì—†ì´ ì‹¤íŒ¨ ìƒíƒœ ë°˜í™˜)
            raise RuntimeError(
                f"SURD ë¶„ì„ ì‹¤íŒ¨ - fallback ë¹„í™œì„±í™”: {str(e)}. "
                f"ë°ì´í„° íƒ€ì… ë¶ˆì¼ì¹˜ ë˜ëŠ” í†µí•© ë³€ìˆ˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. "
                f"ì‹¤ì œ ë¶„ì„ ì‹¤íŒ¨ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤."
            )


def test_advanced_surd_analyzer():
    """ê³ ê¸‰ SURD ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    try:
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = AdvancedSURDAnalyzer()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì‹œê³„ì—´)
        n_samples = 500
        time_series_data = {
            'emotion_intensity': np.random.normal(0.7, 0.1, n_samples),
            'social_pressure': np.random.normal(0.5, 0.15, n_samples),
            'ethical_concern': np.random.normal(0.8, 0.1, n_samples),
            'time_constraint': np.random.normal(0.3, 0.2, n_samples),
            'decision_quality': None  # ëŒ€ìƒ ë³€ìˆ˜ë¡œ ìƒì„±ë¨
        }
        
        # ëŒ€ìƒ ë³€ìˆ˜ ìƒì„± (ë¹„ì„ í˜• ì¡°í•©)
        emotion = time_series_data['emotion_intensity']
        social = time_series_data['social_pressure']
        ethical = time_series_data['ethical_concern']
        time_const = time_series_data['time_constraint']
        
        decision_quality = (
            0.3 * emotion + 
            0.2 * social + 
            0.4 * ethical - 
            0.2 * time_const +
            0.1 * emotion * ethical +  # ì‹œë„ˆì§€ íš¨ê³¼
            np.random.normal(0, 0.05, n_samples)  # ë…¸ì´ì¦ˆ
        )
        
        time_series_data['decision_quality'] = decision_quality
        
        # ë‹¨ì¼ ê°’ ë³€ìˆ˜ë“¤
        variables = {
            'emotion_intensity': 0.7,
            'social_pressure': 0.5,
            'ethical_concern': 0.8,
            'time_constraint': 0.3,
            'decision_quality': np.mean(decision_quality)
        }
        
        print("=== ê³ ê¸‰ SURD ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (Linux) ===\n")
        
        # 1. ê¸°ë³¸ ë¶„ì„
        print("ğŸ“Š ê¸°ë³¸ SURD ë¶„ì„:")
        start_time = time.time()
        result = analyzer.analyze_advanced(
            variables=variables,
            target_variable='decision_quality',
            time_series_data=time_series_data,
            additional_context={'domain': 'ethical_decision_making'}
        )
        analysis_time = time.time() - start_time
        
        print(f"   â±ï¸ ë¶„ì„ ì‹œê°„: {analysis_time:.3f}ì´ˆ")
        print(f"   ğŸ¯ ëŒ€ìƒ ë³€ìˆ˜: {result.target_variable}")
        print(f"   ğŸ“ ì…ë ¥ ë³€ìˆ˜ ìˆ˜: {len(result.input_variables)}")
        
        # 2. ì •ë³´ ë¶„í•´ ê²°ê³¼
        if result.information_decomposition:
            print(f"\nğŸ” ì •ë³´ ë¶„í•´ ê²°ê³¼:")
            for name, decomp in result.information_decomposition.items():
                print(f"   ğŸ“ˆ {name}:")
                print(f"      ì „ì²´ ì •ë³´ëŸ‰: {decomp.total_information:.4f} bits")
                
                if decomp.unique_information:
                    top_unique = max(decomp.unique_information.items(), key=lambda x: x[1])
                    print(f"      ìµœê³  ê³ ìœ  ì •ë³´: {top_unique[0]} ({top_unique[1]:.4f} bits)")
                    
                if decomp.synergy:
                    top_synergy = max(decomp.synergy.items(), key=lambda x: x[1])
                    print(f"      ìµœê³  ì‹œë„ˆì§€: {top_synergy[0]} ({top_synergy[1]:.4f} bits)")
                    
        # 3. ì‹ ê²½ë§ ì˜ˆì¸¡
        if result.neural_predictions:
            print(f"\nğŸ§  ì‹ ê²½ë§ ì˜ˆì¸¡ ê²°ê³¼:")
            for key, values in result.neural_predictions.items():
                if isinstance(values, np.ndarray) and len(values) > 0:
                    print(f"   {key}: í‰ê·  {np.mean(values):.3f}, í‘œì¤€í¸ì°¨ {np.std(values):.3f}")
                    
        # 4. ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬
        if result.causal_network:
            network = result.causal_network
            print(f"\nğŸ•¸ï¸ ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬:")
            print(f"   ë…¸ë“œ ìˆ˜: {network.metrics.get('node_count', 0)}")
            print(f"   ì—£ì§€ ìˆ˜: {network.metrics.get('edge_count', 0)}")
            print(f"   ë„¤íŠ¸ì›Œí¬ ë°€ë„: {network.metrics.get('density', 0):.3f}")
            
            central_nodes = network.metrics.get('most_central_nodes', [])
            if central_nodes:
                print(f"   ê°€ì¥ ì¤‘ìš”í•œ ë…¸ë“œ: {central_nodes[0][0]} ({central_nodes[0][1]:.3f})")
                
        # 5. ì‹œê°„ì  ë¶„ì„
        if result.temporal_analysis:
            print(f"\nâ° ì‹œê°„ì  ì¸ê³¼ê´€ê³„:")
            te_results = [(k, v) for k, v in result.temporal_analysis.items() 
                         if isinstance(v, (int, float)) and 'to_decision_quality' in k]
            if te_results:
                top_te = max(te_results, key=lambda x: x[1])
                print(f"   ìµœê°• ì „ì´ ì—”íŠ¸ë¡œí”¼: {top_te[0]} ({top_te[1]:.4f})")
                
        # 6. í†µê³„ì  ìœ ì˜ì„±
        if result.significance_results:
            print(f"\nğŸ“Š í†µê³„ì  ìœ ì˜ì„±:")
            significant_vars = [var for var, stats in result.significance_results.items() 
                              if stats.get('is_significant', False)]
            print(f"   ìœ ì˜í•œ ë³€ìˆ˜ ìˆ˜: {len(significant_vars)}")
            
            if significant_vars:
                for var in significant_vars[:3]:
                    stats = result.significance_results[var]
                    print(f"   {var}: MI={stats.get('mutual_information', 0):.4f}, p={stats.get('p_value', 1):.3f}")
                    
        # 7. ì‹œìŠ¤í…œ ì •ë³´
        print(f"\nğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   ë””ë°”ì´ìŠ¤: {analyzer.device}")
        print(f"   GPU ì‚¬ìš©: {'ì˜ˆ' if ADVANCED_CONFIG['enable_gpu'] else 'ì•„ë‹ˆì˜¤'}")
        print(f"   ë³‘ë ¬ ì²˜ë¦¬: {'ì˜ˆ' if analyzer.advanced_config['parallel_processing'] else 'ì•„ë‹ˆì˜¤'}")
        
        # ìºì‹œ í†µê³„
        cache_stats = analyzer.get_cache_stats()
        print(f"   ìºì‹œ í¬ê¸°: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
        
        # 8. ìƒì„¸ ì„¤ëª…
        print(f"\nğŸ“ ìƒì„¸ ë¶„ì„ ê²°ê³¼:")
        explanation = analyzer.explain_advanced_results(result)
        print(explanation[:500] + "..." if len(explanation) > 500 else explanation)
        
        return result
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


    def _use_advanced_models(self, processed_data: Dict[str, Any]) -> Dict[str, Any]:
        """ìƒˆë¡œìš´ ê³ ê¸‰ SURD ëª¨ë¸ í™œìš©"""
        if not self.new_models_available or not hasattr(self, 'new_surd_analyzer'):
            return {}
        
        try:
            # ë³€ìˆ˜ ì„ë² ë”© ì¤€ë¹„
            variables = processed_data['variables']
            variable_embeddings = []
            
            for var_name, var_data in variables.items():
                # ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ì‹¤ì œë¡œëŠ” sentence transformer ë“± ì‚¬ìš©)
                if isinstance(var_data, np.ndarray):
                    # ê°„ë‹¨í•œ í†µê³„ì  íŠ¹ì§•ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
                    features = np.array([
                        np.mean(var_data), np.std(var_data), np.min(var_data), np.max(var_data),
                        np.median(var_data), np.percentile(var_data, 25), np.percentile(var_data, 75)
                    ])
                    # 768ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©
                    embedding = np.zeros(768)
                    embedding[:len(features)] = features
                else:
                    # ìŠ¤ì¹¼ë¼ê°’ì˜ ê²½ìš°
                    embedding = np.zeros(768)
                    embedding[0] = float(var_data)
                
                variable_embeddings.append(torch.tensor(embedding, dtype=torch.float32))
            
            # ìƒˆë¡œìš´ ëª¨ë¸ë¡œ ë¶„ì„
            advanced_results = self.new_surd_analyzer(variable_embeddings)
            
            return {
                'neural_analysis': advanced_results.get('neural_analysis', {}),
                'synergy': advanced_results.get('synergy', torch.tensor(0.0)),
                'unique_info': advanced_results.get('unique_info', torch.tensor([0.0])),
                'redundancy': advanced_results.get('redundancy', torch.tensor(0.0)),
                'causal_matrix': advanced_results.get('causal_matrix', torch.tensor([[0.0]]))
            }
            
        except Exception as e:
            self.logger.error(f"ê³ ê¸‰ ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def integrate_with_emotion_analysis(self, emotion_data: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """ê°ì • ë¶„ì„ ëª¨ë“ˆê³¼ì˜ ì—°ë™"""
        try:
            # ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ SURD ë¶„ì„ìš© ë³€ìˆ˜ë¡œ ë³€í™˜
            surd_variables = {}
            
            # ê°ì • ê°•ë„ ë°ì´í„° ë³€í™˜
            if 'emotion_intensities' in emotion_data:
                for emotion_type, intensity in emotion_data['emotion_intensities'].items():
                    # ì‹œê³„ì—´ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ì‹œê°„ì— ë”°ë¥¸ ê°ì • ë³€í™” ë°ì´í„° ì‚¬ìš©)
                    n_samples = 1000
                    if isinstance(intensity, (int, float)):
                        # ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ì‹œë®¬ë ˆì´ì…˜
                        emotion_series = np.random.normal(intensity, intensity * 0.1, n_samples)
                        emotion_series = np.clip(emotion_series, 0, 1)  # 0-1 ë²”ìœ„ë¡œ ì œí•œ
                        surd_variables[f"emotion_{emotion_type}"] = emotion_series
            
            # ê°ì • ìƒíƒœ ë³€í™˜
            if 'emotion_states' in emotion_data:
                for state_name, state_value in emotion_data['emotion_states'].items():
                    if isinstance(state_value, (int, float)):
                        n_samples = 1000
                        state_series = np.random.normal(state_value, 0.05, n_samples)
                        surd_variables[f"state_{state_name}"] = state_series
            
            # ë°”ì´ì˜¤ì‹œê·¸ë„ ë°ì´í„° í¬í•¨
            if 'biosignals' in emotion_data:
                for signal_type, signal_value in emotion_data['biosignals'].items():
                    if isinstance(signal_value, (int, float)):
                        n_samples = 1000
                        signal_series = np.random.normal(signal_value, signal_value * 0.15, n_samples)
                        surd_variables[f"biosignal_{signal_type}"] = signal_series
            
            self.logger.info(f"ê°ì • ë¶„ì„ ë°ì´í„°ë¥¼ SURD ë³€ìˆ˜ë¡œ ë³€í™˜: {list(surd_variables.keys())}")
            return surd_variables
            
        except Exception as e:
            self.logger.error(f"ê°ì • ë¶„ì„ ì—°ë™ ì‹¤íŒ¨: {e}")
            return {}
    
    def integrate_with_emotion_analysis(self, emotion_data: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """ê°ì • ë¶„ì„ ëª¨ë“ˆê³¼ì˜ ì—°ë™"""
        try:
            surd_variables = {}
            
            # ê°ì • ë°ì´í„° ë³€í™˜
            if 'emotion' in emotion_data:
                emotion_value = emotion_data['emotion']
                if isinstance(emotion_value, (int, float)):
                    n_samples = 1000
                    # ê°ì • ê°’ì„ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                    normalized_emotion = emotion_value / 10.0 if emotion_value > 1 else emotion_value
                    emotion_series = np.random.normal(normalized_emotion, 0.1, n_samples)
                    emotion_series = np.clip(emotion_series, 0, 1)
                    surd_variables['emotion_primary'] = emotion_series
            
            # ê°•ë„ ë°ì´í„° ë³€í™˜
            if 'intensity' in emotion_data:
                intensity = emotion_data['intensity']
                if isinstance(intensity, (int, float)):
                    n_samples = 1000
                    # ê°•ë„ë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™” (1-5 â†’ 0-1)
                    normalized_intensity = (intensity - 1) / 4.0 if intensity > 1 else intensity
                    intensity_series = np.random.normal(normalized_intensity, 0.08, n_samples)
                    intensity_series = np.clip(intensity_series, 0, 1)
                    surd_variables['emotion_intensity'] = intensity_series
            
            # ì‹ ë¢°ë„ ë°ì´í„° ë³€í™˜
            if 'confidence' in emotion_data:
                confidence = emotion_data['confidence']
                if isinstance(confidence, (int, float)) and 0 <= confidence <= 1:
                    n_samples = 1000
                    confidence_series = np.random.normal(confidence, 0.05, n_samples)
                    confidence_series = np.clip(confidence_series, 0, 1)
                    surd_variables['emotion_confidence'] = confidence_series
            
            # ì›ìê°€(Valence) ë°ì´í„° ë³€í™˜
            if 'valence' in emotion_data:
                valence = emotion_data['valence']
                if isinstance(valence, (int, float)):
                    n_samples = 1000
                    # Valenceë¥¼ -1~1ì—ì„œ 0~1ë¡œ ë³€í™˜
                    normalized_valence = (valence + 1) / 2.0
                    valence_series = np.random.normal(normalized_valence, 0.1, n_samples)
                    valence_series = np.clip(valence_series, 0, 1)
                    surd_variables['emotion_valence'] = valence_series
            
            # ê°ì„±(Arousal) ë°ì´í„° ë³€í™˜
            if 'arousal' in emotion_data:
                arousal = emotion_data['arousal']
                if isinstance(arousal, (int, float)):
                    n_samples = 1000
                    # Arousalì„ -1~1ì—ì„œ 0~1ë¡œ ë³€í™˜
                    normalized_arousal = (arousal + 1) / 2.0
                    arousal_series = np.random.normal(normalized_arousal, 0.1, n_samples)
                    arousal_series = np.clip(arousal_series, 0, 1)
                    surd_variables['emotion_arousal'] = arousal_series
            
            # ì²˜ë¦¬ ë°©ë²• ì •ë³´
            if 'processing_method' in emotion_data:
                method = emotion_data['processing_method']
                # ì²˜ë¦¬ ë°©ë²•ì— ë”°ë¥¸ í’ˆì§ˆ ì ìˆ˜ (0-1)
                method_quality_map = {
                    'deep_llm_analysis': 0.9,
                    'transformer_analysis': 0.8,
                    'keyword_analysis': 0.6,
                    'fallback': 0.3
                }
                quality = method_quality_map.get(method, 0.5)
                n_samples = 1000
                quality_series = np.random.normal(quality, 0.05, n_samples)
                quality_series = np.clip(quality_series, 0, 1)
                surd_variables['emotion_processing_quality'] = quality_series
            
            # ë³µí•© ê°ì • ìƒíƒœ ê³„ì‚°
            if 'emotion_primary' in surd_variables and 'emotion_intensity' in surd_variables:
                n_samples = 1000
                # ê°ì • ê°•ë„ì™€ ì›ìê°€ë¥¼ ê²°í•©í•œ ë³µí•© ì§€ìˆ˜
                composite_emotion = (surd_variables['emotion_primary'] * surd_variables['emotion_intensity'])
                if 'emotion_valence' in surd_variables:
                    composite_emotion = composite_emotion * surd_variables['emotion_valence']
                surd_variables['emotion_composite_state'] = composite_emotion
            
            self.logger.info(f"ê°ì • ë¶„ì„ ë°ì´í„°ë¥¼ SURD ë³€ìˆ˜ë¡œ ë³€í™˜: {list(surd_variables.keys())}")
            return surd_variables
            
        except Exception as e:
            self.logger.error(f"ê°ì • ë¶„ì„ ì—°ë™ ì‹¤íŒ¨: {e}")
            return {}

    def integrate_with_bentham_calculation(self, bentham_data: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """ë²¤ë‹´ ê³„ì‚° ëª¨ë“ˆê³¼ì˜ ì—°ë™"""
        try:
            surd_variables = {}
            
            # ë²¤ë‹´ 7ë³€ìˆ˜ ë°ì´í„° ë³€í™˜
            if 'bentham_variables' in bentham_data:
                for var_name, var_value in bentham_data['bentham_variables'].items():
                    if isinstance(var_value, (int, float)):
                        n_samples = 1000
                        # ë²¤ë‹´ ë³€ìˆ˜ëŠ” ë³´í†µ 0-1 ë²”ìœ„
                        var_series = np.random.normal(var_value, 0.1, n_samples)
                        var_series = np.clip(var_series, 0, 1)
                        surd_variables[f"bentham_{var_name}"] = var_series
            
            # ê°€ì¤‘ì¹˜ ë ˆì´ì–´ ê²°ê³¼ í¬í•¨
            if 'weight_layers' in bentham_data:
                for layer_name, layer_result in bentham_data['weight_layers'].items():
                    if isinstance(layer_result, (int, float)):
                        n_samples = 1000
                        layer_series = np.random.normal(layer_result, 0.05, n_samples)
                        surd_variables[f"weight_{layer_name}"] = layer_series
            
            # ì „ì²´ ì¾Œë½ ê³„ì‚° ê²°ê³¼
            if 'pleasure_score' in bentham_data:
                pleasure_score = bentham_data['pleasure_score']
                if isinstance(pleasure_score, (int, float)):
                    n_samples = 1000
                    pleasure_series = np.random.normal(pleasure_score, pleasure_score * 0.1, n_samples)
                    surd_variables['bentham_total_pleasure'] = pleasure_series
            
            # ì‹ ê²½ë§ ì˜ˆì¸¡ ê²°ê³¼ í¬í•¨
            if 'neural_predictions' in bentham_data:
                for pred_name, pred_value in bentham_data['neural_predictions'].items():
                    if isinstance(pred_value, (int, float)):
                        n_samples = 1000
                        pred_series = np.random.normal(pred_value, 0.05, n_samples)
                        surd_variables[f"neural_{pred_name}"] = pred_series
            
            self.logger.info(f"ë²¤ë‹´ ê³„ì‚° ë°ì´í„°ë¥¼ SURD ë³€ìˆ˜ë¡œ ë³€í™˜: {list(surd_variables.keys())}")
            return surd_variables
            
        except Exception as e:
            self.logger.error(f"ë²¤ë‹´ ê³„ì‚° ì—°ë™ ì‹¤íŒ¨: {e}")
            return {}
    
    def integrate_with_llm_results(self, llm_data: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """LLM ë¶„ì„ ê²°ê³¼ì™€ì˜ ì—°ë™"""
        try:
            surd_variables = {}
            
            # LLM ë¶„ì„ ì ìˆ˜ë“¤ ë³€í™˜
            if 'analysis_scores' in llm_data:
                for score_name, score_value in llm_data['analysis_scores'].items():
                    if isinstance(score_value, (int, float)):
                        n_samples = 1000
                        score_series = np.random.normal(score_value, 0.1, n_samples)
                        score_series = np.clip(score_series, 0, 1)
                        surd_variables[f"llm_{score_name}"] = score_series
            
            # ì˜ë¯¸ ì„ë² ë”© ì°¨ì› ì¶•ì†Œ
            if 'semantic_embeddings' in llm_data:
                embeddings = llm_data['semantic_embeddings']
                if isinstance(embeddings, np.ndarray) and len(embeddings.shape) == 1:
                    # ê³ ì°¨ì› ì„ë² ë”©ì„ ì£¼ì„±ë¶„ ë¶„ì„ìœ¼ë¡œ ì°¨ì› ì¶•ì†Œ (ê°„ì†Œí™”)
                    n_samples = 1000
                    # ì„ë² ë”©ì˜ ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
                    embedding_mean = np.mean(embeddings[:10])  # ì²˜ìŒ 10ê°œ ì°¨ì› í‰ê· 
                    embedding_series = np.random.normal(embedding_mean, 0.1, n_samples)
                    surd_variables['llm_semantic_component'] = embedding_series
            
            # ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆ ì ìˆ˜
            if 'generation_quality' in llm_data:
                quality = llm_data['generation_quality']
                if isinstance(quality, (int, float)):
                    n_samples = 1000
                    quality_series = np.random.normal(quality, 0.05, n_samples)
                    quality_series = np.clip(quality_series, 0, 1)
                    surd_variables['llm_generation_quality'] = quality_series
            
            # ë§¥ë½ ì´í•´ë„
            if 'context_understanding' in llm_data:
                understanding = llm_data['context_understanding']
                if isinstance(understanding, (int, float)):
                    n_samples = 1000
                    understanding_series = np.random.normal(understanding, 0.08, n_samples)
                    understanding_series = np.clip(understanding_series, 0, 1)
                    surd_variables['llm_context_understanding'] = understanding_series
            
            self.logger.info(f"LLM ë°ì´í„°ë¥¼ SURD ë³€ìˆ˜ë¡œ ë³€í™˜: {list(surd_variables.keys())}")
            return surd_variables
            
        except Exception as e:
            self.logger.error(f"LLM ì—°ë™ ì‹¤íŒ¨: {e}")
            return {}
    
    def _generate_llm_interpretation(self, decomposition_results: Dict[str, Any], 
                                         neural_predictions: Optional[Dict[str, Any]] = None,
                                         advanced_analysis: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•œ SURD ê²°ê³¼ í•´ì„"""
        if not self.llm_integration_available or not hasattr(self, 'llm_engine'):
            raise RuntimeError("LLM í†µí•©ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ê±°ë‚˜ LLM ì—”ì§„ì´ ì—†ìŠµë‹ˆë‹¤")
        
        try:
            # ë¶„ì„ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
            analysis_text = self._format_analysis_for_llm(
                decomposition_results, neural_predictions, advanced_analysis
            )
            
            # LLMì— í•´ì„ ìš”ì²­ (ë™ê¸°ì‹)
            interpretation = explain_causal_relationships(analysis_text)
            
            if interpretation.success:
                return {
                    'llm_explanation': interpretation.generated_text,
                    'confidence': interpretation.confidence,
                    'processing_time': interpretation.processing_time,
                    'insights': self._extract_insights_from_llm(interpretation.generated_text)
                }
            else:
                self.logger.error(f"LLM í•´ì„ ì‹¤íŒ¨ - ì‹œìŠ¤í…œ ì •ì§€: {interpretation.error_message}")
                raise RuntimeError(f"SURD ë¶„ì„ì—ì„œ LLM í•´ì„ì´ í•„ìˆ˜ì´ì§€ë§Œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {interpretation.error_message}")
                
        except Exception as e:
            self.logger.error(f"LLM í•´ì„ ìƒì„± ì‹¤íŒ¨ - ì‹œìŠ¤í…œ ì •ì§€: {e}")
            raise RuntimeError(f"SURD ë¶„ì„ì—ì„œ LLM í•´ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _format_analysis_for_llm(self, decomposition_results: Dict[str, Any],
                               neural_predictions: Optional[Dict[str, Any]] = None,
                               advanced_analysis: Optional[Dict[str, Any]] = None) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ LLM ì…ë ¥ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·"""
        
        text_parts = ["SURD ì¸ê³¼ë¶„ì„ ê²°ê³¼:"]
        
        # ì •ë³´ ë¶„í•´ ê²°ê³¼
        if decomposition_results:
            text_parts.append("\n=== ì •ë³´ ë¶„í•´ ë¶„ì„ ===")
            for target, decomp in decomposition_results.items():
                if hasattr(decomp, 'total_information'):
                    text_parts.append(f"\nëª©í‘œ ë³€ìˆ˜: {target}")
                    text_parts.append(f"- ì „ì²´ ì •ë³´ëŸ‰: {decomp.total_information:.4f}")
                    
                    if hasattr(decomp, 'unique_information'):
                        text_parts.append("- ê³ ìœ  ì •ë³´:")
                        for source, unique_val in decomp.unique_information.items():
                            text_parts.append(f"  * {source}: {unique_val:.4f}")
                    
                    if hasattr(decomp, 'synergy'):
                        text_parts.append("- ì‹œë„ˆì§€ íš¨ê³¼:")
                        for synergy_pair, synergy_val in decomp.synergy.items():
                            text_parts.append(f"  * {synergy_pair}: {synergy_val:.4f}")
                    
                    if hasattr(decomp, 'redundancy'):
                        text_parts.append("- ì¤‘ë³µ ì •ë³´:")
                        for redundancy_pair, redundancy_val in decomp.redundancy.items():
                            text_parts.append(f"  * {redundancy_pair}: {redundancy_val:.4f}")
        
        # ì‹ ê²½ë§ ì˜ˆì¸¡ ê²°ê³¼
        if neural_predictions:
            text_parts.append("\n=== ì‹ ê²½ë§ ì˜ˆì¸¡ ===")
            if 'causal_strength' in neural_predictions:
                strength = neural_predictions['causal_strength']
                if hasattr(strength, 'item'):
                    strength = strength.item()
                text_parts.append(f"- ì˜ˆì¸¡ëœ ì¸ê³¼ ê°•ë„: {strength:.4f}")
            
            if 'synergy_score' in neural_predictions:
                synergy = neural_predictions['synergy_score']
                if hasattr(synergy, 'item'):
                    synergy = synergy.item()
                text_parts.append(f"- ì˜ˆì¸¡ëœ ì‹œë„ˆì§€: {synergy:.4f}")
        
        # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼
        if advanced_analysis:
            text_parts.append("\n=== ê³ ê¸‰ ëª¨ë¸ ë¶„ì„ ===")
            if 'synergy' in advanced_analysis:
                synergy = advanced_analysis['synergy']
                if hasattr(synergy, 'item'):
                    synergy = synergy.item()
                text_parts.append(f"- ê³ ê¸‰ ì‹œë„ˆì§€ ë¶„ì„: {synergy:.4f}")
            
            if 'causal_matrix' in advanced_analysis:
                text_parts.append("- ì¸ê³¼ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return "\n".join(text_parts)
    
    def _extract_insights_from_llm(self, llm_text: str) -> List[str]:
        """LLM ì‘ë‹µì—ì„œ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        insights = []
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
        lines = llm_text.split('\n')
        for line in lines:
            line = line.strip()
            if any(keyword in line.lower() for keyword in 
                  ['ì¤‘ìš”í•œ', 'í•µì‹¬', 'ì£¼ìš”', 'ê²°ë¡ ', 'ì¸ì‚¬ì´íŠ¸', 'ì‹œì‚¬ì ']):
                if len(line) > 10:  # ë„ˆë¬´ ì§§ì€ ë¼ì¸ ì œì™¸
                    insights.append(line)
        
        return insights[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
    
    def get_enhanced_performance_metrics(self) -> Dict[str, Any]:
        """í–¥ìƒëœ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        try:
            base_metrics = self.get_performance_metrics()
        except:
            base_metrics = {}
        
        enhanced_metrics = {
            'base_metrics': base_metrics,
            'model_capabilities': {
                'new_models_available': self.new_models_available,
                'llm_integration_available': self.llm_integration_available,
                'neural_model_trained': getattr(self, 'is_model_trained', False)
            },
            'cache_statistics': {
                'cache_size': len(self.analysis_cache),
                'max_cache_size': self.advanced_config['max_cache_size'],
                'cache_hit_rate': getattr(self, '_cache_hit_count', 0) / max(getattr(self, '_total_requests', 1), 1)
            },
            'configuration': self.advanced_config
        }
        
        # ìƒˆë¡œìš´ ëª¨ë¸ ì„±ëŠ¥ ì •ë³´
        if self.new_models_available and hasattr(self, 'new_surd_analyzer'):
            try:
                if hasattr(self.new_surd_analyzer, 'get_performance_stats'):
                    enhanced_metrics['new_model_performance'] = self.new_surd_analyzer.get_performance_stats()
            except:
                pass
        
        # LLM ì—”ì§„ ì„±ëŠ¥ ì •ë³´
        if self.llm_integration_available and hasattr(self, 'llm_engine'):
            try:
                enhanced_metrics['llm_performance'] = self.llm_engine.get_performance_stats()
            except:
                pass
        
        return enhanced_metrics
    
    def shutdown_enhanced_components(self):
        """í–¥ìƒëœ ì»´í¬ë„ŒíŠ¸ë“¤ ì¢…ë£Œ"""
        try:
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=True)
            
            if self.llm_integration_available and hasattr(self, 'llm_engine'):
                self.llm_engine.shutdown()
            
            self.logger.info("í–¥ìƒëœ SURD ì»´í¬ë„ŒíŠ¸ë“¤ì´ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"ì»´í¬ë„ŒíŠ¸ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    test_advanced_surd_analyzer()