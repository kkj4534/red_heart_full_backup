"""
ê³ ê¸‰ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ - Linux ì „ìš©
Advanced Experience Database System for Linux

ê³ ê¸‰ AI ê¸°ë²•ì„ í™œìš©í•œ ê²½í—˜ ì €ì¥, ê²€ìƒ‰, ì••ì¶•, í•™ìŠµ ì‹œìŠ¤í…œ
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ì‹ ê²½ë§ ê¸°ë°˜ ê²½í—˜ ì¶”ë¡ ì„ ê²°í•©
"""

import os
import json
import time
import asyncio
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Any, Optional, Union, Set
from pathlib import Path
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
import sqlite3
from collections import defaultdict, deque
import hashlib
import uuid

# ê³ ê¸‰ AI ë¼ì´ë¸ŒëŸ¬ë¦¬
# SentenceTransformerëŠ” sentence_transformer_singletonì„ í†µí•´ ì‚¬ìš©
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.neighbors import NearestNeighbors
import networkx as nx
# ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰ - subprocessë¡œ ë¶„ë¦¬ ì²˜ë¦¬
# import faiss  # ì§ì ‘ ì„í¬íŠ¸ ì œê±° - subprocess ì‚¬ìš©
FAISS_AVAILABLE = True
import warnings
warnings.filterwarnings('ignore')

from config import ADVANCED_CONFIG, get_device, TORCH_DTYPE, BATCH_SIZE, MODELS_DIR
from utils import run_faiss_subprocess
# ë°ì´í„° ëª¨ë¸ ì„ íƒì  ì„í¬íŠ¸
try:
    from data_models import (
        Decision, DecisionOutcome, ExperienceMetadata, 
        AdvancedExperience, ExperienceCluster, ExperiencePattern,
        LearningProgress, AdaptationStrategy
    )
    DATA_MODELS_AVAILABLE = True
except ImportError as e:
    # ê¸°ë³¸ ë°ì´í„° ëª¨ë¸ ì‚¬ìš©
    DATA_MODELS_AVAILABLE = False
    from dataclasses import dataclass
    
    @dataclass
    class Decision:
        id: str
        situation: str
        action: str
    
    @dataclass  
    class DecisionOutcome:
        decision_id: str
        result: str
        satisfaction: float
    
    @dataclass
    class ExperienceMetadata:
        timestamp: str = ""
        tags: list = field(default_factory=list)
    
    @dataclass  
    class AdvancedExperience:
        id: str = ""
        content: str = ""
        metadata: dict = field(default_factory=dict)
    
    @dataclass
    class ExperienceCluster:
        id: str = ""
        experiences: list = field(default_factory=list)
    
    @dataclass
    class ExperiencePattern:
        pattern_id: str = ""
        pattern_type: str = ""
        confidence: float = 0.0
    
    @dataclass
    class LearningProgress:
        progress: float = 0.0
        stage: str = ""
    
    @dataclass
    class AdaptationStrategy:
        strategy_id: str = ""
        description: str = ""

logger = logging.getLogger('RedHeart.AdvancedExperienceDB')

@dataclass
class ExperienceVector:
    """ê²½í—˜ ë²¡í„° í‘œí˜„"""
    experience_id: str
    embedding: np.ndarray
    metadata_features: np.ndarray
    timestamp: datetime
    category: str
    importance_score: float = 0.0
    access_count: int = 0
    last_accessed: datetime = field(default_factory=datetime.now)

@dataclass
class ExperienceQuery:
    """ê²½í—˜ ê²€ìƒ‰ ì¿¼ë¦¬"""
    query_text: str = ""
    query_embedding: Optional[np.ndarray] = None
    category_filter: Optional[str] = None
    time_range: Optional[Tuple[datetime, datetime]] = None
    similarity_threshold: float = 0.7
    max_results: int = 10
    include_metadata: bool = True
    boost_recent: bool = True

@dataclass
class ExperienceLearningState:
    """ê²½í—˜ í•™ìŠµ ìƒíƒœ"""
    total_experiences: int = 0
    compressed_experiences: int = 0
    active_patterns: int = 0
    learning_accuracy: float = 0.0
    adaptation_rate: float = 0.0
    memory_efficiency: float = 0.0
    last_update: datetime = field(default_factory=datetime.now)

class AdvancedNeuralMemory(nn.Module):
    """ê³ ê¸‰ ì‹ ê²½ë§ ê¸°ë°˜ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, input_dim=768, hidden_dim=512, memory_slots=1000):
        super().__init__()
        
        # ë©”ëª¨ë¦¬ ì¸ì½”ë”
        self.memory_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim // 4)
        )
        
        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim // 4,
            num_heads=8,
            batch_first=True
        )
        
        # ê²½í—˜ ë¶„ë¥˜ê¸°
        self.experience_classifier = nn.Sequential(
            nn.Linear(hidden_dim // 4, hidden_dim // 8),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 8, 10)  # 10ê°œ ì¹´í…Œê³ ë¦¬
        )
        
        # ì¤‘ìš”ë„ ì˜ˆì¸¡ê¸°
        self.importance_predictor = nn.Sequential(
            nn.Linear(hidden_dim // 4, hidden_dim // 8),
            nn.ReLU(),
            nn.Linear(hidden_dim // 8, 1),
            nn.Sigmoid()
        )
        
        # ë©”ëª¨ë¦¬ ìŠ¬ë¡¯
        self.memory_slots = memory_slots
        self.memory_bank = nn.Parameter(
            torch.randn(memory_slots, hidden_dim // 4)
        )
        
    def forward(self, experience_embeddings):
        """ìˆœì „íŒŒ"""
        # ë©”ëª¨ë¦¬ ì¸ì½”ë”©
        encoded = self.memory_encoder(experience_embeddings)
        
        # ì–´í…ì…˜ ì ìš©
        attended, attention_weights = self.attention(
            encoded, self.memory_bank.unsqueeze(0).repeat(encoded.size(0), 1, 1), 
            self.memory_bank.unsqueeze(0).repeat(encoded.size(0), 1, 1)
        )
        
        # ë¶„ë¥˜ ë° ì¤‘ìš”ë„ ì˜ˆì¸¡
        categories = self.experience_classifier(attended.mean(dim=1))
        importance = self.importance_predictor(attended.mean(dim=1))
        
        return {
            'encoded': encoded,
            'attended': attended,
            'attention_weights': attention_weights,
            'categories': categories,
            'importance': importance
        }

class AdvancedExperienceDatabase:
    """ê³ ê¸‰ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤"""
    
    def __init__(self):
        """ê³ ê¸‰ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        # ê¸°ë³¸ ì„¤ì • ì‚¬ìš© (configì— ì—†ëŠ” ê²½ìš°)
        self.config = ADVANCED_CONFIG.get('experience_db', {
            'max_experiences': 10000,
            'compression_threshold': 1000,
            'similarity_threshold': 0.8,
            'clustering_method': 'kmeans'
        })
        self.device = get_device()
        self.dtype = TORCH_DTYPE
        
        # ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ì„¤ì •
        self.db_dir = os.path.join(MODELS_DIR, 'experience_db')
        self.vector_db_path = os.path.join(self.db_dir, 'vectors')
        self.metadata_db_path = os.path.join(self.db_dir, 'metadata.db')
        self.models_path = os.path.join(self.db_dir, 'models')
        
        os.makedirs(self.db_dir, exist_ok=True)
        os.makedirs(self.vector_db_path, exist_ok=True)
        os.makedirs(self.models_path, exist_ok=True)
        
        # GPU ìˆœì°¨ ì²˜ë¦¬ ë° ë°°ì¹˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë¬´í•œ ëŒ€ê¸° ì´ìŠˆ í•´ê²°)
        self.gpu_semaphore = asyncio.Semaphore(1)  # GPU ë™ì‹œ ì ‘ê·¼ ì œí•œ
        self.embedding_batch_queue = asyncio.Queue()  # ë°°ì¹˜ ì²˜ë¦¬ í
        self.embedding_results = {}  # ìš”ì²­ ID -> ê²°ê³¼ ë§¤í•‘
        self.batch_size = 32  # ë™ì  ë°°ì¹˜ í¬ê¸°
        self.batch_timeout = 1.0  # 1ì´ˆ íƒ€ì„ì•„ì›ƒ (ë” ë‚˜ì€ ì „ëµ)
        self.current_batch = []  # í˜„ì¬ ë°°ì¹˜
        self.batch_lock = asyncio.Lock()  # ë°°ì¹˜ ì¡°ì‘ ë™ê¸°í™”
        self.batch_processor_task = None  # ë°°ì¹˜ ì²˜ë¦¬ íƒœìŠ¤í¬
        
        # ThreadPoolExecutor ì´ˆê¸°í™”
        from concurrent.futures import ThreadPoolExecutor
        self.executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="EmbeddingExecutor")
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_embedding_model()
        
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (FAISS)
        self._initialize_vector_db()
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì›Œì»¤ ì‹œì‘
        self._start_batch_processor()
        
        # ë©”íƒ€ë°ì´í„° SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._initialize_metadata_db()
        
        # ì‹ ê²½ë§ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_neural_memory()
        
        # í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_learning_systems()
        
        # ê²½í—˜ ì €ì¥ì†Œ
        self.experience_vectors = {}
        self.experience_metadata = {}
        self.experience_clusters = {}
        self.learning_patterns = {}
        
        # í•™ìŠµ ìƒíƒœ
        self.learning_state = ExperienceLearningState()
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.query_cache = {}
        self.embedding_cache = {}
        
        # ì‹¤ì‹œê°„ í•™ìŠµì„ ìœ„í•œ ë²„í¼
        self.learning_buffer = deque(maxlen=1000)
        
        logger.info("ê³ ê¸‰ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _initialize_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ì‹±ê¸€í†¤ ë§¤ë‹ˆì € ì‚¬ìš©)"""
        try:
            from sentence_transformer_singleton import get_sentence_transformer
            
            # ì‹±ê¸€í†¤ ë§¤ë‹ˆì €ë¥¼ í†µí•´ ê³µìœ  ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            self.embedding_model = get_sentence_transformer(
                'paraphrase-multilingual-mpnet-base-v2',
                device=str(self.device),
                cache_folder=self.models_path
            )
            
            # ì„ë² ë”© ì°¨ì›
            self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
            
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì‹±ê¸€í†¤, ì°¨ì›: {self.embedding_dim})")
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # fallback ì—†ìŒ - ë°”ë¡œ ì˜ˆì™¸ ë°œìƒ
            raise RuntimeError(f"SentenceTransformer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}") from e
    
    def _initialize_vector_db(self):
        """FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” - subprocess ë¶„ë¦¬ ì²˜ë¦¬"""
        try:
            # subprocessë¥¼ í†µí•´ FAISS ì¸ë±ìŠ¤ ìƒì„±
            result = run_faiss_subprocess('create_index', {
                'dimension': self.embedding_dim,
                'index_type': 'IndexFlatL2'
            })
            
            if result['status'] == 'success':
                logger.info(f"FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {result['index_type']}, ì°¨ì›={result['dimension']}")
                # ì¸ë±ìŠ¤ ìƒíƒœë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ê´€ë¦¬
                self.vector_index_info = {
                    'index_type': result['index_type'],
                    'dimension': result['dimension'],
                    'total_vectors': 0
                }
                self.vector_ids = []
                logger.info("FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (subprocess)")
            else:
                raise RuntimeError(f"FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                
        except Exception as e:
            logger.error(f"FAISS ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _initialize_metadata_db(self):
        """SQLite ë©”íƒ€ë°ì´í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            self.metadata_conn = sqlite3.connect(
                self.metadata_db_path, 
                check_same_thread=False
            )
            
            # í…Œì´ë¸” ìƒì„±
            self.metadata_conn.execute('''
                CREATE TABLE IF NOT EXISTS experiences (
                    id TEXT PRIMARY KEY,
                    text TEXT NOT NULL,
                    category TEXT,
                    importance_score REAL,
                    access_count INTEGER DEFAULT 0,
                    created_at TIMESTAMP,
                    last_accessed TIMESTAMP,
                    is_compressed BOOLEAN DEFAULT FALSE,
                    cluster_id TEXT,
                    metadata_json TEXT
                )
            ''')
            
            self.metadata_conn.execute('''
                CREATE TABLE IF NOT EXISTS experience_relations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source_id TEXT,
                    target_id TEXT,
                    relation_type TEXT,
                    strength REAL,
                    created_at TIMESTAMP,
                    FOREIGN KEY (source_id) REFERENCES experiences (id),
                    FOREIGN KEY (target_id) REFERENCES experiences (id)
                )
            ''')
            
            self.metadata_conn.execute('''
                CREATE TABLE IF NOT EXISTS learning_progress (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    metric_name TEXT,
                    metric_value REAL,
                    timestamp TIMESTAMP
                )
            ''')
            
            # ì¸ë±ìŠ¤ ìƒì„±
            self.metadata_conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_category ON experiences (category)
            ''')
            self.metadata_conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_created_at ON experiences (created_at)
            ''')
            self.metadata_conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_importance ON experiences (importance_score)
            ''')
            
            self.metadata_conn.commit()
            
            logger.info("ë©”íƒ€ë°ì´í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.metadata_conn = None
    
    def _initialize_neural_memory(self):
        """ì‹ ê²½ë§ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            self.neural_memory = AdvancedNeuralMemory(
                input_dim=self.embedding_dim,
                hidden_dim=self.config.get('neural_hidden_dim', 512),
                memory_slots=self.config.get('memory_slots', 1000)
            ).to(self.device)
            
            # ì˜µí‹°ë§ˆì´ì €
            self.memory_optimizer = torch.optim.Adam(
                self.neural_memory.parameters(),
                lr=self.config.get('learning_rate', 0.001)
            )
            
            # ì†ì‹¤ í•¨ìˆ˜ë“¤
            self.classification_loss = nn.CrossEntropyLoss()
            self.importance_loss = nn.MSELoss()
            
            # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint_path = os.path.join(self.models_path, 'neural_memory.pt')
            if os.path.exists(checkpoint_path):
                try:
                    checkpoint = torch.load(checkpoint_path, map_location=self.device)
                    self.neural_memory.load_state_dict(checkpoint['model_state'])
                    self.memory_optimizer.load_state_dict(checkpoint['optimizer_state'])
                    logger.info("ì‹ ê²½ë§ ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                except:
                    logger.warning("ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨, ìƒˆ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            logger.info("ì‹ ê²½ë§ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ì‹ ê²½ë§ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.neural_memory = None
    
    def _initialize_learning_systems(self):
        """í•™ìŠµ ì‹œìŠ¤í…œë“¤ ì´ˆê¸°í™”"""
        # ì´ìƒ íƒì§€ê¸° (ìƒˆë¡œìš´ ê²½í—˜ íŒ¨í„´ ê°ì§€)
        self.anomaly_detector = IsolationForest(
            contamination=0.1,
            random_state=42
        )
        
        # í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜
        self.clusterer = DBSCAN(
            eps=0.3,
            min_samples=3,
            metric='cosine'
        )
        
        # íŒ¨í„´ ë¶„ë¥˜ê¸°
        self.pattern_classifier = RandomForestClassifier(
            n_estimators=100,
            random_state=42
        )
        
        # ê·¼ì‚¬ ì´ì›ƒ ê²€ìƒ‰ê¸°
        self.neighbor_searcher = NearestNeighbors(
            n_neighbors=10,
            metric='cosine'
        )
        
        logger.info("í•™ìŠµ ì‹œìŠ¤í…œë“¤ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    async def store_experience(self, experience_text: str, 
                             metadata: Dict[str, Any] = None,
                             category: str = "general",
                             importance_score: Optional[float] = None) -> str:
        """
        ê³ ê¸‰ ê²½í—˜ ì €ì¥
        
        Args:
            experience_text: ê²½í—˜ í…ìŠ¤íŠ¸
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            category: ì¹´í…Œê³ ë¦¬
            importance_score: ì¤‘ìš”ë„ ì ìˆ˜ (Noneì´ë©´ ìë™ ê³„ì‚°)
            
        Returns:
            ê²½í—˜ ID
        """
        try:
            # ê²½í—˜ ID ìƒì„±
            experience_id = str(uuid.uuid4())
            
            # ì„ë² ë”© ìƒì„±
            embedding = await self._generate_embedding(experience_text)
            
            # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
            if importance_score is None:
                importance_score = await self._calculate_importance_score(
                    experience_text, embedding, metadata
                )
            
            # ë©”íƒ€ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ
            metadata_features = self._extract_metadata_features(metadata or {})
            
            # ê²½í—˜ ë²¡í„° ìƒì„±
            experience_vector = ExperienceVector(
                experience_id=experience_id,
                embedding=embedding,
                metadata_features=metadata_features,
                timestamp=datetime.now(),
                category=category,
                importance_score=importance_score
            )
            
            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ - subprocess í™˜ê²½ ë¶„ë¦¬ ë°©ì‹ ì‚¬ìš©
            if self.vector_index_info is not None:
                try:
                    # FAISS subprocessë¥¼ í†µí•´ ë²¡í„° ì¶”ê°€
                    result = run_faiss_subprocess('add_vectors', {
                        'vectors': [embedding.tolist()],
                        'dimension': self.embedding_dim,
                        'index_type': self.vector_index_info['index_type']
                    })
                    
                    if result['status'] == 'success':
                        self.vector_index_info['total_vectors'] = result['total_vectors']
                        self.vector_ids.append(experience_id)
                        logger.debug(f"ë²¡í„°ê°€ FAISSì— ì¶”ê°€ë¨: {experience_id}")
                    else:
                        logger.warning(f"FAISS ë²¡í„° ì¶”ê°€ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                        # í´ë°±: ë©”ëª¨ë¦¬ ê¸°ë°˜ ì €ì¥
                        self.vectors_memory.append(embedding)
                        self.vector_ids.append(experience_id)
                        
                except Exception as e:
                    logger.warning(f"FAISS subprocess ì˜¤ë¥˜: {e}, ë©”ëª¨ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ì €ì¥")
                    # í´ë°±: ë©”ëª¨ë¦¬ ê¸°ë°˜ ì €ì¥
                    self.vectors_memory.append(embedding)
                    self.vector_ids.append(experience_id)
            else:
                # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì €ì¥
                self.vectors_memory.append(embedding)
                self.vector_ids.append(experience_id)
            
            # ë©”íƒ€ë°ì´í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            if self.metadata_conn:
                self.metadata_conn.execute('''
                    INSERT INTO experiences 
                    (id, text, category, importance_score, created_at, last_accessed, metadata_json)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    experience_id,
                    experience_text,
                    category,
                    importance_score,
                    datetime.now(),
                    datetime.now(),
                    json.dumps(metadata or {})
                ))
                self.metadata_conn.commit()
            
            # ê²½í—˜ ì €ì¥ì†Œì— ì¶”ê°€
            self.experience_vectors[experience_id] = experience_vector
            self.experience_metadata[experience_id] = {
                'text': experience_text,
                'metadata': metadata or {},
                'category': category,
                'importance_score': importance_score,
                'created_at': datetime.now()
            }
            
            # ì‹¤ì‹œê°„ í•™ìŠµ ë²„í¼ì— ì¶”ê°€
            self.learning_buffer.append({
                'id': experience_id,
                'embedding': embedding,
                'importance': importance_score,
                'category': category
            })
            
            # í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.learning_state.total_experiences += 1
            
            # ì£¼ê¸°ì  í•™ìŠµ íŠ¸ë¦¬ê±°
            if len(self.learning_buffer) >= self.config.get('learning_batch_size', 50):
                asyncio.create_task(self._update_learning_systems())
            
            logger.info(f"ê²½í—˜ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {experience_id}")
            return experience_id
            
        except Exception as e:
            logger.error(f"ê²½í—˜ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    async def search_experiences(self, query: ExperienceQuery) -> List[Dict[str, Any]]:
        """
        ê³ ê¸‰ ê²½í—˜ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_query_cache_key(query)
            if cache_key in self.query_cache:
                cached_result = self.query_cache[cache_key]
                # ìºì‹œê°€ ë„ˆë¬´ ì˜¤ë˜ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
                if (datetime.now() - cached_result['timestamp']).seconds < 300:  # 5ë¶„
                    return cached_result['results']
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            if query.query_embedding is None and query.query_text:
                query_embedding = await self._generate_embedding(query.query_text)
            else:
                query_embedding = query.query_embedding
            
            if query_embedding is None:
                return []
            
            # ë²¡í„° ê²€ìƒ‰
            similar_experiences = await self._vector_search(
                query_embedding, 
                query.max_results * 2  # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
            )
            
            # ë©”íƒ€ë°ì´í„° í•„í„°ë§
            filtered_results = await self._apply_metadata_filters(similar_experiences, query)
            
            # ìµœê·¼ì„± ë¶€ìŠ¤íŠ¸ ì ìš©
            if query.boost_recent:
                filtered_results = self._apply_recency_boost(filtered_results)
            
            # ê²°ê³¼ ì •ë ¬ ë° ì œí•œ
            final_results = sorted(
                filtered_results, 
                key=lambda x: x['relevance_score'], 
                reverse=True
            )[:query.max_results]
            
            # ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€ì— ë”°ë¼ ê²°ê³¼ êµ¬ì„±
            if query.include_metadata:
                enriched_results = await self._enrich_results_with_metadata(final_results)
            else:
                enriched_results = final_results
            
            # ìºì‹œ ì €ì¥
            self.query_cache[cache_key] = {
                'results': enriched_results,
                'timestamp': datetime.now()
            }
            
            # ì ‘ê·¼ íšŸìˆ˜ ì—…ë°ì´íŠ¸
            for result in enriched_results:
                await self._update_access_count(result['experience_id'])
            
            return enriched_results
            
        except Exception as e:
            logger.error(f"ê²½í—˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def compress_experiences(self, compression_threshold: float = 0.1) -> Dict[str, Any]:
        """
        ê²½í—˜ ì••ì¶• - ìœ ì‚¬í•œ ê²½í—˜ë“¤ì„ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
        
        Args:
            compression_threshold: ì••ì¶• ì„ê³„ê°’
            
        Returns:
            ì••ì¶• ê²°ê³¼ í†µê³„
        """
        try:
            logger.info("ê²½í—˜ ì••ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # ëª¨ë“  ê²½í—˜ ë²¡í„° ìˆ˜ì§‘
            all_embeddings = []
            all_ids = []
            
            for exp_id, exp_vector in self.experience_vectors.items():
                all_embeddings.append(exp_vector.embedding)
                all_ids.append(exp_id)
            
            if len(all_embeddings) < 10:
                return {'message': 'ì••ì¶•ì— ì¶©ë¶„í•œ ê²½í—˜ì´ ì—†ìŠµë‹ˆë‹¤.', 'compressed': 0}
            
            embeddings_array = np.array(all_embeddings)
            
            # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            cluster_labels = self.clusterer.fit_predict(embeddings_array)
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ì••ì¶•
            compression_stats = {
                'total_experiences': len(all_embeddings),
                'clusters_formed': len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0),
                'compressed_experiences': 0,
                'compression_ratio': 0.0,
                'cluster_details': {}
            }
            
            for cluster_id in set(cluster_labels):
                if cluster_id == -1:  # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ë“¤
                    continue
                
                # í´ëŸ¬ìŠ¤í„° ë©¤ë²„ ì°¾ê¸°
                cluster_members = [all_ids[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
                
                if len(cluster_members) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒì¼ ë•Œë§Œ ì••ì¶•
                    # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ê²½í—˜ ì„ íƒ (ì¤‘ìš”ë„ê°€ ê°€ì¥ ë†’ì€ ê²ƒ)
                    representative_id = max(
                        cluster_members,
                        key=lambda x: self.experience_vectors[x].importance_score
                    )
                    
                    # ë‚˜ë¨¸ì§€ ê²½í—˜ë“¤ì„ ì••ì¶•
                    compressed_members = [m for m in cluster_members if m != representative_id]
                    
                    # í´ëŸ¬ìŠ¤í„° ì •ë³´ ìƒì„±
                    cluster_info = ExperienceCluster(
                        cluster_id=f"cluster_{cluster_id}",
                        representative_id=representative_id,
                        member_ids=cluster_members,
                        compressed_ids=compressed_members,
                        centroid=np.mean([self.experience_vectors[m].embedding for m in cluster_members], axis=0),
                        compression_ratio=len(compressed_members) / len(cluster_members),
                        created_at=datetime.now()
                    )
                    
                    self.experience_clusters[cluster_info.cluster_id] = cluster_info
                    
                    # ì••ì¶•ëœ ê²½í—˜ë“¤ì„ ë©”íƒ€ë°ì´í„° DBì—ì„œ ì••ì¶• ìƒíƒœë¡œ í‘œì‹œ
                    if self.metadata_conn:
                        for compressed_id in compressed_members:
                            self.metadata_conn.execute('''
                                UPDATE experiences 
                                SET is_compressed = TRUE, cluster_id = ?
                                WHERE id = ?
                            ''', (cluster_info.cluster_id, compressed_id))
                        
                        self.metadata_conn.commit()
                    
                    compression_stats['compressed_experiences'] += len(compressed_members)
                    compression_stats['cluster_details'][cluster_info.cluster_id] = {
                        'size': len(cluster_members),
                        'compressed': len(compressed_members),
                        'representative': representative_id
                    }
            
            # ì••ì¶• ë¹„ìœ¨ ê³„ì‚°
            if compression_stats['total_experiences'] > 0:
                compression_stats['compression_ratio'] = (
                    compression_stats['compressed_experiences'] / 
                    compression_stats['total_experiences']
                )
            
            # í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.learning_state.compressed_experiences = compression_stats['compressed_experiences']
            self.learning_state.memory_efficiency = compression_stats['compression_ratio']
            
            logger.info(f"ê²½í—˜ ì••ì¶• ì™„ë£Œ: {compression_stats['compressed_experiences']}ê°œ ì••ì¶•ë¨")
            return compression_stats
            
        except Exception as e:
            logger.error(f"ê²½í—˜ ì••ì¶• ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'compressed': 0}
    
    async def learn_patterns(self) -> Dict[str, Any]:
        """
        ê²½í—˜ íŒ¨í„´ í•™ìŠµ - ì‹ ê²½ë§ì„ í†µí•œ ê³ ê¸‰ íŒ¨í„´ ì¸ì‹
        
        Returns:
            í•™ìŠµ ê²°ê³¼
        """
        try:
            if self.neural_memory is None:
                return {'error': 'ì‹ ê²½ë§ ë©”ëª¨ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}
            
            logger.info("ê²½í—˜ íŒ¨í„´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
            training_data = list(self.learning_buffer)
            if len(training_data) < 10:
                return {'message': 'í•™ìŠµì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'}
            
            # ë°°ì¹˜ ë°ì´í„° êµ¬ì„±
            embeddings = torch.tensor(
                [item['embedding'] for item in training_data],
                dtype=self.dtype,
                device=self.device
            )
            
            importance_targets = torch.tensor(
                [item['importance'] for item in training_data],
                dtype=self.dtype,
                device=self.device
            ).unsqueeze(1)
            
            # ì¹´í…Œê³ ë¦¬ ë ˆì´ë¸” ì¸ì½”ë”©
            unique_categories = list(set(item['category'] for item in training_data))
            category_to_idx = {cat: idx for idx, cat in enumerate(unique_categories)}
            category_targets = torch.tensor(
                [category_to_idx[item['category']] for item in training_data],
                dtype=torch.long,
                device=self.device
            )
            
            # ì‹ ê²½ë§ í•™ìŠµ
            self.neural_memory.train()
            total_loss = 0.0
            num_batches = (len(training_data) + self.config.get('batch_size', 32) - 1) // self.config.get('batch_size', 32)
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * self.config.get('batch_size', 32)
                end_idx = min(start_idx + self.config.get('batch_size', 32), len(training_data))
                
                batch_embeddings = embeddings[start_idx:end_idx]
                batch_importance = importance_targets[start_idx:end_idx]
                batch_categories = category_targets[start_idx:end_idx]
                
                # ìˆœì „íŒŒ
                outputs = self.neural_memory(batch_embeddings)
                
                # ì†ì‹¤ ê³„ì‚°
                classification_loss = self.classification_loss(
                    outputs['categories'], 
                    batch_categories
                )
                importance_loss = self.importance_loss(
                    outputs['importance'], 
                    batch_importance
                )
                
                total_loss_batch = classification_loss + importance_loss
                
                # ì—­ì „íŒŒ
                self.memory_optimizer.zero_grad()
                total_loss_batch.backward()
                self.memory_optimizer.step()
                
                total_loss += total_loss_batch.item()
            
            avg_loss = total_loss / num_batches
            
            # í•™ìŠµ ì •í™•ë„ ê³„ì‚°
            self.neural_memory.eval()
            with torch.no_grad():
                outputs = self.neural_memory(embeddings)
                predicted_categories = torch.argmax(outputs['categories'], dim=1)
                accuracy = (predicted_categories == category_targets).float().mean().item()
            
            # íŒ¨í„´ ì¶”ì¶œ
            patterns = await self._extract_learned_patterns(training_data, outputs)
            
            # í•™ìŠµ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.learning_state.learning_accuracy = accuracy
            self.learning_state.active_patterns = len(patterns)
            self.learning_state.last_update = datetime.now()
            
            # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            checkpoint_path = os.path.join(self.models_path, 'neural_memory.pt')
            torch.save({
                'model_state': self.neural_memory.state_dict(),
                'optimizer_state': self.memory_optimizer.state_dict(),
                'loss': avg_loss,
                'accuracy': accuracy
            }, checkpoint_path)
            
            # í•™ìŠµ ë²„í¼ ì´ˆê¸°í™”
            self.learning_buffer.clear()
            
            learning_result = {
                'training_samples': len(training_data),
                'average_loss': avg_loss,
                'accuracy': accuracy,
                'patterns_learned': len(patterns),
                'model_saved': True
            }
            
            logger.info(f"íŒ¨í„´ í•™ìŠµ ì™„ë£Œ: ì •í™•ë„ {accuracy:.3f}, ì†ì‹¤ {avg_loss:.3f}")
            return learning_result
            
        except Exception as e:
            logger.error(f"íŒ¨í„´ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    async def get_experience_insights(self, experience_id: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ê²½í—˜ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ ë¶„ì„
        
        Args:
            experience_id: ê²½í—˜ ID
            
        Returns:
            ê²½í—˜ ì¸ì‚¬ì´íŠ¸
        """
        try:
            if experience_id not in self.experience_vectors:
                return {'error': 'ê²½í—˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
            
            experience = self.experience_vectors[experience_id]
            metadata = self.experience_metadata[experience_id]
            
            # ìœ ì‚¬ ê²½í—˜ ì°¾ê¸°
            similar_experiences = await self._vector_search(
                experience.embedding, 
                max_results=5
            )
            
            # í´ëŸ¬ìŠ¤í„° ì •ë³´
            cluster_info = None
            for cluster_id, cluster in self.experience_clusters.items():
                if experience_id in cluster.member_ids:
                    cluster_info = {
                        'cluster_id': cluster_id,
                        'is_representative': experience_id == cluster.representative_id,
                        'cluster_size': len(cluster.member_ids),
                        'compression_ratio': cluster.compression_ratio
                    }
                    break
            
            # ì‹ ê²½ë§ ë¶„ì„ (ê°€ëŠ¥í•œ ê²½ìš°)
            neural_analysis = None
            if self.neural_memory is not None:
                with torch.no_grad():
                    embedding_tensor = torch.tensor(
                        experience.embedding,
                        dtype=self.dtype,
                        device=self.device
                    ).unsqueeze(0)
                    
                    outputs = self.neural_memory(embedding_tensor)
                    
                    neural_analysis = {
                        'predicted_importance': float(outputs['importance'].squeeze()),
                        'category_probabilities': torch.softmax(outputs['categories'], dim=1).squeeze().cpu().numpy().tolist(),
                        'attention_weights': outputs['attention_weights'].squeeze().cpu().numpy().tolist()
                    }
            
            # ê²½í—˜ ì˜í–¥ë„ ë¶„ì„
            impact_analysis = await self._analyze_experience_impact(experience_id)
            
            insights = {
                'experience_id': experience_id,
                'basic_info': {
                    'category': experience.category,
                    'importance_score': experience.importance_score,
                    'access_count': experience.access_count,
                    'created_at': experience.timestamp.isoformat(),
                    'last_accessed': experience.last_accessed.isoformat()
                },
                'similar_experiences': similar_experiences,
                'cluster_info': cluster_info,
                'neural_analysis': neural_analysis,
                'impact_analysis': impact_analysis,
                'text_preview': metadata['text'][:200] + "..." if len(metadata['text']) > 200 else metadata['text']
            }
            
            return insights
            
        except Exception as e:
            logger.error(f"ê²½í—˜ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    async def optimize_database(self) -> Dict[str, Any]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” - ì„±ëŠ¥ ê°œì„  ë° ë©”ëª¨ë¦¬ ì •ë¦¬
        
        Returns:
            ìµœì í™” ê²°ê³¼
        """
        try:
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            optimization_results = {
                'cache_cleared': 0,
                'index_rebuilt': False,
                'compression_performed': False,
                'old_data_archived': 0,
                'performance_improvement': 0.0
            }
            
            # ìºì‹œ ì •ë¦¬
            old_cache_size = len(self.query_cache) + len(self.embedding_cache)
            self.query_cache.clear()
            self.embedding_cache.clear()
            optimization_results['cache_cleared'] = old_cache_size
            
            # FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì„± (í•„ìš”í•œ ê²½ìš°)
            if self.vector_index is not None and self.vector_index.ntotal > 1000:
                try:
                    # ì¸ë±ìŠ¤ ì €ì¥
                    index_file = os.path.join(self.vector_db_path, 'experience_index.faiss')
                    faiss.write_index(self.vector_index, index_file)
                    optimization_results['index_rebuilt'] = True
                except Exception as e:
                    logger.warning(f"ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # ì˜¤ë˜ëœ ë°ì´í„° ì•„ì¹´ì´ë¸Œ (90ì¼ ì´ìƒ)
            cutoff_date = datetime.now() - timedelta(days=90)
            archived_count = 0
            
            if self.metadata_conn:
                cursor = self.metadata_conn.execute('''
                    SELECT id FROM experiences 
                    WHERE created_at < ? AND access_count = 0
                ''', (cutoff_date,))
                
                old_experience_ids = [row[0] for row in cursor.fetchall()]
                
                for exp_id in old_experience_ids:
                    # ë©”íƒ€ë°ì´í„°ì—ì„œ ì•„ì¹´ì´ë¸Œ í‘œì‹œ
                    self.metadata_conn.execute('''
                        UPDATE experiences 
                        SET metadata_json = json_set(metadata_json, '$.archived', 'true')
                        WHERE id = ?
                    ''', (exp_id,))
                    
                    # ë©”ëª¨ë¦¬ì—ì„œ ì œê±° (ì„ íƒì )
                    if exp_id in self.experience_vectors:
                        del self.experience_vectors[exp_id]
                    if exp_id in self.experience_metadata:
                        del self.experience_metadata[exp_id]
                    
                    archived_count += 1
                
                self.metadata_conn.commit()
                optimization_results['old_data_archived'] = archived_count
            
            # ê²½í—˜ ì••ì¶• ìˆ˜í–‰
            compression_result = await self.compress_experiences()
            if 'compressed_experiences' in compression_result:
                optimization_results['compression_performed'] = True
            
            # SQLite ìµœì í™”
            if self.metadata_conn:
                self.metadata_conn.execute('VACUUM')
                self.metadata_conn.execute('ANALYZE')
            
            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì™„ë£Œ: {archived_count}ê°œ ì•„ì¹´ì´ë¸Œë¨")
            return optimization_results
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    # =====================================
    # ğŸš€ ë™ì  ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ (ë¬´í•œ ëŒ€ê¸° ì´ìŠˆ í•´ê²°)
    # =====================================
    
    def _start_batch_processor(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ì›Œì»¤ ì‹œì‘"""
        if self.batch_processor_task is None:
            self.batch_processor_task = asyncio.create_task(self._process_embedding_queue())
            logger.info("GPU ë°°ì¹˜ ì²˜ë¦¬ ì›Œì»¤ ì‹œì‘ë¨ (ë¬´í•œ ëŒ€ê¸° ì´ìŠˆ í•´ê²°)")
    
    async def _process_embedding_queue(self):
        """ë°±ê·¸ë¼ìš´ë“œ ë°°ì¹˜ ì²˜ë¦¬ ì›Œì»¤ - ë” ë‚˜ì€ ì „ëµ êµ¬í˜„"""
        while True:
            try:
                # ë°°ì¹˜ ìˆ˜ì§‘ ëŒ€ê¸°
                await asyncio.sleep(0.01)  # CPU ì‚¬ìš©ë¥  ì¡°ì ˆ
                
                async with self.batch_lock:
                    if not self.current_batch:
                        continue
                    
                    # ë™ì  ë°°ì¹˜ ì²˜ë¦¬ ì¡°ê±´ (ë” ë‚˜ì€ ì „ëµ)
                    should_process = (
                        len(self.current_batch) >= self.batch_size  # 32ê°œ ë„ë‹¬ -> ì¦‰ì‹œ ì²˜ë¦¬
                        or (self.current_batch and 
                            time.time() - self.current_batch[0]['timestamp'] >= self.batch_timeout)  # 1ì´ˆ ê²½ê³¼ -> í˜„ì¬ ë°°ì¹˜ ì²˜ë¦¬
                    )
                    
                    if should_process:
                        batch_to_process = self.current_batch.copy()
                        self.current_batch.clear()
                    else:
                        continue
                
                # GPU ìˆœì°¨ ì²˜ë¦¬ (ì„¸ë§ˆí¬ì–´ë¡œ ë³´ì¥)
                await self._process_batch(batch_to_process)
                
            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì›Œì»¤ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)  # ì˜¤ë¥˜ ì‹œ ì ì‹œ ëŒ€ê¸°
    
    async def _process_batch(self, batch_requests):
        """ì‹¤ì œ ë°°ì¹˜ ì²˜ë¦¬ - GPU ìˆœì°¨ ì ‘ê·¼ ë³´ì¥"""
        if not batch_requests:
            return
        
        # GPU ì„¸ë§ˆí¬ì–´ë¡œ ìˆœì°¨ ì ‘ê·¼ ë³´ì¥
        async with self.gpu_semaphore:
            try:
                # ë°°ì¹˜ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                texts = [req['text'] for req in batch_requests]
                
                # GPUì—ì„œ ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ë¸”ë¡œí‚¹ ë°©ì§€ë¥¼ ìœ„í•´ executor ì‚¬ìš©)
                loop = asyncio.get_event_loop()
                with ThreadPoolExecutor(max_workers=1) as executor:
                    embeddings = await loop.run_in_executor(
                        executor, 
                        self.embedding_model.encode, 
                        texts
                    )
                
                # ê²°ê³¼ë¥¼ ê° ìš”ì²­ì— ë§¤í•‘
                for i, req in enumerate(batch_requests):
                    embedding = embeddings[i]
                    req_id = req['request_id']
                    text = req['text']
                    
                    # ìºì‹œì— ì €ì¥
                    self.embedding_cache[text] = embedding
                    
                    # ê²°ê³¼ ì €ì¥ ë° ì´ë²¤íŠ¸ ì‹ í˜¸
                    self.embedding_results[req_id] = embedding
                    req['event'].set()
                
                logger.debug(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(batch_requests)}ê°œ ì„ë² ë”©")
                
            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ìš”ì²­ë“¤ì— ì˜¤ë¥˜ ì‹ í˜¸ (í´ë°± ì—†ìŒ, ì¦‰ì‹œ ì‹¤íŒ¨)
                for req in batch_requests:
                    req_id = req['request_id']
                    # ì‹¤íŒ¨ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ - RuntimeErrorê°€ ë°œìƒí•˜ë„ë¡ í•¨
                    req['event'].set()
    
    # =====================================
    # ğŸ“ ê¸°ì¡´ ë©”ì„œë“œ (ì¸í„°í˜ì´ìŠ¤ ìœ ì§€í•˜ë©° ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë³€ê²½)
    # =====================================
    
    # í—¬í¼ ë©”ì„œë“œë“¤
    async def _generate_embedding(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± - GPU ì„¸ë§ˆí¬ì–´ë¡œ ë¬´í•œ ëŒ€ê¸° ì´ìŠˆ í•´ê²°"""
        # ìºì‹œ í™•ì¸ (ë¹ ë¥¸ ë°˜í™˜)
        if text in self.embedding_cache:
            return self.embedding_cache[text]
        
        # ì„ë² ë”© ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° ì¦‰ì‹œ ì‹¤íŒ¨
        if self.embedding_model is None:
            raise RuntimeError("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í´ë°± ëª¨ë“œ ë¹„í™œì„±í™”ë¨.")
        
        # GPU ì„¸ë§ˆí¬ì–´ë¡œ ìˆœì°¨ ì²˜ë¦¬
        async with self.gpu_semaphore:
            try:
                # ThreadPoolExecutorë¡œ ë¸”ë¡œí‚¹ ë°©ì§€
                loop = asyncio.get_event_loop()
                
                # partial í•¨ìˆ˜ë¡œ í‚¤ì›Œë“œ ì¸ì ì²˜ë¦¬
                from functools import partial
                encode_func = partial(
                    self.embedding_model.encode,
                    convert_to_numpy=True,
                    normalize_embeddings=True
                )
                
                embedding = await loop.run_in_executor(
                    self.executor,
                    encode_func,
                    text
                )
                
                # ìºì‹œì— ì €ì¥
                self.embedding_cache[text] = embedding
                return embedding
                
            except Exception as e:
                logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                # fallback ì—†ìŒ - ë°”ë¡œ ì˜ˆì™¸ ë°œìƒ
                raise RuntimeError(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}") from e
    
    async def _calculate_importance_score(self, text: str, embedding: np.ndarray, 
                                        metadata: Dict[str, Any]) -> float:
        """ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        # ê¸°ë³¸ ì ìˆ˜
        base_score = 0.5
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜
        length_score = min(1.0, len(text) / 1000)
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì ìˆ˜
        metadata_score = 0.0
        if metadata:
            if metadata.get('urgency', 0) > 0.7:
                metadata_score += 0.3
            if metadata.get('impact', 0) > 0.7:
                metadata_score += 0.3
        
        # ìœ ì¼ì„± ì ìˆ˜ (ê¸°ì¡´ ê²½í—˜ê³¼ì˜ ì°¨ì´)
        uniqueness_score = await self._calculate_uniqueness_score(embedding)
        
        # ì¢…í•© ì ìˆ˜
        final_score = (
            base_score * 0.2 +
            length_score * 0.2 +
            metadata_score * 0.3 +
            uniqueness_score * 0.3
        )
        
        return min(1.0, max(0.0, final_score))
    
    async def _calculate_uniqueness_score(self, embedding: np.ndarray) -> float:
        """ìœ ì¼ì„± ì ìˆ˜ ê³„ì‚°"""
        if not self.experience_vectors:
            return 1.0  # ì²« ë²ˆì§¸ ê²½í—˜
        
        # ê¸°ì¡´ ì„ë² ë”©ë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        existing_embeddings = np.array([
            exp.embedding for exp in self.experience_vectors.values()
        ])
        
        similarities = cosine_similarity([embedding], existing_embeddings)[0]
        max_similarity = np.max(similarities)
        
        # ìœ ì¼ì„± = 1 - ìµœëŒ€ ìœ ì‚¬ë„
        return 1.0 - max_similarity
    
    def _extract_metadata_features(self, metadata: Dict[str, Any]) -> np.ndarray:
        """ë©”íƒ€ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ"""
        # ê¸°ë³¸ íŠ¹ì„± ë²¡í„° (í¬ê¸° 10ìœ¼ë¡œ ê³ ì •)
        features = np.zeros(10)
        
        # ìˆ˜ì¹˜í˜• ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        if 'urgency' in metadata:
            features[0] = float(metadata['urgency'])
        if 'impact' in metadata:
            features[1] = float(metadata['impact'])
        if 'confidence' in metadata:
            features[2] = float(metadata['confidence'])
        
        # ë²”ì£¼í˜• ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ë¡œ ë³€í™˜
        if 'emotion' in metadata:
            emotion_map = {'positive': 1.0, 'negative': -1.0, 'neutral': 0.0}
            features[3] = emotion_map.get(metadata['emotion'], 0.0)
        
        # ì‹œê°„ ê´€ë ¨ íŠ¹ì„±
        if 'time_sensitivity' in metadata:
            features[4] = float(metadata['time_sensitivity'])
        
        return features
    
    async def _vector_search(self, query_embedding: np.ndarray, 
                           max_results: int = 10) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ - subprocess í™˜ê²½ ë¶„ë¦¬ ë°©ì‹ ì‚¬ìš©"""
        if self.vector_index_info is not None and self.vector_index_info['total_vectors'] > 0:
            try:
                # FAISS subprocessë¥¼ í†µí•´ ê²€ìƒ‰
                result = run_faiss_subprocess('search_vectors', {
                    'vectors': self.vectors_memory,  # ê¸°ì¡´ ë²¡í„°ë“¤
                    'query_vectors': [query_embedding.tolist()],
                    'dimension': self.embedding_dim,
                    'index_type': self.vector_index_info['index_type'],
                    'k': max_results
                })
                
                if result['status'] == 'success':
                    search_results = []
                    distances = result['distances'][0]  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ê²°ê³¼
                    indices = result['indices'][0]
                    
                    for i, (distance, idx) in enumerate(zip(distances, indices)):
                        if idx >= 0 and idx < len(self.vector_ids):  # ìœ íš¨í•œ ì¸ë±ìŠ¤
                            similarity = 1.0 / (1.0 + distance)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                            search_results.append({
                                'experience_id': self.vector_ids[idx],
                                'similarity': float(similarity),
                                'relevance_score': float(similarity)
                            })
                    
                    return search_results
                else:
                    logger.warning(f"FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                    # í´ë°±: ë©”ëª¨ë¦¬ ê¸°ë°˜ ê²€ìƒ‰
                    
            except Exception as e:
                logger.warning(f"FAISS subprocess ê²€ìƒ‰ ì˜¤ë¥˜: {e}, ë©”ëª¨ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰")
                # í´ë°±: ë©”ëª¨ë¦¬ ê¸°ë°˜ ê²€ìƒ‰
        
        # í´ë°±: ë©”ëª¨ë¦¬ ê¸°ë°˜ ê²€ìƒ‰
        if not self.vectors_memory:
            return []
        
        similarities = cosine_similarity([query_embedding], self.vectors_memory)[0]
        
        # ìƒìœ„ ê²°ê³¼ ì„ íƒ
        top_indices = np.argsort(similarities)[::-1][:max_results]
        
        results = []
        for idx in top_indices:
            if idx < len(self.vector_ids) and similarities[idx] > 0.1:  # ìµœì†Œ ìœ ì‚¬ë„
                results.append({
                    'experience_id': self.vector_ids[idx],
                    'similarity': float(similarities[idx]),
                    'relevance_score': float(similarities[idx])
                })
        
        return results
    
    async def _apply_metadata_filters(self, results: List[Dict[str, Any]], 
                                    query: ExperienceQuery) -> List[Dict[str, Any]]:
        """ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©"""
        filtered = []
        
        for result in results:
            # ì¹´í…Œê³ ë¦¬ í•„í„°
            if query.category_filter:
                exp_id = result.get('experience_id')
                if exp_id and exp_id in self.experience_vectors:
                    exp_category = self.experience_vectors[exp_id].category
                    if exp_category != query.category_filter:
                        continue
            
            # ì‹œê°„ ë²”ìœ„ í•„í„°
            if query.time_range:
                exp_id = result.get('experience_id')
                if exp_id and exp_id in self.experience_vectors:
                    exp_time = self.experience_vectors[exp_id].timestamp
                    if not (query.time_range[0] <= exp_time <= query.time_range[1]):
                        continue
            
            # ìœ ì‚¬ë„ ì„ê³„ê°’
            if result['similarity'] >= query.similarity_threshold:
                filtered.append(result)
        
        return filtered
    
    def _apply_recency_boost(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ìµœê·¼ì„± ë¶€ìŠ¤íŠ¸ ì ìš©"""
        current_time = datetime.now()
        
        for result in results:
            exp_id = result.get('experience_id')
            if exp_id and exp_id in self.experience_vectors:
                exp_time = self.experience_vectors[exp_id].timestamp
                days_ago = (current_time - exp_time).days
                
                # ìµœê·¼ ê²½í—˜ì¼ìˆ˜ë¡ ë†’ì€ ë¶€ìŠ¤íŠ¸
                recency_boost = max(0.1, 1.0 - (days_ago / 365))  # 1ë…„ ê¸°ì¤€
                result['relevance_score'] *= recency_boost
        
        return results
    
    async def _enrich_results_with_metadata(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        enriched = []
        
        for result in results:
            exp_id = result.get('experience_id')
            if exp_id and exp_id in self.experience_metadata:
                metadata = self.experience_metadata[exp_id]
                result.update({
                    'text': metadata['text'],
                    'category': metadata['category'],
                    'importance_score': metadata['importance_score'],
                    'created_at': metadata['created_at'].isoformat(),
                    'metadata': metadata['metadata']
                })
            
            enriched.append(result)
        
        return enriched
    
    async def _update_access_count(self, experience_id: str):
        """ì ‘ê·¼ íšŸìˆ˜ ì—…ë°ì´íŠ¸"""
        if experience_id in self.experience_vectors:
            self.experience_vectors[experience_id].access_count += 1
            self.experience_vectors[experience_id].last_accessed = datetime.now()
        
        if self.metadata_conn:
            self.metadata_conn.execute('''
                UPDATE experiences 
                SET access_count = access_count + 1, last_accessed = ?
                WHERE id = ?
            ''', (datetime.now(), experience_id))
            self.metadata_conn.commit()
    
    def _generate_query_cache_key(self, query: ExperienceQuery) -> str:
        """ì¿¼ë¦¬ ìºì‹œ í‚¤ ìƒì„±"""
        key_components = [
            query.query_text[:50],  # í…ìŠ¤íŠ¸ ì¼ë¶€
            query.category_filter or "",
            str(query.similarity_threshold),
            str(query.max_results)
        ]
        return hashlib.md5("|".join(key_components).encode()).hexdigest()
    
    async def _update_learning_systems(self):
        """í•™ìŠµ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸"""
        try:
            # íŒ¨í„´ í•™ìŠµ ìˆ˜í–‰
            learning_result = await self.learn_patterns()
            
            # ì´ìƒ íƒì§€ ëª¨ë¸ ì—…ë°ì´íŠ¸
            if len(self.experience_vectors) >= 50:
                embeddings = np.array([exp.embedding for exp in self.experience_vectors.values()])
                self.anomaly_detector.fit(embeddings)
            
            logger.info("í•™ìŠµ ì‹œìŠ¤í…œì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    async def _extract_learned_patterns(self, training_data: List[Dict], 
                                      neural_outputs: Dict) -> List[ExperiencePattern]:
        """í•™ìŠµëœ íŒ¨í„´ ì¶”ì¶œ"""
        patterns = []
        
        try:
            # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë¶„ì„
            attention_weights = neural_outputs['attention_weights'].cpu().numpy()
            
            # ë†’ì€ ì–´í…ì…˜ì„ ë°›ëŠ” ë©”ëª¨ë¦¬ ìŠ¬ë¡¯ ì‹ë³„
            avg_attention = np.mean(attention_weights, axis=0)
            high_attention_slots = np.where(avg_attention > np.percentile(avg_attention, 80))[0]
            
            for slot_idx in high_attention_slots:
                pattern = ExperiencePattern(
                    pattern_id=f"pattern_{slot_idx}",
                    pattern_type="attention_based",
                    description=f"ê³ ì–´í…ì…˜ ë©”ëª¨ë¦¬ ìŠ¬ë¡¯ {slot_idx}",
                    confidence=float(avg_attention[slot_idx]),
                    examples=[item['id'] for item in training_data[:5]],  # ì˜ˆì‹œ
                    created_at=datetime.now()
                )
                patterns.append(pattern)
        
        except Exception as e:
            logger.error(f"íŒ¨í„´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return patterns
    
    async def _analyze_experience_impact(self, experience_id: str) -> Dict[str, Any]:
        """ê²½í—˜ ì˜í–¥ë„ ë¶„ì„"""
        try:
            if experience_id not in self.experience_vectors:
                return {'error': 'ê²½í—˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
            
            experience = self.experience_vectors[experience_id]
            
            # ì§ì ‘ì  ì˜í–¥ (ìœ ì‚¬ ê²½í—˜ë“¤)
            similar_experiences = await self._vector_search(experience.embedding, max_results=10)
            
            # ê°„ì ‘ì  ì˜í–¥ (ì°¸ì¡°ëœ íšŸìˆ˜, í´ëŸ¬ìŠ¤í„° ë‚´ ìœ„ì¹˜ ë“±)
            indirect_impact = {
                'access_frequency': experience.access_count,
                'recency_impact': (datetime.now() - experience.timestamp).days,
                'importance_percentile': 0.0
            }
            
            # ì¤‘ìš”ë„ ë°±ë¶„ìœ„ ê³„ì‚°
            all_importance_scores = [exp.importance_score for exp in self.experience_vectors.values()]
            if all_importance_scores:
                importance_rank = sorted(all_importance_scores, reverse=True).index(experience.importance_score)
                indirect_impact['importance_percentile'] = (1 - importance_rank / len(all_importance_scores)) * 100
            
            return {
                'direct_impact': {
                    'similar_experiences_count': len(similar_experiences),
                    'average_similarity': np.mean([sim['similarity'] for sim in similar_experiences]) if similar_experiences else 0.0
                },
                'indirect_impact': indirect_impact,
                'overall_impact_score': self._calculate_overall_impact_score(experience, similar_experiences, indirect_impact)
            }
            
        except Exception as e:
            logger.error(f"ì˜í–¥ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _calculate_overall_impact_score(self, experience: ExperienceVector, 
                                      similar_experiences: List[Dict], 
                                      indirect_impact: Dict) -> float:
        """ì „ì²´ ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚°"""
        # ê¸°ë³¸ ì¤‘ìš”ë„
        base_score = experience.importance_score
        
        # ìœ ì‚¬ ê²½í—˜ ì˜í–¥
        similarity_score = len(similar_experiences) / 10.0  # ì •ê·œí™”
        
        # ì ‘ê·¼ ë¹ˆë„ ì˜í–¥
        access_score = min(1.0, experience.access_count / 10.0)
        
        # ìµœê·¼ì„± ì˜í–¥ (ìµœê·¼ì¼ìˆ˜ë¡ ë†’ìŒ)
        recency_days = indirect_impact['recency_impact']
        recency_score = max(0.1, 1.0 - (recency_days / 365))
        
        # ê°€ì¤‘ í‰ê· 
        overall_score = (
            base_score * 0.4 +
            similarity_score * 0.2 +
            access_score * 0.2 +
            recency_score * 0.2
        )
        
        return min(1.0, overall_score)
    
    def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.metadata_conn:
            self.metadata_conn.close()
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        if self.vector_index is not None:
            try:
                index_file = os.path.join(self.vector_db_path, 'experience_index.faiss')
                faiss.write_index(self.vector_index, index_file)
            except Exception as e:
                logger.error(f"ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        logger.info("ê³ ê¸‰ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

def create_advanced_experience_database() -> AdvancedExperienceDatabase:
    """ê³ ê¸‰ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"""
    return AdvancedExperienceDatabase()

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import asyncio
    
    async def test_advanced_database():
        """ê³ ê¸‰ ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
        db = create_advanced_experience_database()
        
        print("=== ê³ ê¸‰ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ===\n")
        
        # ê²½í—˜ ì €ì¥ í…ŒìŠ¤íŠ¸
        test_experiences = [
            "ì˜¤ëŠ˜ ì–´ë ¤ìš´ ìœ¤ë¦¬ì  ê²°ì •ì„ ë‚´ë ¤ì•¼ í–ˆë‹¤. ê°œì¸ì˜ ì´ìµê³¼ ê³µê³µì˜ ì´ìµ ì‚¬ì´ì—ì„œ ê³ ë¯¼í–ˆì§€ë§Œ, ê²°êµ­ ê³µê³µì˜ ì´ìµì„ ì„ íƒí–ˆë‹¤.",
            "íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ê°ˆë“±ì´ ë°œìƒí–ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ì˜ê²¬ì„ ì¡°ìœ¨í•˜ëŠ” ê³¼ì •ì—ì„œ ë§ì€ ê²ƒì„ ë°°ì› ë‹¤.",
            "ìƒˆë¡œìš´ ê¸°ìˆ ì„ ë°°ìš°ëŠ” ê³¼ì •ì—ì„œ ì‹¤íŒ¨ë¥¼ ë§ì´ ê²½í—˜í–ˆì§€ë§Œ, ê²°êµ­ ì„±ê³µí•  ìˆ˜ ìˆì—ˆë‹¤.",
            "ë„ë•ì  ë”œë ˆë§ˆ ìƒí™©ì—ì„œ ì›ì¹™ì„ ì§€í‚¤ëŠ” ê²ƒì˜ ì¤‘ìš”ì„±ì„ ê¹¨ë‹¬ì•˜ë‹¤.",
            "í˜‘ë ¥ê³¼ ì†Œí†µì˜ ì¤‘ìš”ì„±ì„ ì‹¤ê°í–ˆë˜ ê²½í—˜ì´ì—ˆë‹¤."
        ]
        
        stored_ids = []
        for i, exp_text in enumerate(test_experiences):
            exp_id = await db.store_experience(
                exp_text,
                metadata={'test_id': i, 'urgency': 0.5 + i * 0.1},
                category='learning' if i % 2 == 0 else 'ethical'
            )
            stored_ids.append(exp_id)
            print(f"ê²½í—˜ ì €ì¥ë¨: {exp_id[:8]}...")
        
        print(f"\nì´ {len(stored_ids)}ê°œ ê²½í—˜ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        query = ExperienceQuery(
            query_text="ìœ¤ë¦¬ì  ê²°ì •ê³¼ ë„ë•ì  ë”œë ˆë§ˆ",
            similarity_threshold=0.3,
            max_results=3
        )
        
        search_results = await db.search_experiences(query)
        print(f"ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
        for result in search_results:
            print(f"- ìœ ì‚¬ë„: {result['relevance_score']:.3f}, í…ìŠ¤íŠ¸: {result['text'][:50]}...")
        
        # ì••ì¶• í…ŒìŠ¤íŠ¸
        print(f"\nê²½í—˜ ì••ì¶• ìˆ˜í–‰...")
        compression_result = await db.compress_experiences()
        print(f"ì••ì¶• ê²°ê³¼: {compression_result}")
        
        # íŒ¨í„´ í•™ìŠµ í…ŒìŠ¤íŠ¸
        print(f"\níŒ¨í„´ í•™ìŠµ ìˆ˜í–‰...")
        learning_result = await db.learn_patterns()
        print(f"í•™ìŠµ ê²°ê³¼: {learning_result}")
        
        # ì¸ì‚¬ì´íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸
        if stored_ids:
            print(f"\nê²½í—˜ ì¸ì‚¬ì´íŠ¸ ë¶„ì„...")
            insights = await db.get_experience_insights(stored_ids[0])
            print(f"ì¸ì‚¬ì´íŠ¸: {insights['basic_info']}")
        
        # ìµœì í™” í…ŒìŠ¤íŠ¸
        print(f"\në°ì´í„°ë² ì´ìŠ¤ ìµœì í™”...")
        optimization_result = await db.optimize_database()
        print(f"ìµœì í™” ê²°ê³¼: {optimization_result}")
        
        # ì¢…ë£Œ
        db.close()
        print(f"\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_advanced_database())