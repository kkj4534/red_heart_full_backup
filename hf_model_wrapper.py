"""
HuggingFace ëª¨ë¸ ë¡œë”© ë˜í¼
ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì™€ í†µí•©í•˜ì—¬ ëª¨ë“  ëª¨ë¸ ë¡œë“œë¥¼ ì¶”ì 
"""

import os
import logging
from typing import Any, Dict, Optional, Union, Callable
from functools import wraps
import torch
from transformers import (
    AutoModel, AutoModelForSequenceClassification, 
    AutoModelForTokenClassification, AutoModelForCausalLM,
    AutoTokenizer, pipeline
)

logger = logging.getLogger(__name__)

class HFModelWrapper:
    """HuggingFace ëª¨ë¸ ë¡œë”©ì„ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì™€ í†µí•©"""
    
    def __init__(self):
        self.memory_manager = None
        self._model_registry = {}  # model_id -> (size_mb, device, owner)
        # ì›ë³¸ í•¨ìˆ˜ë“¤ ì €ì¥
        self._original_from_pretrained = {}
        self._original_pipeline = None
        self._original_tokenizer = None
        self._is_patched = False  # ì´ì¤‘ íŒ¨ì¹˜ ë°©ì§€
        
    def set_memory_manager(self, memory_manager):
        """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì„¤ì •"""
        self.memory_manager = memory_manager
        
    def _merge_owner(self, owner: Optional[str], kwargs: Dict) -> str:
        """owner ì¸ì ì¶©ëŒ ë°©ì§€ - kwargsì—ì„œ owner ì¶”ì¶œ ë° ë³‘í•©"""
        if 'owner' in kwargs:
            kw_owner = kwargs.pop('owner')
            if owner and kw_owner != owner:
                logger.warning(f"[HFWrapper] owner conflict: arg='{owner}' vs kwargs='{kw_owner}'. "
                             f"Using kwargs value.")
            owner = kw_owner
        return owner or "unknown"
        
    def _estimate_model_size(self, model: torch.nn.Module) -> float:
        """ëª¨ë¸ í¬ê¸° ì¶”ì • (MB) - dtype ì¸ì‹"""
        total_bytes = 0
        requires_grad_params = 0
        
        for p in model.parameters():
            # dtypeë³„ ë°”ì´íŠ¸ í¬ê¸° ê³„ì‚°
            bytes_per_param = p.element_size()  # dtype í¬ê¸° ìë™ ë°˜ì˜
            param_bytes = p.numel() * bytes_per_param
            total_bytes += param_bytes
            
            # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³µê°„ë„ í•„ìš”
            if p.requires_grad:
                total_bytes += param_bytes  # ê·¸ë˜ë””ì–¸íŠ¸ ê³µê°„
                requires_grad_params += p.numel()
        
        # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ì¶”ê°€ (í•™ìŠµ ëª¨ë“œì¸ ê²½ìš°)
        if requires_grad_params > 0:
            # Adam ì˜µí‹°ë§ˆì´ì € ê¸°ì¤€: momentum + variance
            total_bytes += requires_grad_params * 4 * 2  # float32 ê¸°ì¤€
        
        size_mb = total_bytes / (1024 * 1024)
        logger.debug(f"ëª¨ë¸ í¬ê¸° ì¶”ì •: {size_mb:.1f}MB (í•™ìŠµê°€ëŠ¥: {requires_grad_params > 0})")
        return size_mb
        
    def _estimate_model_size_predicted(self, model_class_or_task: Any, kwargs: Dict) -> float:
        """ëª¨ë¸ ë¡œë“œ ì „ í¬ê¸° ì˜ˆì¸¡ (MB)"""
        # íƒœìŠ¤í¬/ëª¨ë¸ë³„ ê¸°ë³¸ í¬ê¸° ì¶”ì •
        size_estimates = {
            # íƒœìŠ¤í¬ë³„
            'sentiment-analysis': 500,
            'text-classification': 500,
            'token-classification': 600,
            'question-answering': 800,
            'text-generation': 1000,
            'translation': 1200,
            # ëª¨ë¸ í´ë˜ìŠ¤ë³„
            'AutoModelForSequenceClassification': 500,
            'AutoModelForTokenClassification': 600,
            'AutoModelForCausalLM': 1000,
            'AutoModel': 400,
        }
        
        # ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ í¬ê¸° ì¶”ì •
        model_name = kwargs.get('model_name_or_path', '') or kwargs.get('model', '')
        if 'large' in model_name.lower():
            multiplier = 2.0
        elif 'base' in model_name.lower():
            multiplier = 1.0
        elif 'small' in model_name.lower() or 'tiny' in model_name.lower():
            multiplier = 0.5
        else:
            multiplier = 1.0
            
        # í´ë˜ìŠ¤ ë˜ëŠ” íƒœìŠ¤í¬ë¡œ ê¸°ë³¸ í¬ê¸° ê²°ì •
        if hasattr(model_class_or_task, '__name__'):
            base_size = size_estimates.get(model_class_or_task.__name__, 500)
        else:
            base_size = size_estimates.get(str(model_class_or_task), 500)
            
        # dtype ê³ ë ¤
        torch_dtype = kwargs.get('torch_dtype', None)
        if torch_dtype == torch.float16:
            dtype_multiplier = 0.5
        else:
            dtype_multiplier = 1.0
            
        estimated_mb = base_size * multiplier * dtype_multiplier
        
        # í•™ìŠµ ëª¨ë“œ ê³ ë ¤
        if kwargs.get('requires_grad', True):
            estimated_mb *= 2  # ê·¸ë˜ë””ì–¸íŠ¸ ê³µê°„
            
        return estimated_mb
        
    async def _request_memory_async(self, module_name: str, required_mb: float, deps: list = None):
        """ë¹„ë™ê¸° ë©”ëª¨ë¦¬ ìš”ì²­"""
        if self.memory_manager and hasattr(self.memory_manager, 'request_gpu'):
            try:
                success = await self.memory_manager.request_gpu(
                    module_name=module_name,
                    required_mb=required_mb,
                    deps=deps or [],
                    target_util=0.85
                )
                if not success:
                    logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìš”ì²­ ì‹¤íŒ¨: {module_name} ({required_mb:.1f}MB)")
                return success
            except Exception as e:
                logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
                return False
        return True
        
    def _register_model(self, model_id: str, model: Any, owner: str, device: torch.device, force_cpu_init: bool = False):
        """ëª¨ë¸ì„ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— ë“±ë¡"""
        if isinstance(model, torch.nn.Module):
            size_mb = self._estimate_model_size(model)
        else:
            # pipeline ë“±ì˜ ê²½ìš° ë‚´ë¶€ ëª¨ë¸ í™•ì¸
            if hasattr(model, 'model'):
                size_mb = self._estimate_model_size(model.model)
            else:
                size_mb = 500  # ê¸°ë³¸ê°’
                
        self._model_registry[model_id] = {
            'size_mb': size_mb,
            'device': str(device),
            'owner': owner,
            'force_cpu_init': force_cpu_init  # FORCE_CPU_INIT ëª¨ë“œ ì¶”ì 
        }
        
        # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— ë“±ë¡
        if self.memory_manager:
            from workflow_aware_memory_manager import WorkflowAwareMemoryManager
            if isinstance(self.memory_manager, WorkflowAwareMemoryManager):
                # GPU ëª¨ë¸ì¸ ê²½ìš° ì†Œìœ ê¶Œ íƒœê¹…
                if device.type == 'cuda':
                    if hasattr(self.memory_manager, '_gpu_models'):
                        self.memory_manager._gpu_models[model_id] = {
                            'model': model,
                            'size_mb': size_mb,
                            'owner': owner,
                            'last_used': 0
                        }
                        logger.info(f"âœ… ëª¨ë¸ '{model_id}' ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— ë“±ë¡ë¨ (ì†Œìœ ì: {owner}, í¬ê¸°: {size_mb:.1f}MB)")
        
        return model
        
    def _register_tokenizer(self, tokenizer_id: str, tokenizer: Any, owner: str):
        """í† í¬ë‚˜ì´ì €ë¥¼ CPU ì „ìš©ìœ¼ë¡œ ë“±ë¡"""
        # í† í¬ë‚˜ì´ì €ëŠ” GPU ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        self._model_registry[tokenizer_id] = {
            'size_mb': 0,  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ì—†ìŒ
            'device': 'cpu',
            'owner': owner,
            'type': 'tokenizer'  # íƒ€ì… êµ¬ë¶„
        }
        
        logger.info(f"âœ… í† í¬ë‚˜ì´ì € '{tokenizer_id}' CPU ì „ìš©ìœ¼ë¡œ ë“±ë¡ë¨ (ì†Œìœ ì: {owner})")
        
        return tokenizer
        
    def wrapped_from_pretrained(self, model_class, model_name: str, 
                               *, owner: Optional[str] = None, **kwargs) -> Any:
        """AutoModel.from_pretrained ë˜í¼"""
        # TokenizerëŠ” ì›ë³¸ í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        if 'Tokenizer' in model_class.__name__:
            original_fn = self._original_from_pretrained.get(model_class.__name__)
            if original_fn:
                return original_fn(model_name, **kwargs)
            else:
                # ì›ë³¸ í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ ì§ì ‘ í˜¸ì¶œ
                return model_class.from_pretrained(model_name, **kwargs)
        
        # owner ì¶©ëŒ ë°©ì§€
        owner = self._merge_owner(owner, kwargs)
        
        # ì´ì¤‘ ì•ˆì „ì¥ì¹˜
        if 'owner' in kwargs:
            logger.warning("[HFWrapper] 'owner' remained in kwargs after merge. Forcing pop.")
            kwargs.pop('owner', None)
        
        logger.info(f"ğŸ”„ HF ëª¨ë¸ ë¡œë”© ì¤‘: {model_name} (ì†Œìœ ì: {owner})")
        
        # CPU ì „ìš© owner í™•ì¸
        CPU_ONLY_OWNERS = {"translator"}
        
        # FORCE_CPU_INIT í™˜ê²½ë³€ìˆ˜ ì²´í¬
        force_cpu_init = os.environ.get('FORCE_CPU_INIT', '0') == '1'
        
        # ë””ë°”ì´ìŠ¤ í™•ì¸
        device_map = kwargs.get('device_map', None)
        if owner in CPU_ONLY_OWNERS or force_cpu_init:
            device = torch.device('cpu')  # CPU ì „ìš© owner ë˜ëŠ” FORCE_CPU_INIT ëª¨ë“œ
            if force_cpu_init:
                logger.debug(f"FORCE_CPU_INIT ëª¨ë“œ â†’ device=cpu ê°•ì œ (owner: {owner})")
            else:
                logger.debug(f"CPU ì „ìš© owner({owner}) â†’ device=cpu ê°•ì œ")
        elif device_map == "cpu":
            device = torch.device('cpu')
        elif device_map is None:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            device = torch.device('cuda')  # device_mapì´ ìˆìœ¼ë©´ ë³´í†µ GPU
            
        # ëª¨ë¸ ID ë¯¸ë¦¬ ìƒì„±
        model_id = f"{owner}_{model_name.split('/')[-1]}"
        
        # ì‚¬ì „ ë©”ëª¨ë¦¬ ìš”ì²­ (GPU ëª¨ë¸ì¸ ê²½ìš° + CPU ì „ìš© owner ì œì™¸ + FORCE_CPU_INIT ì œì™¸)
        if device.type == 'cuda' and self.memory_manager and owner not in CPU_ONLY_OWNERS and not force_cpu_init:
            estimated_mb = self._estimate_model_size_predicted(model_class, kwargs)
            logger.info(f"ğŸ“Š ëª¨ë¸ ë¡œë“œ ì „ ë©”ëª¨ë¦¬ ìš”ì²­: {model_id} ({estimated_mb:.1f}MB)")
            
            # ë™ê¸°ì  ë©”ëª¨ë¦¬ ìš”ì²­ - í•„ìˆ˜ ëª¨ë“ˆ íŒë‹¨
            is_required = owner in ['emotion_analyzer', 'core_backbone', 'unified_backbone', 'bentham_calculator']
            
            # request_gpu_blocking í•„ìˆ˜ ì‚¬ìš© (STRICT_NO_FALLBACK)
            if not hasattr(self.memory_manager, 'request_gpu_blocking'):
                raise RuntimeError("WorkflowAwareMemoryManagerì— request_gpu_blockingì´ êµ¬í˜„ë˜ì§€ ì•ŠìŒ")
                
            # ë™ê¸° ë©”ëª¨ë¦¬ ìš”ì²­ ì‹¤í–‰
            success = self.memory_manager.request_gpu_blocking(
                module_name=model_id,
                required_mb=estimated_mb,
                deps=kwargs.get('deps', []),
                target_util=0.85,
                timeout=30.0,
                is_required=is_required
            )
            
            # NO FALLBACK - ë©”ëª¨ë¦¬ í™•ë³´ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì˜ˆì™¸ ë°œìƒ
            if not success:
                raise RuntimeError(f"[GPU] ë©”ëª¨ë¦¬ í™•ë³´ ì‹¤íŒ¨: {model_id} (required={is_required})")
            
        # device íŒŒë¼ë¯¸í„° ì œê±° (from_pretrainedì€ deviceë¥¼ ë°›ì§€ ì•ŠìŒ)
        load_kwargs = kwargs.copy()
        if 'device' in load_kwargs:
            del load_kwargs['device']
        
        # ëª¨ë¸ ë¡œë“œ - ì›ë³¸ í•¨ìˆ˜ ì‚¬ìš©
        original_fn = self._original_from_pretrained.get(model_class.__name__)
        if original_fn:
            model = original_fn(model_name, **load_kwargs)
        else:
            # í´ë°± - ê²½ê³  ë¡œê·¸ì™€ í•¨ê»˜
            logger.warning(f"âš ï¸ ì›ë³¸ í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_class.__name__}. ì§ì ‘ í˜¸ì¶œ ì‚¬ìš©.")
            model = model_class.from_pretrained(model_name, **load_kwargs)
        
        # í•„ìš”ì‹œ deviceë¡œ ì´ë™ (TokenizerëŠ” ì œì™¸)
        if device.type != 'cpu' and hasattr(model, 'to'):
            model = model.to(device)
        
        # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— ë“±ë¡ (ì‹¤ì œ í¬ê¸°ë¡œ ì—…ë°ì´íŠ¸)
        self._register_model(model_id, model, owner, device, force_cpu_init)
        
        return model
        
    def wrapped_pipeline(self, task: str, model: str = None, 
                        *, owner: Optional[str] = None, **kwargs) -> Any:
        """pipeline ë˜í¼"""
        # owner ì¶©ëŒ ë°©ì§€
        owner = self._merge_owner(owner, kwargs)
        
        # ì´ì¤‘ ì•ˆì „ì¥ì¹˜
        if 'owner' in kwargs:
            logger.warning("[HFWrapper] 'owner' remained in kwargs after merge. Forcing pop.")
            kwargs.pop('owner', None)
        
        logger.info(f"ğŸ”„ HF íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘: {task} (ëª¨ë¸: {model}, ì†Œìœ ì: {owner})")
        
        # ë””ë°”ì´ìŠ¤ í™•ì¸
        device_num = kwargs.get('device', -1)
        if device_num >= 0:
            device = torch.device(f'cuda:{device_num}')
        else:
            device = torch.device('cpu')
            
        # íŒŒì´í”„ë¼ì¸ ID ë¯¸ë¦¬ ìƒì„±
        model_name = model if model else f"{task}_default"
        pipe_id = f"{owner}_pipeline_{model_name.split('/')[-1]}"
        
        # ì‚¬ì „ ë©”ëª¨ë¦¬ ìš”ì²­ (GPU íŒŒì´í”„ë¼ì¸ì¸ ê²½ìš°)
        if device.type == 'cuda' and self.memory_manager:
            estimated_mb = self._estimate_model_size_predicted(task, kwargs)
            logger.info(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì „ ë©”ëª¨ë¦¬ ìš”ì²­: {pipe_id} ({estimated_mb:.1f}MB)")
            
            # íŒŒì´í”„ë¼ì¸ì€ ëŒ€ë¶€ë¶„ ì„ íƒì ì´ì§€ë§Œ ì¼ë¶€ëŠ” í•„ìˆ˜
            is_required = owner in ['core_nli', 'semantic_search']
            
            # request_gpu_blocking í•„ìˆ˜ ì‚¬ìš© (STRICT_NO_FALLBACK)
            if not hasattr(self.memory_manager, 'request_gpu_blocking'):
                raise RuntimeError("WorkflowAwareMemoryManagerì— request_gpu_blockingì´ êµ¬í˜„ë˜ì§€ ì•ŠìŒ")
                
            # ë™ê¸° ë©”ëª¨ë¦¬ ìš”ì²­ ì‹¤í–‰
            success = self.memory_manager.request_gpu_blocking(
                module_name=pipe_id,
                required_mb=estimated_mb,
                deps=kwargs.get('deps', []),
                target_util=0.85,
                timeout=30.0,
                is_required=is_required
            )
            
            # NO FALLBACK - ë©”ëª¨ë¦¬ í™•ë³´ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì˜ˆì™¸ ë°œìƒ
            if not success:
                raise RuntimeError(f"[GPU] ë©”ëª¨ë¦¬ í™•ë³´ ì‹¤íŒ¨: {pipe_id} (required={is_required})")
            
        # íŒŒì´í”„ë¼ì¸ ìƒì„± - ì›ë³¸ í•¨ìˆ˜ ì‚¬ìš©
        if self._original_pipeline:
            pipe = self._original_pipeline(task, model=model, **kwargs)
        else:
            # í´ë°± - ê²½ê³  ë¡œê·¸ì™€ í•¨ê»˜
            logger.warning("âš ï¸ ì›ë³¸ pipeline í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ì ‘ í˜¸ì¶œ ì‚¬ìš©.")
            pipe = pipeline(task, model=model, **kwargs)
        
        # ë©”ëª¨ë¦¬ ë§¤ë‹ˆì €ì— ë“±ë¡ (ì‹¤ì œ í¬ê¸°ë¡œ ì—…ë°ì´íŠ¸)
        self._register_model(pipe_id, pipe, owner, device)
        
        return pipe
        
    def wrapped_tokenizer(self, model_name: str, *, owner: Optional[str] = None, **kwargs) -> Any:
        """AutoTokenizer.from_pretrained ë˜í¼"""
        # owner ì¶©ëŒ ë°©ì§€
        owner = self._merge_owner(owner, kwargs)
        
        # ì´ì¤‘ ì•ˆì „ì¥ì¹˜ - merge í›„ì—ë„ kwargsì— ownerê°€ ë‚¨ì•„ìˆìœ¼ë©´ ì œê±°
        if 'owner' in kwargs:
            logger.warning("[HFWrapper] 'owner' remained in kwargs after merge. Forcing pop.")
            kwargs.pop('owner', None)
        
        logger.info(f"ğŸ”„ HF í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘: {model_name} (ì†Œìœ ì: {owner})")
        
        # í† í¬ë‚˜ì´ì €ëŠ” í•­ìƒ CPUì—ì„œ ë™ì‘
        device = torch.device('cpu')
        
        # í† í¬ë‚˜ì´ì € ID ìƒì„±
        last_segment = model_name.split('/')[-1]
        tokenizer_id = f"{owner}_tokenizer_{last_segment}"
        
        # í† í¬ë‚˜ì´ì €ëŠ” GPU ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë©”ëª¨ë¦¬ ìš”ì²­ ì—†ìŒ
        logger.debug(f"í† í¬ë‚˜ì´ì €ëŠ” CPU ì „ìš©, GPU ë©”ëª¨ë¦¬ ìš”ì²­ ìŠ¤í‚µ: {tokenizer_id}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ - ì›ë³¸ í•¨ìˆ˜ ì§ì ‘ ì‚¬ìš© (íŒ¨ì¹˜ ìš°íšŒ)
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name, **kwargs)
        
        # CPU ì „ìš© ë©”íƒ€ë°ì´í„°ë¡œ ë“±ë¡
        self._register_tokenizer(tokenizer_id, tokenizer, owner)
        
        return tokenizer
        
    def get_model_info(self, model_id: str) -> Optional[Dict[str, Any]]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        return self._model_registry.get(model_id)
        
    def get_total_gpu_usage(self) -> float:
        """ì „ì²´ GPU ì‚¬ìš©ëŸ‰ ê³„ì‚° (MB) - í† í¬ë‚˜ì´ì € ì œì™¸"""
        total_mb = 0
        for model_id, info in self._model_registry.items():
            # GPU ëª¨ë¸ì´ê³  í† í¬ë‚˜ì´ì €ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ê³„ì‚°
            if info['device'].startswith('cuda') and info.get('type') != 'tokenizer':
                total_mb += info['size_mb']
        return total_mb
        
    def list_models_by_owner(self, owner: str) -> Dict[str, Dict[str, Any]]:
        """íŠ¹ì • ì†Œìœ ìì˜ ëª¨ë“  ëª¨ë¸ ì¡°íšŒ"""
        return {
            model_id: info 
            for model_id, info in self._model_registry.items() 
            if info['owner'] == owner
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_hf_wrapper = HFModelWrapper()

def get_hf_wrapper() -> HFModelWrapper:
    """ì „ì—­ HF ë˜í¼ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return _hf_wrapper

# í¸ì˜ í•¨ìˆ˜ë“¤
def wrapped_auto_model(model_name: str, owner: str = "unknown", **kwargs):
    """AutoModel.from_pretrained í¸ì˜ í•¨ìˆ˜"""
    # ë§Œì•½ í˜¸ì¶œ ì¸¡ì—ì„œ kwargsì— ownerë¥¼ ì‹¤ìˆ˜ë¡œ ë„£ì—ˆìœ¼ë©´ ì œê±°í•´ ì¼ê´€í™”
    owner = kwargs.pop("owner", owner)
    return _hf_wrapper.wrapped_from_pretrained(
        AutoModel, model_name, owner=owner, **kwargs
    )

def wrapped_auto_model_for_sequence_classification(model_name: str, owner: str = "unknown", **kwargs):
    """AutoModelForSequenceClassification.from_pretrained í¸ì˜ í•¨ìˆ˜"""
    # ë§Œì•½ í˜¸ì¶œ ì¸¡ì—ì„œ kwargsì— ownerë¥¼ ì‹¤ìˆ˜ë¡œ ë„£ì—ˆìœ¼ë©´ ì œê±°í•´ ì¼ê´€í™”
    owner = kwargs.pop("owner", owner)
    return _hf_wrapper.wrapped_from_pretrained(
        AutoModelForSequenceClassification, model_name, owner=owner, **kwargs
    )

def wrapped_pipeline(task: str, model: str = None, owner: str = "unknown", **kwargs):
    """pipeline í¸ì˜ í•¨ìˆ˜"""
    # ë§Œì•½ í˜¸ì¶œ ì¸¡ì—ì„œ kwargsì— ownerë¥¼ ì‹¤ìˆ˜ë¡œ ë„£ì—ˆìœ¼ë©´ ì œê±°í•´ ì¼ê´€í™”
    owner = kwargs.pop("owner", owner)
    return _hf_wrapper.wrapped_pipeline(task, model=model, owner=owner, **kwargs)

def wrapped_tokenizer(model_name: str, owner: str = "unknown", **kwargs):
    """AutoTokenizer.from_pretrained í¸ì˜ í•¨ìˆ˜"""
    # ë§Œì•½ í˜¸ì¶œ ì¸¡ì—ì„œ kwargsì— ownerë¥¼ ì‹¤ìˆ˜ë¡œ ë„£ì—ˆìœ¼ë©´ ì œê±°í•´ ì¼ê´€í™”
    owner = kwargs.pop("owner", owner)
    return _hf_wrapper.wrapped_tokenizer(model_name, owner=owner, **kwargs)

def wrapped_from_pretrained(model_class, model_name: str, owner: str = "unknown", **kwargs):
    """ë²”ìš© from_pretrained í¸ì˜ í•¨ìˆ˜"""
    # ë§Œì•½ í˜¸ì¶œ ì¸¡ì—ì„œ kwargsì— ownerë¥¼ ì‹¤ìˆ˜ë¡œ ë„£ì—ˆìœ¼ë©´ ì œê±°í•´ ì¼ê´€í™”
    owner = kwargs.pop("owner", owner)
    return _hf_wrapper.wrapped_from_pretrained(model_class, model_name, owner=owner, **kwargs)

# ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ monkey patching (ì„ íƒì )
def enable_auto_registration():
    """ìë™ ë“±ë¡ í™œì„±í™” - ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ì—†ì´ ì‚¬ìš©"""
    import transformers
    from transformers import AutoTokenizer, AutoProcessor, AutoImageProcessor
    
    # ì´ì¤‘ íŒ¨ì¹˜ ë°©ì§€
    if _hf_wrapper._is_patched:
        logger.info("âš ï¸ HF ëª¨ë¸ ìë™ ë“±ë¡ì´ ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return
    
    # ì›ë³¸ í•¨ìˆ˜ ë°±ì—…ì„ _hf_wrapper ì¸ìŠ¤í„´ìŠ¤ì— ì €ì¥
    # Marianê³¼ Electra import (ê²¬ê³ í•œ ê²½ë¡œ)
    try:
        from transformers import MarianMTModel, MarianTokenizer
    except ImportError:
        logger.warning("transformersì—ì„œ ì§ì ‘ import ì‹¤íŒ¨, í•˜ìœ„ ëª¨ë“ˆì—ì„œ ì‹œë„")
        try:
            from transformers.models.marian import MarianMTModel, MarianTokenizer
        except ImportError as e:
            logger.error(f"MarianMTModel/MarianTokenizer import ì‹¤íŒ¨: {e}")
            MarianMTModel = None
            MarianTokenizer = None
    
    try:
        from transformers import ElectraForSequenceClassification
    except ImportError:
        try:
            from transformers.models.electra import ElectraForSequenceClassification
        except ImportError as e:
            logger.error(f"ElectraForSequenceClassification import ì‹¤íŒ¨: {e}")
            ElectraForSequenceClassification = None
    _hf_wrapper._original_from_pretrained = {
        'AutoModel': AutoModel.from_pretrained,
        'AutoModelForSequenceClassification': AutoModelForSequenceClassification.from_pretrained,
        'AutoModelForTokenClassification': AutoModelForTokenClassification.from_pretrained,
        'AutoModelForCausalLM': AutoModelForCausalLM.from_pretrained,
        'AutoTokenizer': AutoTokenizer.from_pretrained,
        'AutoProcessor': AutoProcessor.from_pretrained if hasattr(transformers, 'AutoProcessor') else None,
        'AutoImageProcessor': AutoImageProcessor.from_pretrained if hasattr(transformers, 'AutoImageProcessor') else None,
        'MarianMTModel': MarianMTModel.from_pretrained if MarianMTModel else None,
        'MarianTokenizer': MarianTokenizer.from_pretrained if MarianTokenizer else None,
        'ElectraForSequenceClassification': ElectraForSequenceClassification.from_pretrained if ElectraForSequenceClassification else None
    }
    _hf_wrapper._original_pipeline = transformers.pipeline
    _hf_wrapper._original_tokenizer = AutoTokenizer.from_pretrained
    
    # ë˜í¼ë¡œ êµì²´
    def patched_from_pretrained(cls, *args, **kwargs):
        # í˜¸ì¶œì ì •ë³´ ì¶”ì¶œ
        import inspect
        frame = inspect.currentframe()
        caller_frame = frame.f_back
        owner = caller_frame.f_globals.get('__name__', 'unknown')
        
        return _hf_wrapper.wrapped_from_pretrained(cls, *args, owner=owner, **kwargs)
    
    def patched_pipeline(*args, **kwargs):
        # í˜¸ì¶œì ì •ë³´ ì¶”ì¶œ
        import inspect
        frame = inspect.currentframe()
        caller_frame = frame.f_back
        owner = caller_frame.f_globals.get('__name__', 'unknown')
        
        return _hf_wrapper.wrapped_pipeline(*args, owner=owner, **kwargs)
    
    # ì´ì¤‘ íŒ¨ì¹˜ ë°©ì§€ë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
    def patch_if_needed(cls, class_name):
        """ì´ì¤‘ íŒ¨ì¹˜ ë°©ì§€í•˜ë©° from_pretrained íŒ¨ì¹˜"""
        if hasattr(cls, 'from_pretrained'):
            current_method = getattr(cls.from_pretrained, '__name__', '')
            if current_method != 'patched_from_pretrained':
                cls.from_pretrained = classmethod(patched_from_pretrained)
                logger.debug(f"âœ… {class_name}.from_pretrained íŒ¨ì¹˜ë¨")
            else:
                logger.debug(f"âš ï¸ {class_name}.from_pretrained ì´ë¯¸ íŒ¨ì¹˜ë¨, ìŠ¤í‚µ")
    
    # Monkey patching - Auto í´ë˜ìŠ¤ë“¤ (í† í¬ë‚˜ì´ì €ëŠ” ì œì™¸)
    patch_if_needed(AutoModel, 'AutoModel')
    patch_if_needed(AutoModelForSequenceClassification, 'AutoModelForSequenceClassification')
    patch_if_needed(AutoModelForTokenClassification, 'AutoModelForTokenClassification')
    patch_if_needed(AutoModelForCausalLM, 'AutoModelForCausalLM')
    # AutoTokenizerëŠ” íŒ¨ì¹˜í•˜ì§€ ì•ŠìŒ - ë¬´í•œ ì¬ê·€ ë°©ì§€
    # patch_if_needed(AutoTokenizer, 'AutoTokenizer')
    
    if hasattr(transformers, 'AutoProcessor'):
        patch_if_needed(AutoProcessor, 'AutoProcessor')
    if hasattr(transformers, 'AutoImageProcessor'):
        patch_if_needed(AutoImageProcessor, 'AutoImageProcessor')
    
    # Marian ëª¨ë¸ë§Œ íŒ¨ì¹˜, í† í¬ë‚˜ì´ì €ëŠ” ì œì™¸
    if MarianMTModel:
        patch_if_needed(MarianMTModel, 'MarianMTModel')
    # MarianTokenizerëŠ” íŒ¨ì¹˜í•˜ì§€ ì•ŠìŒ - ë¬´í•œ ì¬ê·€ ë°©ì§€
    # if MarianTokenizer:
    #     patch_if_needed(MarianTokenizer, 'MarianTokenizer')
    if ElectraForSequenceClassification:
        patch_if_needed(ElectraForSequenceClassification, 'ElectraForSequenceClassification')
    
    transformers.pipeline = patched_pipeline
    
    # íŒ¨ì¹˜ ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
    _hf_wrapper._is_patched = True
    
    logger.info("âœ… HF ëª¨ë¸ ìë™ ë“±ë¡ í™œì„±í™”ë¨ (ëª¨ë¸, í† í¬ë‚˜ì´ì €, í”„ë¡œì„¸ì„œ í¬í•¨)")