#!/usr/bin/env python3
"""
Claude API ìºì‹± ìµœì í™” ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ
- í”„ë¡¬í”„íŠ¸ ìºì‹±ìœ¼ë¡œ 90% ë¹„ìš© ì ˆê°
- í†µí•© API í˜¸ì¶œë¡œ íšŸìˆ˜ ìµœì†Œí™”
"""

import os
import json
import time
import logging
import asyncio
import pickle
import re
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from pathlib import Path
from datetime import datetime
import anthropic
from anthropic import Anthropic, RateLimitError, APIError, AuthenticationError

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('Claude.CachedPreprocessor')

@dataclass
class ProcessingStats:
    """ì²˜ë¦¬ í†µê³„"""
    total_samples: int = 0
    processed: int = 0
    failed: int = 0
    
    # í† í° ì¶”ì  (ìºì‹± êµ¬ë¶„)
    cache_write_tokens: int = 0  # ì²« ìºì‹œ ì“°ê¸°
    cache_read_tokens: int = 0   # ìºì‹œ ì½ê¸°
    regular_tokens: int = 0      # ì¼ë°˜ í† í°
    output_tokens: int = 0       # ì¶œë ¥ í† í°
    
    start_time: Optional[datetime] = None
    credit_exhausted: bool = False
    
    def get_cost_estimate(self) -> float:
        """ìºì‹± ì ìš© ë¹„ìš© ê³„ì‚°"""
        # ìºì‹œ ì“°ê¸°: $3.75/1M
        cache_write_cost = (self.cache_write_tokens / 1_000_000) * 3.75
        # ìºì‹œ ì½ê¸°: $0.30/1M (90% ì ˆê°!)
        cache_read_cost = (self.cache_read_tokens / 1_000_000) * 0.30
        # ì¼ë°˜ ì…ë ¥: $3/1M
        regular_cost = (self.regular_tokens / 1_000_000) * 3.0
        # ì¶œë ¥: $15/1M
        output_cost = (self.output_tokens / 1_000_000) * 15.0
        
        return cache_write_cost + cache_read_cost + regular_cost + output_cost
    
    def get_cost_krw(self) -> float:
        return self.get_cost_estimate() * 1320

class ClaudeCachedPreprocessor:
    """ìºì‹± ìµœì í™” ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # API í‚¤ ë¡œë“œ
        api_key_path = Path("api_key.json")
        if not api_key_path.exists():
            raise ValueError("api_key.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        with open(api_key_path, 'r') as f:
            api_config = json.load(f)
        
        self.api_key = api_config.get('anthropic_api_key')
        if not self.api_key or self.api_key == "YOUR_ANTHROPIC_API_KEY_HERE":
            raise ValueError("api_key.jsonì— ì‹¤ì œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        # Claude í´ë¼ì´ì–¸íŠ¸ (ìºì‹± ë² íƒ€ í—¤ë” í¬í•¨)
        self.client = Anthropic(
            api_key=self.api_key,
            default_headers={
                "anthropic-beta": "prompt-caching-2024-07-31"
            }
        )
        
        # Rate limit (Tier 2: 1000/min ê°€ëŠ¥)
        self.rate_limit_delay = 0.8  # ì•ˆì „í•˜ê²Œ ë¶„ë‹¹ 75ê°œ
        self.last_request_time = 0
        
        # í†µê³„
        self.stats = ProcessingStats()
        
        # ì²´í¬í¬ì¸íŠ¸
        self.checkpoint_path = Path("checkpoint_cached.pkl")
        self.processed_ids = set()
        
        # í†µí•© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìºì‹± ëŒ€ìƒ)
        self.system_prompt = self._create_unified_prompt()
    
    def _create_unified_prompt(self) -> str:
        """í†µí•© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return """You are an advanced AI system for comprehensive text analysis.

For each text, analyze ALL of the following aspects and return a single JSON object:

1. **emotions**: Array of 7 floats (0.0-1.0) for [Joy, Trust, Fear, Surprise, Sadness, Disgust, Anger]
   - Values should sum approximately to 1.0
   - Multiple emotions can coexist
   - Be nuanced - avoid extreme values unless certain

2. **regret_factor**: Single float (0.0-1.0) for regret level
   - Consider guilt, remorse, disappointment with past actions
   - 0.0 = no regret, 1.0 = extreme regret

3. **bentham_scores**: Object with 4 aspects of hedonic calculus (each 0.0-1.0)
   - intensity: strength of happiness/pleasure
   - duration: how long-lasting
   - certainty: how likely/certain
   - propinquity: how close in time

4. **surd_metrics**: Object with 4 decision-making aspects (each 0.0-1.0)
   - selection: amount of choice/agency
   - uncertainty: level of unknown factors
   - risk: potential negative consequences
   - decision: importance of decision

Return ONLY a valid JSON object with this exact structure:
{
  "emotions": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
  "regret_factor": 0.0,
  "bentham_scores": {
    "intensity": 0.0,
    "duration": 0.0,
    "certainty": 0.0,
    "propinquity": 0.0
  },
  "surd_metrics": {
    "selection": 0.0,
    "uncertainty": 0.0,
    "risk": 0.0,
    "decision": 0.0
  }
}

Important: Return ONLY the JSON object, no explanations or additional text."""
    
    def load_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if self.checkpoint_path.exists():
            with open(self.checkpoint_path, 'rb') as f:
                checkpoint = pickle.load(f)
                self.processed_ids = checkpoint.get('processed_ids', set())
                self.stats = checkpoint.get('stats', ProcessingStats())
                logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {len(self.processed_ids)}ê°œ ì™„ë£Œ")
                
                if self.stats.credit_exhausted:
                    logger.warning("âš ï¸ ì´ì „ ì„¸ì…˜ì—ì„œ í¬ë ˆë”§ ì†Œì§„ìœ¼ë¡œ ì¤‘ë‹¨ë¨")
                    response = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
                    if response.lower() != 'y':
                        raise SystemExit("ì‚¬ìš©ìê°€ ì¤‘ë‹¨í•¨")
                    self.stats.credit_exhausted = False
    
    def save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (IDë§Œ)"""
        checkpoint = {
            'processed_ids': self.processed_ids,
            'stats': self.stats,
            'timestamp': datetime.now()
        }
        with open(self.checkpoint_path, 'wb') as f:
            pickle.dump(checkpoint, f)
    
    async def wait_for_rate_limit(self):
        """Rate limit ëŒ€ê¸°"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.rate_limit_delay:
            wait_time = self.rate_limit_delay - time_since_last
            await asyncio.sleep(wait_time)
        
        self.last_request_time = time.time()
    
    async def analyze_unified(self, text: str, is_first: bool = False) -> Optional[Dict[str, Any]]:
        """í†µí•© ë¶„ì„ (ëª¨ë“  ë©”íŠ¸ë¦­ í•œë²ˆì— + ìºì‹±)"""
        await self.wait_for_rate_limit()
        
        try:
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì ˆì•½)
            text_truncated = text[:1500] if len(text) > 1500 else text
            
            # ìºì‹± í™œì„±í™” API í˜¸ì¶œ
            message = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=300,
                temperature=0.7,
                system=[
                    {
                        "type": "text",
                        "text": self.system_prompt,
                        "cache_control": {"type": "ephemeral"}  # ìºì‹±!
                    }
                ],
                messages=[
                    {
                        "role": "user",
                        "content": f"Analyze this text:\n\n{text_truncated}"
                    }
                ]
            )
            
            response_text = message.content[0].text.strip()
            
            # ì‚¬ìš©ëŸ‰ ì¶”ì  (ìºì‹± êµ¬ë¶„)
            if hasattr(message, 'usage'):
                usage = message.usage
                
                # ìºì‹œ ê´€ë ¨ í† í°
                cache_creation = getattr(usage, 'cache_creation_input_tokens', 0)
                cache_read = getattr(usage, 'cache_read_input_tokens', 0)
                
                if cache_creation > 0:
                    self.stats.cache_write_tokens += cache_creation
                    logger.info(f"ğŸ’¾ ìºì‹œ ìƒì„±: {cache_creation} í† í°")
                
                if cache_read > 0:
                    self.stats.cache_read_tokens += cache_read
                    logger.debug(f"ğŸ“– ìºì‹œ ì½ê¸°: {cache_read} í† í° (90% ì ˆê°!)")
                
                # ì¼ë°˜ í† í° (ìºì‹œë˜ì§€ ì•Šì€ ë¶€ë¶„)
                regular_input = usage.input_tokens - cache_creation - cache_read
                self.stats.regular_tokens += regular_input
                
                # ì¶œë ¥ í† í°
                self.stats.output_tokens += usage.output_tokens
            
            # JSON íŒŒì‹±
            try:
                # JSON ë¸”ë¡ ì°¾ê¸°
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                    
                    # ê²€ì¦
                    required_keys = ['emotions', 'regret_factor', 'bentham_scores', 'surd_metrics']
                    if all(key in result for key in required_keys):
                        if len(result['emotions']) == 7:
                            return result
                
                # íŒŒì‹± ì‹¤íŒ¨
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {response_text[:200]}")
                return None
                
            except json.JSONDecodeError as e:
                logger.error(f"JSON ë””ì½”ë“œ ì—ëŸ¬: {e}")
                return None
            
        except AuthenticationError:
            logger.error("âŒ ì¸ì¦ ì‹¤íŒ¨ - API í‚¤ í™•ì¸")
            self.stats.credit_exhausted = True
            self.save_checkpoint()
            raise
            
        except Exception as e:
            if "credit" in str(e).lower() or "billing" in str(e).lower():
                logger.error("ğŸ’³ í¬ë ˆë”§ ì†Œì§„!")
                self.stats.credit_exhausted = True
                self.save_checkpoint()
                raise SystemExit("í¬ë ˆë”§ ì†Œì§„ - ì¶©ì „ í›„ ì¬ì‹¤í–‰í•˜ì„¸ìš”")
            
            logger.error(f"API ì˜¤ë¥˜: {e}")
            return None
    
    async def process_samples(self, input_file: str, limit: int = None):
        """ìƒ˜í”Œ ì²˜ë¦¬"""
        self.stats.start_time = datetime.now()
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        self.load_checkpoint()
        
        # ê¸°ì¡´ JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë³µì›
        success_samples = []
        failed_samples = []
        
        success_file = Path("claude_preprocessed_complete.json")
        if success_file.exists():
            with open(success_file, 'r', encoding='utf-8') as f:
                success_samples = json.load(f)
                # ì´ë¯¸ ì²˜ë¦¬ëœ ID ì¶”ê°€
                for sample in success_samples:
                    self.processed_ids.add(sample['id'])
                logger.info(f"âœ… ê¸°ì¡´ JSONì—ì„œ {len(success_samples)}ê°œ ë³µì›")
        
        # íŒŒì¼ ê²½ë¡œ
        input_path = Path(input_file)
        if not input_path.exists():
            input_path = Path(f"../for_learn_dataset/scruples_real_data/anecdotes/{input_file}")
        
        if not input_path.exists():
            logger.error(f"íŒŒì¼ ì—†ìŒ: {input_path}")
            return
        
        # ì¶œë ¥ íŒŒì¼ (JSON í˜•ì‹ìœ¼ë¡œ ë³€ê²½)
        success_file = Path("claude_preprocessed_complete.json")
        failed_file = Path("claude_failed_complete.json")
        
        logger.info(f"\n=== ìºì‹± ìµœì í™” ì „ì²˜ë¦¬ ì‹œì‘ ===")
        logger.info(f"ì…ë ¥: {input_path}")
        logger.info(f"ì²˜ë¦¬ ëŒ€ìƒ: {limit if limit else 'ì „ì²´'}")
        
        processed_count = 0
        is_first = True  # ì²« ìš”ì²­ ì—¬ë¶€ (ìºì‹œ ìƒì„±)
        
        # JSON ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘
        success_samples = []
        failed_samples = []
        
        with open(input_path, 'r', encoding='utf-8') as f_in:
            
            for i, line in enumerate(f_in):
                if limit and processed_count >= limit:
                    break
                
                sample = json.loads(line)
                sample_id = sample.get('id', str(i))
                
                # ì´ë¯¸ ì²˜ë¦¬ë¨
                if sample_id in self.processed_ids:
                    continue
                
                text = sample.get('text', '')
                if not text:
                    continue
                
                # ì§„í–‰ ìƒí™©
                if processed_count % 10 == 0 and processed_count > 0:
                    current_cost = self.stats.get_cost_estimate()
                    logger.info(f"\nì§„í–‰: {processed_count}/{limit if limit else 'âˆ'}")
                    logger.info(f"ìºì‹œ ì ˆê°: {self.stats.cache_read_tokens:,} í† í°")
                    logger.info(f"í˜„ì¬ ë¹„ìš©: ${current_cost:.2f} ({current_cost*1320:.0f}ì›)")
                    
                    # í¬ë ˆë”§ í•œë„ ì²´í¬ ($51)
                    if current_cost >= 51.0:
                        logger.warning(f"ğŸ’° í¬ë ˆë”§ í•œë„ ë„ë‹¬! ($51)")
                        logger.info(f"ì´ ì²˜ë¦¬: {processed_count}ê°œ")
                        break
                
                # í†µí•© ë¶„ì„ (í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ëª¨ë“  ë©”íŠ¸ë¦­)
                result = await self.analyze_unified(text, is_first)
                is_first = False  # ì²« ë²ˆì§¸ ì´í›„ë¡œëŠ” ìºì‹œ ì½ê¸°
                
                if result:
                    # ì„±ê³µ
                    output = {
                        'id': sample_id,
                        'text': text[:500],
                        'title': sample.get('title', ''),
                        'action': sample.get('action', {}).get('description', '') if sample.get('action') else '',
                        'label': sample.get('label', ''),
                        **result,  # emotions, regret_factor, bentham_scores, surd_metrics
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    success_samples.append(output)
                    
                    self.processed_ids.add(sample_id)
                    self.stats.processed += 1
                    processed_count += 1
                    
                    logger.info(f"âœ… [{processed_count}] {sample_id}")
                    
                else:
                    # ì‹¤íŒ¨
                    failed = {
                        'id': sample_id,
                        'text': text[:500],
                        'error': 'Analysis failed',
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    failed_samples.append(failed)
                    
                    self.stats.failed += 1
                    logger.error(f"âŒ {sample_id}")
                
                # 30ê°œë§ˆë‹¤ JSON + ì²´í¬í¬ì¸íŠ¸ ë™ì‹œ ì €ì¥ (ë™ê¸°í™”)
                if processed_count % 30 == 0:
                    # JSON ì €ì¥
                    with open(success_file, 'w', encoding='utf-8') as f:
                        json.dump(success_samples, f, ensure_ascii=False, indent=2)
                    if failed_samples:
                        with open(failed_file, 'w', encoding='utf-8') as f:
                            json.dump(failed_samples, f, ensure_ascii=False, indent=2)
                    
                    # ì²´í¬í¬ì¸íŠ¸ë„ í•¨ê»˜ ì €ì¥ (ë™ê¸°í™”)
                    self.save_checkpoint()
                    logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {processed_count}ê°œ (JSON + ì²´í¬í¬ì¸íŠ¸)")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        logger.info(f"\nğŸ“ JSON íŒŒì¼ ì €ì¥ ì¤‘...")
        with open(success_file, 'w', encoding='utf-8') as f:
            json.dump(success_samples, f, ensure_ascii=False, indent=2)
        
        if failed_samples:
            with open(failed_file, 'w', encoding='utf-8') as f:
                json.dump(failed_samples, f, ensure_ascii=False, indent=2)
        
        # ìµœì¢… í†µê³„
        duration = (datetime.now() - self.stats.start_time).total_seconds()
        total_cost = self.stats.get_cost_estimate()
        
        logger.info(f"\n=== ì™„ë£Œ ===")
        logger.info(f"ì²˜ë¦¬: {self.stats.processed}")
        logger.info(f"ì‹¤íŒ¨: {self.stats.failed}")
        logger.info(f"ì‹œê°„: {duration/60:.1f}ë¶„")
        logger.info(f"\n=== í† í° ì‚¬ìš©ëŸ‰ ===")
        logger.info(f"ìºì‹œ ìƒì„±: {self.stats.cache_write_tokens:,} í† í° ($3.75/1M)")
        logger.info(f"ìºì‹œ ì½ê¸°: {self.stats.cache_read_tokens:,} í† í° ($0.30/1M) â† 90% ì ˆê°!")
        logger.info(f"ì¼ë°˜ ì…ë ¥: {self.stats.regular_tokens:,} í† í° ($3/1M)")
        logger.info(f"ì¶œë ¥: {self.stats.output_tokens:,} í† í° ($15/1M)")
        logger.info(f"\n=== ë¹„ìš© ===")
        logger.info(f"ì´ ë¹„ìš©: ${total_cost:.2f} ({total_cost*1320:.0f}ì›)")
        
        if self.stats.processed > 0:
            per_sample = total_cost / self.stats.processed
            savings = (self.stats.cache_read_tokens * 2.7) / 1_000_000  # ì ˆê°ì•¡
            logger.info(f"ìƒ˜í”Œë‹¹: ${per_sample:.4f}")
            logger.info(f"ìºì‹± ì ˆê°ì•¡: ${savings:.2f} ({savings*1320:.0f}ì›)")
            logger.info(f"\n20,000ê°œ ì˜ˆìƒ:")
            logger.info(f"  ìºì‹± ì—†ì´: ${per_sample * 20000 * 4:.2f}")  # 4ë°° (ìºì‹± ì—†ì´)
            logger.info(f"  ìºì‹± ì ìš©: ${per_sample * 20000:.2f}")
        
        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸
        self.save_checkpoint()

async def test_cached():
    """ìºì‹± í…ŒìŠ¤íŠ¸"""
    preprocessor = ClaudeCachedPreprocessor()
    
    # 3ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    await preprocessor.process_samples(
        "train.scruples-anecdotes.jsonl",
        limit=3
    )

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--reset":
            # ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™”
            Path("checkpoint_cached.pkl").unlink(missing_ok=True)
            Path("claude_preprocessed_complete.json").unlink(missing_ok=True)
            Path("claude_failed_complete.json").unlink(missing_ok=True)
            print("âœ… ìºì‹œ ê´€ë ¨ íŒŒì¼ ì´ˆê¸°í™”ë¨")
        elif sys.argv[1] == "--full":
            # ì „ì²´ ì‹¤í–‰
            asyncio.run(ClaudeCachedPreprocessor().process_samples(
                "train.scruples-anecdotes.jsonl",
                limit=20000
            ))
        else:
            # ì»¤ìŠ¤í…€ limit ì²˜ë¦¬ (ê°„ë‹¨í•œ ë°©ì‹)
            try:
                limit = int(sys.argv[1])
                print(f"ğŸš€ {limit}ê°œ ìƒ˜í”Œ ì²˜ë¦¬ ì‹œì‘...")
                asyncio.run(ClaudeCachedPreprocessor().process_samples(
                    "../for_learn_dataset/scruples_real_data/anecdotes/train.scruples-anecdotes.jsonl",
                    limit=limit
                ))
            except ValueError:
                print(f"âŒ ì˜ëª»ëœ ì¸ì: {sys.argv[1]}")
                print("ì‚¬ìš©ë²•: python3 claude_cached_preprocessor.py [ìˆ«ì|--reset|--full]")
    else:
        # ê¸°ë³¸: 3ê°œ í…ŒìŠ¤íŠ¸
        asyncio.run(test_cached())