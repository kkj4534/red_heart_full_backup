# Red Heart AI μ›ν¬ν”λ΅μ° κ²€μ¦ λ¬Έμ„
μ‘μ„±μΌ: 2025-08-21

## 1. μ „μ²΄ μ›ν¬ν”λ΅μ°

### 1.1 μ‹¤ν–‰ λ…λ Ήμ–΄
```bash
# κΈ°λ³Έ λ…λ Ήμ–΄ (μμ • μ „)
bash run_learning.sh unified-test --samples 3 --no-param-update --debug --verbose

# LR μ¤μ• ν¬ν•¨ λ…λ Ήμ–΄ (μμ • ν›„) β…
bash run_learning.sh unified-test --lr-sweep --samples 3 --no-param-update --debug --verbose

# nohupμΌλ΅ λ°±κ·ΈλΌμ΄λ“ μ‹¤ν–‰
nohup timeout 3600 bash run_learning.sh unified-test --lr-sweep --samples 3 --no-param-update --debug --verbose > test_$(date +%Y%m%d_%H%M%S).txt 2>&1 &
```

### 1.2 μ‹¤ν–‰ νλ¦„

#### Stage 1: μ΄κΈ°ν™”
1. **run_learning.sh μ‹¤ν–‰**
   - unified-test λ¨λ“ μ§„μ…
   - --lr-sweep μµμ… ν™•μΈ β…
   - training/run_hierarchical_lr_sweep.py μ‹¤ν–‰

2. **ν™κ²½ μ„¤μ •**
   - venv/conda ν™κ²½ ν™μ„±ν™”
   - GPU λ©”λ¨λ¦¬ ν™•μΈ λ° μ΄κΈ°ν™”
   - λ΅κΉ… μ„¤μ •

#### Stage 2: λ°μ΄ν„° μ¤€λΉ„
3. **λ°μ΄ν„° λ΅λ“**
   - claude_preprocessed_complete.json λλ” .embedded.json ν™•μΈ
   - μ„λ² λ”© μƒνƒ μ²΄ν¬
     - μμΌλ©΄: μ¬μ‚¬μ© β…
     - μ—†μΌλ©΄: SentenceTransformerλ΅ μƒμ„± β…
   - μƒμ„±λ μ„λ² λ”© .embedded.jsonμ— μ €μ¥ β…

#### Stage 3: LR μ¤μ•
4. **κ³„μΈµμ  LR μ¤μ• (5-5-5-5)**
   - lr_sweep_cumulative.json λ΅λ“ (κΈ°μ΅΄ κ²°κ³Ό) β…
   - μ¤‘λ³µ LR μ²΄ν¬ λ° 10% κ°„κ²© μ΅°μ • β…
   - Stage 0-4 μμ°¨ μ‹¤ν–‰ (μ΄ 25ν¬μΈνΈ)
   - κ° LRμ€ λ…λ¦½μ μΌλ΅ μ΄κΈ° κ°€μ¤‘μΉμ—μ„ μ‹μ‘
   - κ²°κ³Ό λ„μ  μ €μ¥ β…
   - Stageλ³„ PNG μƒμ„± β…

5. **μµμ  LR μ„ νƒ**
   - optimal_lr.json μ €μ¥
   - μµμ  LRλ΅ λ³Έ ν•™μµ μ¤€λΉ„

#### Stage 4: λ³Έ ν•™μµ
6. **λ¨λΈ μ΄κΈ°ν™”**
   - UnifiedModel μƒμ„±
   - 730M νλΌλ―Έν„° κ²€μ¦ β…
     - λ©ν‘: 730M
     - μ‹¤μ  μΉ΄μ΄νΈ λ° μ°¨μ΄ ν‘μ‹
     - λ¨λ“λ³„ μƒμ„Έ λ¶„μ„
   - GPU λ©”λ¨λ¦¬ λ°°μΉ

7. **ν•™μµ μ‹¤ν–‰**
   - 60 μ—ν­ ν•™μµ (--samplesλ΅ μ ν• κ°€λ¥)
   - νλΌλ―Έν„° μ—…λ°μ΄νΈ λ¨λ‹ν„°λ§ β…
     - 100λ°°μΉλ§λ‹¤ μ—…λ°μ΄νΈ ν™•μΈ
     - λ―Έμ—…λ°μ΄νΈ λ¨λ“ κ²½κ³ 
   - Gradient norm μ¶”μ 
   - μ²΄ν¬ν¬μΈνΈ μ €μ¥

#### Stage 5: λ¶„μ„
8. **Sweet Spot λ¶„μ„**
   - λ¨λ“λ³„ μµμ  μ—ν­ νƒμƒ‰
   - 5κ°€μ§€ κ³ κΈ‰ λ¶„μ„ κΈ°λ²• μ μ©

9. **Parameter Crossover**
   - μµμ  μ—ν­μ νλΌλ―Έν„° κ²°ν•©
   - crossover_final.pth μ €μ¥

10. **μΆ…λ£**
    - μ„λ² λ”© μ €μ¥ ν™•μΈ β…
    - ν•™μµ κ³΅μ„  export
    - OOM ν†µκ³„ μ €μ¥
    - μµμΆ… λ¦¬ν¬νΈ μƒμ„±

## 2. ν•µμ‹¬ νμΌ λ° κΈ°λ¥

### 2.1 μμ •λ νμΌ
- β… **run_learning.sh**: --lr-sweep μµμ… μ¶”κ°€
- β… **unified_training_final.py**: 
  - SentenceTransformer μ„λ² λ”© ν†µν•©
  - νλΌλ―Έν„° μ—…λ°μ΄νΈ λ¨λ‹ν„°λ§
  - 730M κ²€μ¦ κ°•ν™”
- β… **run_hierarchical_lr_sweep.py**: μ‹¤μ  λ°μ΄ν„° μ‚¬μ©
- β… **hierarchical_lr_sweep.py**: 
  - λ„μ  κ²°κ³Ό μ €μ¥/λ΅λ“
  - μ¤‘λ³µ LR 10% μ΅°μ •

### 2.2 μƒμ„±λλ” νμΌ
- `lr_sweep_cumulative.json`: λ¨λ“  LR ν…μ¤νΈ λ„μ  κ²°κ³Ό
- `optimal_lr.json`: μµμ  LR μ •λ³΄
- `*.embedded.json`: μ„λ² λ”©μ΄ ν¬ν•¨λ λ°μ΄ν„°μ…‹
- `hierarchical_lr_sweep_stage*.png`: Stageλ³„ μ‹κ°ν™”
- `checkpoints/`: μ—ν­λ³„ μ²΄ν¬ν¬μΈνΈ
- `crossover_final.pth`: Parameter Crossover κ²°κ³Ό

## 3. μμƒ λ¬Έμ μ  λ° ν•΄κ²°μ±…

### 3.1 λ©”λ¨λ¦¬ λ¬Έμ 
- **λ¬Έμ **: 730M λ¨λΈ + μ„λ² λ”© μƒμ„± μ‹ OOM
- **ν•΄κ²°**: 
  - λ°°μΉ μ‚¬μ΄μ¦ μλ™ μ΅°μ •
  - μ„λ² λ”© λ°°μΉ μ²λ¦¬
  - GPU/CPU ν•μ΄λΈλ¦¬λ“ λ°°μΉ

### 3.2 μ„λ² λ”© μ‹κ°„
- **λ¬Έμ **: 150,000κ° μƒν” μ„λ² λ”© μ‹κ°„
- **ν•΄κ²°**: 
  - μ²« μ‹¤ν–‰λ§ μƒμ„±, μ΄ν›„ μ¬μ‚¬μ©
  - LR μ¤μ•μ€ μΌλ¶€ μƒν”λ§ μ‚¬μ© (1000κ°)

### 3.3 νλΌλ―Έν„° λ„λ½
- **λ¬Έμ **: neural_analyzers, advanced_wrappers λ―Έμ‚¬μ©
- **ν•΄κ²°**: 
  - nn.ModuleDict μ‚¬μ© β…
  - νλΌλ―Έν„° μ—…λ°μ΄νΈ λ¨λ‹ν„°λ§ β…
  - Optimizer λ“±λ΅ ν™•μΈ β…

## 4. κ²€μ¦ μ²΄ν¬λ¦¬μ¤νΈ

- [x] λ…λ Ήμ–΄μ— --lr-sweep μΈμ ν¬ν•¨
- [x] μ„λ² λ”© μƒμ„±/μ¬μ‚¬μ© λ΅μ§
- [x] LR μ¤‘λ³µ μ²΄ν¬ λ° μ΅°μ •
- [x] 730M νλΌλ―Έν„° κ²€μ¦
- [x] νλΌλ―Έν„° μ—…λ°μ΄νΈ λ¨λ‹ν„°λ§
- [x] λ„μ  κ²°κ³Ό μ €μ¥
- [ ] μ‹¤μ  ν…μ¤νΈ μ‹¤ν–‰
- [ ] λ΅κ·Έ ν™•μΈ
- [ ] κ²°κ³Ό λ¶„μ„

## 5. ν…μ¤νΈ λ…λ Ήμ–΄

### μµμ† ν…μ¤νΈ (1 μ—ν­, LR μ¤μ•)
```bash
bash run_learning.sh unified-test --lr-sweep --samples 1 --no-param-update --debug --verbose
```

### ν‘μ¤€ ν…μ¤νΈ (3 μ—ν­, LR μ¤μ•)
```bash
bash run_learning.sh unified-test --lr-sweep --samples 3 --no-param-update --debug --verbose
```

### μ „μ²΄ ν•™μµ (60 μ—ν­, μµμ  LR μ‚¬μ©)
```bash
bash run_learning.sh unified-train --epochs 60 --lr $(cat training/lr_sweep_results/optimal_lr.json | grep optimal_lr | cut -d'"' -f4)
```

## 6. λ¨λ‹ν„°λ§ ν¬μΈνΈ

1. **μ„λ² λ”© μƒμ„±**
   - "π“ μ„λ² λ”© μƒνƒ:" λ΅κ·Έ ν™•μΈ
   - "β… μ„λ² λ”©μ΄ μ €μ¥λμ—μµλ‹λ‹¤" ν™•μΈ

2. **LR μ¤μ•**
   - "π“‚ κΈ°μ΅΄ LR ν…μ¤νΈ κ²°κ³Ό λ΅λ“" ν™•μΈ
   - "β οΈ LR * μ΄λ―Έ ν…μ¤νΈλ¨" κ²½κ³  ν™•μΈ
   - "π’Ύ λ„μ  κ²°κ³Ό μ €μ¥ μ™„λ£" ν™•μΈ

3. **νλΌλ―Έν„° κ²€μ¦**
   - "β… λ©ν‘ νλΌλ―Έν„° μ λ‹¬μ„±: *M β‰ 730M" λλ”
   - "β οΈ νλΌλ―Έν„° κ°μ λ¶μΌμΉ!" κ²½κ³  ν™•μΈ
   - "β… νλΌλ―Έν„° μ—…λ°μ΄νΈλ¨" λ΅κ·Έ ν™•μΈ

4. **ν•™μµ μ§„ν–‰**
   - Epochλ³„ loss κ°μ†
   - Gradient norm λ³€ν™”
   - μ²΄ν¬ν¬μΈνΈ μ €μ¥

---
μ‘μ„±: 2025-08-21
μƒνƒ: ν…μ¤νΈ μ¤€λΉ„ μ™„λ£