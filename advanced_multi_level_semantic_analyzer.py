"""
ê³ ê¸‰ ë‹¤ì¤‘ìˆ˜ì¤€ ì˜ë¯¸ ë¶„ì„ ì‹œìŠ¤í…œ - Linux ì „ìš©
Advanced Multi-Level Semantic Analysis System for Linux

ìµœì‹  AI ê¸°ë²•ì„ í™œìš©í•œ 4ìˆ˜ì¤€(í‘œë©´ì , ìœ¤ë¦¬ì , ê°ì •ì , ì¸ê³¼ì ) ì˜ë¯¸ ë¶„ì„
Transformer ê¸°ë°˜ ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ê³¼ ê³ ê¸‰ ML ê¸°ë²•ì„ ê²°í•©í•œ ì‹¬ì¸µ ì˜ë¯¸ ì´í•´
"""

__all__ = ['AdvancedMultiLevelSemanticAnalyzer', 'create_advanced_semantic_analyzer']

import os
import logging
import json
import time
import asyncio
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Any, Optional, Union, Set
from dataclasses import dataclass, field
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
from collections import defaultdict

# ê³ ê¸‰ AI ë¼ì´ë¸ŒëŸ¬ë¦¬
# SentenceTransformerëŠ” sentence_transformer_singletonì„ í†µí•´ ì‚¬ìš©
from transformers import (
    pipeline, AutoTokenizer, AutoModel, AutoModelForSequenceClassification,
    AutoModelForCausalLM, BertTokenizer, BertModel, GPT2LMHeadModel
)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import scipy.stats as stats
from scipy.spatial.distance import pdist, squareform
import networkx as nx
import warnings
warnings.filterwarnings('ignore')

from config import ADVANCED_CONFIG, SYSTEM_CONFIG, DEVICE, TORCH_DTYPE, BATCH_SIZE, MODELS_DIR, get_smart_device, ModelPriority, get_priority_based_device
from data_models import (
    SemanticRepresentationData, SemanticLevel, IntentionCategory,
    AdvancedSemanticResult, SemanticCluster, SemanticNetwork,
    CausalRelation, EthicalDimension, EmotionalProfile
)

logger = logging.getLogger('RedHeart.AdvancedMultiLevelSemantic')

@dataclass
class SemanticAnalysisState:
    """ì˜ë¯¸ ë¶„ì„ ìƒíƒœ ì •ë³´"""
    current_level: str = ""
    processing_time: float = 0.0
    confidence_scores: Dict[str, float] = field(default_factory=dict)
    intermediate_results: Dict[str, Any] = field(default_factory=dict)
    error_log: List[str] = field(default_factory=list)

@dataclass
class CrossLevelSemanticRelation:
    """ìˆ˜ì¤€ ê°„ ì˜ë¯¸ ê´€ê³„"""
    source_level: str
    target_level: str
    relation_type: str  # "supports", "contradicts", "neutral", "amplifies"
    strength: float  # 0.0 ~ 1.0
    evidence: List[str] = field(default_factory=list)
    confidence: float = 0.0

class AdvancedMultiLevelSemanticAnalyzer:
    """ê³ ê¸‰ ë‹¤ì¤‘ìˆ˜ì¤€ ì˜ë¯¸ë¡ ì  ë¶„ì„ê¸°"""
    
    def __init__(self):
        """ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.config = SYSTEM_CONFIG['semantic']
        self.device = DEVICE
        self.dtype = TORCH_DTYPE
        self.batch_size = BATCH_SIZE
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.models_dir = os.path.join(MODELS_DIR, 'semantic_models')
        self.cache_dir = os.path.join(MODELS_DIR, 'semantic_cache')
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.embedding_cache = {}
        self.analysis_cache = {}
        self.relation_cache = {}
        
        # ë¹„ë™ê¸° ì´ˆê¸°í™”ë¥¼ ìœ„í•œ í”Œë˜ê·¸ë“¤
        self.models_initialized = False
        self.classifiers_initialized = False
        self.networks_initialized = False
        self.fully_initialized = False
        
        # ë¶„ì„ ìƒíƒœ ì¶”ì 
        self.analysis_state = SemanticAnalysisState()
        self.cross_level_relations = []
        
        # GPU ë¡œë” ë°”ì¸ë”© (ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°/CONFIG ì œê³µ)
        try:
            from config import get_gpu_loader
            self.gpu_loader = get_gpu_loader()
            logger.info("GPU Loader ë°”ì¸ë”© ì™„ë£Œ")
        except Exception as e:
            self.gpu_loader = None
            logger.warning(f"GPU Loader ë¯¸ë°”ì¸ë”© (ì´ˆê¸°í™” ì¤‘ ì¬ì‹œë„ ì˜ˆì •): {e}")
        
        logger.info("AdvancedMultiLevelSemanticAnalyzer ê¸°ë³¸ ì´ˆê¸°í™” ì™„ë£Œ (ë¹„ë™ê¸° ì´ˆê¸°í™” ëŒ€ê¸° ì¤‘)")
    
    async def initialize(self):
        """ë¹„ë™ê¸° ì´ˆê¸°í™” - ëª¨ë“  ëª¨ë¸ê³¼ ë„¤íŠ¸ì›Œí¬ ì„¤ì •"""
        # ì·¨ì†Œ/ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ lockê³¼ event ì´ˆê¸°í™”
        if not hasattr(self, '_init_lock'):
            self._init_lock = asyncio.Lock()
        if not hasattr(self, '_cancel_event'):
            self._cancel_event = asyncio.Event()
        
        # ì´ë¯¸ ì´ˆê¸°í™”ë¨
        if self.fully_initialized:
            return
        
        # Lockìœ¼ë¡œ ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€
        async with self._init_lock:
            if self.fully_initialized:
                return
            
            logger.info("AdvancedMultiLevelSemanticAnalyzer ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹œì‘...")
            start_ts = time.time()
            self.last_heartbeat_ts = time.time()  # í•˜íŠ¸ë¹„íŠ¸ ê°’ ì´ˆê¸°í™”
            
            # GPU ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· (ì´ˆê¸°í™” ì‹œì‘)
            from config import get_gpu_memory_info
            gpu_info = get_gpu_memory_info()
            if gpu_info:
                logger.info(f"ğŸ“Š [semantic ì´ˆê¸°í™” ì‹œì‘] GPU: {gpu_info['usage_percent']:.1f}% ì‚¬ìš©, {gpu_info['free_mb']:.0f}MB ì—¬ìœ ")
        
        # DSM placeholder ì„ ë“±ë¡ (ì´ˆê¸°í™” ì‹œì‘ ì‹œì )
        try:
            from dynamic_swap_manager import get_swap_manager, SwapPriority
            swap = get_swap_manager()
            if swap and "semantic_analyzer" not in getattr(swap, 'models', {}):
                class _TinyPlaceholder(nn.Module):
                    def __init__(self):
                        super().__init__()
                        self.dummy = nn.Linear(8, 8, bias=False)
                    def forward(self, x):
                        return x if not isinstance(x, torch.Tensor) else self.dummy(x[..., :8])
                swap.register_model("semantic_analyzer", _TinyPlaceholder(), priority=SwapPriority.MEDIUM)
                logger.info("ğŸ§© semantic_analyzer DSM ì„ ë“±ë¡(placeholder) ì™„ë£Œ")
                logger.info(f"   DSM keys after placeholder: {list(swap.models.keys())[:10]}")
        except Exception as e:
            logger.warning(f"DSM placeholder ì„ ë“±ë¡ ìŠ¤í‚µ: {e}")
        
        # GPU ë©”ëª¨ë¦¬ ì˜ˆì•½ (device_policy ìŠ¤í™ì— ë”°ë¼)
        from workflow_aware_memory_manager import WorkflowAwareMemoryManager
        try:
            from module_specs import MODULE_SPECS
        except Exception:
            MODULE_SPECS = []  # í´ë°±: ìŠ¤í™ ì—†ì–´ë„ ë™ì‘
        
        # GPU ë¡œë” ì¬ë°”ì¸ë”© ì‹œë„ (í•„ìš”ì‹œ)
        if self.gpu_loader is None:
            try:
                from config import get_gpu_loader
                self.gpu_loader = get_gpu_loader()
                logger.info("GPU Loader ì¬ë°”ì¸ë”© ì„±ê³µ")
            except Exception as e:
                logger.warning(f"GPU Loader ì¬ë°”ì¸ë”© ì‹¤íŒ¨: {e} (NoopLoader í´ë°± ì ìš©)")
                # NoopLoader í´ë°± ì²˜ë¦¬
                class _NoopLoader:
                    def request_gpu_loading(self, *a, **kw): return ("cpu", {})
                self.gpu_loader = _NoopLoader()
        
        # gpu_loader ë³´ì¦ í™•ì¸
        assert hasattr(self, 'gpu_loader') and self.gpu_loader is not None, "gpu_loader must be available"
        logger.info(f"âœ… gpu_loader ë³´ì¦ í™•ì¸ ì™„ë£Œ (type: {type(self.gpu_loader).__name__})")
        
        # DSM/ë¡œë” ì‹¤ì¸¡ ê¸°ë°˜ìœ¼ë¡œ ìŠ¹ê²© ì²˜ë¦¬ (ì¶”ì •ì¹˜ ì„ í• ë‹¹ ì œê±°)
        # CPU ì„ ì ì¬ í›„ í•„ìš”ì‹œ GPUë¡œ í”„ë¡œëª¨ì…˜í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½ë¨
        logger.info("ğŸ”„ DSM ì‹¤ì¸¡ ê¸°ë°˜ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì •ì±… ì ìš© (ì¶”ì •ì¹˜ ì„ í• ë‹¹ ì œê±°)")
        
        # í•˜íŠ¸ë¹„íŠ¸ íƒœìŠ¤í¬ ì‹œì‘ (ê°’ ë…¸ì¶œ í¬í•¨)
        async def _heartbeat():
            while not self.fully_initialized and not self._cancel_event.is_set():
                elapsed = int(time.time() - start_ts)
                self.last_heartbeat_ts = time.time()  # orchestratorê°€ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ ê°’ ê°±ì‹ 
                logger.info(f"â³ semantic_analyzer initâ€¦ {elapsed}s ê²½ê³¼ (heartbeat_ts={self.last_heartbeat_ts:.1f})")
                await asyncio.sleep(2.0)
        heartbeat_task = asyncio.create_task(_heartbeat())
        
        try:
            # ìˆœì°¨ì ìœ¼ë¡œ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”) - asyncio.to_threadë¡œ ë¹„ë™ê¸°í™”
            if not self.models_initialized:
                models_start = time.time()
                await asyncio.to_thread(self._initialize_models_cpu_safe)
                self.models_initialized = True
                
                # ì·¨ì†Œ ì²´í¬
                if self._cancel_event.is_set():
                    raise asyncio.CancelledError("ì´ˆê¸°í™” ì·¨ì†Œë¨")
                
                models_elapsed = int((time.time() - models_start) * 1000)
                logger.info(f"ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (+{models_elapsed}ms)")
                
                # ë‹¨ê³„ë³„ RAM ì„ ë“±ë¡ - ëª¨ë¸ ì´ˆê¸°í™” í›„
                try:
                    from dynamic_swap_manager import get_swap_manager, SwapPriority
                    swap = get_swap_manager()
                    if swap:
                        # embedding_model ë“±ë¡
                        if hasattr(self, 'embedding_model'):
                            embed_core = getattr(self.embedding_model, 'auto_model', None) or \
                                       getattr(self.embedding_model, 'model', None) or \
                                       getattr(self.embedding_model, '_model', None)
                            if embed_core:
                                swap.register_model("semantic_embedding", embed_core, priority=SwapPriority.LOW)
                                logger.info("âœ… semantic_embedding RAM ì„ ë“±ë¡")
                        
                        # emotion_pipeline ë“±ë¡
                        if hasattr(self, 'emotion_pipeline') and hasattr(self.emotion_pipeline, 'model'):
                            swap.register_model("semantic_emotion", self.emotion_pipeline.model, priority=SwapPriority.LOW)
                            logger.info("âœ… semantic_emotion RAM ì„ ë“±ë¡")
                except Exception as e:
                    logger.warning(f"ëª¨ë¸ ë‹¨ê³„ RAM ì„ ë“±ë¡ ìŠ¤í‚µ: {e}")
                
                # ì´ë²¤íŠ¸ ë£¨í”„ ì¦‰ì‹œ ì–‘ë³´ (ë¸”ë¡œí‚¹ ë°©ì§€)
                await asyncio.sleep(0)
            
            if not self.classifiers_initialized:
                classifiers_start = time.time()
                await asyncio.to_thread(self._initialize_classifiers)
                self.classifiers_initialized = True
                
                # ì·¨ì†Œ ì²´í¬
                if self._cancel_event.is_set():
                    raise asyncio.CancelledError("ì´ˆê¸°í™” ì·¨ì†Œë¨")
                
                classifiers_elapsed = int((time.time() - classifiers_start) * 1000)
                logger.info(f"ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ (+{classifiers_elapsed}ms)")
                
                # ë‹¨ê³„ë³„ RAM ì„ ë“±ë¡ - ë¶„ë¥˜ê¸° ì´ˆê¸°í™” í›„
                try:
                    from dynamic_swap_manager import get_swap_manager, SwapPriority
                    swap = get_swap_manager()
                    if swap:
                        # ethical_model ë“±ë¡
                        if hasattr(self, 'ethical_model'):
                            swap.register_model("semantic_ethical", self.ethical_model, priority=SwapPriority.LOW)
                            logger.info("âœ… semantic_ethical RAM ì„ ë“±ë¡")
                        
                        # causal_pipeline ë“±ë¡
                        if hasattr(self, 'causal_pipeline') and hasattr(self.causal_pipeline, 'model'):
                            swap.register_model("semantic_causal", self.causal_pipeline.model, priority=SwapPriority.LOW)
                            logger.info("âœ… semantic_causal RAM ì„ ë“±ë¡")
                except Exception as e:
                    logger.warning(f"ë¶„ë¥˜ê¸° ë‹¨ê³„ RAM ì„ ë“±ë¡ ìŠ¤í‚µ: {e}")
                
                # ì´ë²¤íŠ¸ ë£¨í”„ ì¦‰ì‹œ ì–‘ë³´
                await asyncio.sleep(0)
            
            if not self.networks_initialized:
                networks_start = time.time()
                # CPU-bound ì‘ì—…ë§Œ ì›Œì»¤ ìŠ¤ë ˆë“œì—ì„œ ìˆ˜í–‰
                await asyncio.to_thread(self._setup_neural_networks_cpu)
                # GPU ë¡œë”/ë””ë°”ì´ìŠ¤ ì´ë™/DSM ë“±ë¡ì€ ë©”ì¸ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ìˆ˜í–‰
                await self._setup_neural_networks_gpu()
                self.networks_initialized = True
                
                # ì·¨ì†Œ ì²´í¬
                if self._cancel_event.is_set():
                    raise asyncio.CancelledError("ì´ˆê¸°í™” ì·¨ì†Œë¨")
                
                networks_elapsed = int((time.time() - networks_start) * 1000)
                logger.info(f"ì‹ ê²½ë§ ì´ˆê¸°í™” ì™„ë£Œ (+{networks_elapsed}ms)")
                
                # ë‹¨ê³„ë³„ RAM ì„ ë“±ë¡ - ì‹ ê²½ë§ ì´ˆê¸°í™” í›„
                try:
                    from dynamic_swap_manager import get_swap_manager, SwapPriority
                    swap = get_swap_manager()
                    if swap:
                        # fusion_network ë“±ë¡
                        if hasattr(self, 'fusion_network'):
                            swap.register_model("semantic_fusion", self.fusion_network, priority=SwapPriority.LOW)
                            logger.info("âœ… semantic_fusion RAM ì„ ë“±ë¡")
                        
                        # cross_attention ë“±ë¡
                        if hasattr(self, 'cross_attention'):
                            swap.register_model("semantic_cross_attention", self.cross_attention, priority=SwapPriority.LOW)
                            logger.info("âœ… semantic_cross_attention RAM ì„ ë“±ë¡")
                except Exception as e:
                    logger.warning(f"ì‹ ê²½ë§ ë‹¨ê³„ RAM ì„ ë“±ë¡ ìŠ¤í‚µ: {e}")
                
                # ì´ë²¤íŠ¸ ë£¨í”„ ì¦‰ì‹œ ì–‘ë³´
                await asyncio.sleep(0)
            
            self.fully_initialized = True
            total_elapsed = int((time.time() - start_ts) * 1000)
            logger.info(f"AdvancedMultiLevelSemanticAnalyzer ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ (ì´ {total_elapsed}ms ì†Œìš”)")
            
            # GPU ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· (ì´ˆê¸°í™” ì™„ë£Œ)
            gpu_info = get_gpu_memory_info()
            if gpu_info:
                logger.info(f"ğŸ“Š [semantic ì´ˆê¸°í™” ì™„ë£Œ] GPU: {gpu_info['usage_percent']:.1f}% ì‚¬ìš©, {gpu_info['free_mb']:.0f}MB ì—¬ìœ ")
            
            # ë§ˆì§€ë§‰: core_moduleë¡œ êµì²´ ë“±ë¡
            try:
                from dynamic_swap_manager import get_swap_manager, SwapPriority
                swap = get_swap_manager()
                core_module = self.get_pytorch_network()
                
                if swap and core_module:
                    old_type = type(swap.models.get('semantic_analyzer', {}).model).__name__ if 'semantic_analyzer' in swap.models else 'None'
                    swap.register_model("semantic_analyzer", core_module, priority=SwapPriority.HIGH)
                    new_type = type(swap.models['semantic_analyzer'].model).__name__
                    logger.info(f"âœ… semantic_analyzer DSM ë“±ë¡ ì™„ë£Œ (placeholder â†’ ë³¸ì²´ êµì²´: {old_type} â†’ {new_type})")
                    logger.info(f"   DSM keys after replacement: {list(swap.models.keys())[:10]}")
                    
                    # GPU ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· (DSM êµì²´ í›„)
                    gpu_info = get_gpu_memory_info()
                    if gpu_info:
                        logger.info(f"ğŸ“Š [DSM êµì²´ í›„] GPU: {gpu_info['usage_percent']:.1f}% ì‚¬ìš©, {gpu_info['free_mb']:.0f}MB ì—¬ìœ ")
                else:
                    logger.warning(f"âš ï¸ semantic_analyzer DSM ë“±ë¡ ìŠ¤í‚µ (swap={swap is not None}, core={core_module is not None})")
            except Exception as e:
                logger.error(f"semantic_analyzer DSM ë“±ë¡ ì‹¤íŒ¨: {e}")
            
        except asyncio.CancelledError:
            self._cancel_event.set()
            logger.warning("â›” semantic_analyzer ì´ˆê¸°í™” ì·¨ì†Œë¨ (íƒ€ì„ì•„ì›ƒ/ì¤‘ë‹¨)")
            raise
        except Exception as e:
            logger.error(f"AdvancedMultiLevelSemanticAnalyzer ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
        finally:
            # í•˜íŠ¸ë¹„íŠ¸ ì•ˆì „ ì •ë¦¬
            if 'heartbeat_task' in locals():
                heartbeat_task.cancel()
                from contextlib import suppress
                with suppress(asyncio.CancelledError):
                    await heartbeat_task
    
    def _initialize_models_cpu_safe(self):
        """ëª¨ë“  ëª¨ë¸ì„ CPUë¡œ ê°•ì œ ë¡œë“œí•˜ëŠ” ì•ˆì „í•œ ì´ˆê¸°í™”"""
        # ê¸°ì¡´ _initialize_models í˜¸ì¶œ í›„ CPU ê°•ì œ
        self._initialize_models()
        
        # ëª¨ë“  ëª¨ë¸ì„ CPUë¡œ ì´ë™ (GPU ìš°íšŒ ë°©ì§€)
        try:
            # embedding_modelì„ CPUë¡œ
            if hasattr(self, 'embedding_model') and hasattr(self.embedding_model, 'to'):
                self.embedding_model.to('cpu')
                logger.info("embedding_model CPU ê°•ì œ ì™„ë£Œ")
            
            # emotion_pipeline ë‚´ë¶€ ëª¨ë¸ì„ CPUë¡œ  
            if hasattr(self, 'emotion_pipeline') and hasattr(self.emotion_pipeline, 'model'):
                self.emotion_pipeline.model.to('cpu')
                logger.info("emotion_pipeline.model CPU ê°•ì œ ì™„ë£Œ")
                
            # ethical_modelì„ CPUë¡œ
            if hasattr(self, 'ethical_model') and hasattr(self.ethical_model, 'to'):
                self.ethical_model.to('cpu')
                logger.info("ethical_model CPU ê°•ì œ ì™„ë£Œ")
                
        except Exception as e:
            logger.warning(f"CPU ê°•ì œ ì¤‘ ê²½ê³ : {e}")
        
        logger.info("âœ… ëª¨ë“  ëª¨ë¸ CPU ê°•ì œ ë¡œë“œ ì™„ë£Œ (DSMì„ í†µí•œ GPU ìŠ¹ê²© ëŒ€ê¸°)")
    
    def _initialize_models(self):
        """ê³ ê¸‰ AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        try:
            # ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ íŠ¹í™”)
            from sentence_transformer_singleton import get_sentence_transformer
            
            self.embedding_model = get_sentence_transformer(
                'paraphrase-multilingual-mpnet-base-v2',
                device='cpu',  # CPUë¡œ ê°•ì œ ë¡œë“œ (gpu_on_demand ì •ì±…)
                cache_folder=self.models_dir
            )
            
            # ê°ì • ë¶„ì„ ëª¨ë¸ (gpu_on_demand: CPU ì„ ì ì¬)
            logger.info("ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì‹œì‘ (gpu_on_demand)")
            try:
                # HF ë˜í¼ ì‚¬ìš© ì‹œë„
                try:
                    from hf_model_wrapper import wrapped_pipeline
                    self.emotion_pipeline = wrapped_pipeline(
                        task="text-classification",
                        model="j-hartmann/emotion-english-distilroberta-base",
                        owner="semantic_analyzer",
                        device=-1,  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                        torch_dtype=torch.float32  # CPUëŠ” float32 ì‚¬ìš©
                    )
                    logger.info("ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì„±ê³µ (HF ë˜í¼, CPU)")
                except ImportError:
                    from transformers import pipeline
                    self.emotion_pipeline = pipeline(
                        "text-classification",
                        model="j-hartmann/emotion-english-distilroberta-base",
                        device=-1  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                    )
                    logger.info("ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì„±ê³µ (CPU)")
                
                # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ í™•ì¸
                if self.emotion_pipeline is None:
                    raise RuntimeError("ê°ì • ë¶„ì„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨")
                
                logger.info("âœ… emotion pipeline CPU ì„ ì ì¬ ì™„ë£Œ (gpu_on_demand)")
                
            except Exception as e:
                logger.error(f"ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ìœ¤ë¦¬ì  ë¶„ë¥˜ ëª¨ë¸ (gpu_on_demand: CPU ì„ ì ì¬)
            logger.info("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹œì‘ (gpu_on_demand)")
            self.ethical_tokenizer = AutoTokenizer.from_pretrained(
                'bert-base-uncased',
                cache_dir=self.models_dir
            )
            
            try:
                # HF ë˜í¼ ìš°ì„  ì‹œë„
                try:
                    from hf_model_wrapper import wrapped_from_pretrained
                    self.ethical_model = wrapped_from_pretrained(
                        model_class=AutoModel,
                        model_name='bert-base-uncased',
                        owner="semantic_analyzer",
                        cache_dir=self.models_dir
                    ).to(torch.device('cpu'))  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                    logger.info("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ë¡œë”© ì„±ê³µ (HF ë˜í¼, CPU)")
                except ImportError:
                    # ë˜í¼ ì—†ìœ¼ë©´ ì§ì ‘ ë¡œë”©
                    self.ethical_model = AutoModel.from_pretrained(
                        'bert-base-uncased',
                        cache_dir=self.models_dir
                    ).to(torch.device('cpu'))  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                    logger.info("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ë¡œë”© ì„±ê³µ (ì§ì ‘, CPU)")
                
                # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ í™•ì¸
                if self.ethical_model is None:
                    raise RuntimeError("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨")
                
                logger.info("âœ… ethical model CPU ì„ ì ì¬ ì™„ë£Œ (gpu_on_demand)")
                
            except Exception as e:
                logger.error(f"ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ì¸ê³¼ê´€ê³„ ì¶”ë¡  ëª¨ë¸ (gpu_on_demand: CPU ì„ ì ì¬)
            logger.info("ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì‹œì‘ (gpu_on_demand)")
            try:
                # HF ë˜í¼ ìš°ì„  ì‹œë„
                try:
                    from hf_model_wrapper import wrapped_pipeline
                    self.causal_pipeline = wrapped_pipeline(
                        task="zero-shot-classification",
                        model="facebook/bart-large-mnli",
                        owner="semantic_analyzer",
                        device=-1  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                    )
                    logger.info("ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì„±ê³µ (HF ë˜í¼, CPU)")
                except ImportError:
                    # ë˜í¼ ì—†ìœ¼ë©´ ì§ì ‘ ë¡œë”©
                    from transformers import pipeline
                    self.causal_pipeline = pipeline(
                        "zero-shot-classification",
                        model="facebook/bart-large-mnli",
                        device=-1  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                    )
                    logger.info("ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì„±ê³µ (ì§ì ‘, CPU)")
                
                # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ í™•ì¸
                if self.causal_pipeline is None:
                    raise RuntimeError("ì¸ê³¼ê´€ê³„ ë¶„ì„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨")
                
                logger.info("âœ… causal pipeline CPU ì„ ì ì¬ ì™„ë£Œ (gpu_on_demand)")
                
            except Exception as e:
                logger.error(f"ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"ì¸ê³¼ê´€ê³„ ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # causal_device ë³´ì •: ê°’ì´ í•­ìƒ ì •ì˜ë˜ë„ë¡ ì²˜ë¦¬
            try:
                causal_device = torch.device('cpu')  # ê¸°ë³¸ê°’
                if hasattr(self, 'causal_pipeline') and hasattr(self.causal_pipeline, 'model'):
                    if hasattr(self.causal_pipeline.model, 'device'):
                        causal_device = self.causal_pipeline.model.device
            except Exception:
                causal_device = torch.device('cpu')
            logger.info(f"ì¸ê³¼ê´€ê³„ ëª¨ë¸ ìˆœì°¨ ë¡œë“œ ì™„ë£Œ: {causal_device}")
            
            logger.info("ê³ ê¸‰ AI ëª¨ë¸ë“¤ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # í”„ë¡œì íŠ¸ ê·œì¹™: fallback ì—†ëŠ” ìˆœìˆ˜ ì¬ì‹œë„ ë°©ì‹
            retry_count = 0
            max_retries = 3
            while retry_count < max_retries:
                retry_count += 1
                logger.info(f"ëª¨ë¸ ì´ˆê¸°í™” ì¬ì‹œë„ {retry_count}/{max_retries}")
                try:
                    # ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ íŠ¹í™”)
                    from sentence_transformer_singleton import get_sentence_transformer
                    
                    self.embedding_model = get_sentence_transformer(
                        'paraphrase-multilingual-mpnet-base-v2',
                        device=str(self.device),
                        cache_folder=self.models_dir
                    )
                    
                    # ê°ì • ë¶„ì„ ëª¨ë¸ (ì¬ì‹œë„, gpu_on_demand: CPU ì„ ì ì¬)
                    logger.info("ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì‹œì‘ (gpu_on_demand)")
                    try:
                        # HF ë˜í¼ ìš°ì„  ì‹œë„
                        try:
                            from hf_model_wrapper import wrapped_pipeline
                            self.emotion_pipeline = wrapped_pipeline(
                                task="text-classification",
                                model="j-hartmann/emotion-english-distilroberta-base",
                                owner="semantic_analyzer",
                                device=-1  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                            )
                            logger.info("ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (HF ë˜í¼, CPU)")
                        except ImportError:
                            # ë˜í¼ ì—†ìœ¼ë©´ ì§ì ‘ ë¡œë”©
                            from transformers import pipeline
                            self.emotion_pipeline = pipeline(
                                "text-classification",
                                model="j-hartmann/emotion-english-distilroberta-base",
                                device=-1  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                            )
                            logger.info("ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (ì§ì ‘, CPU)")
                        
                        if self.emotion_pipeline is None:
                            raise RuntimeError("ê°ì • ë¶„ì„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„)")
                        
                        logger.info("âœ… emotion pipeline CPU ì„ ì ì¬ ì™„ë£Œ (ì¬ì‹œë„, gpu_on_demand)")
                        
                    except Exception as e:
                        logger.error(f"ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì‹¤íŒ¨: {e}")
                        raise
                    
                    # ìœ¤ë¦¬ì  ë¶„ë¥˜ ëª¨ë¸ (ì¬ì‹œë„, gpu_on_demand: CPU ì„ ì ì¬)
                    logger.info("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ì¬ì‹œë„ ë¡œë”© ì‹œì‘ (gpu_on_demand)")
                    self.ethical_tokenizer = AutoTokenizer.from_pretrained(
                        'bert-base-uncased',
                        cache_dir=self.models_dir
                    )
                    
                    try:
                        # HF ë˜í¼ ìš°ì„  ì‹œë„
                        try:
                            from hf_model_wrapper import wrapped_from_pretrained
                            self.ethical_model = wrapped_from_pretrained(
                                model_class=AutoModel,
                                model_name='bert-base-uncased',
                                owner="semantic_analyzer",
                                cache_dir=self.models_dir
                            ).to(torch.device('cpu'))  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                            logger.info("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (HF ë˜í¼, CPU)")
                        except ImportError:
                            # ë˜í¼ ì—†ìœ¼ë©´ ì§ì ‘ ë¡œë”©
                            self.ethical_model = AutoModel.from_pretrained(
                                'bert-base-uncased',
                                cache_dir=self.models_dir
                            ).to(torch.device('cpu'))  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                            logger.info("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (ì§ì ‘, CPU)")
                        
                        if self.ethical_model is None:
                            raise RuntimeError("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„)")
                        
                        logger.info("âœ… ethical model CPU ì„ ì ì¬ ì™„ë£Œ (ì¬ì‹œë„, gpu_on_demand)")
                        
                    except Exception as e:
                        logger.error(f"ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ì¬ì‹œë„ ë¡œë”© ì‹¤íŒ¨: {e}")
                        raise
                    
                    # ì¸ê³¼ê´€ê³„ ì¶”ë¡  ëª¨ë¸ (ì¬ì‹œë„, gpu_on_demand: CPU ì„ ì ì¬)
                    logger.info("ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì‹œì‘ (gpu_on_demand)")
                    try:
                        # HF ë˜í¼ ìš°ì„  ì‹œë„
                        try:
                            from hf_model_wrapper import wrapped_pipeline
                            self.causal_pipeline = wrapped_pipeline(
                                task="zero-shot-classification",
                                model="facebook/bart-large-mnli",
                                owner="semantic_analyzer",
                                device=-1  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                            )
                            logger.info("ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (HF ë˜í¼, CPU)")
                        except ImportError:
                            # ë˜í¼ ì—†ìœ¼ë©´ ì§ì ‘ ë¡œë”©
                            from transformers import pipeline
                            self.causal_pipeline = pipeline(
                                "zero-shot-classification",
                                model="facebook/bart-large-mnli",
                                device=-1  # CPUë¡œ ë¨¼ì € ë¡œë“œ (gpu_on_demand)
                            )
                            logger.info("ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (ì§ì ‘, CPU)")
                        
                        if self.causal_pipeline is None:
                            raise RuntimeError("ì¸ê³¼ê´€ê³„ ë¶„ì„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„)")
                        
                        logger.info("âœ… causal pipeline CPU ì„ ì ì¬ ì™„ë£Œ (ì¬ì‹œë„, gpu_on_demand)")
                        
                    except Exception as e:
                        logger.error(f"ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì‹¤íŒ¨: {e}")
                        raise
                    
                    logger.info("ê³ ê¸‰ AI ëª¨ë¸ë“¤ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤ (ì¬ì‹œë„).")
                    break
                    
                except Exception as retry_error:
                    logger.error(f"ì¬ì‹œë„ {retry_count} ì‹¤íŒ¨: {retry_error}")
                    if retry_count >= max_retries:
                        logger.error("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ - ì‹œìŠ¤í…œ ì¢…ë£Œ")
                        raise Exception(f"ëª¨ë¸ ì´ˆê¸°í™” ìµœì¢… ì‹¤íŒ¨: {retry_error}")
                    import time
                    time.sleep(1)  # ì¬ì‹œë„ ê°„ê²©
    
# í”„ë¡œì íŠ¸ ê·œì¹™ ì¤€ìˆ˜: fallback ë©”ì†Œë“œ ì œê±°ë¨ - ìˆœìˆ˜ ì¬ì‹œë„ ë°©ì‹ë§Œ ì‚¬ìš©

# Lazy loading ì œê±° - ëª¨ë“  ëª¨ë¸ì€ ì´ˆê¸°í™” ì‹œ ë¡œë”©ë¨
                retry_count += 1
                logger.info(f"ëª¨ë¸ ì´ˆê¸°í™” ì¬ì‹œë„ {retry_count}/{max_retries}")
                try:
                    # ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ íŠ¹í™”)
                    from sentence_transformer_singleton import get_sentence_transformer
                    
                    self.embedding_model = get_sentence_transformer(
                        'paraphrase-multilingual-mpnet-base-v2',
                        device=str(self.device),
                        cache_folder=self.models_dir
                    )
                    
                    # ê°ì • ë¶„ì„ ëª¨ë¸ (ìˆœì°¨ì  ë¡œë”© ì¬ì‹œë„)
                    def load_semantic_emotion_pipeline_retry():
                        # GPU ë¡œë” ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ í•„ìš”í•œ ëª¨ë“  import í¬í•¨
                        from transformers import pipeline
                        import torch
                        import logging
                        logger = logging.getLogger(__name__)
                        logger.info("GPU ë¡œë”ì—ì„œ ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì‹œì‘")
                        try:
                            # HF ë˜í¼ ìš°ì„  ì‹œë„
                            try:
                                from hf_model_wrapper import wrapped_pipeline
                                model = wrapped_pipeline(
                                    task="text-classification",
                                    model="j-hartmann/emotion-english-distilroberta-base",
                                    owner="semantic_analyzer",
                                    device=0 if torch.cuda.is_available() else -1
                                )
                                logger.info("ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (HF ë˜í¼)")
                            except ImportError:
                                # ë˜í¼ ì—†ìœ¼ë©´ ì§ì ‘ ë¡œë”©
                                model = pipeline(
                                    "text-classification",
                                    model="j-hartmann/emotion-english-distilroberta-base",
                                    device=0 if torch.cuda.is_available() else -1
                                )
                                logger.info("ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (ì§ì ‘)")
                            return model
                        except Exception as e:
                            logger.error(f"ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì‹¤íŒ¨: {e}")
                            raise
                    
                    # GPU ë¡œë” í•„ë“œ ë³´ì¥ í›„ ì‚¬ìš©
                    if self.gpu_loader is None:
                        from config import get_gpu_loader
                        self.gpu_loader = get_gpu_loader()
                    
                    if self.gpu_loader is not None:
                        emotion_device, emotion_model = self.gpu_loader.request_gpu_loading(
                            model_id="semantic_emotion_pipeline_retry",
                            priority=ModelPriority.MEDIUM,
                            estimated_memory_mb=732,
                            loading_function=load_semantic_emotion_pipeline_retry
                        )
                        
                        # gpu_on_demand ì •ì±…: CPU ì„ ì ì¬ í—ˆìš© (ì¬ì‹œë„)
                        if emotion_model is None:
                            logger.warning("ê°ì • ë¶„ì„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ, CPU í´ë°±")
                            emotion_model = load_semantic_emotion_pipeline_retry()
                            emotion_device = torch.device('cpu')
                    else:
                        logger.warning("GPU ë¡œë” ì—†ìŒ, CPU ëª¨ë“œë¡œ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ (ì¬ì‹œë„)")
                        emotion_device = torch.device('cpu')
                        emotion_model = load_semantic_emotion_pipeline_retry()
                    
                    if getattr(emotion_device, "type", "cpu") != "cuda":
                        logger.warning("emotion pipeline on CPU (gpu_on_demand, retry). Will promote via DSM when needed.")
                    self.emotion_pipeline = emotion_model
                    logger.info(f"ê°ì • ë¶„ì„ ëª¨ë¸ ìˆœì°¨ ë¡œë“œ ì™„ë£Œ (ì¬ì‹œë„): {emotion_device}")
                    
                    # ìœ¤ë¦¬ì  ë¶„ë¥˜ ëª¨ë¸ (BERT ê¸°ë°˜)
                    self.ethical_tokenizer = AutoTokenizer.from_pretrained(
                        'bert-base-uncased',
                        cache_dir=self.models_dir
                    )
                    def load_semantic_ethical_model_retry():
                        # GPU ë¡œë” ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ í•„ìš”í•œ ëª¨ë“  import í¬í•¨
                        from transformers import AutoModel
                        import torch
                        import logging
                        logger = logging.getLogger(__name__)
                        logger.info("GPU ë¡œë”ì—ì„œ ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ì¬ì‹œë„ ë¡œë”© ì‹œì‘")
                        try:
                            # HF ë˜í¼ ìš°ì„  ì‹œë„
                            try:
                                from hf_model_wrapper import wrapped_from_pretrained
                                model = wrapped_from_pretrained(
                                    model_class=AutoModel,
                                    model_name='bert-base-uncased',
                                    owner="semantic_analyzer",
                                    cache_dir=models_dir_cache
                                ).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
                                logger.info("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (HF ë˜í¼)")
                            except ImportError:
                                # ë˜í¼ ì—†ìœ¼ë©´ ì§ì ‘ ë¡œë”©
                                model = AutoModel.from_pretrained(
                                    'bert-base-uncased',
                                    cache_dir=models_dir_cache
                                ).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
                                logger.info("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (ì§ì ‘)")
                            return model
                        except Exception as e:
                            logger.error(f"ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ì¬ì‹œë„ ë¡œë”© ì‹¤íŒ¨: {e}")
                            raise
                    
                    # GPU ë¡œë” í•„ë“œ ë³´ì¥ í›„ ì‚¬ìš©
                    if self.gpu_loader is None:
                        from config import get_gpu_loader
                        self.gpu_loader = get_gpu_loader()
                    
                    if self.gpu_loader is not None:
                        ethical_device, ethical_model = self.gpu_loader.request_gpu_loading(
                            model_id="semantic_ethical_model_retry",
                            priority=ModelPriority.LOW,
                            estimated_memory_mb=732,
                            loading_function=load_semantic_ethical_model_retry
                        )
                        
                        # gpu_on_demand ì •ì±…: CPU ì„ ì ì¬ í—ˆìš© (ì¬ì‹œë„)
                        if ethical_model is None:
                            logger.warning("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ, CPU í´ë°±")
                            ethical_model = load_semantic_ethical_model_retry()
                            ethical_device = torch.device('cpu')
                    else:
                        logger.warning("GPU ë¡œë” ì—†ìŒ, CPU ëª¨ë“œë¡œ ìœ¤ë¦¬ í‰ê°€ ëª¨ë¸ ë¡œë“œ (ì¬ì‹œë„)")
                        ethical_device = torch.device('cpu')
                        ethical_model = load_semantic_ethical_model_retry()
                    
                    if getattr(ethical_device, "type", "cpu") != "cuda":
                        logger.warning("ethical model on CPU (gpu_on_demand, retry). Will promote via DSM when needed.")
                    self.ethical_model = ethical_model
                    logger.info(f"ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ ìˆœì°¨ ë¡œë“œ ì™„ë£Œ (ì¬ì‹œë„): {ethical_device}")
                    
                    # ì¸ê³¼ê´€ê³„ ì¶”ë¡  ëª¨ë¸ (ìˆœì°¨ì  ë¡œë”© ì¬ì‹œë„)
                    def load_semantic_causal_pipeline_retry():
                        # GPU ë¡œë” ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ í•„ìš”í•œ ëª¨ë“  import í¬í•¨
                        from transformers import pipeline
                        import torch
                        import logging
                        logger = logging.getLogger(__name__)
                        logger.info("GPU ë¡œë”ì—ì„œ ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì‹œì‘")
                        try:
                            # HF ë˜í¼ ìš°ì„  ì‹œë„
                            try:
                                from hf_model_wrapper import wrapped_pipeline
                                model = wrapped_pipeline(
                                    task="zero-shot-classification",
                                    model="facebook/bart-large-mnli",
                                    owner="semantic_analyzer",
                                    device=0 if torch.cuda.is_available() else -1
                                )
                                logger.info("ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (HF ë˜í¼)")
                            except ImportError:
                                # ë˜í¼ ì—†ìœ¼ë©´ ì§ì ‘ ë¡œë”©
                                model = pipeline(
                                    "zero-shot-classification",
                                    model="facebook/bart-large-mnli",
                                    device=0 if torch.cuda.is_available() else -1
                                )
                                logger.info("ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì„±ê³µ (ì§ì ‘)")
                            return model
                        except Exception as e:
                            logger.error(f"ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¬ì‹œë„ ë¡œë”© ì‹¤íŒ¨: {e}")
                            raise
                    
                    # GPU ë¡œë” í•„ë“œ ë³´ì¥ í›„ ì‚¬ìš©
                    if self.gpu_loader is None:
                        from config import get_gpu_loader
                        self.gpu_loader = get_gpu_loader()
                    
                    if self.gpu_loader is not None:
                        causal_device, causal_model = self.gpu_loader.request_gpu_loading(
                            model_id="semantic_causal_pipeline_retry",
                            priority=ModelPriority.LOW,
                            estimated_memory_mb=732,
                            loading_function=load_semantic_causal_pipeline_retry
                        )
                        
                        # gpu_on_demand ì •ì±…: CPU ì„ ì ì¬ í—ˆìš© (ì¬ì‹œë„)
                        if causal_model is None:
                            logger.warning("ì¸ê³¼ê´€ê³„ ë¶„ì„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ, CPU í´ë°±")
                            causal_model = load_semantic_causal_pipeline_retry()
                            causal_device = torch.device('cpu')
                    else:
                        logger.warning("GPU ë¡œë” ì—†ìŒ, CPU ëª¨ë“œë¡œ ì¸ê³¼ê´€ê³„ ëª¨ë¸ ë¡œë“œ (ì¬ì‹œë„)")
                        causal_device = torch.device('cpu')
                        causal_model = load_semantic_causal_pipeline_retry()
                    
                    if getattr(causal_device, "type", "cpu") != "cuda":
                        logger.warning("causal model on CPU (gpu_on_demand, retry). Will promote via DSM when needed.")
                    self.causal_pipeline = causal_model
                    logger.info(f"ì¸ê³¼ê´€ê³„ ëª¨ë¸ ìˆœì°¨ ë¡œë“œ ì™„ë£Œ (ì¬ì‹œë„): {causal_device}")
                    
                    logger.info(f"ì¬ì‹œë„ {retry_count}: ê³ ê¸‰ AI ëª¨ë¸ë“¤ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break
                    
                except Exception as retry_error:
                    logger.error(f"ì¬ì‹œë„ {retry_count} ì‹¤íŒ¨: {retry_error}")
                    if retry_count >= max_retries:
                        logger.error("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ - ì‹œìŠ¤í…œ ì¢…ë£Œ")
                        raise Exception(f"ëª¨ë¸ ì´ˆê¸°í™” ìµœì¢… ì‹¤íŒ¨: {retry_error}")
                    import time
                    time.sleep(1)  # ì¬ì‹œë„ ê°„ê²©
    
# í”„ë¡œì íŠ¸ ê·œì¹™ ì¤€ìˆ˜: fallback ë©”ì†Œë“œ ì œê±°ë¨ - ìˆœìˆ˜ ì¬ì‹œë„ ë°©ì‹ë§Œ ì‚¬ìš©

# Lazy loading ì œê±° - ëª¨ë“  ëª¨ë¸ì€ ì´ˆê¸°í™” ì‹œ ë¡œë”©ë¨
    
    def _initialize_classifiers(self):
        """ê³ ê¸‰ ë¶„ë¥˜ê¸°ë“¤ ì´ˆê¸°í™”"""
        # í‘œë©´ì  ì˜ë¯¸ ë¶„ë¥˜ê¸°
        self.surface_classifier = MLPClassifier(
            hidden_layer_sizes=(512, 256, 128),
            activation='relu',
            solver='adam',
            random_state=42
        )
        
        # ìœ¤ë¦¬ì  íŒë‹¨ ë¶„ë¥˜ê¸°
        self.ethical_classifier = GradientBoostingClassifier(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=6,
            random_state=42
        )
        
        # ê°ì • ê°•ë„ íšŒê·€ê¸°
        self.emotion_regressor = MLPRegressor(
            hidden_layer_sizes=(256, 128, 64),
            activation='relu',
            solver='adam',
            random_state=42
        )
        
        # ì¸ê³¼ê´€ê³„ ê°•ë„ ì˜ˆì¸¡ê¸°
        self.causal_regressor = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
    
    def _setup_neural_networks_cpu(self):
        """ì»¤ìŠ¤í…€ ì‹ ê²½ë§ ì„¤ì • - CPU ì‘ì—…ë§Œ (ì›Œì»¤ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
        GPU ë¡œë”, torch.cuda, DSM ë“±ë¡ ë“±ì€ ì ˆëŒ€ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
        """
        
        class SemanticFusionNetwork(nn.Module):
            """ì˜ë¯¸ ìœµí•© ì‹ ê²½ë§"""
            def __init__(self, input_dim=768, hidden_dim=512, output_dim=256):
                super().__init__()
                self.fusion_layers = nn.Sequential(
                    nn.Linear(input_dim * 4, hidden_dim),  # 4ê°œ ìˆ˜ì¤€ ìœµí•©
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(hidden_dim, hidden_dim // 2),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(hidden_dim // 2, output_dim),
                    nn.Tanh()
                )
                
            def forward(self, surface, ethical, emotional, causal):
                fused = torch.cat([surface, ethical, emotional, causal], dim=-1)
                return self.fusion_layers(fused)
        
        class CrossLevelAttention(nn.Module):
            """ìˆ˜ì¤€ ê°„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜"""
            def __init__(self, d_model=768, num_heads=8):
                super().__init__()
                self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
                self.norm = nn.LayerNorm(d_model)
                
            def forward(self, query, key, value):
                attn_output, attn_weights = self.attention(query, key, value)
                return self.norm(attn_output + query), attn_weights
        
        # ë„¤íŠ¸ì›Œí¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìˆœì°¨ì  ë¡œë”©)
        def load_semantic_fusion_network():
            # GPU ë¡œë” ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ í•„ìš”í•œ ëª¨ë“  import í¬í•¨
            import torch
            import logging
            logger = logging.getLogger(__name__)
            logger.info("GPU ë¡œë”ì—ì„œ ì˜ë¯¸ ìœµí•© ë„¤íŠ¸ì›Œí¬ ë¡œë”© ì‹œì‘")
            try:
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                fusion_network = SemanticFusionNetwork().to(device)
                cross_attention = CrossLevelAttention().to(device)
                logger.info(f"ì˜ë¯¸ ìœµí•© ë„¤íŠ¸ì›Œí¬ ë¡œë”© ì„±ê³µ (device: {device})")
                return {'fusion_network': fusion_network, 'cross_attention': cross_attention}
            except Exception as e:
                logger.error(f"ì˜ë¯¸ ìœµí•© ë„¤íŠ¸ì›Œí¬ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise
        
        # CPUì—ì„œë§Œ ë„¤íŠ¸ì›Œí¬ ìƒì„± (GPU ì´ë™ì€ ë‚˜ì¤‘ì—)
        self.fusion_network = SemanticFusionNetwork()
        self.cross_attention = CrossLevelAttention()
        logger.info("ì˜ë¯¸ í“¨ì „ ë„¤íŠ¸ì›Œí¬ CPU ìƒì„± ì™„ë£Œ")
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.fusion_optimizer = torch.optim.Adam(
            self.fusion_network.parameters(), 
            lr=self.config.get('learning_rate', 0.001)
        )
        
        logger.info("ì»¤ìŠ¤í…€ ì‹ ê²½ë§ì´ CPUì—ì„œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    async def _setup_neural_networks_gpu(self):
        """ì‹ ê²½ë§ GPU ì´ë™ ë° DSM ë“±ë¡ (ë©”ì¸ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ì‹¤í–‰)
        GPU ë¡œë” í˜¸ì¶œ, ë””ë°”ì´ìŠ¤ ì´ë™, DSM ë“±ë¡ ë“±ì„ ìˆ˜í–‰
        """
        try:
            # DSMì„ í†µí•œ GPU ìŠ¹ê²© ì‹œë„
            from dynamic_swap_manager import get_swap_manager, SwapPriority
            swap = get_swap_manager()
            
            if swap:
                # fusion_network ë“±ë¡
                if hasattr(self, 'fusion_network'):
                    swap.register_model("semantic_fusion", self.fusion_network, priority=SwapPriority.LOW)
                    logger.info("âœ… semantic_fusion DSM ë“±ë¡ ì™„ë£Œ")
                
                # cross_attention ë“±ë¡
                if hasattr(self, 'cross_attention'):
                    swap.register_model("semantic_cross_attention", self.cross_attention, priority=SwapPriority.LOW)
                    logger.info("âœ… semantic_cross_attention DSM ë“±ë¡ ì™„ë£Œ")
                    
                # GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë©´ GPUë¡œ ì´ë™ ì‹œë„
                if torch.cuda.is_available():
                    from config import get_gpu_memory_info
                    gpu_info = get_gpu_memory_info()
                    if gpu_info and gpu_info['free_mb'] > 500:  # 500MB ì´ìƒ ì—¬ìœ  ì‹œ
                        if hasattr(self, 'fusion_network'):
                            self.fusion_network = self.fusion_network.cuda()
                            logger.info("ğŸš€ fusion_network GPUë¡œ ì´ë™ ì„±ê³µ")
                        if hasattr(self, 'cross_attention'):
                            self.cross_attention = self.cross_attention.cuda()
                            logger.info("ğŸš€ cross_attention GPUë¡œ ì´ë™ ì„±ê³µ")
                    else:
                        logger.info("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ CPUì— ìœ ì§€")
            else:
                logger.warning("DSMì„ ì°¾ì„ ìˆ˜ ì—†ì–´ GPU ìŠ¹ê²© ìŠ¤í‚µ")
                
        except Exception as e:
            logger.warning(f"GPU ì„¤ì • ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
    
    async def _ensure_model_on_gpu(self, model_name: str):
        """ëª¨ë¸ì„ GPUë¡œ ìŠ¹ê²© (gpu_on_demand ì •ì±…)
        
        Args:
            model_name: ëª¨ë¸ ì†ì„± ì´ë¦„ ('emotion_pipeline', 'ethical_model', 'causal_pipeline')
        """
        try:
            # DSMì„ í†µí•œ GPU ìŠ¹ê²©
            from dynamic_swap_manager import get_swap_manager
            swap = get_swap_manager()
            
            if swap and hasattr(self, model_name):
                model = getattr(self, model_name)
                
                # ì´ë¯¸ GPUì— ìˆëŠ”ì§€ í™•ì¸
                if hasattr(model, 'device'):
                    if str(model.device).startswith('cuda'):
                        return  # ì´ë¯¸ GPUì— ìˆìŒ
                
                # DSMì„ í†µí•œ GPU ìŠ¹ê²© ì‹œë„
                success = await swap.ensure_on_gpu("semantic_analyzer")
                
                if success:
                    # ëª¨ë¸ì„ GPUë¡œ ì´ë™
                    if hasattr(model, 'to'):
                        setattr(self, model_name, model.to('cuda'))
                        logger.info(f"âœ… {model_name} GPU ìŠ¹ê²© ì™„ë£Œ")
                    elif hasattr(model, 'model') and hasattr(model.model, 'to'):
                        # pipelineì˜ ê²½ìš°
                        model.model = model.model.to('cuda')
                        model.device = 0  # pipeline device ì„¤ì •
                        logger.info(f"âœ… {model_name} pipeline GPU ìŠ¹ê²© ì™„ë£Œ")
                else:
                    logger.warning(f"âš ï¸ {model_name} GPU ìŠ¹ê²© ì‹¤íŒ¨ - CPUì—ì„œ ì‹¤í–‰")
                    
        except Exception as e:
            logger.warning(f"GPU ìŠ¹ê²© ì¤‘ ì˜¤ë¥˜ (CPU ê³„ì†): {e}")
    
    async def analyze_text_advanced(self, text: str, metadata: Dict[str, Any] = None) -> AdvancedSemanticResult:
        """
        ê³ ê¸‰ ë‹¤ì¤‘ìˆ˜ì¤€ ì˜ë¯¸ ë¶„ì„ (ë¹„ë™ê¸°)
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„ ê²°ê³¼
        """
        start_time = time.time()
        self.analysis_state = SemanticAnalysisState()
        
        if not text:
            return self._get_empty_advanced_result(text)
        
        
        # ìºì‹œ í™•ì¸
        cache_key = self._generate_cache_key(text, metadata)
        if cache_key in self.analysis_cache:
            logger.info("ìºì‹œì—ì„œ ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            return self.analysis_cache[cache_key]
        
        try:
            # ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰
            analysis_tasks = [
                self._analyze_surface_level_advanced(text),
                self._analyze_ethical_level_advanced(text),
                self._analyze_emotional_level_advanced(text),
                self._analyze_causal_level_advanced(text)
            ]
            
            # ëª¨ë“  ìˆ˜ì¤€ ë¶„ì„ ì™„ë£Œ ëŒ€ê¸°
            results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
            
            surface_result, ethical_result, emotional_result, causal_result = results
            
            # ì˜ˆì™¸ ì²˜ë¦¬
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    level_name = ['surface', 'ethical', 'emotional', 'causal'][i]
                    logger.error(f"{level_name} ìˆ˜ì¤€ ë¶„ì„ ì‹¤íŒ¨: {result}")
                    self.analysis_state.error_log.append(f"{level_name}: {str(result)}")
            
            # ìˆ˜ì¤€ ê°„ ê´€ê³„ ë¶„ì„
            cross_relations = await self._analyze_cross_level_relations(
                surface_result, ethical_result, emotional_result, causal_result
            )
            
            # ì˜ë¯¸ ìœµí•© ë° ì¢…í•©
            fused_representation = await self._fuse_semantic_levels(
                surface_result, ethical_result, emotional_result, causal_result
            )
            
            # ê³ ê¸‰ ê²°ê³¼ êµ¬ì„±
            advanced_result = AdvancedSemanticResult(
                text=text,
                surface_analysis=surface_result,
                ethical_analysis=ethical_result,
                emotional_analysis=emotional_result,
                causal_analysis=causal_result,
                cross_level_relations=cross_relations,
                fused_representation=fused_representation,
                confidence_score=self._calculate_overall_confidence(),
                processing_time=time.time() - start_time,
                metadata=metadata or {}
            )
            
            # ìºì‹œ ì €ì¥
            self.analysis_cache[cache_key] = advanced_result
            
            return advanced_result
            
        except Exception as e:
            logger.error(f"ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_error_result(text, str(e))
    
    async def _analyze_surface_level_advanced(self, text: str) -> Dict[str, Any]:
        """ê³ ê¸‰ í‘œë©´ì  ìˆ˜ì¤€ ë¶„ì„"""
        self.analysis_state.current_level = "surface"
        
        try:
            # ì„ë² ë”© ìƒì„±
            embedding = await self._get_embedding_async(text)
            
            # í†µê³„ì  íŠ¹ì„± ì¶”ì¶œ
            stats_features = self._extract_statistical_features(text)
            
            # ì–¸ì–´í•™ì  íŠ¹ì„± ë¶„ì„
            linguistic_features = self._analyze_linguistic_features(text)
            
            # TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords_advanced(text)
            
            # ì˜ë¯¸ í´ëŸ¬ìŠ¤í„°ë§
            semantic_clusters = await self._perform_semantic_clustering([text])
            
            result = {
                'text_length': len(text),
                'word_count': len(text.split()),
                'sentence_count': len(re.findall(r'[.!?]+', text)),
                'embedding': embedding.tolist(),  # embeddingì€ í•„ìˆ˜ - Noneì´ë©´ ì‹¤íŒ¨
                'statistical_features': stats_features,
                'linguistic_features': linguistic_features,
                'keywords': keywords,
                'semantic_clusters': semantic_clusters,
                'complexity_score': self._calculate_complexity_score(text),
                'readability_score': self._calculate_readability_score(text),
                'confidence': 0.85
            }
            
            self.analysis_state.confidence_scores['surface'] = result['confidence']
            return result
            
        except Exception as e:
            logger.error(f"í‘œë©´ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'confidence': 0.0}
    
    async def _analyze_ethical_level_advanced(self, text: str) -> Dict[str, Any]:
        """ê³ ê¸‰ ìœ¤ë¦¬ì  ìˆ˜ì¤€ ë¶„ì„"""
        self.analysis_state.current_level = "ethical"
        
        try:
            # ìœ¤ë¦¬ì  í‚¤ì›Œë“œ ê°ì§€
            ethical_keywords = self._detect_ethical_keywords(text)
            
            # ë„ë•ì  ê°ì • ë¶„ì„
            moral_emotions = await self._analyze_moral_emotions(text)
            
            # ê¶Œë¦¬ì™€ ì˜ë¬´ ë¶„ì„
            rights_duties = self._analyze_rights_duties(text)
            
            # ê²°ê³¼ì£¼ì˜ vs ì˜ë¬´ë¡ ì  ë¶„ì„
            ethical_framework = await self._classify_ethical_framework(text)
            
            # ë„ë•ì  ê°ˆë“± ê°ì§€
            moral_conflicts = self._detect_moral_conflicts(text)
            
            # BERT ê¸°ë°˜ ìœ¤ë¦¬ì  ì„ë² ë”©
            ethical_embedding = await self._get_ethical_embedding(text)
            
            result = {
                'ethical_keywords': ethical_keywords,
                'moral_emotions': moral_emotions,
                'rights_duties': rights_duties,
                'ethical_framework': ethical_framework,
                'moral_conflicts': moral_conflicts,
                'ethical_embedding': ethical_embedding.tolist(),  # ethical_embeddingì€ í•„ìˆ˜ - Noneì´ë©´ ì‹¤íŒ¨
                'moral_judgment': self._make_moral_judgment(text),
                'ethical_dimensions': self._analyze_ethical_dimensions(text),
                'confidence': 0.78
            }
            
            self.analysis_state.confidence_scores['ethical'] = result['confidence']
            return result
            
        except Exception as e:
            logger.error(f"ìœ¤ë¦¬ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'confidence': 0.0}
    
    async def _analyze_emotional_level_advanced(self, text: str) -> Dict[str, Any]:
        """ê³ ê¸‰ ê°ì •ì  ìˆ˜ì¤€ ë¶„ì„"""
        self.analysis_state.current_level = "emotional"
        
        try:
            # ë©€í‹° ëª¨ë¸ ê°ì • ë¶„ì„ - emotion_pipelineì€ í•„ìˆ˜
            if not self.emotion_pipeline:
                raise RuntimeError("ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            
            # GPU on-demand: í•„ìš”ì‹œ GPUë¡œ ìŠ¹ê²©
            await self._ensure_model_on_gpu('emotion_pipeline')
            
            emotion_result = self.emotion_pipeline(text)
            primary_emotion = emotion_result[0]['label']
            emotion_confidence = emotion_result[0]['score']
            
            # ê°ì • ê°•ë„ ë¶„ì„
            emotion_intensity = self._calculate_emotion_intensity(text)
            
            # ê°ì • ê·¹ì„± ë¶„ì„ (Valence-Arousal)
            valence_arousal = self._analyze_valence_arousal(text)
            
            # ë³µí•© ê°ì • ê°ì§€
            complex_emotions = self._detect_complex_emotions(text)
            
            # ê°ì • ë³€í™” ì¶”ì  (ë¬¸ì¥ ë‹¨ìœ„)
            emotion_dynamics = self._track_emotion_dynamics(text)
            
            # ê°ì • ì„ë² ë”©
            emotion_embedding = await self._get_emotion_embedding(text)
            
            result = {
                'primary_emotion': primary_emotion,
                'emotion_confidence': emotion_confidence,
                'emotion_intensity': emotion_intensity,
                'valence': valence_arousal['valence'],
                'arousal': valence_arousal['arousal'],
                'complex_emotions': complex_emotions,
                'emotion_dynamics': emotion_dynamics,
                'emotion_embedding': emotion_embedding.tolist(),  # emotion_embeddingì€ í•„ìˆ˜ - Noneì´ë©´ ì‹¤íŒ¨
                'emotional_stability': self._calculate_emotional_stability(emotion_dynamics),
                'confidence': 0.82
            }
            
            self.analysis_state.confidence_scores['emotional'] = result['confidence']
            return result
            
        except Exception as e:
            logger.error(f"ê°ì •ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'confidence': 0.0}
    
    async def _analyze_causal_level_advanced(self, text: str) -> Dict[str, Any]:
        """ê³ ê¸‰ ì¸ê³¼ì  ìˆ˜ì¤€ ë¶„ì„"""
        self.analysis_state.current_level = "causal"
        
        try:
            # ì¸ê³¼ê´€ê³„ í‚¤ì›Œë“œ ê°ì§€
            causal_keywords = self._detect_causal_keywords(text)
            
            # ì›ì¸-ê²°ê³¼ ê´€ê³„ ì¶”ì¶œ
            cause_effect_pairs = self._extract_cause_effect_pairs(text)
            
            # ì¡°ê±´ë¶€ ê´€ê³„ ë¶„ì„
            conditional_relations = self._analyze_conditional_relations(text)
            
            # ì‹œê°„ì  ìˆœì„œ ë¶„ì„
            temporal_order = self._analyze_temporal_order(text)
            
            # ì¸ê³¼ê´€ê³„ ê°•ë„ ê³„ì‚°
            causal_strength = self._calculate_causal_strength(text)
            
            # ì¸ê³¼ ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
            causal_network = self._build_causal_network(cause_effect_pairs)
            
            # Zero-shot ì¸ê³¼ê´€ê³„ ë¶„ë¥˜ - causal_pipelineì€ í•„ìˆ˜
            if not self.causal_pipeline:
                raise RuntimeError("ì¸ê³¼ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            
            # GPU on-demand: í•„ìš”ì‹œ GPUë¡œ ìŠ¹ê²©
            await self._ensure_model_on_gpu('causal_pipeline')
            
            causal_labels = ["cause", "effect", "correlation", "independence"]
            causal_classification = self.causal_pipeline(text, causal_labels)
            
            result = {
                'causal_keywords': causal_keywords,
                'cause_effect_pairs': cause_effect_pairs,
                'conditional_relations': conditional_relations,
                'temporal_order': temporal_order,
                'causal_strength': causal_strength,
                'causal_network': causal_network,
                'causal_classification': causal_classification,
                'causal_confidence': self._calculate_causal_confidence(causal_keywords, cause_effect_pairs),
                'confidence': 0.75
            }
            
            self.analysis_state.confidence_scores['causal'] = result['confidence']
            return result
            
        except Exception as e:
            logger.error(f"ì¸ê³¼ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'confidence': 0.0}
    
    async def _analyze_cross_level_relations(self, surface, ethical, emotional, causal) -> List[CrossLevelSemanticRelation]:
        """ìˆ˜ì¤€ ê°„ ì˜ë¯¸ ê´€ê³„ ë¶„ì„"""
        relations = []
        
        try:
            # ê° ìˆ˜ì¤€ ì¡°í•©ì— ëŒ€í•´ ê´€ê³„ ë¶„ì„
            level_pairs = [
                ('surface', 'ethical', surface, ethical),
                ('surface', 'emotional', surface, emotional),
                ('surface', 'causal', surface, causal),
                ('ethical', 'emotional', ethical, emotional),
                ('ethical', 'causal', ethical, causal),
                ('emotional', 'causal', emotional, causal)
            ]
            
            for source_level, target_level, source_data, target_data in level_pairs:
                relation = await self._compute_cross_level_relation(
                    source_level, target_level, source_data, target_data
                )
                if relation:
                    relations.append(relation)
            
            return relations
            
        except Exception as e:
            logger.error(f"ìˆ˜ì¤€ ê°„ ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    async def _compute_cross_level_relation(self, source_level: str, target_level: str, 
                                         source_data: Dict, target_data: Dict) -> Optional[CrossLevelSemanticRelation]:
        """ë‘ ìˆ˜ì¤€ ê°„ì˜ êµ¬ì²´ì  ê´€ê³„ ê³„ì‚°"""
        
        if 'error' in source_data or 'error' in target_data:
            return None
        
        try:
            # ì‹ ë¢°ë„ ê¸°ë°˜ ê´€ê³„ ê°•ë„ ê³„ì‚°
            source_confidence = source_data.get('confidence', 0.0)
            target_confidence = target_data.get('confidence', 0.0)
            base_strength = (source_confidence + target_confidence) / 2
            
            # ê´€ê³„ ìœ í˜• ê²°ì • ë¡œì§
            relation_type = "neutral"
            evidence = []
            
            # íŠ¹ì • ìˆ˜ì¤€ ì¡°í•©ë³„ ê´€ê³„ ë¶„ì„
            if source_level == "emotional" and target_level == "ethical":
                # ê°ì •ê³¼ ìœ¤ë¦¬ì˜ ê´€ê³„
                emotion = source_data.get('primary_emotion', 'NEUTRAL')
                moral_judgment = target_data.get('moral_judgment', 'neutral')
                
                if emotion in ['JOY', 'LOVE'] and moral_judgment == 'positive':
                    relation_type = "supports"
                    evidence.append("ê¸ì •ì  ê°ì •ì´ ê¸ì •ì  ë„ë• íŒë‹¨ì„ ì§€ì§€")
                elif emotion in ['ANGER', 'FEAR'] and moral_judgment == 'negative':
                    relation_type = "supports" 
                    evidence.append("ë¶€ì •ì  ê°ì •ì´ ë¶€ì •ì  ë„ë• íŒë‹¨ê³¼ ì¼ì¹˜")
                elif emotion in ['JOY'] and moral_judgment == 'negative':
                    relation_type = "contradicts"
                    evidence.append("ê¸ì •ì  ê°ì •ê³¼ ë¶€ì •ì  ë„ë• íŒë‹¨ ê°„ ëª¨ìˆœ")
            
            elif source_level == "causal" and target_level == "ethical":
                # ì¸ê³¼ê´€ê³„ì™€ ìœ¤ë¦¬ì˜ ê´€ê³„
                causal_strength = source_data.get('causal_strength', 0.0)
                moral_conflicts = target_data.get('moral_conflicts', [])
                
                if causal_strength > 0.7 and len(moral_conflicts) > 0:
                    relation_type = "amplifies"
                    evidence.append("ê°•í•œ ì¸ê³¼ê´€ê³„ê°€ ë„ë•ì  ê°ˆë“±ì„ ì¦í­")
            
            # ê¸°ë³¸ ê´€ê³„ ì„¤ì •
            if relation_type == "neutral":
                if base_strength > 0.7:
                    relation_type = "supports"
                    evidence.append("ë†’ì€ ì‹ ë¢°ë„ ê¸°ë°˜ ì§€ì§€ ê´€ê³„")
            
            return CrossLevelSemanticRelation(
                source_level=source_level,
                target_level=target_level,
                relation_type=relation_type,
                strength=base_strength,
                evidence=evidence,
                confidence=min(source_confidence, target_confidence)
            )
            
        except Exception as e:
            logger.error(f"ìˆ˜ì¤€ ê°„ ê´€ê³„ ê³„ì‚° ì‹¤íŒ¨ ({source_level}-{target_level}): {e}")
            return None
    
    async def _fuse_semantic_levels(self, surface, ethical, emotional, causal) -> Dict[str, Any]:
        """ì˜ë¯¸ ìˆ˜ì¤€ë“¤ì„ ê³ ê¸‰ ì‹ ê²½ë§ìœ¼ë¡œ ìœµí•©"""
        try:
            # ê° ìˆ˜ì¤€ì˜ ì„ë² ë”© ì¶”ì¶œ
            embeddings = []
            
            for level_data in [surface, ethical, emotional, causal]:
                if 'error' in level_data:
                    # ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš° 0 ë²¡í„° ì‚¬ìš©
                    embeddings.append(torch.zeros(768))
                else:
                    # ì„ë² ë”©ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ 0 ë²¡í„°
                    emb_key = None
                    for key in ['embedding', 'ethical_embedding', 'emotion_embedding']:
                        if key in level_data and level_data[key] is not None:
                            emb_key = key
                            break
                    
                    if not emb_key:
                        raise RuntimeError(f"ìˆ˜ì¤€ë³„ ì„ë² ë”©ì´ ì—†ìŒ: {level_data.keys()}")
                    embedding = torch.tensor(level_data[emb_key][:768])  # ì°¨ì› ë§ì¶¤
                    
                    embeddings.append(embedding)
            
            # ì„ë² ë”©ë“¤ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            # Fusion networkì™€ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            fusion_device = next(self.fusion_network.parameters()).device
            embeddings = [emb.to(fusion_device) for emb in embeddings]
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            embeddings = [emb.unsqueeze(0) for emb in embeddings]
            
            # ìœµí•© ë„¤íŠ¸ì›Œí¬ ì ìš©
            with torch.no_grad():
                fused_repr = self.fusion_network(*embeddings)
                fused_repr = fused_repr.squeeze(0).cpu().numpy()
            
            # í¬ë¡œìŠ¤ ì–´í…ì…˜ ì ìš©
            attention_weights = await self._compute_cross_attention(embeddings)
            
            return {
                'fused_embedding': fused_repr.tolist(),
                'attention_weights': attention_weights,
                'fusion_confidence': self._calculate_fusion_confidence(surface, ethical, emotional, causal),
                'semantic_coherence': self._calculate_semantic_coherence(surface, ethical, emotional, causal)
            }
            
        except Exception as e:
            logger.error(f"ì˜ë¯¸ ìœµí•© ì‹¤íŒ¨: {e}")
            return {
                'fused_embedding': None,
                'attention_weights': None,
                'fusion_confidence': 0.0,
                'semantic_coherence': 0.0,
                'error': str(e)
            }
    
    async def _compute_cross_attention(self, embeddings: List[torch.Tensor]) -> Dict[str, Any]:
        """í¬ë¡œìŠ¤ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        try:
            # ëª¨ë“  ì„ë² ë”©ì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ê²°í•©
            combined = torch.cat(embeddings, dim=1)  # [1, 4, 768]
            
            with torch.no_grad():
                attended, weights = self.cross_attention(combined, combined, combined)
            
            # ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì¤€ë³„ë¡œ ë¶„ë¦¬
            level_names = ['surface', 'ethical', 'emotional', 'causal']
            attention_matrix = {}
            
            weights_np = weights.squeeze(0).cpu().numpy()  # [4, 4]
            
            for i, source in enumerate(level_names):
                attention_matrix[source] = {}
                for j, target in enumerate(level_names):
                    attention_matrix[source][target] = float(weights_np[i, j])
            
            return {
                'attention_matrix': attention_matrix,
                'dominant_relations': self._identify_dominant_relations(attention_matrix)
            }
            
        except Exception as e:
            logger.error(f"í¬ë¡œìŠ¤ ì–´í…ì…˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'attention_matrix': None, 'dominant_relations': []}
    
    def _identify_dominant_relations(self, attention_matrix: Dict[str, Dict[str, float]]) -> List[str]:
        """ì§€ë°°ì ì¸ ì–´í…ì…˜ ê´€ê³„ ì‹ë³„"""
        dominant_relations = []
        threshold = 0.3
        
        for source, targets in attention_matrix.items():
            for target, weight in targets.items():
                if source != target and weight > threshold:
                    dominant_relations.append(f"{source} â†’ {target} ({weight:.3f})")
        
        return sorted(dominant_relations, key=lambda x: float(x.split('(')[1].split(')')[0]), reverse=True)
    
    # í—¬í¼ ë©”ì„œë“œë“¤
    async def _get_embedding_async(self, text: str) -> np.ndarray:
        """ë¹„ë™ê¸° ì„ë² ë”© ìƒì„±"""
        if self.embedding_model is None:
            raise RuntimeError("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        
        try:
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(
                None, lambda: self.embedding_model.encode(text)
            )
            return embedding
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_statistical_features(self, text: str) -> Dict[str, float]:
        """í†µê³„ì  íŠ¹ì„± ì¶”ì¶œ"""
        words = text.split()
        sentences = re.split(r'[.!?]+', text)
        
        return {
            'avg_word_length': np.mean([len(word) for word in words]) if words else 0,
            'avg_sentence_length': np.mean([len(sent.split()) for sent in sentences if sent.strip()]),
            'lexical_diversity': len(set(words)) / len(words) if words else 0,
            'punctuation_ratio': len(re.findall(r'[^\w\s]', text)) / len(text) if text else 0
        }
    
    def _analyze_linguistic_features(self, text: str) -> Dict[str, Any]:
        """ì–¸ì–´í•™ì  íŠ¹ì„± ë¶„ì„"""
        return {
            'contains_questions': '?' in text,
            'contains_exclamations': '!' in text,
            'contains_quotes': '"' in text or "'" in text,
            'uppercase_ratio': sum(1 for c in text if c.isupper()) / len(text) if text else 0,
            'digit_ratio': sum(1 for c in text if c.isdigit()) / len(text) if text else 0
        }
    
    def _extract_keywords_advanced(self, text: str) -> List[str]:
        """ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # TF-IDF ë²¡í„°ë¼ì´ì €
            vectorizer = TfidfVectorizer(
                max_features=10,
                stop_words='english',
                ngram_range=(1, 2)
            )
            
            # ë‹¨ì¼ ë¬¸ì„œì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê¸°
            tfidf_matrix = vectorizer.fit_transform([text])
            feature_names = vectorizer.get_feature_names_out()
            
            # TF-IDF ì ìˆ˜ ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œ
            scores = tfidf_matrix.toarray()[0]
            keyword_scores = list(zip(feature_names, scores))
            keyword_scores.sort(key=lambda x: x[1], reverse=True)
            
            return [kw for kw, score in keyword_scores if score > 0][:5]
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def _perform_semantic_clustering(self, texts: List[str]) -> Dict[str, Any]:
        """ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë§"""
        if len(texts) < 2:
            return {'clusters': [], 'cluster_centers': []}
        
        try:
            if self.embedding_model:
                embeddings = self.embedding_model.encode(texts)
                
                # K-means í´ëŸ¬ìŠ¤í„°ë§
                n_clusters = min(3, len(texts))
                kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                cluster_labels = kmeans.fit_predict(embeddings)
                
                return {
                    'clusters': cluster_labels.tolist(),
                    'cluster_centers': kmeans.cluster_centers_.tolist(),
                    'inertia': float(kmeans.inertia_)
                }
            else:
                return {'clusters': [], 'cluster_centers': []}
                
        except Exception as e:
            logger.error(f"ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return {'clusters': [], 'cluster_centers': []}
    
    def _calculate_complexity_score(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°"""
        words = text.split()
        sentences = re.split(r'[.!?]+', text)
        
        if not words or not sentences:
            return 0.0
        
        # ë³µì¡ë„ ì§€í‘œë“¤
        avg_word_length = np.mean([len(word) for word in words])
        avg_sentence_length = len(words) / len([s for s in sentences if s.strip()])
        unique_words_ratio = len(set(words)) / len(words)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë³µì¡ë„ ê³„ì‚°
        complexity = (
            (avg_word_length / 10) * 0.3 +
            (avg_sentence_length / 20) * 0.4 +
            unique_words_ratio * 0.3
        )
        
        return min(1.0, complexity)
    
    def _calculate_readability_score(self, text: str) -> float:
        """ê°€ë…ì„± ì ìˆ˜ ê³„ì‚° (Flesch Reading Ease ë³€í˜•)"""
        words = text.split()
        sentences = re.split(r'[.!?]+', text)
        syllables = sum(self._count_syllables(word) for word in words)
        
        if not sentences or not words:
            return 0.5
        
        avg_sentence_length = len(words) / len([s for s in sentences if s.strip()])
        avg_syllables_per_word = syllables / len(words)
        
        # ê°„ë‹¨í•œ ê°€ë…ì„± ê³µì‹
        readability = 1.0 - (avg_sentence_length * 0.05 + avg_syllables_per_word * 0.1)
        return max(0.0, min(1.0, readability))
    
    def _count_syllables(self, word: str) -> int:
        """ë‹¨ì–´ì˜ ìŒì ˆ ìˆ˜ ì¶”ì •"""
        word = word.lower().strip(".,!?;:")
        if not word:
            return 0
        
        # ê°„ë‹¨í•œ ìŒì ˆ ì¹´ìš´íŒ… ê·œì¹™
        vowels = "aeiouy"
        syllable_count = 0
        prev_was_vowel = False
        
        for char in word:
            if char in vowels:
                if not prev_was_vowel:
                    syllable_count += 1
                prev_was_vowel = True
            else:
                prev_was_vowel = False
        
        # ìµœì†Œ 1ìŒì ˆ
        return max(1, syllable_count)
    
    def _detect_ethical_keywords(self, text: str) -> List[str]:
        """ìœ¤ë¦¬ì  í‚¤ì›Œë“œ ê°ì§€"""
        ethical_keywords = [
            'ê¶Œë¦¬', 'ì˜ë¬´', 'ì •ì˜', 'ê³µì •', 'ì±…ì„', 'ë„ë•', 'ìœ¤ë¦¬', 'ì„ ', 'ì•…',
            'ì˜³ë‹¤', 'ê·¸ë¥´ë‹¤', 'í•´ì•¼', 'í•˜ì§€ë§ì•„ì•¼', 'ì˜¬ë°”ë¥¸', 'ì˜ëª»ëœ',
            'right', 'wrong', 'should', 'ought', 'must', 'duty', 'moral',
            'ethical', 'justice', 'fair', 'unfair', 'responsibility'
        ]
        
        found_keywords = []
        text_lower = text.lower()
        
        for keyword in ethical_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    async def _analyze_moral_emotions(self, text: str) -> Dict[str, float]:
        """ë„ë•ì  ê°ì • ë¶„ì„"""
        moral_emotions = {
            'guilt': 0.0, 'shame': 0.0, 'pride': 0.0, 
            'indignation': 0.0, 'compassion': 0.0, 'disgust': 0.0
        }
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ë„ë•ì  ê°ì • ê°ì§€
        emotion_keywords = {
            'guilt': ['ì£„ì±…ê°', 'ë¯¸ì•ˆ', 'ì˜ëª»', 'guilt', 'sorry', 'regret'],
            'shame': ['ë¶€ë„ëŸ½', 'ì°½í”¼', 'shame', 'embarrassed'],
            'pride': ['ìë‘', 'ë¿Œë“¯', 'proud', 'pride'],
            'indignation': ['ë¶„ë…¸', 'ì–µìš¸', 'angry', 'unfair', 'outrage'],
            'compassion': ['ë™ì •', 'ì—°ë¯¼', 'ì•ˆíƒ€ê¹', 'compassion', 'sympathy', 'pity'],
            'disgust': ['í˜ì˜¤', 'ì—­ê²¨ìš´', 'disgust', 'disgusting']
        }
        
        text_lower = text.lower()
        for emotion, keywords in emotion_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    moral_emotions[emotion] += 0.3
        
        # ì •ê·œí™”
        for emotion in moral_emotions:
            moral_emotions[emotion] = min(1.0, moral_emotions[emotion])
        
        return moral_emotions
    
    def _analyze_rights_duties(self, text: str) -> Dict[str, List[str]]:
        """ê¶Œë¦¬ì™€ ì˜ë¬´ ë¶„ì„"""
        rights_keywords = ['ê¶Œë¦¬', 'ììœ ', 'ìê²©', 'right', 'freedom', 'liberty', 'entitled']
        duties_keywords = ['ì˜ë¬´', 'ì±…ì„', 'í•´ì•¼', 'duty', 'obligation', 'responsibility', 'must', 'should']
        
        rights = []
        duties = []
        
        text_lower = text.lower()
        
        for keyword in rights_keywords:
            if keyword in text_lower:
                rights.append(keyword)
        
        for keyword in duties_keywords:
            if keyword in text_lower:
                duties.append(keyword)
        
        return {'rights': rights, 'duties': duties}
    
    async def _classify_ethical_framework(self, text: str) -> Dict[str, float]:
        """ìœ¤ë¦¬ì  í”„ë ˆì„ì›Œí¬ ë¶„ë¥˜"""
        frameworks = {
            'consequentialist': 0.0,  # ê²°ê³¼ì£¼ì˜
            'deontological': 0.0,     # ì˜ë¬´ë¡ 
            'virtue_ethics': 0.0      # ë•ìœ¤ë¦¬
        }
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        consequentialist_keywords = ['ê²°ê³¼', 'íš¨ê³¼', 'ì´ìµ', 'ì†í•´', 'result', 'consequence', 'outcome', 'benefit']
        deontological_keywords = ['ì˜ë¬´', 'ê·œì¹™', 'ë²•', 'ì›ì¹™', 'duty', 'rule', 'law', 'principle']
        virtue_keywords = ['ë•', 'ì„±í’ˆ', 'ì¸ê²©', 'ì„ ëŸ‰', 'virtue', 'character', 'integrity', 'honor']
        
        text_lower = text.lower()
        
        for keyword in consequentialist_keywords:
            if keyword in text_lower:
                frameworks['consequentialist'] += 0.2
        
        for keyword in deontological_keywords:
            if keyword in text_lower:
                frameworks['deontological'] += 0.2
        
        for keyword in virtue_keywords:
            if keyword in text_lower:
                frameworks['virtue_ethics'] += 0.2
        
        # ì •ê·œí™”
        total = sum(frameworks.values())
        if total > 0:
            for framework in frameworks:
                frameworks[framework] /= total
        
        return frameworks
    
    def _detect_moral_conflicts(self, text: str) -> List[str]:
        """ë„ë•ì  ê°ˆë“± ê°ì§€"""
        conflict_indicators = [
            'ë”œë ˆë§ˆ', 'ê°ˆë“±', 'ëª¨ìˆœ', 'ì–´ë ¤ìš´', 'í˜ë“ ', 
            'dilemma', 'conflict', 'contradiction', 'difficult', 'torn'
        ]
        
        conflicts = []
        text_lower = text.lower()
        
        for indicator in conflict_indicators:
            if indicator in text_lower:
                conflicts.append(f"ë„ë•ì  ê°ˆë“± ê°ì§€: '{indicator}' í‚¤ì›Œë“œ")
        
        return conflicts
    
    async def _get_ethical_embedding(self, text: str) -> np.ndarray:
        """ìœ¤ë¦¬ì  ì„ë² ë”© ìƒì„±"""
        if self.ethical_model is None:
            raise RuntimeError("ìœ¤ë¦¬ ë¶„ì„ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        
        # GPU on-demand: í•„ìš”ì‹œ GPUë¡œ ìŠ¹ê²©
        await self._ensure_model_on_gpu('ethical_model')
        
        try:
            # BERT í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì‚¬ìš©
            inputs = self.ethical_tokenizer(
                text, return_tensors='pt', 
                max_length=512, truncation=True, padding=True
            )
            # Ethical modelê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            ethical_device = next(self.ethical_model.parameters()).device
            inputs = {k: v.to(ethical_device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.ethical_model(**inputs)
                # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            return embedding.squeeze()
            
        except Exception as e:
            logger.error(f"ìœ¤ë¦¬ì  ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _make_moral_judgment(self, text: str) -> str:
        """ë„ë•ì  íŒë‹¨"""
        positive_keywords = ['ì¢‹ë‹¤', 'ì˜³ë‹¤', 'ì„ ', 'ë°”ëŒì§', 'good', 'right', 'moral', 'ethical']
        negative_keywords = ['ë‚˜ì˜ë‹¤', 'ê·¸ë¥´ë‹¤', 'ì•…', 'ë°”ëŒì§í•˜ì§€ì•Šì€', 'bad', 'wrong', 'immoral', 'unethical']
        
        text_lower = text.lower()
        
        positive_count = sum(1 for kw in positive_keywords if kw in text_lower)
        negative_count = sum(1 for kw in negative_keywords if kw in text_lower)
        
        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'
    
    def _analyze_ethical_dimensions(self, text: str) -> Dict[str, float]:
        """ìœ¤ë¦¬ì  ì°¨ì› ë¶„ì„"""
        dimensions = {
            'harm_care': 0.0,       # í”¼í•´/ë³´ì‚´í•Œ
            'fairness_cheating': 0.0, # ê³µì •/ë¶€ì •
            'loyalty_betrayal': 0.0,   # ì¶©ì„±/ë°°ì‹ 
            'authority_subversion': 0.0, # ê¶Œìœ„/ì „ë³µ
            'sanctity_degradation': 0.0  # ì‹ ì„±/íƒ€ë½
        }
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì°¨ì› ë¶„ì„
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['í•´', 'ìƒì²˜', 'harm', 'hurt', 'care', 'protect']):
            dimensions['harm_care'] = 0.7
        
        if any(word in text_lower for word in ['ê³µì •', 'ë¶ˆê³µì •', 'fair', 'unfair', 'justice']):
            dimensions['fairness_cheating'] = 0.7
        
        if any(word in text_lower for word in ['ì¶©ì„±', 'ë°°ì‹ ', 'loyal', 'betray', 'faithful']):
            dimensions['loyalty_betrayal'] = 0.7
        
        if any(word in text_lower for word in ['ê¶Œìœ„', 'ë³µì¢…', 'authority', 'obey', 'respect']):
            dimensions['authority_subversion'] = 0.7
        
        if any(word in text_lower for word in ['ì‹ ì„±', 'ìˆœìˆ˜', 'sacred', 'pure', 'holy']):
            dimensions['sanctity_degradation'] = 0.7
        
        return dimensions
    
    def _calculate_emotion_intensity(self, text: str) -> float:
        """ê°ì • ê°•ë„ ê³„ì‚°"""
        intensity_indicators = {
            'very': 0.8, 'extremely': 0.9, 'incredibly': 0.9,
            'quite': 0.6, 'rather': 0.5, 'somewhat': 0.4,
            'ë§¤ìš°': 0.8, 'ì •ë§': 0.7, 'ë„ˆë¬´': 0.8, 'ì¡°ê¸ˆ': 0.3
        }
        
        text_lower = text.lower()
        max_intensity = 0.5  # ê¸°ë³¸ ê°•ë„
        
        for indicator, intensity in intensity_indicators.items():
            if indicator in text_lower:
                max_intensity = max(max_intensity, intensity)
        
        # ëŠë‚Œí‘œ ê°œìˆ˜ë„ ê³ ë ¤
        exclamation_count = text.count('!')
        max_intensity = min(1.0, max_intensity + exclamation_count * 0.1)
        
        return max_intensity
    
    def _analyze_valence_arousal(self, text: str) -> Dict[str, float]:
        """ê°ì • ê·¹ì„±(Valence)ê³¼ ê°ì„±ë„(Arousal) ë¶„ì„"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
        positive_words = ['ì¢‹ë‹¤', 'í–‰ë³µ', 'ê¸°ì˜ë‹¤', 'happy', 'good', 'love', 'excellent']
        negative_words = ['ë‚˜ì˜ë‹¤', 'ìŠ¬í”„ë‹¤', 'í™”ë‚˜ë‹¤', 'sad', 'angry', 'bad', 'hate']
        
        high_arousal_words = ['í¥ë¶„', 'ì—´ì •', 'excited', 'passionate', 'intense', 'thrilled']
        low_arousal_words = ['í‰ì˜¨', 'ì°¨ë¶„', 'calm', 'peaceful', 'relaxed', 'serene']
        
        text_lower = text.lower()
        
        # Valence ê³„ì‚°
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count + negative_count > 0:
            valence = (positive_count - negative_count) / (positive_count + negative_count)
        else:
            valence = 0.0
        
        # Arousal ê³„ì‚°
        high_arousal_count = sum(1 for word in high_arousal_words if word in text_lower)
        low_arousal_count = sum(1 for word in low_arousal_words if word in text_lower)
        
        if high_arousal_count + low_arousal_count > 0:
            arousal = (high_arousal_count - low_arousal_count) / (high_arousal_count + low_arousal_count)
        else:
            arousal = 0.0
        
        return {
            'valence': valence,   # -1 (ë§¤ìš° ë¶€ì •) ~ 1 (ë§¤ìš° ê¸ì •)
            'arousal': arousal    # -1 (ë§¤ìš° ë‚®ìŒ) ~ 1 (ë§¤ìš° ë†’ìŒ)
        }
    
    def _detect_complex_emotions(self, text: str) -> List[Dict[str, Any]]:
        """ë³µí•© ê°ì • ê°ì§€"""
        complex_emotions = []
        
        # ë³µí•© ê°ì • íŒ¨í„´
        patterns = [
            {
                'name': 'bittersweet',
                'keywords': ['ì”ì“¸', 'ë‹¬ì½¤ìŒ‰ìŒ€', 'bittersweet'],
                'components': ['sadness', 'happiness']
            },
            {
                'name': 'nostalgic',
                'keywords': ['ê·¸ë¦¬ìš´', 'ê·¸ë¦½ë‹¤', 'nostalgic', 'miss'],
                'components': ['sadness', 'love']
            },
            {
                'name': 'anxious_excitement',
                'keywords': ['ë–¨ë¦°ë‹¤', 'ê¸´ì¥', 'nervous', 'anxious', 'excited'],
                'components': ['fear', 'joy']
            }
        ]
        
        text_lower = text.lower()
        
        for pattern in patterns:
            for keyword in pattern['keywords']:
                if keyword in text_lower:
                    complex_emotions.append({
                        'emotion': pattern['name'],
                        'components': pattern['components'],
                        'confidence': 0.6
                    })
                    break
        
        return complex_emotions
    
    def _track_emotion_dynamics(self, text: str) -> List[Dict[str, Any]]:
        """ê°ì • ë³€í™” ì¶”ì  (ë¬¸ì¥ ë‹¨ìœ„)"""
        sentences = re.split(r'[.!?]+', text)
        emotion_timeline = []
        
        for i, sentence in enumerate(sentences):
            if sentence.strip():
                # ê° ë¬¸ì¥ì˜ ê°ì • ë¶„ì„
                sentence_emotion = self._simple_emotion_analysis(sentence)
                emotion_timeline.append({
                    'sentence_index': i,
                    'sentence': sentence.strip(),
                    'emotion': sentence_emotion,
                    'timestamp': i  # ìˆœì„œë¥¼ ì‹œê°„ìœ¼ë¡œ ê°„ì£¼
                })
        
        return emotion_timeline
    
    def _simple_emotion_analysis(self, text: str) -> str:
        """ê°„ë‹¨í•œ ê°ì • ë¶„ì„"""
        emotion_keywords = {
            'joy': ['ê¸°ì˜ë‹¤', 'í–‰ë³µ', 'ì¢‹ë‹¤', 'happy', 'joy', 'good'],
            'sadness': ['ìŠ¬í”„ë‹¤', 'ìš°ìš¸', 'í˜ë“¤ë‹¤', 'sad', 'depressed', 'difficult'],
            'anger': ['í™”ë‚˜ë‹¤', 'ë¶„ë…¸', 'ì§œì¦', 'angry', 'mad', 'frustrated'],
            'fear': ['ë¬´ì„­ë‹¤', 'ë‘ë µë‹¤', 'ê±±ì •', 'fear', 'scared', 'worried']
        }
        
        text_lower = text.lower()
        emotion_scores = {}
        
        for emotion, keywords in emotion_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            emotion_scores[emotion] = score
        
        if not any(emotion_scores.values()):
            return 'neutral'
        
        return max(emotion_scores, key=emotion_scores.get)
    
    def _calculate_emotional_stability(self, emotion_dynamics: List[Dict[str, Any]]) -> float:
        """ê°ì • ì•ˆì •ì„± ê³„ì‚°"""
        if len(emotion_dynamics) < 2:
            return 1.0
        
        emotions = [ed['emotion'] for ed in emotion_dynamics]
        unique_emotions = len(set(emotions))
        total_emotions = len(emotions)
        
        # ê°ì • ë³€í™”ê°€ ì ì„ìˆ˜ë¡ ì•ˆì •ì„±ì´ ë†’ìŒ
        stability = 1.0 - (unique_emotions / total_emotions)
        return stability
    
    async def _get_emotion_embedding(self, text: str) -> Optional[np.ndarray]:
        """ê°ì • ì„ë² ë”© ìƒì„±"""
        # ì¼ë°˜ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (ê°ì • íŠ¹í™” ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°)
        return await self._get_embedding_async(text)
    
    def _detect_causal_keywords(self, text: str) -> List[str]:
        """ì¸ê³¼ê´€ê³„ í‚¤ì›Œë“œ ê°ì§€"""
        causal_keywords = [
            'ë•Œë¬¸ì—', 'ì›ì¸', 'ê²°ê³¼', 'ì˜í–¥', 'ì´ìœ ', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ',
            'because', 'cause', 'effect', 'result', 'due to', 'therefore', 'thus'
        ]
        
        found_keywords = []
        text_lower = text.lower()
        
        for keyword in causal_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def _extract_cause_effect_pairs(self, text: str) -> List[Dict[str, str]]:
        """ì›ì¸-ê²°ê³¼ ìŒ ì¶”ì¶œ"""
        pairs = []
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­
        causal_patterns = [
            r'(.+?)\s*ë•Œë¬¸ì—\s*(.+)',
            r'(.+?)\s*ì´ìœ ë¡œ\s*(.+)',
            r'(.+?)\s*because\s*(.+)',
            r'(.+?)\s*due to\s*(.+)'
        ]
        
        for pattern in causal_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match) == 2:
                    pairs.append({
                        'cause': match[1].strip(),
                        'effect': match[0].strip()
                    })
        
        return pairs
    
    def _analyze_conditional_relations(self, text: str) -> List[Dict[str, str]]:
        """ì¡°ê±´ë¶€ ê´€ê³„ ë¶„ì„"""
        conditions = []
        
        conditional_patterns = [
            r'ë§Œì•½\s*(.+?)\s*ë¼ë©´\s*(.+)',
            r'(.+?)\s*ì´ë©´\s*(.+)',
            r'if\s*(.+?)\s*then\s*(.+)',
            r'(.+?)\s*if\s*(.+)'
        ]
        
        for pattern in conditional_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match) == 2:
                    conditions.append({
                        'condition': match[0].strip(),
                        'consequence': match[1].strip()
                    })
        
        return conditions
    
    def _analyze_temporal_order(self, text: str) -> List[str]:
        """ì‹œê°„ì  ìˆœì„œ ë¶„ì„"""
        temporal_indicators = [
            'ë¨¼ì €', 'ê·¸ë‹¤ìŒ', 'ë‚˜ì¤‘ì—', 'ì´í›„', 'ì „ì—',
            'first', 'then', 'next', 'later', 'after', 'before'
        ]
        
        found_indicators = []
        text_lower = text.lower()
        
        for indicator in temporal_indicators:
            if indicator in text_lower:
                found_indicators.append(indicator)
        
        return found_indicators
    
    def _calculate_causal_strength(self, text: str) -> float:
        """ì¸ê³¼ê´€ê³„ ê°•ë„ ê³„ì‚°"""
        strength_indicators = {
            'ê°•í•˜ê²Œ': 0.9, 'í™•ì‹¤íˆ': 0.8, 'ë¶„ëª…íˆ': 0.8,
            'strongly': 0.9, 'definitely': 0.8, 'clearly': 0.8,
            'ì•½ê°„': 0.3, 'ì¡°ê¸ˆ': 0.3, 'ì•„ë§ˆ': 0.4,
            'slightly': 0.3, 'maybe': 0.4, 'perhaps': 0.4
        }
        
        text_lower = text.lower()
        max_strength = 0.5  # ê¸°ë³¸ ê°•ë„
        
        for indicator, strength in strength_indicators.items():
            if indicator in text_lower:
                max_strength = max(max_strength, strength)
        
        return max_strength
    
    def _build_causal_network(self, cause_effect_pairs: List[Dict[str, str]]) -> Dict[str, Any]:
        """ì¸ê³¼ê´€ê³„ ë„¤íŠ¸ì›Œí¬ êµ¬ì„±"""
        if not cause_effect_pairs:
            return {'nodes': [], 'edges': [], 'network_metrics': {}}
        
        # NetworkX ê·¸ë˜í”„ ìƒì„±
        G = nx.DiGraph()
        
        for pair in cause_effect_pairs:
            cause = pair['cause']
            effect = pair['effect']
            G.add_edge(cause, effect)
        
        # ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        try:
            metrics = {
                'node_count': G.number_of_nodes(),
                'edge_count': G.number_of_edges(),
                'density': nx.density(G),
                'is_dag': nx.is_directed_acyclic_graph(G)
            }
            
            # ì¤‘ì‹¬ì„± ì¸¡ì • (ì—°ê²°ëœ ê·¸ë˜í”„ì¸ ê²½ìš°)
            if G.number_of_nodes() > 0:
                try:
                    betweenness = nx.betweenness_centrality(G)
                    metrics['most_central_node'] = max(betweenness, key=betweenness.get)
                except:
                    metrics['most_central_node'] = None
        except:
            metrics = {'node_count': 0, 'edge_count': 0}
        
        return {
            'nodes': list(G.nodes()),
            'edges': list(G.edges()),
            'network_metrics': metrics
        }
    
    def _calculate_causal_confidence(self, causal_keywords: List[str], 
                                   cause_effect_pairs: List[Dict[str, str]]) -> float:
        """ì¸ê³¼ê´€ê³„ ì‹ ë¢°ë„ ê³„ì‚°"""
        keyword_score = min(1.0, len(causal_keywords) * 0.2)
        pair_score = min(1.0, len(cause_effect_pairs) * 0.3)
        
        return (keyword_score + pair_score) / 2
    
    def _calculate_overall_confidence(self) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidences = list(self.analysis_state.confidence_scores.values())
        if not confidences:
            return 0.0
        
        return sum(confidences) / len(confidences)
    
    def _calculate_fusion_confidence(self, surface, ethical, emotional, causal) -> float:
        """ìœµí•© ì‹ ë¢°ë„ ê³„ì‚°"""
        confidences = []
        
        for level_data in [surface, ethical, emotional, causal]:
            if 'confidence' in level_data:
                confidences.append(level_data['confidence'])
        
        if not confidences:
            return 0.0
        
        # ìµœì†Œê°’ì´ ì „ì²´ ì‹ ë¢°ë„ë¥¼ ê²°ì • (ì•½í•œ ê³ ë¦¬ ì›ì¹™)
        return min(confidences) * 0.7 + (sum(confidences) / len(confidences)) * 0.3
    
    def _calculate_semantic_coherence(self, surface, ethical, emotional, causal) -> float:
        """ì˜ë¯¸ì  ì¼ê´€ì„± ê³„ì‚°"""
        # ê°„ë‹¨í•œ ì¼ê´€ì„± ì¸¡ì •
        error_count = sum(1 for level in [surface, ethical, emotional, causal] if 'error' in level)
        total_levels = 4
        
        basic_coherence = (total_levels - error_count) / total_levels
        
        # ì¶”ê°€ ì¼ê´€ì„± ê²€ì‚¬ (ì˜ˆ: ê°ì •ê³¼ ìœ¤ë¦¬ íŒë‹¨ì˜ ì¼ì¹˜ì„±)
        coherence_bonus = 0.0
        
        if 'error' not in emotional and 'error' not in ethical:
            emotion = emotional.get('primary_emotion', '')
            moral_judgment = ethical.get('moral_judgment', '')
            
            # ê¸ì •ì  ê°ì •ê³¼ ê¸ì •ì  ë„ë• íŒë‹¨ì˜ ì¼ì¹˜
            if emotion in ['JOY', 'LOVE'] and moral_judgment == 'positive':
                coherence_bonus += 0.2
            elif emotion in ['ANGER', 'SADNESS'] and moral_judgment == 'negative':
                coherence_bonus += 0.2
        
        return min(1.0, basic_coherence + coherence_bonus)
    
    def _generate_cache_key(self, text: str, metadata: Dict[str, Any] = None) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_components = [text[:100]]  # í…ìŠ¤íŠ¸ ì¼ë¶€ë§Œ ì‚¬ìš©
        
        if metadata:
            key_components.append(str(sorted(metadata.items())))
        
        return str(hash(tuple(key_components)))
    
    def _get_empty_advanced_result(self, text: str) -> AdvancedSemanticResult:
        """ë¹ˆ ê²°ê³¼ ë°˜í™˜"""
        return AdvancedSemanticResult(
            text=text,
            surface_analysis={'confidence': 0.0},
            ethical_analysis={'confidence': 0.0},
            emotional_analysis={'confidence': 0.0},
            causal_analysis={'confidence': 0.0},
            cross_level_relations=[],
            fused_representation={},
            confidence_score=0.0,
            processing_time=0.0,
            metadata={}
        )
    
    def _get_error_result(self, text: str, error_msg: str) -> AdvancedSemanticResult:
        """ì˜¤ë¥˜ ê²°ê³¼ ë°˜í™˜"""
        return AdvancedSemanticResult(
            text=text,
            surface_analysis={'error': error_msg, 'confidence': 0.0},
            ethical_analysis={'error': error_msg, 'confidence': 0.0},
            emotional_analysis={'error': error_msg, 'confidence': 0.0},
            causal_analysis={'error': error_msg, 'confidence': 0.0},
            cross_level_relations=[],
            fused_representation={'error': error_msg},
            confidence_score=0.0,
            processing_time=0.0,
            metadata={'error': error_msg}
        )
    
    def get_pytorch_network(self) -> Optional[nn.Module]:
        """PyTorch ë„¤íŠ¸ì›Œí¬ ë°˜í™˜ (HeadAdapterì™€ì˜ í˜¸í™˜ì„±)
        DSM ëŒ€í‘œ nn.Module ë°˜í™˜ - ê°€ì¥ í° ëª¨ë¸ ìš°ì„ 
        """
        # ethical_model ìš°ì„  (ê°€ì¥ í° AutoModel)
        if hasattr(self, 'ethical_model') and isinstance(self.ethical_model, nn.Module):
            logger.info("AdvancedMultiLevelSemanticAnalyzer: ethical_model ë°˜í™˜")
            return self.ethical_model
        
        # SentenceTransformer ë‚´ë¶€ backbone
        if hasattr(self, 'embedding_model'):
            try:
                # SentenceTransformerì˜ ë‚´ë¶€ ëª¨ë¸ ì¶”ì¶œ
                if hasattr(self.embedding_model, '_modules'):
                    for module in self.embedding_model._modules.values():
                        if hasattr(module, 'auto_model') and isinstance(module.auto_model, nn.Module):
                            logger.info("AdvancedMultiLevelSemanticAnalyzer: embedding_model.auto_model ë°˜í™˜")
                            return module.auto_model
                # ë˜ëŠ” ì§ì ‘ model ì†ì„±
                if hasattr(self.embedding_model, 'model') and isinstance(self.embedding_model.model, nn.Module):
                    logger.info("AdvancedMultiLevelSemanticAnalyzer: embedding_model.model ë°˜í™˜")
                    return self.embedding_model.model
            except Exception as e:
                logger.debug(f"embedding_model ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # fusion_network
        if hasattr(self, 'fusion_network') and isinstance(self.fusion_network, nn.Module):
            logger.info("AdvancedMultiLevelSemanticAnalyzer: fusion_network ë°˜í™˜")
            return self.fusion_network
        
        # cross_attention
        if hasattr(self, 'cross_attention') and isinstance(self.cross_attention, nn.Module):
            logger.info("AdvancedMultiLevelSemanticAnalyzer: cross_attention ë°˜í™˜")
            return self.cross_attention
        
        # ê¸°íƒ€ ê°€ëŠ¥í•œ ë„¤íŠ¸ì›Œí¬ ì†ì„±
        for attr_name in ['main_network', 'model', 'neural_model']:
            if hasattr(self, attr_name):
                model = getattr(self, attr_name)
                if isinstance(model, nn.Module):
                    logger.info(f"AdvancedMultiLevelSemanticAnalyzer: {attr_name} ë°˜í™˜")
                    return model
        
        # PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì§€ ëª»í•¨
        logger.warning("AdvancedMultiLevelSemanticAnalyzer: PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
        return None

def create_advanced_semantic_analyzer() -> AdvancedMultiLevelSemanticAnalyzer:
    """ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„ê¸° ìƒì„±"""
    return AdvancedMultiLevelSemanticAnalyzer()

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import asyncio
    
    async def test_advanced_analyzer():
        """ê³ ê¸‰ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
        analyzer = create_advanced_semantic_analyzer()
        
        test_text = """
        ì´ ìƒí™©ì—ì„œ ìš°ë¦¬ëŠ” ê°œì¸ì˜ ììœ ì™€ ê³µê³µì˜ ì•ˆì „ ì‚¬ì´ì—ì„œ ì–´ë ¤ìš´ ì„ íƒì„ í•´ì•¼ í•œë‹¤.
        í•œí¸ìœ¼ë¡œëŠ” ê°œì¸ì˜ ê¶Œë¦¬ë¥¼ ì¡´ì¤‘í•´ì•¼ í•˜ì§€ë§Œ, ë‹¤ë¥¸ í•œí¸ìœ¼ë¡œëŠ” ì‚¬íšŒ ì „ì²´ì˜ ë³µì§€ë¥¼ ê³ ë ¤í•´ì•¼ í•œë‹¤.
        ì´ëŸ° ë”œë ˆë§ˆëŠ” ì •ë§ ê°€ìŠ´ ì•„í”„ê³  ë³µì¡í•œ ë¬¸ì œë‹¤.
        """
        
        print("=== ê³ ê¸‰ ë‹¤ì¤‘ìˆ˜ì¤€ ì˜ë¯¸ ë¶„ì„ í…ŒìŠ¤íŠ¸ ===\n")
        
        result = await analyzer.analyze_text_advanced(test_text)
        
        print(f"ë¶„ì„ í…ìŠ¤íŠ¸: {result.text}")
        print(f"ì „ì²´ ì‹ ë¢°ë„: {result.confidence_score:.3f}")
        print(f"ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.3f}ì´ˆ\n")
        
        print("=== ìˆ˜ì¤€ë³„ ë¶„ì„ ê²°ê³¼ ===")
        print(f"í‘œë©´ì : ì‹ ë¢°ë„ {result.surface_analysis.get('confidence', 0):.3f}")
        print(f"ìœ¤ë¦¬ì : ì‹ ë¢°ë„ {result.ethical_analysis.get('confidence', 0):.3f}")
        print(f"ê°ì •ì : ì‹ ë¢°ë„ {result.emotional_analysis.get('confidence', 0):.3f}")
        print(f"ì¸ê³¼ì : ì‹ ë¢°ë„ {result.causal_analysis.get('confidence', 0):.3f}\n")
        
        print("=== ìˆ˜ì¤€ ê°„ ê´€ê³„ ===")
        for relation in result.cross_level_relations:
            print(f"{relation.source_level} â†’ {relation.target_level}: {relation.relation_type} (ê°•ë„: {relation.strength:.3f})")
        
        print(f"\n=== ìœµí•© ê²°ê³¼ ===")
        fusion = result.fused_representation
        print(f"ìœµí•© ì‹ ë¢°ë„: {fusion.get('fusion_confidence', 0):.3f}")
        print(f"ì˜ë¯¸ì  ì¼ê´€ì„±: {fusion.get('semantic_coherence', 0):.3f}")
        
        if fusion.get('attention_weights'):
            print("\nì§€ë°°ì  ì–´í…ì…˜ ê´€ê³„:")
            for relation in fusion['attention_weights'].get('dominant_relations', [])[:3]:
                print(f"  {relation}")
    
    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_advanced_analyzer())