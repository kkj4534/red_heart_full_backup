"""
ë™ì  ìœ¤ë¦¬ì  ì„ íƒì§€ ë¶„ì„ ì‹œìŠ¤í…œ
Dynamic Ethical Choice Analyzer System

ëª¨ë“  ìœ¤ë¦¬ì  ë”œë ˆë§ˆì— ëŒ€í•´ ë™ì ìœ¼ë¡œ ì„ íƒì§€ë¥¼ ì¶”ì¶œí•˜ê³  ë¶„ì„í•˜ëŠ” ë²”ìš© ì‹œìŠ¤í…œ
- ìì—°ì–´ ì²˜ë¦¬ë¥¼ í†µí•œ ì„ íƒì§€ ìë™ ì¶”ì¶œ
- ëŸ¼ë°”ìš° êµ¬ì¡°ì  ë¶„ì„ìœ¼ë¡œ ì´í•´ê´€ê³„ì ë° ì œì•½ì‚¬í•­ íŒŒì•…
- ê° ì„ íƒì§€ë³„ 12ê°œ ëª¨ë“ˆ ë³‘ë ¬ ë¶„ì„
- ë°˜ì‚¬ì‹¤ì  ì¶”ë¡ ì„ í†µí•œ ëŒ€ì•ˆ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
"""

import asyncio
import logging
import time
import json
import re
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import uuid
from enum import Enum

# ê³ ê¸‰ ë¶„ì„ ëª¨ë“ˆë“¤
try:
    from advanced_emotion_analyzer import AdvancedEmotionAnalyzer
    from advanced_bentham_calculator import AdvancedBenthamCalculator
    from advanced_regret_analyzer import AdvancedRegretAnalyzer
    from advanced_surd_analyzer import AdvancedSURDAnalyzer
    from advanced_counterfactual_reasoning import AdvancedCounterfactualReasoning
    from advanced_rumbaugh_analyzer import AdvancedRumbaughAnalyzer
    from integrated_system_orchestrator import IntegratedSystemOrchestrator, IntegrationContext
    from llm_module.advanced_llm_engine import get_llm_engine
    ADVANCED_MODULES_AVAILABLE = True
except ImportError as e:
    ADVANCED_MODULES_AVAILABLE = False
    logging.error(f"ê³ ê¸‰ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

logger = logging.getLogger('DynamicEthicalChoiceAnalyzer')

class DilemmaType(Enum):
    """ë”œë ˆë§ˆ ìœ í˜•"""
    TROLLEY_PROBLEM = "trolley_problem"
    RESOURCE_ALLOCATION = "resource_allocation"
    PRIVACY_SECURITY = "privacy_security"
    MEDICAL_ETHICS = "medical_ethics"
    ENVIRONMENTAL_CHOICE = "environmental_choice"
    PERSONAL_RELATIONSHIP = "personal_relationship"
    PROFESSIONAL_DUTY = "professional_duty"
    SACRIFICE_CHOICE = "sacrifice_choice"
    TRUTH_VS_KINDNESS = "truth_vs_kindness"
    INDIVIDUAL_VS_COLLECTIVE = "individual_vs_collective"
    UNKNOWN = "unknown"

@dataclass
class EthicalChoice:
    """ìœ¤ë¦¬ì  ì„ íƒì§€"""
    id: str
    name: str
    description: str
    action_type: str  # "action", "inaction", "compromise"
    stakeholders_affected: List[str]
    expected_outcomes: Dict[str, Any]
    moral_weight: float = 0.0
    feasibility: float = 1.0
    risk_level: float = 0.0
    time_sensitivity: float = 0.0
    
    # ëŸ¼ë°”ìš° êµ¬ì¡° ë¶„ì„ ê²°ê³¼
    structural_elements: Dict[str, Any] = field(default_factory=dict)
    object_relationships: List[Dict[str, Any]] = field(default_factory=list)
    constraint_factors: List[str] = field(default_factory=list)

@dataclass
class EthicalDilemma:
    """ìœ¤ë¦¬ì  ë”œë ˆë§ˆ"""
    id: str
    title: str
    description: str
    context: Dict[str, Any]
    dilemma_type: DilemmaType
    
    # ë™ì  ì¶”ì¶œëœ ì •ë³´
    extracted_choices: List[EthicalChoice] = field(default_factory=list)
    stakeholders: Dict[str, float] = field(default_factory=dict)
    moral_complexity: float = 0.0
    urgency_level: float = 0.0
    
    # ë¶„ì„ ê²°ê³¼
    choice_analyses: Dict[str, Any] = field(default_factory=dict)
    recommended_choice: Optional[EthicalChoice] = None
    reasoning_chain: List[str] = field(default_factory=list)
    
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class ChoiceAnalysisResult:
    """ì„ íƒì§€ ë¶„ì„ ê²°ê³¼"""
    choice: EthicalChoice
    emotion_analysis: Dict[str, Any]
    bentham_analysis: Dict[str, Any]
    regret_analysis: Dict[str, Any]
    surd_analysis: Dict[str, Any]
    counterfactual_scenarios: List[Dict[str, Any]]
    
    # í†µí•© ì ìˆ˜
    utility_score: float = 0.0
    confidence_score: float = 0.0
    risk_adjusted_score: float = 0.0
    
    processing_time: float = 0.0

class DynamicEthicalChoiceAnalyzer:
    """ë™ì  ìœ¤ë¦¬ì  ì„ íƒì§€ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.emotion_analyzer = None
        self.bentham_calculator = None
        self.regret_analyzer = None
        self.surd_analyzer = None
        self.counterfactual_reasoner = None
        self.rumbaugh_analyzer = None
        self.integrated_orchestrator = None
        self.llm_engine = None
        
        # ì„ íƒì§€ ì¶”ì¶œ íŒ¨í„´
        self.choice_patterns = [
            r'(?:ì„ íƒ|ì˜µì…˜|ë°©ë²•|ë°©ì•ˆ).*?(?:\d+|í•˜ë‚˜|ë‘˜|ì…‹)',
            r'(?:í•´ì•¼|í• ì§€|ê²ƒì¸ê°€|ì¸ê°€).*?(?:ì•„ë‹ˆë©´|ë˜ëŠ”|í˜¹ì€)',
            r'(?:ë°Ÿì„ì§€|í‹€ì§€|í• ì§€|ê°ˆì§€).*?(?:ì•„ë‹ˆë©´|ë˜ëŠ”|í˜¹ì€)',
            r'(?:ë³´í˜¸|ì„ íƒ|ìš°ì„ |ê³ ë ¤).*?(?:vs|ëŒ€|versus)',
            r'(?:í¬ê¸°|í¬ìƒ|ì–‘ë³´).*?(?:ìœ„í•´|í•˜ê³ |í•´ì„œ)',
        ]
        
        # ì´í•´ê´€ê³„ì ì¶”ì¶œ íŒ¨í„´
        self.stakeholder_patterns = [
            r'(?:ìŠ¹ê°|íƒ‘ìŠ¹ì|ìš´ì „ì|ë³´í–‰ì|ì‹œë¯¼|í™˜ì|ì˜ì‚¬|ê°€ì¡±|ì¹œêµ¬|ë™ë£Œ|íšŒì‚¬|ì •ë¶€|ì‚¬íšŒ|ê³µë™ì²´|ê°œì¸|ì§‘ë‹¨)',
            r'(?:ì–´ë¦°ì´|ì•„ì´|ì²­ì†Œë…„|ì„±ì¸|ë…¸ì¸|ì¥ì• ì¸|ì„ì‚°ë¶€)',
            r'(?:ì§ì›|ê´€ë¦¬ì|CEO|ìƒì‚¬|ë¶€í•˜|ë™ë£Œ|íŒŒíŠ¸ë„ˆ)',
            r'(?:í•™ìƒ|êµì‚¬|êµìˆ˜|ì—°êµ¬ì|ì „ë¬¸ê°€)',
        ]
        
        # ë”œë ˆë§ˆ ìœ í˜• í‚¤ì›Œë“œ
        self.dilemma_keywords = {
            DilemmaType.TROLLEY_PROBLEM: ['íŠ¸ë¡¤ë¦¬', 'ììœ¨ì£¼í–‰', 'ë¸Œë ˆì´í¬', 'ì¶©ëŒ', 'ì‚¬ê³ '],
            DilemmaType.RESOURCE_ALLOCATION: ['ìì›', 'ë°°ë¶„', 'ë¶„ë°°', 'í• ë‹¹', 'ì˜ˆì‚°'],
            DilemmaType.PRIVACY_SECURITY: ['ê°œì¸ì •ë³´', 'í”„ë¼ì´ë²„ì‹œ', 'ë³´ì•ˆ', 'ê°ì‹œ', 'ì •ë³´'],
            DilemmaType.MEDICAL_ETHICS: ['ì˜ë£Œ', 'ì¹˜ë£Œ', 'í™˜ì', 'ìƒëª…', 'ê±´ê°•'],
            DilemmaType.ENVIRONMENTAL_CHOICE: ['í™˜ê²½', 'ê¸°í›„', 'ì˜¤ì—¼', 'ìì—°', 'ìƒíƒœ'],
            DilemmaType.PERSONAL_RELATIONSHIP: ['ì¹œêµ¬', 'ê°€ì¡±', 'ì—°ì¸', 'ê´€ê³„', 'ì‹ ë¢°'],
            DilemmaType.PROFESSIONAL_DUTY: ['ì§ì—…', 'ì—…ë¬´', 'ì˜ë¬´', 'ì±…ì„', 'ìœ¤ë¦¬'],
            DilemmaType.SACRIFICE_CHOICE: ['í¬ìƒ', 'í¬ê¸°', 'ì–‘ë³´', 'ì†ì‹¤', 'ëŒ€ê°€'],
            DilemmaType.TRUTH_VS_KINDNESS: ['ì§„ì‹¤', 'ê±°ì§“ë§', 'ì¹œì ˆ', 'ìƒì²˜', 'ì†”ì§'],
            DilemmaType.INDIVIDUAL_VS_COLLECTIVE: ['ê°œì¸', 'ì§‘ë‹¨', 'ê³µë™ì²´', 'ì‚¬íšŒ', 'ì´ìµ']
        }
        
        self._initialize_modules()
    
    def _initialize_modules(self):
        """ëª¨ë“  ë¶„ì„ ëª¨ë“ˆ ì´ˆê¸°í™” - CPU ê¸°ë°˜ + Lazy GPU Loading"""
        if not ADVANCED_MODULES_AVAILABLE:
            logger.error("ê³ ê¸‰ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            logger.info("ë™ì  ìœ¤ë¦¬ì  ì„ íƒì§€ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (CPU ê¸°ë°˜)...")
            
            # ê°œë³„ ë¶„ì„ ëª¨ë“ˆë“¤ì€ ì´ë¯¸ ì´ˆê¸°í™”ëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¬ì‚¬ìš©í•˜ê±°ë‚˜ í•„ìš”ì‹œ ìƒì„±
            # ì‹¤ì œ ë¶„ì„ ì‹œì—ë§Œ GPU ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©
            logger.info("  ğŸ“‹ ë¶„ì„ ëª¨ë“ˆë“¤ì„ ë ˆì´ì§€ ë¡œë”© ëª¨ë“œë¡œ ì„¤ì •...")
            
            # ëª¨ë“ˆë“¤ì„ Noneìœ¼ë¡œ ì´ˆê¸°í™” (ì‹¤ì œ ì‚¬ìš©ì‹œ ë¡œë“œ)
            self.emotion_analyzer = None
            self.bentham_calculator = None  
            self.regret_analyzer = None
            self.surd_analyzer = None
            self.counterfactual_reasoner = None
            self.rumbaugh_analyzer = None
            
            # í†µí•© ì‹œìŠ¤í…œë„ lazy loading
            self.integrated_orchestrator = None
            
            # LLM ì—”ì§„ë„ í•„ìš”ì‹œì—ë§Œ ë¡œë“œ
            self.llm_engine = None
            
            logger.info("âœ… ë™ì  ìœ¤ë¦¬ì  ì„ íƒì§€ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (Lazy Loading ëª¨ë“œ)")
            
        except Exception as e:
            logger.error(f"ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _get_emotion_analyzer(self):
        """ê°ì • ë¶„ì„ê¸° lazy loading"""
        if self.emotion_analyzer is None:
            logger.info("  ğŸ”„ ê°ì • ë¶„ì„ê¸° ë¡œë”©...")
            self.emotion_analyzer = AdvancedEmotionAnalyzer()
        return self.emotion_analyzer
    
    def _get_bentham_calculator(self):
        """ë²¤ë‹´ ê³„ì‚°ê¸° lazy loading"""
        if self.bentham_calculator is None:
            logger.info("  ğŸ”„ ë²¤ë‹´ ê³„ì‚°ê¸° ë¡œë”©...")
            self.bentham_calculator = AdvancedBenthamCalculator()
        return self.bentham_calculator
    
    def _get_regret_analyzer(self):
        """í›„íšŒ ë¶„ì„ê¸° lazy loading"""
        if self.regret_analyzer is None:
            logger.info("  ğŸ”„ í›„íšŒ ë¶„ì„ê¸° ë¡œë”©...")
            self.regret_analyzer = AdvancedRegretAnalyzer()
        return self.regret_analyzer
    
    def _get_surd_analyzer(self):
        """SURD ë¶„ì„ê¸° lazy loading"""
        if self.surd_analyzer is None:
            logger.info("  ğŸ”„ SURD ë¶„ì„ê¸° ë¡œë”©...")
            self.surd_analyzer = AdvancedSURDAnalyzer()
        return self.surd_analyzer
    
    def _get_counterfactual_reasoner(self):
        """ë°˜ì‚¬ì‹¤ì  ì¶”ë¡ ê¸° lazy loading"""
        if self.counterfactual_reasoner is None:
            logger.info("  ğŸ”„ ë°˜ì‚¬ì‹¤ì  ì¶”ë¡ ê¸° ë¡œë”©...")
            self.counterfactual_reasoner = AdvancedCounterfactualReasoning()
        return self.counterfactual_reasoner
    
    def _get_rumbaugh_analyzer(self):
        """ëŸ¼ë°”ìš° ë¶„ì„ê¸° lazy loading"""
        if self.rumbaugh_analyzer is None:
            logger.info("  ğŸ”„ ëŸ¼ë°”ìš° ë¶„ì„ê¸° ë¡œë”©...")
            self.rumbaugh_analyzer = AdvancedRumbaughAnalyzer()
        return self.rumbaugh_analyzer
    
    def _get_integrated_orchestrator(self):
        """í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° lazy loading"""
        if self.integrated_orchestrator is None:
            logger.info("  ğŸ”„ í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë¡œë”©...")
            self.integrated_orchestrator = IntegratedSystemOrchestrator()
        return self.integrated_orchestrator
    
    def _get_llm_engine(self):
        """LLM ì—”ì§„ lazy loading"""
        if self.llm_engine is None:
            logger.info("  ğŸ”„ LLM ì—”ì§„ ë¡œë”©...")
            self.llm_engine = get_llm_engine()
        return self.llm_engine
    
    async def analyze_ethical_dilemma(self, dilemma_text: str, title: str = "", context: Dict[str, Any] = None) -> EthicalDilemma:
        """ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ì¢…í•© ë¶„ì„"""
        
        logger.info(f"ğŸ¯ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ ì‹œì‘: {title or 'ì œëª© ì—†ìŒ'}")
        
        # 1. ë”œë ˆë§ˆ ê¸°ë³¸ ì •ë³´ ìƒì„±
        dilemma = EthicalDilemma(
            id=str(uuid.uuid4()),
            title=title or "ìœ¤ë¦¬ì  ë”œë ˆë§ˆ",
            description=dilemma_text,
            context=context or {},
            dilemma_type=self._classify_dilemma_type(dilemma_text)
        )
        
        # 2. ë™ì  ì„ íƒì§€ ì¶”ì¶œ
        logger.info("ğŸ“‹ ë™ì  ì„ íƒì§€ ì¶”ì¶œ...")
        dilemma.extracted_choices = await self._extract_choices(dilemma_text)
        logger.info(f"   ì¶”ì¶œëœ ì„ íƒì§€: {len(dilemma.extracted_choices)}ê°œ")
        
        # 3. ì´í•´ê´€ê³„ì ì¶”ì¶œ
        logger.info("ğŸ‘¥ ì´í•´ê´€ê³„ì ì¶”ì¶œ...")
        dilemma.stakeholders = self._extract_stakeholders(dilemma_text)
        logger.info(f"   ì‹ë³„ëœ ì´í•´ê´€ê³„ì: {len(dilemma.stakeholders)}ëª…")
        
        # 4. ëŸ¼ë°”ìš° êµ¬ì¡°ì  ë¶„ì„
        logger.info("ğŸ—ï¸ ëŸ¼ë°”ìš° êµ¬ì¡°ì  ë¶„ì„...")
        await self._rumbaugh_structural_analysis(dilemma)
        
        # 5. ê° ì„ íƒì§€ë³„ ì‹¬ì¸µ ë¶„ì„
        logger.info("ğŸ” ê° ì„ íƒì§€ë³„ ì‹¬ì¸µ ë¶„ì„...")
        for choice in dilemma.extracted_choices:
            analysis_result = await self._analyze_individual_choice(dilemma, choice)
            dilemma.choice_analyses[choice.id] = analysis_result
            logger.info(f"   âœ… {choice.name} ë¶„ì„ ì™„ë£Œ (ìœ í‹¸ë¦¬í‹°: {analysis_result.utility_score:.3f})")
        
        # 6. ìµœì  ì„ íƒì§€ ì¶”ì²œ
        logger.info("ğŸ¯ ìµœì  ì„ íƒì§€ ì¶”ì²œ...")
        dilemma.recommended_choice = self._recommend_optimal_choice(dilemma)
        
        # 7. ì¶”ë¡  ì²´ì¸ ìƒì„±
        logger.info("ğŸ§  ì¶”ë¡  ì²´ì¸ ìƒì„±...")
        dilemma.reasoning_chain = await self._generate_reasoning_chain(dilemma)
        
        logger.info(f"âœ… ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ ì™„ë£Œ: {dilemma.title}")
        if dilemma.recommended_choice:
            logger.info(f"   ğŸ¯ ìµœì¢… ì¶”ì²œ: {dilemma.recommended_choice.name}")
        
        return dilemma
    
    async def _extract_choices(self, text: str) -> List[EthicalChoice]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì„ íƒì§€ ë™ì  ì¶”ì¶œ"""
        
        choices = []
        
        # 1. íŒ¨í„´ ê¸°ë°˜ ì„ íƒì§€ ì¶”ì¶œ
        for pattern in self.choice_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                choices.extend(self._parse_choice_from_match(match, text))
        
        # 2. LLM ê¸°ë°˜ ì„ íƒì§€ ì¶”ì¶œ (ë” ì •í™•í•œ ì¶”ì¶œ)
        if self.llm_engine:
            llm_choices = await self._extract_choices_with_llm(text)
            choices.extend(llm_choices)
        
        # 3. ì¤‘ë³µ ì œê±° ë° ì •ê·œí™”
        unique_choices = self._deduplicate_choices(choices)
        
        # 4. ì„ íƒì§€ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„ íƒì§€ ìƒì„±
        if not unique_choices:
            unique_choices = self._generate_default_choices(text)
        
        return unique_choices
    
    async def _extract_choices_with_llm(self, text: str) -> List[EthicalChoice]:
        """LLMì„ ì‚¬ìš©í•œ ì„ íƒì§€ ì¶”ì¶œ"""
        
        if not self.llm_engine:
            return []
        
        prompt = f"""
ë‹¤ìŒ ìœ¤ë¦¬ì  ë”œë ˆë§ˆì—ì„œ ê°€ëŠ¥í•œ ì„ íƒì§€ë“¤ì„ ì¶”ì¶œí•˜ì„¸ìš”:

ìƒí™©: {text}

ê°€ëŠ¥í•œ ì„ íƒì§€ë“¤ì„ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{{
    "choices": [
        {{
            "name": "ì„ íƒì§€ ì´ë¦„",
            "description": "ì„ íƒì§€ ìƒì„¸ ì„¤ëª…",
            "action_type": "action/inaction/compromise",
            "stakeholders_affected": ["ì´í•´ê´€ê³„ì1", "ì´í•´ê´€ê³„ì2"],
            "expected_outcomes": {{"ê²°ê³¼1": "ì„¤ëª…1", "ê²°ê³¼2": "ì„¤ëª…2"}}
        }}
    ]
}}
"""
        
        try:
            response = await self.llm_engine.generate_response(prompt)
            # JSON íŒŒì‹± ë° EthicalChoice ê°ì²´ ìƒì„±
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” JSON íŒŒì‹± ë° ê°ì²´ ìƒì„± ë¡œì§ ì¶”ê°€
            return []
        except Exception as e:
            logger.error(f"LLM ì„ íƒì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_choice_from_match(self, match: str, full_text: str) -> List[EthicalChoice]:
        """ë§¤ì¹˜ëœ í…ìŠ¤íŠ¸ì—ì„œ ì„ íƒì§€ íŒŒì‹±"""
        
        choices = []
        
        # ê°„ë‹¨í•œ ì„ íƒì§€ íŒŒì‹± ë¡œì§
        if "ì•„ë‹ˆë©´" in match or "ë˜ëŠ”" in match or "í˜¹ì€" in match:
            parts = re.split(r'(?:ì•„ë‹ˆë©´|ë˜ëŠ”|í˜¹ì€)', match)
            for i, part in enumerate(parts):
                if part.strip():
                    choice = EthicalChoice(
                        id=f"choice_{i}_{uuid.uuid4().hex[:8]}",
                        name=part.strip(),
                        description=f"ì„ íƒì§€: {part.strip()}",
                        action_type="action",
                        stakeholders_affected=[],
                        expected_outcomes={}
                    )
                    choices.append(choice)
        
        return choices
    
    def _deduplicate_choices(self, choices: List[EthicalChoice]) -> List[EthicalChoice]:
        """ì¤‘ë³µ ì„ íƒì§€ ì œê±°"""
        
        seen_names = set()
        unique_choices = []
        
        for choice in choices:
            if choice.name not in seen_names:
                seen_names.add(choice.name)
                unique_choices.append(choice)
        
        return unique_choices
    
    def _generate_default_choices(self, text: str) -> List[EthicalChoice]:
        """ê¸°ë³¸ ì„ íƒì§€ ìƒì„±"""
        
        return [
            EthicalChoice(
                id=f"default_action_{uuid.uuid4().hex[:8]}",
                name="ì ê·¹ì  í–‰ë™",
                description="ìƒí™©ì— ì ê·¹ì ìœ¼ë¡œ ê°œì…í•˜ì—¬ í–‰ë™í•œë‹¤",
                action_type="action",
                stakeholders_affected=[],
                expected_outcomes={"ê°œì…": "ìƒí™©ì— ì§ì ‘ì  ì˜í–¥"}
            ),
            EthicalChoice(
                id=f"default_inaction_{uuid.uuid4().hex[:8]}",
                name="ì†Œê·¹ì  ëŒ€ì‘",
                description="ìƒí™©ì„ ì§€ì¼œë³´ë©° ìµœì†Œí•œì˜ ê°œì…ë§Œ í•œë‹¤",
                action_type="inaction",
                stakeholders_affected=[],
                expected_outcomes={"ê´€ì°°": "ìƒí™© ë³€í™” ê´€ì°°"}
            ),
            EthicalChoice(
                id=f"default_compromise_{uuid.uuid4().hex[:8]}",
                name="íƒ€í˜‘ì  í•´ê²°",
                description="ì—¬ëŸ¬ ì´í•´ê´€ê³„ìì˜ ì…ì¥ì„ ê³ ë ¤í•œ íƒ€í˜‘ì•ˆì„ ì°¾ëŠ”ë‹¤",
                action_type="compromise",
                stakeholders_affected=[],
                expected_outcomes={"íƒ€í˜‘": "ë¶€ë¶„ì  ë§Œì¡±"}
            )
        ]
    
    def _extract_stakeholders(self, text: str) -> Dict[str, float]:
        """ì´í•´ê´€ê³„ì ì¶”ì¶œ"""
        
        stakeholders = {}
        
        for pattern in self.stakeholder_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # ê¸°ë³¸ ì¤‘ìš”ë„ í• ë‹¹
                importance = 0.8
                if any(keyword in match for keyword in ['ì–´ë¦°ì´', 'ì•„ì´', 'ì„ì‚°ë¶€', 'ì¥ì• ì¸']):
                    importance = 0.9
                elif any(keyword in match for keyword in ['í™˜ì', 'í”¼í•´ì']):
                    importance = 0.95
                
                stakeholders[match] = importance
        
        return stakeholders
    
    def _classify_dilemma_type(self, text: str) -> DilemmaType:
        """ë”œë ˆë§ˆ ìœ í˜• ë¶„ë¥˜"""
        
        text_lower = text.lower()
        
        for dilemma_type, keywords in self.dilemma_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                return dilemma_type
        
        return DilemmaType.UNKNOWN
    
    async def _rumbaugh_structural_analysis(self, dilemma: EthicalDilemma):
        """ëŸ¼ë°”ìš° êµ¬ì¡°ì  ë¶„ì„ - Lazy Loading + GPU Context"""
        
        try:
            # Lazy loadingìœ¼ë¡œ ë¶„ì„ê¸° ë¡œë“œ
            rumbaugh_analyzer = self._get_rumbaugh_analyzer()
            
            # ê° ì„ íƒì§€ì— ëŒ€í•œ êµ¬ì¡°ì  ë¶„ì„
            for choice in dilemma.extracted_choices:
                # GPU context manager ì‚¬ìš©
                from config import gpu_model_context
                
                # í•„ìš”ì‹œ GPU ì‚¬ìš©, ê¸°ë³¸ì€ CPU
                with gpu_model_context(rumbaugh_analyzer, memory_required_mb=300) as gpu_analyzer:
                    analysis_result = await gpu_analyzer.analyze_ethical_structure(
                        situation=dilemma.description,
                        choice_context=choice.description,
                        stakeholders=list(dilemma.stakeholders.keys())
                    )
                
                choice.structural_elements = analysis_result.get('structural_elements', {})
                choice.object_relationships = analysis_result.get('object_relationships', [])
                choice.constraint_factors = analysis_result.get('constraint_factors', [])
                
        except Exception as e:
            logger.error(f"ëŸ¼ë°”ìš° êµ¬ì¡°ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    async def _analyze_individual_choice(self, dilemma: EthicalDilemma, choice: EthicalChoice) -> ChoiceAnalysisResult:
        """ê°œë³„ ì„ íƒì§€ ì‹¬ì¸µ ë¶„ì„"""
        
        start_time = time.time()
        
        # ì„ íƒì§€ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        choice_context = f"""
ìƒí™©: {dilemma.description}
ì„ íƒì§€: {choice.name}
ìƒì„¸ ì„¤ëª…: {choice.description}
ì´í•´ê´€ê³„ì: {', '.join(dilemma.stakeholders.keys())}
"""
        
        # ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰
        analyses = await asyncio.gather(
            self._analyze_choice_emotion(choice_context),
            self._analyze_choice_bentham(choice_context, dilemma),
            self._analyze_choice_regret(choice_context, dilemma, choice),
            self._analyze_choice_surd(choice_context, dilemma),
            self._generate_choice_counterfactuals(choice_context, dilemma, choice),
            return_exceptions=True
        )
        
        # ê²°ê³¼ ì²˜ë¦¬
        emotion_analysis = analyses[0] if not isinstance(analyses[0], Exception) else {}
        bentham_analysis = analyses[1] if not isinstance(analyses[1], Exception) else {}
        regret_analysis = analyses[2] if not isinstance(analyses[2], Exception) else {}
        surd_analysis = analyses[3] if not isinstance(analyses[3], Exception) else {}
        counterfactual_scenarios = analyses[4] if not isinstance(analyses[4], Exception) else []
        
        # í†µí•© ì ìˆ˜ ê³„ì‚°
        utility_score = self._calculate_utility_score(emotion_analysis, bentham_analysis, regret_analysis)
        confidence_score = self._calculate_confidence_score(emotion_analysis, bentham_analysis, regret_analysis)
        risk_adjusted_score = utility_score * confidence_score * (1 - choice.risk_level)
        
        processing_time = time.time() - start_time
        
        return ChoiceAnalysisResult(
            choice=choice,
            emotion_analysis=emotion_analysis,
            bentham_analysis=bentham_analysis,
            regret_analysis=regret_analysis,
            surd_analysis=surd_analysis,
            counterfactual_scenarios=counterfactual_scenarios,
            utility_score=utility_score,
            confidence_score=confidence_score,
            risk_adjusted_score=risk_adjusted_score,
            processing_time=processing_time
        )
    
    async def _analyze_choice_emotion(self, choice_context: str) -> Dict[str, Any]:
        """ì„ íƒì§€ ê°ì • ë¶„ì„ - Lazy Loading + GPU Context"""
        
        try:
            # Lazy loadingìœ¼ë¡œ ê°ì • ë¶„ì„ê¸° ë¡œë“œ
            emotion_analyzer = self._get_emotion_analyzer()
            
            # GPU context manager ì‚¬ìš© (ê°ì • ë¶„ì„ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŒ)
            from config import gpu_model_context
            
            with gpu_model_context(emotion_analyzer, memory_required_mb=800) as gpu_analyzer:
                emotion_result = gpu_analyzer.analyze_emotion(
                    text=choice_context,
                    language="ko",
                    use_cache=True
                )
            
            return {
                'emotion': getattr(emotion_result, 'dominant_emotion', 'unknown'),
                'intensity': getattr(emotion_result, 'intensity', 0.0),
                'confidence': getattr(emotion_result, 'confidence', 0.0),
                'arousal': getattr(emotion_result, 'arousal', 0.0),
                'valence': getattr(emotion_result, 'valence', 0.0)
            }
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _analyze_choice_bentham(self, choice_context: str, dilemma: EthicalDilemma) -> Dict[str, Any]:
        """ì„ íƒì§€ ë²¤ë‹´ ë¶„ì„ - Lazy Loading + GPU Context"""
        
        try:
            # Lazy loadingìœ¼ë¡œ ë²¤ë‹´ ê³„ì‚°ê¸° ë¡œë“œ
            bentham_calculator = self._get_bentham_calculator()
            
            bentham_input = {
                'situation': choice_context,
                'context': dilemma.context,
                'stakeholders': dilemma.stakeholders
            }
            
            # GPU context manager ì‚¬ìš© (ë²¤ë‹´ ê³„ì‚°ì€ ì¤‘ê°„ ì •ë„ ë©”ëª¨ë¦¬ ì‚¬ìš©)
            from config import gpu_model_context
            
            with gpu_model_context(bentham_calculator, memory_required_mb=400) as gpu_calculator:
                bentham_result = gpu_calculator.calculate_with_advanced_layers(
                    input_data=bentham_input,
                    use_cache=True
                )
            
            return {
                'final_score': getattr(bentham_result, 'final_score', 0.0),
                'base_score': getattr(bentham_result, 'base_score', 0.0),
                'confidence': getattr(bentham_result, 'confidence', 0.0),
                'intensity': getattr(bentham_result, 'intensity', 0.0),
                'duration': getattr(bentham_result, 'duration', 0.0),
                'certainty': getattr(bentham_result, 'certainty', 0.0)
            }
        except Exception as e:
            logger.error(f"ë²¤ë‹´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _analyze_choice_regret(self, choice_context: str, dilemma: EthicalDilemma, choice: EthicalChoice) -> Dict[str, Any]:
        """ì„ íƒì§€ í›„íšŒ ë¶„ì„ - Lazy Loading + GPU Context"""
        
        try:
            # Lazy loadingìœ¼ë¡œ í›„íšŒ ë¶„ì„ê¸° ë¡œë“œ
            regret_analyzer = self._get_regret_analyzer()
            
            regret_input = {
                'scenario': dilemma.description,
                'text': choice_context,
                'chosen_action': choice.name,
                'alternative_actions': [c.name for c in dilemma.extracted_choices if c.id != choice.id],
                'context': dilemma.context
            }
            
            # GPU context manager ì‚¬ìš© (í›„íšŒ ë¶„ì„ì€ ì¤‘ê°„ ì •ë„ ë©”ëª¨ë¦¬ ì‚¬ìš©)
            from config import gpu_model_context
            
            with gpu_model_context(regret_analyzer, memory_required_mb=450) as gpu_analyzer:
                regret_result = await gpu_analyzer.analyze_regret(
                    decision_data=regret_input,
                    outcome_data=None
                )
            
            return {
                'regret_intensity': getattr(regret_result, 'regret_intensity', 0.0),
                'anticipated_regret': getattr(regret_result, 'anticipated_regret', 0.0),
                'experienced_regret': getattr(regret_result, 'experienced_regret', 0.0),
                'model_confidence': getattr(regret_result, 'model_confidence', 0.0)
            }
        except Exception as e:
            logger.error(f"í›„íšŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _analyze_choice_surd(self, choice_context: str, dilemma: EthicalDilemma) -> Dict[str, Any]:
        """ì„ íƒì§€ SURD ë¶„ì„ - Lazy Loading + GPU Context"""
        
        try:
            # Lazy loadingìœ¼ë¡œ SURD ë¶„ì„ê¸° ë¡œë“œ
            surd_analyzer = self._get_surd_analyzer()
            
            surd_variables = {
                'ethical_decision_quality': 0.5,
                'stakeholder_satisfaction': 0.5,
                'moral_complexity': dilemma.moral_complexity or 0.5
            }
            
            # GPU context manager ì‚¬ìš© (SURD ë¶„ì„ì€ ìƒëŒ€ì ìœ¼ë¡œ ê°€ë²¼ì›€)
            from config import gpu_model_context
            
            with gpu_model_context(surd_analyzer, memory_required_mb=300) as gpu_analyzer:
                surd_result = gpu_analyzer.analyze_advanced(
                    variables=surd_variables,
                    target_variable='ethical_decision_quality',
                    additional_context=dilemma.context
                )
            
            return {
                'synergy': getattr(surd_result, 'synergy', 0.0),
                'uniqueness': getattr(surd_result, 'uniqueness', 0.0),
                'redundancy': getattr(surd_result, 'redundancy', 0.0),
                'determinism': getattr(surd_result, 'determinism', 0.0)
            }
        except Exception as e:
            logger.error(f"SURD ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _generate_choice_counterfactuals(self, choice_context: str, dilemma: EthicalDilemma, choice: EthicalChoice) -> List[Dict[str, Any]]:
        """ì„ íƒì§€ ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        
        try:
            if not self.counterfactual_reasoner:
                return []
            
            # ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
            scenarios = await self.counterfactual_reasoner.generate_counterfactual_scenarios(
                base_situation=dilemma.description,
                chosen_action=choice.name,
                alternative_actions=[c.name for c in dilemma.extracted_choices if c.id != choice.id],
                context=dilemma.context
            )
            
            return scenarios or []
        except Exception as e:
            logger.error(f"ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _calculate_utility_score(self, emotion_analysis: Dict, bentham_analysis: Dict, regret_analysis: Dict) -> float:
        """ìœ í‹¸ë¦¬í‹° ì ìˆ˜ ê³„ì‚°"""
        
        bentham_score = bentham_analysis.get('final_score', 0.0)
        emotion_intensity = emotion_analysis.get('intensity', 0.0)
        regret_intensity = regret_analysis.get('regret_intensity', 0.0)
        
        # ìœ í‹¸ë¦¬í‹° = ë²¤ë‹´ ì ìˆ˜ + ê°ì • ê°•ë„ - í›„íšŒ ê°•ë„
        utility = bentham_score + emotion_intensity - regret_intensity
        return max(0.0, min(1.0, utility))
    
    def _calculate_confidence_score(self, emotion_analysis: Dict, bentham_analysis: Dict, regret_analysis: Dict) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        
        confidences = []
        
        if 'confidence' in emotion_analysis:
            confidences.append(emotion_analysis['confidence'])
        if 'confidence' in bentham_analysis:
            confidences.append(bentham_analysis['confidence'])
        if 'model_confidence' in regret_analysis:
            confidences.append(regret_analysis['model_confidence'])
        
        return sum(confidences) / len(confidences) if confidences else 0.5
    
    def _recommend_optimal_choice(self, dilemma: EthicalDilemma) -> Optional[EthicalChoice]:
        """ìµœì  ì„ íƒì§€ ì¶”ì²œ"""
        
        if not dilemma.choice_analyses:
            return None
        
        # ìœ„í—˜ ì¡°ì • ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìµœì  ì„ íƒì§€ ì„ íƒ
        best_choice = None
        best_score = -1
        
        for choice_id, analysis in dilemma.choice_analyses.items():
            if analysis.risk_adjusted_score > best_score:
                best_score = analysis.risk_adjusted_score
                best_choice = analysis.choice
        
        return best_choice
    
    async def _generate_reasoning_chain(self, dilemma: EthicalDilemma) -> List[str]:
        """ì¶”ë¡  ì²´ì¸ ìƒì„±"""
        
        reasoning = []
        
        # 1. ìƒí™© ë¶„ì„
        reasoning.append(f"ìƒí™© ë¶„ì„: {dilemma.dilemma_type.value} ìœ í˜•ì˜ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ")
        
        # 2. ì„ íƒì§€ ë¶„ì„
        reasoning.append(f"ì‹ë³„ëœ ì„ íƒì§€: {len(dilemma.extracted_choices)}ê°œ")
        
        # 3. ì´í•´ê´€ê³„ì ë¶„ì„
        reasoning.append(f"ì£¼ìš” ì´í•´ê´€ê³„ì: {', '.join(dilemma.stakeholders.keys())}")
        
        # 4. ìµœì  ì„ íƒì§€ ê·¼ê±°
        if dilemma.recommended_choice:
            best_analysis = dilemma.choice_analyses.get(dilemma.recommended_choice.id)
            if best_analysis:
                reasoning.append(f"ìµœì  ì„ íƒì§€: {dilemma.recommended_choice.name}")
                reasoning.append(f"ìœ í‹¸ë¦¬í‹° ì ìˆ˜: {best_analysis.utility_score:.3f}")
                reasoning.append(f"ì‹ ë¢°ë„: {best_analysis.confidence_score:.3f}")
                reasoning.append(f"ìœ„í—˜ ì¡°ì • ì ìˆ˜: {best_analysis.risk_adjusted_score:.3f}")
        
        return reasoning

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_dynamic_analyzer():
    """ë™ì  ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    
    analyzer = DynamicEthicalChoiceAnalyzer()
    
    # ë‹¤ì–‘í•œ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ í…ŒìŠ¤íŠ¸
    test_scenarios = [
        {
            "title": "ì˜ë£Œì§„ ìì› ë°°ë¶„ ë”œë ˆë§ˆ",
            "description": "ì½”ë¡œë‚˜19 ìƒí™©ì—ì„œ ì¸ê³µí˜¸í¡ê¸° 1ëŒ€ë¥¼ ë‘ê³  90ì„¸ í™˜ìì™€ 30ì„¸ í™˜ì ì¤‘ ëˆ„êµ¬ë¥¼ ì„ íƒí•  ê²ƒì¸ê°€? ë‚˜ì´ë¥¼ ê³ ë ¤í•  ê²ƒì¸ê°€, ì•„ë‹ˆë©´ ì„ ì°©ìˆœìœ¼ë¡œ í•  ê²ƒì¸ê°€?",
        },
        {
            "title": "ê°œì¸ì •ë³´ vs ê³µê³µì•ˆì „",
            "description": "í…ŒëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ì‹œë¯¼ë“¤ì˜ ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ê°ì‹œí•  ê²ƒì¸ê°€, ì•„ë‹ˆë©´ ê°œì¸ì˜ í”„ë¼ì´ë²„ì‹œë¥¼ ë³´í˜¸í•  ê²ƒì¸ê°€?",
        },
        {
            "title": "ì¹œêµ¬ì˜ ë¶€ì •í–‰ìœ„",
            "description": "ì‹œí—˜ì—ì„œ ì¹œêµ¬ê°€ ë¶€ì •í–‰ìœ„ë¥¼ í•˜ëŠ” ê²ƒì„ ëª©ê²©í–ˆë‹¤. ì‹ ê³ í•´ì•¼ í• ê¹Œ, ì•„ë‹ˆë©´ ì¹œêµ¬ë¥¼ ë³´í˜¸í•´ì•¼ í• ê¹Œ?",
        }
    ]
    
    for scenario in test_scenarios:
        logger.info(f"\n{'='*60}")
        logger.info(f"í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {scenario['title']}")
        logger.info(f"{'='*60}")
        
        result = await analyzer.analyze_ethical_dilemma(
            dilemma_text=scenario['description'],
            title=scenario['title']
        )
        
        logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ")
        logger.info(f"ë”œë ˆë§ˆ ìœ í˜•: {result.dilemma_type.value}")
        logger.info(f"ì¶”ì¶œëœ ì„ íƒì§€: {len(result.extracted_choices)}ê°œ")
        logger.info(f"ì´í•´ê´€ê³„ì: {len(result.stakeholders)}ëª…")
        
        if result.recommended_choice:
            logger.info(f"ğŸ¯ ìµœì¢… ì¶”ì²œ: {result.recommended_choice.name}")
            
            best_analysis = result.choice_analyses.get(result.recommended_choice.id)
            if best_analysis:
                logger.info(f"   ìœ í‹¸ë¦¬í‹° ì ìˆ˜: {best_analysis.utility_score:.3f}")
                logger.info(f"   ì‹ ë¢°ë„: {best_analysis.confidence_score:.3f}")
        
        logger.info(f"ğŸ§  ì¶”ë¡  ì²´ì¸:")
        for i, reason in enumerate(result.reasoning_chain):
            logger.info(f"   {i+1}. {reason}")

if __name__ == "__main__":
    asyncio.run(test_dynamic_analyzer())