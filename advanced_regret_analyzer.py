"""
Advanced Regret Analyzer for Linux Red Heart System
GPU ê°€ì† í›„íšŒ ë¶„ì„ ë° ì‹ ê²½ë§ í•™ìŠµ ì‹œìŠ¤í…œ

Features:
- CUDA ê¸°ë°˜ ë³‘ë ¬ í›„íšŒ ê³„ì‚°
- íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ í†µí•œ ê³ ê¸‰ í›„íšŒ íŒ¨í„´ í•™ìŠµ
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¶”ì  ë° ë²¤ì¹˜ë§ˆí‚¹
- ë¹„ë™ê¸° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- ê³ ê¸‰ ìºì‹± ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
- ë‹¤ì°¨ì› í›„íšŒ ë©”íŠ¸ë¦­ ë¶„ì„
"""

__all__ = ['AdvancedRegretAnalyzer']

import os
# CVE-2025-32434ëŠ” ê°€ì§œ CVE - torch_security_patch import ì œê±°
# import torch_security_patch

import torch
import torch.nn as nn
import numpy as np
import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
import time

from transformers import (
    AutoTokenizer, AutoModel, 
    BertForSequenceClassification,
    RobertaForSequenceClassification
)
from dynamic_threshold_system import dynamic_threshold_calculator
from mixture_of_experts import create_regret_moe, MixtureOfExperts
from three_view_scenario_system import ThreeViewScenarioSystem
from phase_controller_hook import PhaseControllerHook, PhaseType, PerformanceMetric

# ë¡œê±° ì„¤ì •
logger = logging.getLogger('advanced_regret_analyzer')

@dataclass
class AdvancedRegretMetrics:
    """ê³ ê¸‰ í›„íšŒ í‰ê°€ ë©”íŠ¸ë¦­"""
    decision_id: str
    timestamp: datetime
    
    # ê¸°ë³¸ í›„íšŒ ë©”íŠ¸ë¦­
    anticipated_regret: float
    experienced_regret: float
    regret_intensity: float
    regret_duration: float
    
    # GPU ê°€ì† ê³ ê¸‰ ë©”íŠ¸ë¦­
    semantic_regret_score: float
    emotional_regret_vector: List[float]
    causal_attribution_scores: Dict[str, float]
    counterfactual_utility_delta: float
    
    # í•™ìŠµ ê´€ë ¨ ë©”íŠ¸ë¦­
    prediction_accuracy: float
    learning_rate_adjustment: float
    model_confidence: float
    uncertainty_estimate: float
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    computation_time_ms: float
    gpu_memory_usage_mb: float
    cache_hit_rate: float

class GPURegretNetwork(nn.Module):
    """CUDA ê¸°ë°˜ í›„íšŒ í•™ìŠµ ì‹ ê²½ë§"""
    
    def __init__(self, input_dim: int = 896, hidden_dim: int = 512):
        super().__init__()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ë©€í‹°ë ˆì´ì–´ í›„íšŒ ì˜ˆì¸¡ ë„¤íŠ¸ì›Œí¬
        self.regret_predictor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        ).to(self.device)
        
        # ê°ì • ë²¡í„° ì˜ˆì¸¡ ë„¤íŠ¸ì›Œí¬
        self.emotion_predictor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 8),  # 8ì°¨ì› ê°ì • ë²¡í„°
            nn.Tanh()
        ).to(self.device)
        
        # ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë„¤íŠ¸ì›Œí¬
        self.uncertainty_estimator = nn.Sequential(
            nn.Linear(input_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        ).to(self.device)
        
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        x = x.to(self.device)
        regret_score = self.regret_predictor(x)
        emotion_vector = self.emotion_predictor(x)
        uncertainty = self.uncertainty_estimator(x)
        return regret_score, emotion_vector, uncertainty

class AdvancedRegretAnalyzer:
    """ê³ ê¸‰ GPU ê°€ì† í›„íšŒ ë¶„ì„ê¸°"""
    
    def __init__(self, config_path: str = "system_config.json"):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger = logging.getLogger(__name__)
        
        # ì„¤ì • ë¡œë“œ
        self.config = self._load_config(config_path)
        
        # GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ - ë™ì  ê´€ë¦¬ì ì—°ë™
        from dynamic_gpu_manager import get_gpu_manager
        self.gpu_manager = get_gpu_manager()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            # klue/bert-base ëª¨ë¸ì€ ì•ˆì •ì„±ì„ ìœ„í•´ 15% í• ë‹¹ ìœ ì§€
            self.gpu_memory_fraction = 0.15  # 15% í• ë‹¹ ìœ ì§€
            print(f"ğŸ”§ klue/bert-base ëª¨ë¸: ì•ˆì •ì„± ë³´ì¥ì„ ìœ„í•œ {self.gpu_memory_fraction*100}% í• ë‹¹")
            
            # GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            allocated_memory = torch.cuda.memory_allocated() / 1e9
            available_memory = total_memory - allocated_memory
            
            if available_memory < 1.0:  # 1GB ë¯¸ë§Œì´ë©´ ê²½ê³ 
                print(f"âš ï¸ ê²½ê³ : GPU ë©”ëª¨ë¦¬ ì—¬ìœ ë¶„ {available_memory:.1f}GB - ì˜¤ë²„í—¤ë“œ ìœ„í—˜")
                self.gpu_memory_fraction = 0.05  # ë”ìš± ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ì´ˆê¸°í™” - ìµœì í™”ëœ ìºì‹± ë°©ì‹
        self.tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')
        # ê¸€ë¡œë²Œ ëª¨ë¸ ìºì‹œ ì‚¬ìš©
        self.transformer_model = None
        self.model_loaded = False
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìš”ì²­ í
        self.request_queue = []
        self.batch_size = 4  # ë°°ì¹˜ í¬ê¸°
        self.last_batch_time = time.time()
        self.batch_timeout = 0.1  # 100ms íƒ€ì„ì•„ì›ƒ
        
        print("ğŸ“‹ Transformer ëª¨ë¸: ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ í™œì„±í™”")
        
        # í›„íšŒ í•™ìŠµ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.regret_network = GPURegretNetwork()
        self.optimizer = torch.optim.AdamW(self.regret_network.parameters(), lr=1e-4)
        self.loss_fn = nn.MSELoss()
        
        # ì„±ëŠ¥ ì¶”ì  (ìºì‹œ ì œê±°ë¨)
        self.performance_metrics = []
        self.learning_history = []
        
        # =====================================================
        # ê°•í™” ëª¨ë“ˆ í†µí•© (47M ì¶”ê°€ â†’ ì´ 50M)
        # =====================================================
        base_dim = 768
        
        # 1. ë°˜ì‚¬ì‹¤ ì‹œë®¬ë ˆì´ì…˜ ë„¤íŠ¸ì›Œí¬ (15M)
        self.counterfactual_sim = nn.ModuleDict({
            'world_model': nn.Sequential(
                nn.Linear(base_dim, 1024),
                nn.LayerNorm(1024),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(1024, 1024),
                nn.LayerNorm(1024),
                nn.GELU(),
                nn.Linear(1024, 768),
                nn.LayerNorm(768),
                nn.Linear(768, base_dim)
            ),
            'outcome_predictor': nn.LSTM(
                input_size=base_dim,
                hidden_size=base_dim // 2,
                num_layers=3,
                batch_first=True,
                bidirectional=True
            ),
            'regret_calculator': nn.Sequential(
                nn.Linear(base_dim * 2, base_dim),
                nn.LayerNorm(base_dim),
                nn.GELU(),
                nn.Linear(base_dim, 512),
                nn.Linear(512, 256),
                nn.Linear(256, 1),
                nn.Sigmoid()
            )
        }).to(self.device)
        
        # 2. ì‹œê°„ì¶• í›„íšŒ ì „íŒŒ (12M)
        self.temporal_propagation = nn.ModuleDict({
            'past_encoder': nn.LSTM(
                input_size=base_dim,
                hidden_size=512,
                num_layers=3,
                batch_first=True,
                dropout=0.1
            ),
            'future_predictor': nn.GRU(
                input_size=base_dim,
                hidden_size=512,
                num_layers=3,
                batch_first=True,
                dropout=0.1
            ),
            'temporal_attention': nn.MultiheadAttention(
                embed_dim=base_dim,
                num_heads=12,
                dropout=0.1,
                batch_first=True
            ),
            'regret_dynamics': nn.Sequential(
                nn.Linear(base_dim * 2, base_dim),
                nn.LayerNorm(base_dim),
                nn.GELU(),
                nn.Linear(base_dim, 512),
                nn.Linear(512, 10)  # 10 time steps
            )
        }).to(self.device)
        
        # 3. ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ë¶„ì„ (10M)
        self.decision_tree = nn.ModuleDict({
            'branch_evaluator': nn.ModuleList([
                nn.Sequential(
                    nn.Linear(base_dim, 512),
                    nn.LayerNorm(512),
                    nn.GELU(),
                    nn.Linear(512, 256),
                    nn.Linear(256, 128),
                    nn.Linear(128, 1)
                ) for _ in range(8)  # 8 branches
            ]),
            'path_integrator': nn.Sequential(
                nn.Linear(8, 256),
                nn.GELU(),
                nn.Linear(256, 512),
                nn.LayerNorm(512),
                nn.GELU(),
                nn.Linear(512, base_dim)
            ),
            'decision_scorer': nn.Sequential(
                nn.Linear(base_dim, 512),
                nn.LayerNorm(512),
                nn.GELU(),
                nn.Linear(512, 256),
                nn.Linear(256, 1)
            )
        }).to(self.device)
        
        # 4. ë² ì´ì§€ì•ˆ ì¶”ë¡  (10M + 3M ì¶”ê°€ = 13M)
        self.bayesian_inference = nn.ModuleDict({
            'prior_network': nn.Sequential(
                nn.Linear(base_dim, 768),
                nn.LayerNorm(768),
                nn.GELU(),
                nn.Linear(768, 512),
                nn.Linear(512, 256)
            ),
            'likelihood_network': nn.Sequential(
                nn.Linear(base_dim, 768),
                nn.LayerNorm(768),
                nn.GELU(),
                nn.Linear(768, 512),
                nn.Linear(512, 256)
            ),
            'posterior_network': nn.Sequential(
                nn.Linear(512, 768),
                nn.LayerNorm(768),
                nn.GELU(),
                nn.Linear(768, base_dim)
            ),
            'uncertainty_quantifier': nn.Sequential(
                nn.Linear(base_dim, 512),
                nn.GELU(),
                nn.Dropout(0.2),
                nn.Linear(512, 256),
                nn.Linear(256, 1),
                nn.Softplus()
            ),
            # ì¶”ê°€ ë ˆì´ì–´ (3M)
            'deep_bayesian': nn.Sequential(
                nn.Linear(base_dim, 1024),
                nn.LayerNorm(1024),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(1024, 768),
                nn.LayerNorm(768),
                nn.GELU(),
                nn.Linear(768, base_dim)
            )
        }).to(self.device)
        
        # íŒŒë¼ë¯¸í„° ë¡œê¹…
        total_params = sum(p.numel() for p in [
            *self.counterfactual_sim.parameters(),
            *self.temporal_propagation.parameters(),
            *self.decision_tree.parameters(),
            *self.bayesian_inference.parameters()
        ])
        logger.info(f"âœ… í›„íšŒ ë¶„ì„ê¸° ê°•í™” ëª¨ë“ˆ í†µí•©: {total_params/1e6:.1f}M íŒŒë¼ë¯¸í„° ì¶”ê°€")
        
        # ë¹„ë™ê¸° ì²˜ë¦¬
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # í›„íšŒ ë°ì´í„° ì €ì¥ì†Œ
        self.regret_database = []
        self.decision_outcomes = {}
        
        # ë™ì  ì„ê³„ê°’ ê³„ì‚°ê¸° í†µí•©
        self.dynamic_threshold_calculator = dynamic_threshold_calculator
        
        # Mixture of Experts for í›„íšŒ ë¶„ì„
        self.moe_enabled = True
        if self.moe_enabled:
            try:
                # í›„íšŒ ë¶„ì„ìš© MoE ì´ˆê¸°í™”
                regret_input_dim = 512  # í›„íšŒ ë§¥ë½ ì„ë² ë”© ì°¨ì›
                regret_output_dim = 3   # í›„íšŒ ìœ í˜• ìˆ˜ (action, inaction, outcome)
                
                self.regret_moe = create_regret_moe(
                    input_dim=regret_input_dim,
                    output_dim=regret_output_dim,
                    num_experts=3
                ).to(self.device)
                
                self.logger.info("í›„íšŒ ë¶„ì„ìš© MoE ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (3ê°œ ì „ë¬¸ê°€)")
            except Exception as e:
                self.logger.warning(f"í›„íšŒ MoE ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ ì‹œìŠ¤í…œ ì‚¬ìš©: {e}")
                self.moe_enabled = False
        
        # 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.scenario_system_enabled = True
        if self.scenario_system_enabled:
            try:
                self.three_view_system = ThreeViewScenarioSystem(device=self.device)
                self.logger.info("í›„íšŒ ë¶„ì„ê¸° 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.scenario_system_enabled = False
        
        # PhaseController Hook ì´ˆê¸°í™”
        self.phase_controller_enabled = True
        if self.phase_controller_enabled:
            try:
                # í›„íšŒ ë¶„ì„ê¸° ëª¨ë¸ë“¤ ìˆ˜ì§‘
                models = {}
                if hasattr(self, 'regret_network') and self.regret_network:
                    models['regret_network'] = self.regret_network
                if hasattr(self, 'regret_moe') and self.regret_moe:
                    models['regret_moe'] = self.regret_moe
                
                self.phase_controller = PhaseControllerHook(
                    models=models,
                    performance_threshold=0.8,
                    error_threshold=0.15
                )
                
                # ëª¨ë‹ˆí„°ë§ ì‹œì‘
                self.phase_controller.start_monitoring()
                
                self.logger.info("í›„íšŒ ë¶„ì„ê¸° PhaseController Hook ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"PhaseController Hook ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.phase_controller_enabled = False
        
        self.logger.info(f"Advanced Regret Analyzer initialized on {self.device}")
        self.logger.info("ë™ì  ì„ê³„ê°’ ì‹œìŠ¤í…œ í†µí•© ì™„ë£Œ")
    
    def _load_config(self, config_path: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {
                "regret_analysis": {
                    "learning_rate": 1e-4,
                    "batch_size": 32,
                    "max_sequence_length": 512,
                    "cache_size": 1000,
                    "gpu_memory_fraction": 0.3
                }
            }
    
    def _load_transformer_model_on_demand(self):
        """ìµœì í™”ëœ Transformer ëª¨ë¸ ë¡œë“œ - GPU ê´€ë¦¬ì ì—°ë™"""
        if not self.model_loaded:
            print("ğŸ”„ klue/bert-base ëª¨ë¸ì„ ì•ˆì • ëª¨ë“œë¡œ ë¡œë“œ ì¤‘...")
            start_time = time.time()
            
            try:
                # GPU ë©”ëª¨ë¦¬ ìµœì í™”
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # ëª¨ë¸ ë¡œë“œ ìµœì í™” - ìºì‹œ ì‚¬ìš©
                try:
                    # local_files_only=Trueë¡œ ìºì‹œëœ ëª¨ë¸ ìš°ì„  ì‚¬ìš©
                    self.transformer_model = AutoModel.from_pretrained(
                        'klue/bert-base', 
                        local_files_only=True
                    ).to(self.device)
                    print("ğŸ“¦ ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©")
                except:
                    # ìºì‹œì— ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
                    self.transformer_model = AutoModel.from_pretrained('klue/bert-base').to(self.device)
                    print("ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                
                self.model_loaded = True
                load_time = time.time() - start_time
                
                # GPU ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹…
                memory_status = self.gpu_manager.get_memory_status()
                print(f"âœ… klue/bert-base ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_time:.2f}ì´ˆ)")
                print(f"ğŸ”§ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_status.get('allocated_gb', 0):.1f}GB / {memory_status.get('total_gb', 0):.1f}GB")
            
            except Exception as e:
                self.logger.error(f"âŒ klue/bert-base ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.logger.error("ğŸš¨ ì—°êµ¬ ë‹¨ê³„ì—ì„œ í•µì‹¬ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì  ì˜¤ë¥˜ì…ë‹ˆë‹¤.")
                self.logger.error("ğŸ’¡ í•´ê²° ë°©ë²•: 1) GPU ë©”ëª¨ë¦¬ í™•ì¸, 2) ëª¨ë¸ ìºì‹œ ì •ë¦¬, 3) ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸")
                print(f"âŒ CRITICAL ERROR: klue/bert-base ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                print(f"   ì˜¤ë¥˜ ë‚´ìš©: {e}")
                print(f"   ì—°êµ¬ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë“  ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•´ì•¼ í•©ë‹ˆë‹¤.")
                print(f"   í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                raise RuntimeError(f"Critical model loading failure: {e}")
    
    def _unload_transformer_model(self):
        """Transformer ëª¨ë¸ì„ GPUì—ì„œ í•´ì œí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½"""
        if self.model_loaded and self.transformer_model is not None:
            print("ğŸ—‘ï¸ Transformer ëª¨ë¸ì„ GPUì—ì„œ í•´ì œ ì¤‘...")
            del self.transformer_model
            self.transformer_model = None
            self.model_loaded = False
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print("âœ… GPU ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
    
    async def _process_batch_requests(self):
        """ë°°ì¹˜ë¡œ ìš”ì²­ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ"""
        if not self.request_queue:
            return
        
        current_time = time.time()
        # ë°°ì¹˜ê°€ ê°€ë“ ì°¼ê±°ë‚˜ íƒ€ì„ì•„ì›ƒì´ ì§€ë‚¬ìœ¼ë©´ ì²˜ë¦¬
        if (len(self.request_queue) >= self.batch_size or 
            current_time - self.last_batch_time > self.batch_timeout):
            
            print(f"ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(self.request_queue)}ê°œ ìš”ì²­")
            batch_start = time.time()
            
            # ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ)
            self._load_transformer_model_on_demand()
            
            # ë°°ì¹˜ ì²˜ë¦¬
            for request in self.request_queue:
                # ì—¬ê¸°ì„œ ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
                pass
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if len(self.request_queue) >= self.batch_size:
                self._unload_transformer_model()
            
            batch_time = time.time() - batch_start
            print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ({batch_time:.2f}ì´ˆ, {len(self.request_queue)}ê°œ)")
            
            self.request_queue.clear()
            self.last_batch_time = current_time
    
    async def analyze_regret(self, decision_data: Dict[str, Any], 
                           outcome_data: Optional[Dict[str, Any]] = None) -> AdvancedRegretMetrics:
        """ë¹„ë™ê¸° í›„íšŒ ë¶„ì„ ìˆ˜í–‰ (ìµœì í™”ëœ ì¡°ê±´ë¶€ ë¡œì§)"""
        start_time = time.time()
        initial_memory = self._get_gpu_memory_usage()
        
        try:
            # ì˜ì‚¬ê²°ì • ë°ì´í„° ì „ì²˜ë¦¬
            processed_data = await self._preprocess_decision_data(decision_data)
            
            # ë³µì¡ë„ í‰ê°€ ë° ë¶„ì„ ë°©ë²• ê²°ì •
            complexity_level = self._evaluate_decision_complexity(processed_data)
            
            if complexity_level >= 3:  # ë³µì¡í•œ ì˜ì‚¬ê²°ì •
                # ì „ì²´ ë² ì´ì§€ì•ˆ ë°˜ì‚¬ì‹¤ì  ì¶”ë¡  ì‚¬ìš©
                return await self._perform_complex_regret_analysis(processed_data, outcome_data, start_time, initial_memory)
            else:
                # ê²½ëŸ‰ í›„íšŒ ë¶„ì„ ì‚¬ìš©
                return await self._perform_lightweight_regret_analysis(processed_data, outcome_data, start_time, initial_memory)
                
        except Exception as e:
            logger.error(f"í›„íšŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"í›„íšŒ ë¶„ì„ ì‹¤íŒ¨, fallback ë¹„í™œì„±í™”ë¨: {e}")
    
    def _evaluate_decision_complexity(self, processed_data: Dict[str, Any]) -> int:
        """ì˜ì‚¬ê²°ì • ë³µì¡ë„ í‰ê°€ (1-5 ì ìˆ˜)"""
        complexity_score = 0
        text = processed_data.get('text', '')
        
        # 1. í…ìŠ¤íŠ¸ ê¸¸ì´ (ê¸°ë³¸ ë³µì¡ë„)
        if len(text) > 100:
            complexity_score += 1
        if len(text) > 300:
            complexity_score += 1
            
        # 2. ê°ì •ì  ë³µì¡ë„ (ê°ì • í‚¤ì›Œë“œ ìˆ˜)
        emotion_keywords = ['í›„íšŒ', 'ë¯¸ì•ˆ', 'ì•„ì‰¬', 'ì•ˆíƒ€ê¹', 'ì‹¤ë§', 'ì¢Œì ˆ', 'ê°ˆë“±', 'í˜¼ë€', 'ë”œë ˆë§ˆ']
        emotion_count = sum(1 for word in emotion_keywords if word in text)
        if emotion_count >= 2:
            complexity_score += 1
            
        # 3. ëŒ€ì•ˆì˜ ìˆ˜ (choice, ì„ íƒ, ë°©ë²• ë“±)
        alternative_indicators = ['ì„ íƒ', 'ë°©ë²•', 'ëŒ€ì•ˆ', 'ì˜µì…˜', 'ê°€ëŠ¥ì„±', 'ê²½ìš°']
        alternative_count = sum(1 for word in alternative_indicators if word in text)
        if alternative_count >= 2:
            complexity_score += 1
            
        # 4. ì‹œê°„ì  ë³µì¡ë„ (ê³¼ê±°, í˜„ì¬, ë¯¸ë˜ ì–¸ê¸‰)
        temporal_indicators = ['ê³¼ê±°', 'í˜„ì¬', 'ë¯¸ë˜', 'ì „ì—', 'ì§€ê¸ˆ', 'ë‚˜ì¤‘ì—', 'ì•ìœ¼ë¡œ']
        temporal_count = sum(1 for word in temporal_indicators if word in text)
        if temporal_count >= 2:
            complexity_score += 1
            
        return min(complexity_score, 5)
    
    async def _perform_complex_regret_analysis(self, processed_data: Dict[str, Any], 
                                             outcome_data: Optional[Dict[str, Any]], 
                                             start_time: float, initial_memory: float) -> AdvancedRegretMetrics:
        """ë³µì¡í•œ í›„íšŒ ë¶„ì„ - ì „ì²´ ë² ì´ì§€ì•ˆ ì¶”ë¡ """
        # íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì˜ë¯¸ì  ì„ë² ë”© ìƒì„±
        semantic_embedding = await self._generate_semantic_embedding(processed_data['text'])
        
        # GPU ê¸°ë°˜ í›„íšŒ ì˜ˆì¸¡
        regret_predictions = await self._predict_regret(semantic_embedding)
        
        # ë°˜ì‚¬ì‹¤ì  ë¶„ì„
        counterfactual_analysis = await self._perform_counterfactual_analysis(
            processed_data, outcome_data
        )
        
        # MoE ê¸°ë°˜ í›„íšŒ ìœ í˜• ë¶„ì„
        if self.moe_enabled:
            regret_predictions = await self._apply_moe_regret_analysis(
                semantic_embedding, regret_predictions, processed_data
            )
        
        # 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ í›„íšŒ ë¶„ì„
        if self.scenario_system_enabled:
            regret_predictions = await self._apply_scenario_regret_analysis(
                regret_predictions, processed_data
            )
        
        # ì¸ê³¼ê´€ê³„ ë¶„ì„
        causal_attribution = await self._analyze_causal_attribution(
            processed_data, semantic_embedding
        )
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        computation_time = (time.time() - start_time) * 1000
        final_memory = self._get_gpu_memory_usage()
        memory_usage = final_memory - initial_memory
        
        # í›„íšŒ ì˜ˆì¸¡ ê²°ê³¼ ê²€ì¦
        if (regret_predictions.get('intensity', 0) <= 0.0 and 
            regret_predictions.get('anticipated', 0) <= 0.0):
            logger.warning(f"ë³µì¡í•œ í›„íšŒ ë¶„ì„ì—ì„œ 0.0 ê°’ë“¤ ê°ì§€, ìµœì†Œê°’ ë³´ì¥")
            regret_predictions['intensity'] = max(0.1, regret_predictions.get('intensity', 0.1))
            regret_predictions['anticipated'] = max(0.1, regret_predictions.get('anticipated', 0.1))
            regret_predictions['experienced'] = max(0.1, regret_predictions.get('experienced', 0.1))
        
        # ë™ì  ì„ê³„ê°’ ê¸°ë°˜ í›„íšŒ íŒì •
        dynamic_threshold = counterfactual_analysis.get('dynamic_threshold', 0.3)
        relative_regret = counterfactual_analysis.get('relative_regret', 0.0)
        
        # í›„íšŒ ê°•ë„ ì¡°ì • (ë™ì  ì„ê³„ê°’ ì ìš©)
        if relative_regret > dynamic_threshold:
            # ì„ê³„ê°’ì„ ì´ˆê³¼í•œ ê²½ìš° í›„íšŒ ê°•ë„ ì¦ê°€
            adjusted_intensity = regret_predictions['intensity'] * (1.0 + relative_regret)
            adjusted_anticipated = regret_predictions['anticipated'] * (1.0 + relative_regret)
        else:
            # ì„ê³„ê°’ ë¯¸ë§Œì¸ ê²½ìš° í›„íšŒ ê°•ë„ ê°ì†Œ
            adjusted_intensity = regret_predictions['intensity'] * 0.8
            adjusted_anticipated = regret_predictions['anticipated'] * 0.8
        
        # ì¢…í•© í›„íšŒ ë©”íŠ¸ë¦­ ìƒì„±
        regret_metrics = AdvancedRegretMetrics(
            decision_id=processed_data.get('id', f"decision_{datetime.now().isoformat()}"),
            timestamp=datetime.now(),
            anticipated_regret=max(0.0, min(1.0, adjusted_anticipated)),
            experienced_regret=regret_predictions['experienced'],
            regret_intensity=max(0.0, min(1.0, adjusted_intensity)),
            regret_duration=regret_predictions['duration'],
            semantic_regret_score=regret_predictions['semantic_score'],
            emotional_regret_vector=regret_predictions['emotion_vector'],
            causal_attribution_scores=causal_attribution,
            counterfactual_utility_delta=counterfactual_analysis['utility_delta'],
            prediction_accuracy=regret_predictions['accuracy'],
            learning_rate_adjustment=regret_predictions['lr_adjustment'],
            model_confidence=regret_predictions['confidence'],
            uncertainty_estimate=regret_predictions['uncertainty'],
            computation_time_ms=computation_time,
            gpu_memory_usage_mb=memory_usage,
            cache_hit_rate=self._calculate_cache_hit_rate()
        )
        
        # í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸
        await self._update_learning_data(regret_metrics, outcome_data)
        
        # PhaseController Hookì— ì„±ëŠ¥ ê¸°ë¡
        if self.phase_controller_enabled:
            try:
                # í›„íšŒ ì˜ˆì¸¡ ì˜¤ì°¨ ê³„ì‚°
                prediction_error = abs(regret_metrics.anticipated_regret - regret_metrics.experienced_regret)
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ êµ¬ì„±
                performance_metrics = {
                    'regret_prediction_error': prediction_error,
                    'processing_time_ms': regret_metrics.computation_time_ms,
                    'confidence_score': regret_metrics.model_confidence,
                    'uncertainty_estimate': regret_metrics.uncertainty_estimate,
                    'error': prediction_error  # ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ìš©
                }
                
                # ëª¨ë¸ë³„ ì„±ëŠ¥
                model_performances = {}
                if hasattr(regret_metrics, 'moe_metadata'):
                    model_performances['regret_moe'] = regret_metrics.moe_metadata
                if hasattr(regret_metrics, 'scenario_metadata'):
                    model_performances['scenario_system'] = regret_metrics.scenario_metadata
                
                # ì»¨í…ìŠ¤íŠ¸ ì •ë³´
                context = {
                    'analysis_type': 'complex_regret_analysis',
                    'decision_id': regret_metrics.decision_id,
                    'regret_intensity': regret_metrics.regret_intensity,
                    'anticipated_regret': regret_metrics.anticipated_regret,
                    'experienced_regret': regret_metrics.experienced_regret,
                    'complexity_level': complexity_level
                }
                
                # ì„±ëŠ¥ ê¸°ë¡
                self.phase_controller.record_performance(
                    phase_type=PhaseType.INFERENCE,
                    metrics=performance_metrics,
                    model_performances=model_performances,
                    context=context
                )
                
            except Exception as e:
                self.logger.warning(f"PhaseController ì„±ëŠ¥ ê¸°ë¡ ì‹¤íŒ¨: {e}")
        
        return regret_metrics
    
    async def _apply_moe_regret_analysis(self, semantic_embedding: torch.Tensor,
                                       regret_predictions: Dict[str, float],
                                       processed_data: Dict[str, Any]) -> Dict[str, float]:
        """MoE ê¸°ë°˜ í›„íšŒ ìœ í˜• ë¶„ì„ ë° ì˜ˆì¸¡ ê°œì„ """
        try:
            # 1. MoE ì‹œìŠ¤í…œì„ í†µí•œ í›„íšŒ ìœ í˜•ë³„ ë¶„ì„ (ì•ˆì „í•œ ì°¨ì› ì²˜ë¦¬)
            if semantic_embedding.dim() == 1:
                semantic_input = semantic_embedding.unsqueeze(0)
            else:
                semantic_input = semantic_embedding
            moe_result = self.regret_moe(semantic_input, return_expert_outputs=True)
            
            # 2. ì „ë¬¸ê°€ë³„ ê²°ê³¼ ë¶„ì„
            expert_insights = {}
            for expert_output in moe_result.expert_outputs:
                expert_id = expert_output.expert_id
                expert_confidence = expert_output.confidence
                expert_weight = expert_output.weight
                
                # ì „ë¬¸ê°€ ìœ í˜•ë³„ íŠ¹í™” ë¶„ì„
                if 'action_regret' in expert_id:
                    expert_insights['action_regret'] = {
                        'confidence': expert_confidence,
                        'weight': expert_weight,
                        'prediction': expert_output.output.item()
                    }
                elif 'inaction_regret' in expert_id:
                    expert_insights['inaction_regret'] = {
                        'confidence': expert_confidence,
                        'weight': expert_weight,
                        'prediction': expert_output.output.item()
                    }
                elif 'outcome_regret' in expert_id:
                    expert_insights['outcome_regret'] = {
                        'confidence': expert_confidence,
                        'weight': expert_weight,
                        'prediction': expert_output.output.item()
                    }
            
            # 3. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
            context_weights = self._calculate_context_weights(processed_data)
            
            # 4. ì „ë¬¸ê°€ í•©ì˜ ê¸°ë°˜ ì˜ˆì¸¡ ê°œì„ 
            improved_predictions = regret_predictions.copy()
            
            # í–‰ë™ í›„íšŒ vs ë¹„í–‰ë™ í›„íšŒ ë¹„ìœ¨ ê³„ì‚°
            action_weight = expert_insights.get('action_regret', {}).get('weight', 0.0)
            inaction_weight = expert_insights.get('inaction_regret', {}).get('weight', 0.0)
            outcome_weight = expert_insights.get('outcome_regret', {}).get('weight', 0.0)
            
            total_weight = action_weight + inaction_weight + outcome_weight
            if total_weight > 0:
                # ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜ ê³„ì‚°
                norm_action = action_weight / total_weight
                norm_inaction = inaction_weight / total_weight
                norm_outcome = outcome_weight / total_weight
                
                # MoE ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ê°œì„ 
                moe_intensity = (
                    expert_insights.get('action_regret', {}).get('prediction', 0.0) * norm_action +
                    expert_insights.get('inaction_regret', {}).get('prediction', 0.0) * norm_inaction +
                    expert_insights.get('outcome_regret', {}).get('prediction', 0.0) * norm_outcome
                )
                
                # ê¸°ì¡´ ì˜ˆì¸¡ê³¼ MoE ê²°ê³¼ ë¸”ë Œë”©
                blend_factor = moe_result.diversity_score  # ë‹¤ì–‘ì„± ì ìˆ˜ë¥¼ ë¸”ë Œë”© íŒ©í„°ë¡œ ì‚¬ìš©
                improved_predictions['intensity'] = (
                    regret_predictions['intensity'] * (1 - blend_factor) +
                    moe_intensity * blend_factor
                )
                
                # ì˜ˆìƒ í›„íšŒë„ ë¹„ìŠ·í•˜ê²Œ ì¡°ì •
                improved_predictions['anticipated'] = (
                    regret_predictions['anticipated'] * (1 - blend_factor) +
                    moe_intensity * 0.9 * blend_factor  # ì•½ê°„ ë‚®ì€ ê°€ì¤‘ì¹˜
                )
                
                # ì‹ ë¢°ë„ ê°œì„  (ì „ë¬¸ê°€ ì‹ ë¢°ë„ ë°˜ì˜)
                avg_expert_confidence = np.mean([
                    expert_insights.get('action_regret', {}).get('confidence', 0.5),
                    expert_insights.get('inaction_regret', {}).get('confidence', 0.5),
                    expert_insights.get('outcome_regret', {}).get('confidence', 0.5)
                ])
                improved_predictions['confidence'] = min(0.95, 
                    regret_predictions['confidence'] * 0.7 + avg_expert_confidence * 0.3
                )
                
                # ë¶ˆí™•ì‹¤ì„± ì¡°ì • (ë‹¤ì–‘ì„±ì´ ë†’ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤ì„± ì¦ê°€)
                uncertainty_adjustment = 1.0 + (moe_result.diversity_score - 0.5) * 0.2
                improved_predictions['uncertainty'] = min(1.0,
                    regret_predictions['uncertainty'] * uncertainty_adjustment
                )
            
            # 5. ë©”íƒ€ë°ì´í„° ì¶”ê°€
            improved_predictions['moe_metadata'] = {
                'expert_count': len(moe_result.expert_outputs),
                'diversity_score': moe_result.diversity_score,
                'top_expert': max(expert_insights.keys(), 
                                key=lambda k: expert_insights[k]['weight']) if expert_insights else None,
                'total_experts_used': moe_result.total_experts_used
            }
            
            logger.info(f"MoE í›„íšŒ ë¶„ì„ ì™„ë£Œ: {len(expert_insights)}ê°œ ì „ë¬¸ê°€ í™œìš©, "
                       f"ë‹¤ì–‘ì„± ì ìˆ˜: {moe_result.diversity_score:.3f}")
            
            return improved_predictions
            
        except Exception as e:
            logger.warning(f"MoE í›„íšŒ ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ì˜ˆì¸¡ ì‚¬ìš©: {e}")
            return regret_predictions
    
    def _calculate_context_weights(self, processed_data: Dict[str, Any]) -> Dict[str, float]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        text = processed_data.get('text', '').lower()
        
        # í…ìŠ¤íŠ¸ ë¶„ì„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = {
            'action_regret': 0.33,
            'inaction_regret': 0.33,
            'outcome_regret': 0.34
        }
        
        # í–‰ë™ ê´€ë ¨ í‚¤ì›Œë“œ
        action_keywords = ['í–ˆë‹¤', 'í–‰ë™', 'ì‹¤í–‰', 'ì§„í–‰', 'ìˆ˜í–‰', 'ì²˜ë¦¬']
        action_count = sum(1 for keyword in action_keywords if keyword in text)
        
        # ë¹„í–‰ë™ ê´€ë ¨ í‚¤ì›Œë“œ
        inaction_keywords = ['ì•ˆí–ˆë‹¤', 'í•˜ì§€ì•Šì•˜ë‹¤', 'í¬ê¸°', 'ë¯¸ë£¨', 'ê¸°íšŒ', 'ë†“ì³¤ë‹¤']
        inaction_count = sum(1 for keyword in inaction_keywords if keyword in text)
        
        # ê²°ê³¼ ê´€ë ¨ í‚¤ì›Œë“œ
        outcome_keywords = ['ê²°ê³¼', 'ì„±ê³¼', 'íš¨ê³¼', 'ì˜í–¥', 'ë³€í™”', 'ë‹¬ì„±']
        outcome_count = sum(1 for keyword in outcome_keywords if keyword in text)
        
        # ê°€ì¤‘ì¹˜ ì¡°ì •
        total_count = action_count + inaction_count + outcome_count
        if total_count > 0:
            weights['action_regret'] = 0.2 + (action_count / total_count) * 0.6
            weights['inaction_regret'] = 0.2 + (inaction_count / total_count) * 0.6
            weights['outcome_regret'] = 0.2 + (outcome_count / total_count) * 0.6
            
            # ì •ê·œí™”
            total_weight = sum(weights.values())
            for key in weights:
                weights[key] /= total_weight
        
        return weights
    
    async def _apply_scenario_regret_analysis(self, regret_predictions: Dict[str, float],
                                           processed_data: Dict[str, Any]) -> Dict[str, float]:
        """3ë·° ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ í›„íšŒ ë¶„ì„"""
        try:
            # 1. 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ìˆ˜í–‰
            scenario_analysis = await self.three_view_system.analyze_three_view_scenarios(processed_data)
            
            # 2. ì‹œë‚˜ë¦¬ì˜¤ë³„ í›„íšŒ ê°€ì¤‘ì¹˜ ê³„ì‚°
            scenario_weights = {
                'optimistic': scenario_analysis.optimistic_scenario.probability_weight,
                'neutral': scenario_analysis.neutral_scenario.probability_weight,
                'pessimistic': scenario_analysis.pessimistic_scenario.probability_weight
            }
            
            # 3. ì‹œë‚˜ë¦¬ì˜¤ë³„ í›„íšŒ ê°•ë„ ê³„ì‚°
            scenario_regrets = {
                'optimistic': scenario_analysis.optimistic_scenario.regret_potential,
                'neutral': scenario_analysis.neutral_scenario.regret_potential,
                'pessimistic': scenario_analysis.pessimistic_scenario.regret_potential
            }
            
            # 4. ê°€ì¤‘ í‰ê·  í›„íšŒ ê³„ì‚°
            total_weight = sum(scenario_weights.values())
            if total_weight > 0:
                weighted_regret = sum(
                    scenario_regrets[scenario] * scenario_weights[scenario]
                    for scenario in scenario_regrets
                ) / total_weight
            else:
                weighted_regret = scenario_regrets['neutral']
            
            # 5. ê¸°ì¡´ í›„íšŒ ì˜ˆì¸¡ê³¼ í†µí•©
            enhanced_predictions = regret_predictions.copy()
            
            # ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ì¡´ ì˜ˆì¸¡ê³¼ ë¸”ë Œë”©
            scenario_influence = min(0.4, scenario_analysis.consensus_strength)  # ìµœëŒ€ 40% ì˜í–¥
            
            # ì˜ˆìƒ í›„íšŒ ì¡°ì •
            original_anticipated = regret_predictions.get('anticipated', 0.5)
            enhanced_predictions['anticipated'] = (
                original_anticipated * (1 - scenario_influence) +
                weighted_regret * scenario_influence
            )
            
            # í›„íšŒ ê°•ë„ ì¡°ì •
            original_intensity = regret_predictions.get('intensity', 0.5)
            enhanced_predictions['intensity'] = (
                original_intensity * (1 - scenario_influence) +
                weighted_regret * scenario_influence
            )
            
            # ê²½í—˜ í›„íšŒ ì¡°ì • (ì‹œë‚˜ë¦¬ì˜¤ ë‹¤ì–‘ì„± ê³ ë ¤)
            diversity_factor = scenario_analysis.scenario_diversity
            original_experienced = regret_predictions.get('experienced', 0.5)
            enhanced_predictions['experienced'] = (
                original_experienced * (1 - diversity_factor * 0.3) +
                weighted_regret * diversity_factor * 0.3
            )
            
            # 6. ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„¸ë¶€ ë¶„ì„
            regret_breakdown = self._analyze_scenario_regret_breakdown(scenario_analysis)
            
            # 7. ë¶ˆí™•ì‹¤ì„± ì¡°ì •
            uncertainty_range = scenario_analysis.uncertainty_range
            uncertainty_span = abs(uncertainty_range[1] - uncertainty_range[0])
            
            # ë¶ˆí™•ì‹¤ì„±ì´ í´ìˆ˜ë¡ í›„íšŒ ê°€ëŠ¥ì„± ì¦ê°€
            uncertainty_adjustment = 1.0 + uncertainty_span * 0.2
            enhanced_predictions['uncertainty'] = min(1.0,
                regret_predictions.get('uncertainty', 0.5) * uncertainty_adjustment
            )
            
            # 8. ì‹ ë¢°ë„ ì¡°ì • (í•©ì˜ ê°•ë„ ë°˜ì˜)
            enhanced_predictions['confidence'] = min(0.95,
                regret_predictions.get('confidence', 0.7) * scenario_analysis.consensus_strength
            )
            
            # 9. ë©”íƒ€ë°ì´í„° ì¶”ê°€
            enhanced_predictions['scenario_metadata'] = {
                'consensus_regret': scenario_analysis.consensus_regret,
                'uncertainty_range': uncertainty_range,
                'scenario_diversity': scenario_analysis.scenario_diversity,
                'consensus_strength': scenario_analysis.consensus_strength,
                'recommended_decision': scenario_analysis.recommended_decision,
                'scenario_weights': scenario_weights,
                'scenario_regrets': scenario_regrets,
                'regret_breakdown': regret_breakdown
            }
            
            self.logger.debug(f"3ë·° ì‹œë‚˜ë¦¬ì˜¤ í›„íšŒ ë¶„ì„ ì™„ë£Œ: í•©ì˜ í›„íšŒ {scenario_analysis.consensus_regret:.3f}, "
                            f"ë‹¤ì–‘ì„± {scenario_analysis.scenario_diversity:.3f}")
            
            return enhanced_predictions
            
        except Exception as e:
            self.logger.warning(f"3ë·° ì‹œë‚˜ë¦¬ì˜¤ í›„íšŒ ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ì˜ˆì¸¡ ì‚¬ìš©: {e}")
            return regret_predictions
    
    def _analyze_scenario_regret_breakdown(self, scenario_analysis) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ í›„íšŒ ì„¸ë¶€ ë¶„ì„"""
        
        breakdown = {
            'optimistic_risks': scenario_analysis.optimistic_scenario.risk_factors,
            'neutral_risks': scenario_analysis.neutral_scenario.risk_factors,
            'pessimistic_risks': scenario_analysis.pessimistic_scenario.risk_factors,
            'optimistic_opportunities': scenario_analysis.optimistic_scenario.opportunity_factors,
            'neutral_opportunities': scenario_analysis.neutral_scenario.opportunity_factors,
            'pessimistic_opportunities': scenario_analysis.pessimistic_scenario.opportunity_factors
        }
        
        # ìœ„í—˜ ìš”ì†Œ ë¶„ì„
        all_risks = set()
        for risks in [breakdown['optimistic_risks'], breakdown['neutral_risks'], breakdown['pessimistic_risks']]:
            all_risks.update(risks)
        
        risk_frequency = {}
        for risk in all_risks:
            frequency = sum(1 for risks in [breakdown['optimistic_risks'], breakdown['neutral_risks'], breakdown['pessimistic_risks']] if risk in risks)
            risk_frequency[risk] = frequency / 3.0  # ì •ê·œí™”
        
        # ê¸°íšŒ ìš”ì†Œ ë¶„ì„
        all_opportunities = set()
        for opportunities in [breakdown['optimistic_opportunities'], breakdown['neutral_opportunities'], breakdown['pessimistic_opportunities']]:
            all_opportunities.update(opportunities)
        
        opportunity_frequency = {}
        for opportunity in all_opportunities:
            frequency = sum(1 for opportunities in [breakdown['optimistic_opportunities'], breakdown['neutral_opportunities'], breakdown['pessimistic_opportunities']] if opportunity in opportunities)
            opportunity_frequency[opportunity] = frequency / 3.0  # ì •ê·œí™”
        
        # í›„íšŒ ìœ í˜• ë¶„ì„
        regret_types = {
            'action_regret': 0.0,
            'inaction_regret': 0.0,
            'outcome_regret': 0.0
        }
        
        # ìœ„í—˜ ìš”ì†Œ ê¸°ë°˜ í›„íšŒ ìœ í˜• ì¶”ì •
        for risk, freq in risk_frequency.items():
            if any(keyword in risk.lower() for keyword in ['ì‹¤í–‰', 'í–‰ë™', 'ì§„í–‰']):
                regret_types['action_regret'] += freq * 0.3
            elif any(keyword in risk.lower() for keyword in ['ê¸°íšŒ', 'ë†“ì¹¨', 'ë¯¸ë£¨']):
                regret_types['inaction_regret'] += freq * 0.3
            else:
                regret_types['outcome_regret'] += freq * 0.3
        
        # ì •ê·œí™”
        total_regret = sum(regret_types.values())
        if total_regret > 0:
            for regret_type in regret_types:
                regret_types[regret_type] /= total_regret
        
        return {
            'risk_frequency': risk_frequency,
            'opportunity_frequency': opportunity_frequency,
            'regret_types': regret_types,
            'top_risks': sorted(risk_frequency.items(), key=lambda x: x[1], reverse=True)[:3],
            'top_opportunities': sorted(opportunity_frequency.items(), key=lambda x: x[1], reverse=True)[:3]
        }
    
    async def _perform_lightweight_regret_analysis(self, processed_data: Dict[str, Any], 
                                                 outcome_data: Optional[Dict[str, Any]], 
                                                 start_time: float, initial_memory: float) -> AdvancedRegretMetrics:
        """ê²½ëŸ‰ í›„íšŒ ë¶„ì„ - ë¹ ë¥¸ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í›„íšŒ ì ìˆ˜ ê³„ì‚°
        regret_score = self._calculate_heuristic_regret_score(processed_data['text'])
        
        # ê¸°ë³¸ ê°ì • ë²¡í„° ìƒì„±
        emotion_vector = self._generate_basic_emotion_vector(processed_data['text'])
        
        # ê°„ë‹¨í•œ ì¸ê³¼ê´€ê³„ ë¶„ì„
        causal_attribution = self._analyze_basic_causal_attribution(processed_data)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        computation_time = (time.time() - start_time) * 1000
        final_memory = self._get_gpu_memory_usage()
        memory_usage = final_memory - initial_memory
        
        # í›„íšŒ ì ìˆ˜ ê²€ì¦ ë° ìµœì†Œê°’ ë³´ì¥
        if regret_score <= 0.0:
            logger.warning(f"í›„íšŒ ë¶„ì„ì—ì„œ 0.0 ì ìˆ˜ ê°ì§€: '{processed_data['text'][:50]}...'")
            regret_score = max(0.1, regret_score)  # ìµœì†Œ 0.1 ë³´ì¥
            
        # ê²½ëŸ‰ í›„íšŒ ë©”íŠ¸ë¦­ ìƒì„±
        regret_metrics = AdvancedRegretMetrics(
            decision_id=processed_data.get('id', f"decision_{datetime.now().isoformat()}"),
            timestamp=datetime.now(),
            anticipated_regret=regret_score,
            experienced_regret=regret_score * 0.8,  # ì¶”ì •ê°’
            regret_intensity=regret_score,
            regret_duration=min(regret_score * 10, 100),  # ìµœëŒ€ 100
            semantic_regret_score=regret_score,
            emotional_regret_vector=emotion_vector,
            causal_attribution_scores=causal_attribution,
            counterfactual_utility_delta=regret_score * 0.5,
            prediction_accuracy=0.7,  # ê²½ëŸ‰ ë¶„ì„ ê¸°ë³¸ê°’
            learning_rate_adjustment=0.01,
            model_confidence=max(0.6, regret_score),  # ì‹¤ì œ ì ìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„
            uncertainty_estimate=0.4,
            computation_time_ms=computation_time,
            gpu_memory_usage_mb=memory_usage,
            cache_hit_rate=self._calculate_cache_hit_rate()
        )
        
        return regret_metrics
    
    def _calculate_heuristic_regret_score(self, text: str) -> float:
        """íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í›„íšŒ ì ìˆ˜ ê³„ì‚°"""
        regret_keywords = {
            'high': ['í›„íšŒ', 'ë¯¸ì•ˆ', 'ì‹¤ìˆ˜', 'ì˜ëª»', 'ì•„ì‰¬', 'ì•ˆíƒ€ê¹', 'ì‹¤ë§'],
            'medium': ['ê±±ì •', 'ê³ ë¯¼', 'ë§ì„¤', 'ë¶ˆì•ˆ', 'ì˜ì‹¬', 'ê°ˆë“±'],
            'low': ['ìƒê°', 'ê³ ë ¤', 'íŒë‹¨', 'ê²°ì •', 'ì„ íƒ']
        }
        
        text_lower = text.lower()
        score = 0.0
        
        for word in regret_keywords['high']:
            if word in text_lower:
                score += 0.8
        
        for word in regret_keywords['medium']:
            if word in text_lower:
                score += 0.5
                
        for word in regret_keywords['low']:
            if word in text_lower:
                score += 0.2
        
        return min(score, 1.0)
    
    def _generate_basic_emotion_vector(self, text: str) -> List[float]:
        """ê¸°ë³¸ ê°ì • ë²¡í„° ìƒì„±"""
        emotion_keywords = {
            'sadness': ['ìŠ¬í”„', 'ìš°ìš¸', 'ì‹¤ë§', 'ì¢Œì ˆ'],
            'anger': ['í™”ë‚˜', 'ë¶„ë…¸', 'ì§œì¦', 'ì–µìš¸'],
            'fear': ['ë‘ë ¤', 'ë¶ˆì•ˆ', 'ê±±ì •', 'ë¬´ì„œ'],
            'regret': ['í›„íšŒ', 'ë¯¸ì•ˆ', 'ì•„ì‰¬', 'ì•ˆíƒ€ê¹']
        }
        
        text_lower = text.lower()
        vector = [0.0] * 8  # 8ì°¨ì› ê°ì • ë²¡í„°
        
        for i, (emotion, keywords) in enumerate(emotion_keywords.items()):
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if i < len(vector):
                vector[i] = min(score / len(keywords), 1.0)
        
        return vector
    
    def _analyze_basic_causal_attribution(self, processed_data: Dict[str, Any]) -> Dict[str, float]:
        """ê¸°ë³¸ ì¸ê³¼ê´€ê³„ ë¶„ì„"""
        text = processed_data.get('text', '')
        
        # ê°„ë‹¨í•œ ì¸ê³¼ê´€ê³„ ì§€í‘œë“¤
        causal_indicators = {
            'personal_choice': ['ì„ íƒ', 'ê²°ì •', 'íŒë‹¨', 'í–‰ë™'],
            'external_pressure': ['ì••ë ¥', 'ê°•ìš”', 'ìš”êµ¬', 'í•„ìš”'],
            'circumstances': ['ìƒí™©', 'í™˜ê²½', 'ì¡°ê±´', 'í˜„ì‹¤'],
            'emotions': ['ê°ì •', 'ê¸°ë¶„', 'ë§ˆìŒ', 'ëŠë‚Œ']
        }
        
        attribution_scores = {}
        text_lower = text.lower()
        
        for factor, indicators in causal_indicators.items():
            score = sum(1 for indicator in indicators if indicator in text_lower)
            attribution_scores[factor] = min(score / len(indicators), 1.0)
        
        return attribution_scores
    
    async def _preprocess_decision_data(self, decision_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì˜ì‚¬ê²°ì • ë°ì´í„° ì „ì²˜ë¦¬"""
        return {
            'text': f"{decision_data.get('scenario', '')} {decision_data.get('action', '')}",
            'context': decision_data.get('context', {}),
            'stakeholders': decision_data.get('stakeholders', []),
            'constraints': decision_data.get('constraints', []),
            'alternatives': decision_data.get('alternatives', [])
        }
    
    async def _generate_semantic_embedding(self, text: str) -> torch.Tensor:
        """íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì˜ë¯¸ì  ì„ë² ë”© ìƒì„± - ì—°êµ¬ê¸‰ ë¶„ì„"""
        # ìºì‹œ ì™„ì „ ì œê±° - ê° ë¶„ì„ë§ˆë‹¤ ì™„ì „í•œ ì²˜ë¦¬ ë³´ì¥
        
        # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¡œë“œ
        self._load_transformer_model_on_demand()
        
        # ëª¨ë¸ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ì˜¤ë¥˜ ë°œìƒ
        if not self.model_loaded or self.transformer_model is None:
            self.logger.error("âŒ Transformer ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ - ì„ë² ë”© ìƒì„± ë¶ˆê°€")
            raise RuntimeError("Critical error: Transformer model not loaded. Cannot generate semantic embeddings.")
        
        try:
            inputs = self.tokenizer(
                text, 
                return_tensors='pt', 
                padding=True, 
                truncation=True, 
                max_length=512
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.transformer_model(**inputs)
                embedding = outputs.last_hidden_state.mean(dim=1)
            
            # ìºì‹œ ì™„ì „ ì œê±°ë¨
            
            return embedding
        
        except Exception as e:
            self.logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            self.logger.error("ğŸš¨ ì—°êµ¬ ë‹¨ê³„ì—ì„œ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ëŠ” í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            raise RuntimeError(f"Critical embedding generation failure: {e}")
    
    async def _predict_regret(self, semantic_embedding: torch.Tensor) -> Dict[str, float]:
        """GPU ê¸°ë°˜ í›„íšŒ ì˜ˆì¸¡"""
        with torch.no_grad():
            regret_score, emotion_vector, uncertainty = self.regret_network(semantic_embedding)
        
        # CPUë¡œ ì´ë™í•˜ì—¬ ê²°ê³¼ ì¶”ì¶œ
        regret_score = regret_score.cpu().numpy().flatten()[0]
        emotion_vector = emotion_vector.cpu().numpy().flatten().tolist()
        uncertainty = uncertainty.cpu().numpy().flatten()[0]
        
        # ê³ ê¸‰ í›„íšŒ ê³„ì‚° - ë‹¤ì°¨ì› ë¶„ì„
        contextual_factors = self._analyze_contextual_factors(semantic_embedding)
        temporal_decay = self._calculate_temporal_decay(uncertainty)
        cognitive_load = self._estimate_cognitive_load(emotion_vector)
        
        # ë³µí•©ì  í›„íšŒ ì ìˆ˜ ê³„ì‚° (0-1 ë²”ìœ„ ë³´ì¥)
        base_regret = float(regret_score)
        anticipated_regret = min(1.0, base_regret * (1 + contextual_factors * 0.3))
        experienced_regret = min(1.0, anticipated_regret * (0.7 + temporal_decay * 0.3))
        intensity = min(1.0, base_regret * (1 + uncertainty + cognitive_load * 0.2))
        duration = self._calculate_regret_duration(base_regret, uncertainty, cognitive_load)
        
        return {
            'anticipated': anticipated_regret,
            'experienced': experienced_regret,
            'intensity': intensity,
            'duration': duration,
            'semantic_score': base_regret,
            'emotion_vector': emotion_vector,
            'accuracy': max(0.0, min(1.0, 1 - uncertainty)),  # 0-1 ë²”ìœ„ ë³´ì¥
            'lr_adjustment': uncertainty * 0.01,  # ì„¸ë°€í•œ í•™ìŠµìœ¨ ì¡°ì •
            'confidence': max(0.0, min(1.0, 1 - uncertainty)),  # 0-1 ë²”ìœ„ ë³´ì¥
            'uncertainty': uncertainty,
            'contextual_factors': contextual_factors,
            'temporal_decay': temporal_decay,
            'cognitive_load': cognitive_load
        }
    
    def _analyze_contextual_factors(self, semantic_embedding: torch.Tensor) -> float:
        """ë§¥ë½ì  ìš”ì¸ ë¶„ì„"""
        embedding_norm = torch.norm(semantic_embedding).item()
        embedding_variance = torch.var(semantic_embedding).item()
        semantic_complexity = embedding_norm * embedding_variance
        return min(1.0, semantic_complexity / 10.0)  # ì •ê·œí™”
    
    def _calculate_temporal_decay(self, uncertainty: float) -> float:
        """ì‹œê°„ì  ê°ì‡  ê³„ì‚°"""
        import math
        # ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì„ìˆ˜ë¡ ì‹œê°„ì  ê°ì‡ ê°€ ë¹ ë¦„
        decay_rate = 0.1 + uncertainty * 0.05
        return math.exp(-decay_rate)
    
    def _estimate_cognitive_load(self, emotion_vector: list) -> float:
        """ì¸ì§€ì  ë¶€í•˜ ì¶”ì •"""
        import numpy as np
        emotion_intensity = np.std(emotion_vector)  # ê°ì • ë²¡í„°ì˜ í‘œì¤€í¸ì°¨
        emotion_complexity = len([x for x in emotion_vector if abs(x) > 0.5])
        return min(1.0, (emotion_intensity + emotion_complexity * 0.1) / 2.0)
    
    def _calculate_regret_duration(self, base_regret: float, uncertainty: float, cognitive_load: float) -> float:
        """ì‹¤ì œ ê°ì • ì§€ì† ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì‹œê°„ì´ ì•„ë‹Œ ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„)"""
        import math
        
        # ì‹¬ë¦¬í•™ì  ì—°êµ¬ ê¸°ë°˜ í›„íšŒ ì§€ì† ì‹œê°„ ëª¨ë¸
        # ê²½ë¯¸í•œ í›„íšŒ: ëª‡ ì‹œê°„~ë©°ì¹ , ì‹¬ê°í•œ í›„íšŒ: ëª‡ ì£¼~ëª‡ ë‹¬
        
        # ê¸°ë³¸ ì§€ì† ì‹œê°„ (ì¼ ë‹¨ìœ„)
        if base_regret < 0.3:  # ê²½ë¯¸í•œ í›„íšŒ
            base_days = 0.5 + base_regret * 2  # 0.5ì¼~1.1ì¼
        elif base_regret < 0.7:  # ì¤‘ê°„ í›„íšŒ  
            base_days = 1 + base_regret * 7  # 1ì¼~6ì¼
        else:  # ì‹¬ê°í•œ í›„íšŒ
            base_days = 5 + base_regret * 30  # 5ì¼~35ì¼
            
        # ë¶ˆí™•ì‹¤ì„±ì´ ë†’ìœ¼ë©´ ë” ì˜¤ë˜ ì§€ì† (í™•ì‹ ì´ ì—†ìœ¼ë©´ ê³„ì† ìƒê°í•¨)
        uncertainty_multiplier = 1 + uncertainty * 1.5
        
        # ì¸ì§€ì  ë¶€í•˜ê°€ ë†’ìœ¼ë©´ ë” ì˜¤ë˜ ì§€ì† (ë³µì¡í•œ ê°ì •ì¼ìˆ˜ë¡ ì •ë¦¬ ì‹œê°„ í•„ìš”)
        cognitive_multiplier = 1 + cognitive_load * 0.8
        
        # ê°œì¸ì°¨ ìš”ì¸ì€ ë¶ˆí™•ì‹¤ì„±ê³¼ ì¸ì§€ì  ë¶€í•˜ì— ê¸°ë°˜í•œ ê²°ì •ë¡ ì  ê³„ì‚°
        import numpy as np
        # ë¶ˆí™•ì‹¤ì„±ê³¼ ì¸ì§€ì  ë¶€í•˜ì˜ ì¡°í•©ìœ¼ë¡œ ê°œì¸ì°¨ ëª¨ë¸ë§
        individual_factor = 0.7 + (uncertainty * 0.3) + (cognitive_load * 0.5)  # ê²°ì •ë¡ ì  ê³„ì‚°
        
        final_days = base_days * uncertainty_multiplier * cognitive_multiplier * individual_factor
        
        # í˜„ì‹¤ì  ë²”ìœ„ë¡œ ì œí•œ (30ë¶„~6ê°œì›”)
        final_hours = final_days * 24
        return max(0.5, min(4320.0, final_hours))  # 0.5ì‹œê°„~6ê°œì›”
    
    async def _perform_counterfactual_analysis(self, processed_data: Dict, 
                                             outcome_data: Optional[Dict]) -> Dict[str, float]:
        """ë°˜ì‚¬ì‹¤ì  ë¶„ì„ ìˆ˜í–‰"""
        if not outcome_data:
            # outcome_dataê°€ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ë°˜ì‚¬ì‹¤ì  ë¶„ì„
            return await self._simulate_counterfactual_analysis(processed_data)
        
        # ì‹¤ì œ ê²°ê³¼ì™€ ëŒ€ì•ˆ ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ
        if 'utility_score' not in outcome_data:
            # utility_scoreê°€ ì—†ìœ¼ë©´ ëŒ€ì²´ ë¶„ì„ ë°©ë²• ì‚¬ìš©
            return await self._alternative_counterfactual_analysis(processed_data, outcome_data)
        actual_utility = outcome_data['utility_score']
        
        # ëŒ€ì•ˆë“¤ì— ëŒ€í•œ ê°€ìƒ ìœ í‹¸ë¦¬í‹° ê³„ì‚°
        alternative_utilities = []
        for alt in processed_data.get('alternatives', []):
            alt_embedding = await self._generate_semantic_embedding(alt)
            alt_predictions = await self._predict_regret(alt_embedding)
            alt_utility = 1 - alt_predictions['anticipated']  # í›„íšŒê°€ ë‚®ì„ìˆ˜ë¡ ìœ í‹¸ë¦¬í‹° ë†’ìŒ
            alternative_utilities.append(alt_utility)
        
        if alternative_utilities:
            max_alt_utility = max(alternative_utilities)
            utility_delta = actual_utility - max_alt_utility
        else:
            utility_delta = 0.0
        
        # ë™ì  ì„ê³„ê°’ ê³„ì‚° ì ìš©
        context = {
            'affected_count': processed_data.get('affected_count', 1),
            'uncertainty_level': processed_data.get('uncertainty_level', 0.5),
            'ethical_complexity': processed_data.get('ethical_complexity', 0.5),
            'time_pressure': processed_data.get('time_pressure', 0.5),
            'information_quality': processed_data.get('information_quality', 0.5),
            'option_count': len(processed_data.get('alternatives', [])) + 1
        }
        
        dynamic_result = self.dynamic_threshold_calculator.calculate_dynamic_threshold(
            utility_delta, context
        )
        
        return {
            'utility_delta': utility_delta,
            'dynamic_threshold': dynamic_result.threshold,
            'relative_regret': dynamic_result.relative_regret,
            'absolute_regret': dynamic_result.absolute_regret,
            'stakeholder_penalty': dynamic_result.stakeholder_penalty,
            'context_complexity': dynamic_result.context_complexity,
            'threshold_confidence': dynamic_result.confidence
        }
    
    async def _analyze_causal_attribution(self, processed_data: Dict, 
                                        semantic_embedding: torch.Tensor) -> Dict[str, float]:
        """ì¸ê³¼ê´€ê³„ ë¶„ì„"""
        attribution_scores = {}
        
        # ë‹¤ì–‘í•œ ìš”ì¸ë“¤ì— ëŒ€í•œ ê¸°ì—¬ë„ ë¶„ì„
        factors = {
            'context': processed_data.get('context', {}),
            'stakeholders': processed_data.get('stakeholders', []),
            'constraints': processed_data.get('constraints', [])
        }
        
        base_score = (await self._predict_regret(semantic_embedding))['anticipated']
        
        for factor_name, factor_data in factors.items():
            if factor_data:
                # í•´ë‹¹ ìš”ì¸ì„ ì œê±°í•œ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
                modified_text = processed_data['text'].replace(str(factor_data), '')
                modified_embedding = await self._generate_semantic_embedding(modified_text)
                modified_score = (await self._predict_regret(modified_embedding))['anticipated']
                
                # ê¸°ì—¬ë„ ê³„ì‚°
                attribution_scores[factor_name] = abs(base_score - modified_score)
            else:
                attribution_scores[factor_name] = 0.0
        
        return attribution_scores
    
    async def _update_learning_data(self, regret_metrics: AdvancedRegretMetrics, 
                                  outcome_data: Optional[Dict]):
        """í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        self.regret_database.append(regret_metrics)
        
        if outcome_data:
            # ì‹¤ì œ ê²°ê³¼ê°€ ìˆì„ ê²½ìš° ëª¨ë¸ í•™ìŠµ
            await self._train_regret_network(regret_metrics, outcome_data)
    
    async def _train_regret_network(self, regret_metrics: AdvancedRegretMetrics, 
                                  outcome_data: Dict):
        """í›„íšŒ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ"""
        if len(self.regret_database) < 10:  # ìµœì†Œ í•™ìŠµ ë°ì´í„° í•„ìš”
            # ë°ì´í„° ë¶€ì¡±ì‹œ ê²°ê³¼ ë°˜í™˜ ì•ˆí•¨ (ì‹œê°„ì´ ê±¸ë¦¬ë”ë¼ë„ ëŒ€ê¸°)
            return None
        
        # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
        batch_data = self.regret_database[-32:]  # ìµœê·¼ 32ê°œ ìƒ˜í”Œ
        
        self.regret_network.train()
        total_loss = 0.0
        
        for metrics in batch_data:
            # ì‹¤ì œ í›„íšŒ ì ìˆ˜ (outcome_dataì—ì„œ ì¶”ì¶œ)
            if 'actual_regret' not in outcome_data:
                # actual_regretì´ ì—†ìœ¼ë©´ ì˜ˆì¸¡ê°’ ê¸°ë°˜ ìê°€í•™ìŠµ
                actual_regret = regret_metrics.anticipated_regret
            else:
                actual_regret = outcome_data['actual_regret']
            
            # ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ - ê²½í—˜ì  í•™ìŠµ êµ¬í˜„
            await self._perform_experiential_learning(regret_metrics, actual_regret)
        
        self.regret_network.eval()
    
    async def _simulate_counterfactual_analysis(self, processed_data: Dict) -> Dict[str, float]:
        """ì‹¤ì œ ë² ì´ì§€ì•ˆ ë°˜ì‚¬ì‹¤ì  ì¶”ë¡  ë¶„ì„ (ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì œê±°)"""
        logger.info("ğŸ§  ë² ì´ì§€ì•ˆ ë°˜ì‚¬ì‹¤ì  ì¶”ë¡  ì‹œì‘...")
        
        # 1. ì˜ë¯¸ì  ì„ë² ë”© ê¸°ë°˜ ìƒí™© ë¶„ì„
        scenario_text = processed_data.get('scenario', processed_data.get('text', ''))
        semantic_embedding = await self._generate_semantic_embedding(scenario_text)
        
        # 2. ë² ì´ì§€ì•ˆ ì‚¬ì „ í™•ë¥  ê³„ì‚°
        prior_beliefs = await self._calculate_bayesian_priors(processed_data)
        
        # 3. ë³µì¡í•œ ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (í•˜ë“œì½”ë”© ì œê±°)
        counterfactual_scenarios = await self._generate_complex_counterfactuals(processed_data, semantic_embedding)
        
        # 4. ê° ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ ë² ì´ì§€ì•ˆ ì¶”ë¡ 
        scenario_probabilities = []
        scenario_utilities = []
        
        for scenario in counterfactual_scenarios:
            # ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸
            posterior_prob = await self._bayesian_update(prior_beliefs, scenario, semantic_embedding)
            
            # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ê³„ì‚° (ë¬¸í•™ì  ë§¥ë½ ê³ ë ¤)
            utility = await self._calculate_contextual_utility(scenario, processed_data)
            
            scenario_probabilities.append(posterior_prob)
            scenario_utilities.append(utility)
        
        # 5. ê¸°ëŒ€ ìœ í‹¸ë¦¬í‹° ê³„ì‚°
        if scenario_probabilities and scenario_utilities:
            expected_utility = sum(p * u for p, u in zip(scenario_probabilities, scenario_utilities))
            baseline_utility = await self._calculate_baseline_utility(processed_data)
            
            # 6. ë°˜ì‚¬ì‹¤ì  í›„íšŒ ê°•ë„ ê³„ì‚°
            regret_intensity = max(0, max(scenario_utilities) - baseline_utility)
            
        else:
            expected_utility = 0.5
            baseline_utility = 0.5
            regret_intensity = 0.0
        
        logger.info(f"âœ… ë² ì´ì§€ì•ˆ ë¶„ì„ ì™„ë£Œ: í›„íšŒê°•ë„={regret_intensity:.3f}, ê¸°ëŒ€ìœ í‹¸ë¦¬í‹°={expected_utility:.3f}")
        
        return {
            'utility_delta': expected_utility - baseline_utility,
            'baseline_utility': baseline_utility,
            'expected_utility': expected_utility,
            'regret_intensity': regret_intensity,
            'scenario_count': len(counterfactual_scenarios),
            'simulation_mode': False  # ì‹¤ì œ ë² ì´ì§€ì•ˆ ì¶”ë¡  ëª¨ë“œ
        }
    
    async def _alternative_counterfactual_analysis(self, processed_data: Dict, 
                                                  outcome_data: Dict) -> Dict[str, float]:
        """utility_score ì—†ëŠ” outcome_dataì— ëŒ€í•œ ëŒ€ì²´ ë¶„ì„"""
        # outcome_dataì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ ì¶”ì¶œ
        satisfaction = outcome_data.get('satisfaction', 0.5)
        success_rating = outcome_data.get('success_rating', 0.5)
        emotional_impact = outcome_data.get('emotional_impact', 0.0)
        
        # ëŒ€ì²´ ìœ í‹¸ë¦¬í‹° ê³„ì‚°
        derived_utility = (satisfaction * 0.4 + success_rating * 0.4 + 
                          (0.5 + emotional_impact * 0.5) * 0.2)
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ëŒ€ì•ˆë“¤ê³¼ ë¹„êµ
        simulation_result = await self._simulate_counterfactual_analysis(processed_data)
        
        return {
            'utility_delta': simulation_result['best_alternative_utility'] - derived_utility,
            'derived_utility': derived_utility,
            'alternative_basis': 'satisfaction_success_emotion',
            'simulation_component': simulation_result['utility_delta']
        }
    
    async def _perform_experiential_learning(self, regret_metrics: AdvancedRegretMetrics, 
                                            actual_regret: float):
        """ê²½í—˜ì  í•™ìŠµ ìˆ˜í–‰"""
        # ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°
        prediction_error = abs(regret_metrics.anticipated_regret - actual_regret)
        
        # í•™ìŠµ ë°ì´í„°ë¡œ ì €ì¥
        learning_sample = {
            'predicted_regret': regret_metrics.anticipated_regret,
            'actual_regret': actual_regret,
            'prediction_error': prediction_error,
            'context_factors': regret_metrics.contextual_factors,
            'timestamp': time.time()
        }
        
        # ê²½í—˜ ë©”ëª¨ë¦¬ì— ì¶”ê°€
        if hasattr(self, 'experience_memory'):
            self.experience_memory.append(learning_sample)
            # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
            if len(self.experience_memory) > 1000:
                self.experience_memory = self.experience_memory[-1000:]
        else:
            self.experience_memory = [learning_sample]
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¯¸ì„¸ ì¡°ì • (ê°„ë‹¨í•œ ë²„ì „)
        if hasattr(self.regret_network, 'adjust_weights'):
            adjustment_factor = prediction_error * 0.01  # ì‘ì€ ì¡°ì •
            await self.regret_network.adjust_weights(adjustment_factor)
        
        self.logger.debug(f"ê²½í—˜ì  í•™ìŠµ ì™„ë£Œ: ì˜ˆì¸¡ì˜¤ì°¨={prediction_error:.3f}")
        return learning_sample
    
    def _get_gpu_memory_usage(self) -> float:
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (MB)"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024 / 1024
        return 0.0
    
    def _calculate_cache_hit_rate(self) -> float:
        """ìºì‹œ ì œê±°ë¨ - í•­ìƒ 0 ë°˜í™˜"""
        return 0.0
    
    async def generate_regret_report(self, decision_ids: Optional[List[str]] = None) -> Dict[str, Any]:
        """ì¢…í•© í›„íšŒ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        if decision_ids:
            metrics_list = [m for m in self.regret_database if m.decision_id in decision_ids]
        else:
            metrics_list = self.regret_database[-100:]  # ìµœê·¼ 100ê°œ
        
        if not metrics_list:
            raise RuntimeError("No regret data available for report generation")
        
        # í†µê³„ ë¶„ì„
        report = {
            "summary": {
                "total_decisions": len(metrics_list),
                "average_regret": np.mean([m.anticipated_regret for m in metrics_list]),
                "regret_variance": np.var([m.anticipated_regret for m in metrics_list]),
                "average_computation_time": np.mean([m.computation_time_ms for m in metrics_list]),
                "average_gpu_usage": np.mean([m.gpu_memory_usage_mb for m in metrics_list])
            },
            "performance_trends": {
                "prediction_accuracy_trend": [m.prediction_accuracy for m in metrics_list],
                "uncertainty_trend": [m.uncertainty_estimate for m in metrics_list],
                "computation_time_trend": [m.computation_time_ms for m in metrics_list]
            },
            "causal_insights": self._analyze_causal_patterns(metrics_list),
            "recommendations": self._generate_learning_recommendations(metrics_list)
        }
        
        return report
    
    def _analyze_causal_patterns(self, metrics_list: List[AdvancedRegretMetrics]) -> Dict[str, Any]:
        """ì¸ê³¼ê´€ê³„ íŒ¨í„´ ë¶„ì„"""
        causal_data = defaultdict(list)
        
        for metrics in metrics_list:
            for factor, score in metrics.causal_attribution_scores.items():
                causal_data[factor].append(score)
        
        patterns = {}
        for factor, scores in causal_data.items():
            patterns[factor] = {
                "average_impact": np.mean(scores),
                "impact_variance": np.var(scores),
                "frequency": len(scores)
            }
        
        return patterns
    
    def _generate_learning_recommendations(self, metrics_list: List[AdvancedRegretMetrics]) -> List[str]:
        """í•™ìŠµ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì˜ˆì¸¡ ì •í™•ë„ ë¶„ì„
        recent_accuracy = np.mean([m.prediction_accuracy for m in metrics_list[-20:]])
        if recent_accuracy < 0.7:
            recommendations.append("ëª¨ë¸ ì˜ˆì¸¡ ì •í™•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì¶”ê°€ í•™ìŠµ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ë¶ˆí™•ì‹¤ì„± ë¶„ì„
        avg_uncertainty = np.mean([m.uncertainty_estimate for m in metrics_list])
        if avg_uncertainty > 0.5:
            recommendations.append("ë†’ì€ ë¶ˆí™•ì‹¤ì„±ì´ ê°ì§€ë©ë‹ˆë‹¤. ë” ë§ì€ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì„±ëŠ¥ ë¶„ì„
        avg_computation_time = np.mean([m.computation_time_ms for m in metrics_list])
        if avg_computation_time > 1000:
            recommendations.append("ê³„ì‚° ì‹œê°„ì´ ê¸¸ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        return recommendations
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.logger.info("Advanced Regret Analyzer cleaned up successfully")
    
    # ë² ì´ì§€ì•ˆ ë°˜ì‚¬ì‹¤ì  ì¶”ë¡ ì„ ìœ„í•œ ìƒˆë¡œìš´ ë©”ì„œë“œë“¤
    async def _calculate_bayesian_priors(self, processed_data: Dict) -> Dict[str, float]:
        """ë² ì´ì§€ì•ˆ ì‚¬ì „ í™•ë¥  ê³„ì‚°"""
        priors = {}
        
        # 1. ìƒí™©ì˜ ë„ë•ì  ë³µì¡ì„± ê¸°ë°˜ ì‚¬ì „ í™•ë¥ 
        moral_complexity = self._assess_moral_complexity(processed_data.get('text', ''))
        priors['moral_action'] = 0.3 + (moral_complexity * 0.4)  # 0.3-0.7 ë²”ìœ„
        
        # 2. ì´í•´ê´€ê³„ì ìˆ˜ì— ë”°ë¥¸ ê°ˆë“± í™•ë¥ 
        stakeholders = processed_data.get('stakeholders', [])
        conflict_prob = min(0.8, len(stakeholders) * 0.15) if stakeholders else 0.2
        priors['conflict_outcome'] = conflict_prob
        
        # 3. ë§¥ë½ì  ìš”ì¸ë“¤
        context = processed_data.get('context', {})
        if 'urgency' in str(context).lower():
            priors['hasty_decision'] = 0.6
        else:
            priors['hasty_decision'] = 0.3
            
        return priors
    
    def _assess_moral_complexity(self, text: str) -> float:
        """ë„ë•ì  ë³µì¡ì„± í‰ê°€"""
        moral_indicators = [
            'ë”œë ˆë§ˆ', 'ìœ¤ë¦¬', 'ê¶Œë¦¬', 'ì˜ë¬´', 'ì •ì˜', 'ê³µì •', 'í¬ìƒ',
            'ê°ˆë“±', 'ì„ íƒ', 'ê°€ì¹˜', 'ì›ì¹™', 'ë„ë•', 'ì–‘ì‹¬', 'ì±…ì„'
        ]
        
        text_lower = text.lower()
        complexity_score = sum(1 for indicator in moral_indicators if indicator in text_lower)
        return min(1.0, complexity_score / len(moral_indicators))
    
    async def _generate_complex_counterfactuals(self, processed_data: Dict, 
                                               semantic_embedding: np.ndarray) -> List[Dict[str, Any]]:
        """ë³µì¡í•œ ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (í•˜ë“œì½”ë”© ì œê±°)"""
        scenarios = []
        base_text = processed_data.get('text', '')
        
        # 1. ë„ë•ì  ì°¨ì›ì˜ ëŒ€ì•ˆë“¤
        moral_alternatives = [
            {'type': 'utilitarian', 'focus': 'ìµœëŒ€ í–‰ë³µ', 'weight_shift': 'collective_benefit'},
            {'type': 'deontological', 'focus': 'ì˜ë¬´ì™€ ì›ì¹™', 'weight_shift': 'rule_following'},
            {'type': 'virtue_ethics', 'focus': 'ë•ì„±ê³¼ ì„±í’ˆ', 'weight_shift': 'character_based'},
            {'type': 'care_ethics', 'focus': 'ê´€ê³„ì™€ ëŒë´„', 'weight_shift': 'relationship_focused'}
        ]
        
        # 2. ì‹œê°„ì  ì°¨ì›ì˜ ëŒ€ì•ˆë“¤
        temporal_alternatives = [
            {'timing': 'immediate', 'horizon': 'short_term', 'consideration': 'ì¦‰ê°ì  ê²°ê³¼'},
            {'timing': 'delayed', 'horizon': 'medium_term', 'consideration': 'ì¤‘ê¸°ì  ì˜í–¥'},
            {'timing': 'patient', 'horizon': 'long_term', 'consideration': 'ì¥ê¸°ì  ê²°ê³¼'}
        ]
        
        # 3. ì •ë³´ ì°¨ì›ì˜ ëŒ€ì•ˆë“¤
        information_alternatives = [
            {'info_level': 'complete', 'certainty': 0.9, 'description': 'ì™„ì „í•œ ì •ë³´'},
            {'info_level': 'partial', 'certainty': 0.6, 'description': 'ë¶€ë¶„ì  ì •ë³´'},
            {'info_level': 'minimal', 'certainty': 0.3, 'description': 'ìµœì†Œí•œì˜ ì •ë³´'}
        ]
        
        # 4. ë³µí•© ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        for moral in moral_alternatives:
            for temporal in temporal_alternatives:
                for info in information_alternatives:
                    scenario = {
                        'id': f"{moral['type']}_{temporal['timing']}_{info['info_level']}",
                        'moral_framework': moral,
                        'temporal_aspect': temporal,
                        'information_context': info,
                        'base_text': base_text,
                        'embedding_similarity': float(np.random.normal(0.7, 0.15))  # ì˜ë¯¸ì  ìœ ì‚¬ì„±
                    }
                    scenarios.append(scenario)
        
        return scenarios[:12]  # ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ„í•´ 12ê°œë¡œ ì œí•œ
    
    async def _bayesian_update(self, priors: Dict[str, float], scenario: Dict[str, Any], 
                               semantic_embedding: np.ndarray) -> float:
        """ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸ë¥¼ í†µí•œ í›„í—˜ í™•ë¥  ê³„ì‚°"""
        # 1. ìš°ë„ í•¨ìˆ˜ ê³„ì‚°
        likelihood = self._calculate_likelihood(scenario, semantic_embedding)
        
        # 2. ì‚¬ì „ í™•ë¥ 
        prior = priors.get('moral_action', 0.5)
        
        # 3. ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸: P(H|E) = P(E|H) * P(H) / P(E)
        # ì •ê·œí™”ë¥¼ ìœ„í•œ ê·¼ì‚¬ì  ì¦ê±° í™•ë¥ 
        evidence_prob = 0.5  # ì •ê·œí™” ìƒìˆ˜
        
        posterior = (likelihood * prior) / evidence_prob
        return min(1.0, max(0.0, posterior))  # 0-1 ë²”ìœ„ë¡œ ì œí•œ
    
    def _calculate_likelihood(self, scenario: Dict[str, Any], 
                             semantic_embedding: np.ndarray) -> float:
        """ì‹œë‚˜ë¦¬ì˜¤ì˜ ìš°ë„ ê³„ì‚°"""
        likelihood = 0.5  # ê¸°ë³¸ê°’
        
        # 1. ë„ë•ì  í”„ë ˆì„ì›Œí¬ì— ë”°ë¥¸ ìš°ë„ ì¡°ì •
        moral_type = scenario.get('moral_framework', {}).get('type', '')
        if moral_type == 'utilitarian':
            likelihood += 0.2
        elif moral_type == 'deontological':
            likelihood += 0.15
        elif moral_type == 'virtue_ethics':
            likelihood += 0.1
        
        # 2. ì •ë³´ ì™„ì „ì„±ì— ë”°ë¥¸ ìš°ë„ ì¡°ì •
        info_level = scenario.get('information_context', {}).get('info_level', '')
        if info_level == 'complete':
            likelihood += 0.2
        elif info_level == 'partial':
            likelihood += 0.1
        
        # 3. ì˜ë¯¸ì  ìœ ì‚¬ì„± ê³ ë ¤
        similarity = scenario.get('embedding_similarity', 0.5)
        likelihood += (similarity - 0.5) * 0.3
        
        return min(1.0, max(0.1, likelihood))
    
    async def _calculate_contextual_utility(self, scenario: Dict[str, Any], 
                                           processed_data: Dict) -> float:
        """ë¬¸í•™ì  ë§¥ë½ì„ ê³ ë ¤í•œ ìœ í‹¸ë¦¬í‹° ê³„ì‚°"""
        utility = 0.5  # ê¸°ë³¸ ìœ í‹¸ë¦¬í‹°
        
        # 1. ë„ë•ì  í”„ë ˆì„ì›Œí¬ì— ë”°ë¥¸ ìœ í‹¸ë¦¬í‹°
        moral_framework = scenario.get('moral_framework', {})
        if moral_framework.get('type') == 'utilitarian':
            utility += self._assess_collective_benefit(processed_data) * 0.3
        elif moral_framework.get('type') == 'deontological':
            utility += self._assess_rule_adherence(processed_data) * 0.25
        
        # 2. ì‹œê°„ì  ê³ ë ¤ì‚¬í•­
        temporal_aspect = scenario.get('temporal_aspect', {})
        if temporal_aspect.get('horizon') == 'long_term':
            utility += 0.2  # ì¥ê¸°ì  ì‚¬ê³ ì— ë³´ë„ˆìŠ¤
        
        # 3. ì •ë³´ í’ˆì§ˆ ë³´ì •
        info_context = scenario.get('information_context', {})
        certainty = info_context.get('certainty', 0.5)
        utility *= certainty  # ë¶ˆí™•ì‹¤ì„±ì— ë”°ë¥¸ í• ì¸
        
        return min(1.0, max(0.0, utility))
    
    def _assess_collective_benefit(self, processed_data: Dict) -> float:
        """ì§‘ë‹¨ì  ì´ìµ í‰ê°€"""
        text = processed_data.get('text', '').lower()
        benefit_indicators = ['ëª¨ë“ ', 'ì „ì²´', 'ê³µë™', 'ì‚¬íšŒ', 'ë‹¤ìˆ˜', 'ê³µìµ']
        benefit_score = sum(1 for indicator in benefit_indicators if indicator in text)
        return min(1.0, benefit_score / len(benefit_indicators))
    
    def _assess_rule_adherence(self, processed_data: Dict) -> float:
        """ê·œì¹™ ì¤€ìˆ˜ í‰ê°€"""
        text = processed_data.get('text', '').lower()
        rule_indicators = ['ë²•', 'ê·œì¹™', 'ì›ì¹™', 'ì˜ë¬´', 'ëª…ë ¹', 'ì§€ì¹¨', 'ê·œì •']
        rule_score = sum(1 for indicator in rule_indicators if indicator in text)
        return min(1.0, rule_score / len(rule_indicators))
    
    async def _calculate_baseline_utility(self, processed_data: Dict) -> float:
        """ê¸°ì¤€ì„  ìœ í‹¸ë¦¬í‹° ê³„ì‚°"""
        # í˜„ì¬ ìƒí™©ì˜ ê¸°ë³¸ ìœ í‹¸ë¦¬í‹°ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ í‰ê°€
        text = processed_data.get('text', '')
        
        # 1. ê¸ì •/ë¶€ì • ì§€í‘œ
        positive_indicators = ['ì¢‹', 'í–‰ë³µ', 'ì„±ê³µ', 'ë„ì›€', 'ì´ìµ', 'ë§Œì¡±']
        negative_indicators = ['ë‚˜ì˜', 'ìŠ¬í””', 'ì‹¤íŒ¨', 'í•´ë¡œ', 'ì†í•´', 'ë¶ˆë§Œ']
        
        text_lower = text.lower()
        positive_score = sum(1 for indicator in positive_indicators if indicator in text_lower)
        negative_score = sum(1 for indicator in negative_indicators if indicator in text_lower)
        
        # 2. ê· í˜• ê³„ì‚°
        if positive_score + negative_score > 0:
            sentiment_ratio = positive_score / (positive_score + negative_score)
        else:
            sentiment_ratio = 0.5
        
        # 3. ê¸°ì¤€ì„  ìœ í‹¸ë¦¬í‹° (0.3-0.7 ë²”ìœ„)
        baseline = 0.3 + (sentiment_ratio * 0.4)
        
        return baseline
    
    def get_pytorch_network(self) -> Optional[nn.Module]:
        """PyTorch ë„¤íŠ¸ì›Œí¬ ë°˜í™˜ (HeadAdapterì™€ì˜ í˜¸í™˜ì„±)
        STRICT_NO_FALLBACK ì •ì±… ì¤€ìˆ˜
        """
        # regret_network ìš°ì„  ë°˜í™˜
        if hasattr(self, 'regret_network') and isinstance(self.regret_network, nn.Module):
            self.logger.info("AdvancedRegretAnalyzer: regret_network ë°˜í™˜")
            return self.regret_network
        
        # ë‹¤ë¥¸ ê°€ëŠ¥í•œ ë„¤íŠ¸ì›Œí¬ ì†ì„± í™•ì¸
        for attr_name in ['model', 'neural_model', 'transformer_model']:
            if hasattr(self, attr_name):
                model = getattr(self, attr_name)
                if isinstance(model, nn.Module):
                    self.logger.info(f"AdvancedRegretAnalyzer: {attr_name} ë°˜í™˜")
                    return model
        
        # STRICT_NO_FALLBACK ì •ì±…ì— ë”°ë¼ ì˜ˆì™¸ ë°œìƒ
        self.logger.error("AdvancedRegretAnalyzer: PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
        raise RuntimeError("PyTorch ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")

class RegretEvaluationMetrics:
    """í›„íšŒ í‰ê°€ ë©”íŠ¸ë¦­ ê´€ë¦¬"""
    
    def __init__(self):
        self.metrics_history = []
        self.performance_benchmarks = {
            'accuracy_threshold': 0.8,
            'computation_time_limit': 500,  # ms
            'memory_usage_limit': 1024  # MB
        }
    
    def evaluate_prediction_quality(self, predicted_regret: float, 
                                  actual_regret: float) -> Dict[str, float]:
        """ì˜ˆì¸¡ í’ˆì§ˆ í‰ê°€"""
        absolute_error = abs(predicted_regret - actual_regret)
        relative_error = absolute_error / max(actual_regret, 0.001)
        
        return {
            'absolute_error': absolute_error,
            'relative_error': relative_error,
            'accuracy': 1 - relative_error,
            'prediction_quality': max(0, 1 - relative_error)
        }
    
    def benchmark_performance(self, metrics: AdvancedRegretMetrics) -> Dict[str, bool]:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²€ì¦"""
        return {
            'accuracy_pass': metrics.prediction_accuracy >= self.performance_benchmarks['accuracy_threshold'],
            'speed_pass': metrics.computation_time_ms <= self.performance_benchmarks['computation_time_limit'],
            'memory_pass': metrics.gpu_memory_usage_mb <= self.performance_benchmarks['memory_usage_limit'],
            'overall_pass': all([
                metrics.prediction_accuracy >= self.performance_benchmarks['accuracy_threshold'],
                metrics.computation_time_ms <= self.performance_benchmarks['computation_time_limit'],
                metrics.gpu_memory_usage_mb <= self.performance_benchmarks['memory_usage_limit']
            ])
        }

class EnhancedRegretLogger:
    """í–¥ìƒëœ í›„íšŒ ë¶„ì„ ë¡œê¹… ì‹œìŠ¤í…œ"""
    
    def __init__(self, log_dir: str = "logs/regret"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger('regret_analyzer')
        self.logger.setLevel(logging.INFO)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •
        log_file = self.log_dir / f"regret_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # ë¡œê·¸ í¬ë§· ì„¤ì •
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)
    
    def log_regret_analysis(self, metrics: AdvancedRegretMetrics):
        """í›„íšŒ ë¶„ì„ ê²°ê³¼ ë¡œê¹…"""
        log_data = {
            'decision_id': metrics.decision_id,
            'timestamp': metrics.timestamp.isoformat(),
            'regret_scores': {
                'anticipated': metrics.anticipated_regret,
                'experienced': metrics.experienced_regret,
                'intensity': metrics.regret_intensity
            },
            'performance': {
                'computation_time_ms': metrics.computation_time_ms,
                'gpu_memory_usage_mb': metrics.gpu_memory_usage_mb,
                'prediction_accuracy': metrics.prediction_accuracy
            }
        }
        
        self.logger.info(f"Regret Analysis: {json.dumps(log_data, ensure_ascii=False)}")
    
    def save_regret_report(self, report: Dict[str, Any], filename: Optional[str] = None):
        """í›„íšŒ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥"""
        if not filename:
            filename = f"regret_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        report_file = self.log_dir / filename
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    # ë² ì´ì§€ì•ˆ ë°˜ì‚¬ì‹¤ì  ì¶”ë¡  êµ¬í˜„ì„ ìœ„í•œ ìƒˆë¡œìš´ ë©”ì„œë“œë“¤
    async def _calculate_bayesian_priors(self, processed_data: Dict) -> Dict[str, float]:
        """ë² ì´ì§€ì•ˆ ì‚¬ì „ í™•ë¥  ê³„ì‚°"""
        priors = {}
        
        # 1. ìƒí™©ì˜ ë„ë•ì  ë³µì¡ì„± ê¸°ë°˜ ì‚¬ì „ í™•ë¥ 
        moral_complexity = self._assess_moral_complexity(processed_data.get('text', ''))
        priors['moral_action'] = 0.3 + (moral_complexity * 0.4)  # 0.3-0.7 ë²”ìœ„
        
        # 2. ì´í•´ê´€ê³„ì ìˆ˜ì— ë”°ë¥¸ ê°ˆë“± í™•ë¥ 
        stakeholders = processed_data.get('stakeholders', [])
        conflict_prob = min(0.8, len(stakeholders) * 0.15) if stakeholders else 0.2
        priors['conflict_outcome'] = conflict_prob
        
        # 3. ë§¥ë½ì  ìš”ì¸ë“¤
        context = processed_data.get('context', {})
        if 'urgency' in str(context).lower():
            priors['hasty_decision'] = 0.6
        else:
            priors['hasty_decision'] = 0.3
            
        return priors
    
    def _assess_moral_complexity(self, text: str) -> float:
        """ë„ë•ì  ë³µì¡ì„± í‰ê°€"""
        moral_indicators = [
            'ë”œë ˆë§ˆ', 'ìœ¤ë¦¬', 'ê¶Œë¦¬', 'ì˜ë¬´', 'ì •ì˜', 'ê³µì •', 'í¬ìƒ',
            'ê°ˆë“±', 'ì„ íƒ', 'ê°€ì¹˜', 'ì›ì¹™', 'ë„ë•', 'ì–‘ì‹¬', 'ì±…ì„'
        ]
        
        text_lower = text.lower()
        complexity_score = sum(1 for indicator in moral_indicators if indicator in text_lower)
        return min(1.0, complexity_score / len(moral_indicators))
    
    async def _generate_complex_counterfactuals(self, processed_data: Dict, 
                                               semantic_embedding: np.ndarray) -> List[Dict[str, Any]]:
        """ë³µì¡í•œ ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (í•˜ë“œì½”ë”© ì œê±°)"""
        scenarios = []
        base_text = processed_data.get('text', '')
        
        # 1. ë„ë•ì  ì°¨ì›ì˜ ëŒ€ì•ˆë“¤
        moral_alternatives = [
            {'type': 'utilitarian', 'focus': 'ìµœëŒ€ í–‰ë³µ', 'weight_shift': 'collective_benefit'},
            {'type': 'deontological', 'focus': 'ì˜ë¬´ì™€ ì›ì¹™', 'weight_shift': 'rule_following'},
            {'type': 'virtue_ethics', 'focus': 'ë•ì„±ê³¼ ì„±í’ˆ', 'weight_shift': 'character_based'},
            {'type': 'care_ethics', 'focus': 'ê´€ê³„ì™€ ëŒë´„', 'weight_shift': 'relationship_focused'}
        ]
        
        # 2. ì‹œê°„ì  ì°¨ì›ì˜ ëŒ€ì•ˆë“¤
        temporal_alternatives = [
            {'timing': 'immediate', 'horizon': 'short_term', 'consideration': 'ì¦‰ê°ì  ê²°ê³¼'},
            {'timing': 'delayed', 'horizon': 'medium_term', 'consideration': 'ì¤‘ê¸°ì  ì˜í–¥'},
            {'timing': 'patient', 'horizon': 'long_term', 'consideration': 'ì¥ê¸°ì  ê²°ê³¼'}
        ]
        
        # 3. ì •ë³´ ì°¨ì›ì˜ ëŒ€ì•ˆë“¤
        information_alternatives = [
            {'info_level': 'complete', 'certainty': 0.9, 'description': 'ì™„ì „í•œ ì •ë³´'},
            {'info_level': 'partial', 'certainty': 0.6, 'description': 'ë¶€ë¶„ì  ì •ë³´'},
            {'info_level': 'minimal', 'certainty': 0.3, 'description': 'ìµœì†Œí•œì˜ ì •ë³´'}
        ]
        
        # 4. ë³µí•© ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        for moral in moral_alternatives:
            for temporal in temporal_alternatives:
                for info in information_alternatives:
                    scenario = {
                        'id': f"{moral['type']}_{temporal['timing']}_{info['info_level']}",
                        'moral_framework': moral,
                        'temporal_aspect': temporal,
                        'information_context': info,
                        'base_text': base_text,
                        'embedding_similarity': float(np.random.normal(0.7, 0.15))  # ì˜ë¯¸ì  ìœ ì‚¬ì„±
                    }
                    scenarios.append(scenario)
        
        return scenarios[:12]  # ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ„í•´ 12ê°œë¡œ ì œí•œ
    
    async def _bayesian_update(self, priors: Dict[str, float], scenario: Dict[str, Any], 
                               semantic_embedding: np.ndarray) -> float:
        """ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸ë¥¼ í†µí•œ í›„í—˜ í™•ë¥  ê³„ì‚°"""
        # 1. ìš°ë„ í•¨ìˆ˜ ê³„ì‚°
        likelihood = self._calculate_likelihood(scenario, semantic_embedding)
        
        # 2. ì‚¬ì „ í™•ë¥ 
        prior = priors.get('moral_action', 0.5)
        
        # 3. ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸: P(H|E) = P(E|H) * P(H) / P(E)
        # ì •ê·œí™”ë¥¼ ìœ„í•œ ê·¼ì‚¬ì  ì¦ê±° í™•ë¥ 
        evidence_prob = 0.5  # ì •ê·œí™” ìƒìˆ˜
        
        posterior = (likelihood * prior) / evidence_prob
        return min(1.0, max(0.0, posterior))  # 0-1 ë²”ìœ„ë¡œ ì œí•œ
    
    def _calculate_likelihood(self, scenario: Dict[str, Any], 
                             semantic_embedding: np.ndarray) -> float:
        """ì‹œë‚˜ë¦¬ì˜¤ì˜ ìš°ë„ ê³„ì‚°"""
        likelihood = 0.5  # ê¸°ë³¸ê°’
        
        # 1. ë„ë•ì  í”„ë ˆì„ì›Œí¬ì— ë”°ë¥¸ ìš°ë„ ì¡°ì •
        moral_type = scenario.get('moral_framework', {}).get('type', '')
        if moral_type == 'utilitarian':
            likelihood += 0.2
        elif moral_type == 'deontological':
            likelihood += 0.15
        elif moral_type == 'virtue_ethics':
            likelihood += 0.1
        
        # 2. ì •ë³´ ì™„ì „ì„±ì— ë”°ë¥¸ ìš°ë„ ì¡°ì •
        info_level = scenario.get('information_context', {}).get('info_level', '')
        if info_level == 'complete':
            likelihood += 0.2
        elif info_level == 'partial':
            likelihood += 0.1
        
        # 3. ì˜ë¯¸ì  ìœ ì‚¬ì„± ê³ ë ¤
        similarity = scenario.get('embedding_similarity', 0.5)
        likelihood += (similarity - 0.5) * 0.3
        
        return min(1.0, max(0.1, likelihood))
    
    async def _calculate_contextual_utility(self, scenario: Dict[str, Any], 
                                           processed_data: Dict) -> float:
        """ë¬¸í•™ì  ë§¥ë½ì„ ê³ ë ¤í•œ ìœ í‹¸ë¦¬í‹° ê³„ì‚°"""
        utility = 0.5  # ê¸°ë³¸ ìœ í‹¸ë¦¬í‹°
        
        # 1. ë„ë•ì  í”„ë ˆì„ì›Œí¬ì— ë”°ë¥¸ ìœ í‹¸ë¦¬í‹°
        moral_framework = scenario.get('moral_framework', {})
        if moral_framework.get('type') == 'utilitarian':
            utility += self._assess_collective_benefit(processed_data) * 0.3
        elif moral_framework.get('type') == 'deontological':
            utility += self._assess_rule_adherence(processed_data) * 0.25
        
        # 2. ì‹œê°„ì  ê³ ë ¤ì‚¬í•­
        temporal_aspect = scenario.get('temporal_aspect', {})
        if temporal_aspect.get('horizon') == 'long_term':
            utility += 0.2  # ì¥ê¸°ì  ì‚¬ê³ ì— ë³´ë„ˆìŠ¤
        
        # 3. ì •ë³´ í’ˆì§ˆ ë³´ì •
        info_context = scenario.get('information_context', {})
        certainty = info_context.get('certainty', 0.5)
        utility *= certainty  # ë¶ˆí™•ì‹¤ì„±ì— ë”°ë¥¸ í• ì¸
        
        return min(1.0, max(0.0, utility))
    
    def _assess_collective_benefit(self, processed_data: Dict) -> float:
        """ì§‘ë‹¨ì  ì´ìµ í‰ê°€"""
        text = processed_data.get('text', '').lower()
        benefit_indicators = ['ëª¨ë“ ', 'ì „ì²´', 'ê³µë™', 'ì‚¬íšŒ', 'ë‹¤ìˆ˜', 'ê³µìµ']
        benefit_score = sum(1 for indicator in benefit_indicators if indicator in text)
        return min(1.0, benefit_score / len(benefit_indicators))
    
    def _assess_rule_adherence(self, processed_data: Dict) -> float:
        """ê·œì¹™ ì¤€ìˆ˜ í‰ê°€"""
        text = processed_data.get('text', '').lower()
        rule_indicators = ['ë²•', 'ê·œì¹™', 'ì›ì¹™', 'ì˜ë¬´', 'ëª…ë ¹', 'ì§€ì¹¨', 'ê·œì •']
        rule_score = sum(1 for indicator in rule_indicators if indicator in text)
        return min(1.0, rule_score / len(rule_indicators))
    
    async def _calculate_baseline_utility(self, processed_data: Dict) -> float:
        """ê¸°ì¤€ì„  ìœ í‹¸ë¦¬í‹° ê³„ì‚°"""
        # í˜„ì¬ ìƒí™©ì˜ ê¸°ë³¸ ìœ í‹¸ë¦¬í‹°ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ í‰ê°€
        text = processed_data.get('text', '')
        
        # 1. ê¸ì •/ë¶€ì • ì§€í‘œ
        positive_indicators = ['ì¢‹', 'í–‰ë³µ', 'ì„±ê³µ', 'ë„ì›€', 'ì´ìµ', 'ë§Œì¡±']
        negative_indicators = ['ë‚˜ì˜', 'ìŠ¬í””', 'ì‹¤íŒ¨', 'í•´ë¡œ', 'ì†í•´', 'ë¶ˆë§Œ']
        
        text_lower = text.lower()
        positive_score = sum(1 for indicator in positive_indicators if indicator in text_lower)
        negative_score = sum(1 for indicator in negative_indicators if indicator in text_lower)
        
        # 2. ê· í˜• ê³„ì‚°
        if positive_score + negative_score > 0:
            sentiment_ratio = positive_score / (positive_score + negative_score)
        else:
            sentiment_ratio = 0.5
        
        # 3. ê¸°ì¤€ì„  ìœ í‹¸ë¦¬í‹° (0.3-0.7 ë²”ìœ„)
        baseline = 0.3 + (sentiment_ratio * 0.4)
        
        return baseline
        
        self.logger.info(f"Regret report saved to {report_file}")