#!/usr/bin/env python3
"""
μ¬κ° λ΅λ“ ν…μ¤νΈ - λ¨λ“λ³„ μ²΄ν¬ν¬μΈνΈ ν”λ« λ³€ν™ ν™•μΈ
"""

import sys
import torch
from pathlib import Path

# ν”„λ΅μ νΈ λ£¨νΈ μ¶”κ°€
sys.path.insert(0, str(Path(__file__).parent))

def test_modular_to_flat():
    """λ¨λ“λ³„ stateλ¥Ό ν”λ« κµ¬μ΅°λ΅ λ³€ν™ ν…μ¤νΈ"""
    
    checkpoint_path = "training/checkpoints_final/checkpoint_epoch_0023_lr_0.000011_20250824_204202.pt"
    
    print("=" * 60)
    print("μ²΄ν¬ν¬μΈνΈ κµ¬μ΅° λ³€ν™ ν…μ¤νΈ")
    print("=" * 60)
    
    # μ²΄ν¬ν¬μΈνΈ λ΅λ“
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    model_state = checkpoint['model_state']
    
    print(f"β… μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ™„λ£")
    print(f"   - μ—ν­: {checkpoint['epoch']}")
    print(f"   - LR: {checkpoint.get('lr', 'N/A')}")
    
    # κµ¬μ΅° ν™•μΈ
    if isinstance(model_state, dict) and 'backbone' in model_state:
        print(f"\nπ“¦ λ¨λ“λ³„ κµ¬μ΅° κ°μ§€:")
        for module_name in model_state.keys():
            param_count = len(model_state[module_name])
            print(f"   - {module_name}: {param_count}κ° νλΌλ―Έν„°")
        
        # ν”λ« κµ¬μ΅°λ΅ λ³€ν™
        flat_state = {}
        for module_name, module_state in model_state.items():
            for param_name, param_value in module_state.items():
                flat_state[f"{module_name}.{param_name}"] = param_value
        
        print(f"\nβ… ν”λ« κµ¬μ΅°λ΅ λ³€ν™ μ™„λ£:")
        print(f"   - μ΄ νλΌλ―Έν„°: {len(flat_state)}κ°")
        
        # μƒν” ν‚¤ μ¶λ ¥
        print(f"\nπ“‹ λ³€ν™λ ν‚¤ μƒν” (μ²μ 5κ°):")
        for i, key in enumerate(list(flat_state.keys())[:5]):
            print(f"   - {key}")
        
        # optimizer_state ν™•μΈ
        if 'optimizer_state' in checkpoint:
            print(f"\nβ… Optimizer State: μ΅΄μ¬ (μ¬κ° κ°€λ¥)")
        else:
            print(f"\nβ Optimizer State: μ—†μ (μ¬κ° λ¶κ°€)")
            
    else:
        print(f"\nβ οΈ μ΄λ―Έ ν”λ«ν• κµ¬μ΅°")
    
    print("\nν…μ¤νΈ μ™„λ£!")

if __name__ == "__main__":
    test_modular_to_flat()