#!/usr/bin/env python3
"""
Red Heart AI í•„ìˆ˜ ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
Pre-download essential models for Red Heart AI

í…ŒìŠ¤íŠ¸ ë° í•™ìŠµ ì‹œ ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ ì—†ì´ ë¡œì»¬ ìºì‹œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡
í•„ìš”í•œ transformers ë° sentence-transformers ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
"""

import os
import sys
import logging
from pathlib import Path
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def download_transformers_models():
    """Transformers ëª¨ë¸ë“¤ ë‹¤ìš´ë¡œë“œ"""
    
    print("ğŸš€ Transformers ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    
    models_to_download = [
        "Helsinki-NLP/opus-mt-ko-en",  # í•œêµ­ì–´-ì˜ì–´ ë²ˆì—­
        "jhgan/ko-sroberta-multitask",  # í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©
        "beomi/KcELECTRA-base-v2022",  # í•œêµ­ì–´ ELECTRA
        "j-hartmann/emotion-english-distilroberta-base",  # ê°ì • ë¶„ì„
        "klue/bert-base-kor-ner"  # í•œêµ­ì–´ NER
    ]
    
    try:
        from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
        from transformers import MarianTokenizer, MarianMTModel
        
        for i, model_name in enumerate(models_to_download, 1):
            print(f"\nğŸ“¥ [{i}/{len(models_to_download)}] {model_name} ë‹¤ìš´ë¡œë“œ ì¤‘...")
            start_time = time.time()
            
            try:
                # ëª¨ë¸ë³„ íŠ¹ë³„ ì²˜ë¦¬
                if "opus-mt" in model_name:
                    # ë²ˆì—­ ëª¨ë¸
                    tokenizer = MarianTokenizer.from_pretrained(model_name)
                    model = MarianMTModel.from_pretrained(model_name)
                    print(f"   âœ… ë²ˆì—­ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                elif "emotion" in model_name:
                    # ê°ì • ë¶„ì„ ëª¨ë¸
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    model = AutoModelForSequenceClassification.from_pretrained(model_name)
                    print(f"   âœ… ê°ì • ë¶„ì„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                else:
                    # ì¼ë°˜ ëª¨ë¸
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    model = AutoModel.from_pretrained(model_name)
                    print(f"   âœ… ì¼ë°˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                
                elapsed = time.time() - start_time
                print(f"   â±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
                
            except Exception as e:
                print(f"   âŒ {model_name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                logger.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
                continue
        
        print(f"\nâœ… Transformers ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        
    except ImportError as e:
        print(f"âŒ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: {e}")
        return False
    
    return True

def download_sentence_transformers():
    """SentenceTransformer ëª¨ë¸ë“¤ ë‹¤ìš´ë¡œë“œ"""
    
    print("\nğŸ”¤ SentenceTransformer ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    
    sentence_models = [
        "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",  # ë‹¤êµ­ì–´ ë¬¸ì¥ ì„ë² ë”©
        "jhgan/ko-sroberta-multitask"  # í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© (SentenceTransformer ë²„ì „)
    ]
    
    try:
        from sentence_transformers import SentenceTransformer
        
        for i, model_name in enumerate(sentence_models, 1):
            print(f"\nğŸ“¥ [{i}/{len(sentence_models)}] {model_name} ë‹¤ìš´ë¡œë“œ ì¤‘...")
            start_time = time.time()
            
            try:
                # SentenceTransformer ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
                model = SentenceTransformer(model_name)
                
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ ëª¨ë¸ ë¡œë”© í™•ì¸
                test_encoding = model.encode(["í…ŒìŠ¤íŠ¸ ë¬¸ì¥"])
                print(f"   âœ… SentenceTransformer ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                print(f"   ğŸ“Š ì„ë² ë”© ì°¨ì›: {test_encoding.shape[1]}")
                
                elapsed = time.time() - start_time
                print(f"   â±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
                
            except Exception as e:
                print(f"   âŒ {model_name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                logger.error(f"SentenceTransformer ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
                continue
        
        print(f"\nâœ… SentenceTransformer ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        
    except ImportError as e:
        print(f"âŒ sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: {e}")
        return False
    
    return True

def check_cache_status():
    """ìºì‹œ ìƒíƒœ í™•ì¸"""
    print("\nğŸ“‚ ìºì‹œ ìƒíƒœ í™•ì¸...")
    
    cache_paths = [
        Path.home() / '.cache' / 'huggingface' / 'transformers',
        Path.home() / '.cache' / 'torch' / 'sentence_transformers'
    ]
    
    total_size = 0
    for cache_path in cache_paths:
        if cache_path.exists():
            # ìºì‹œ í¬ê¸° ê³„ì‚°
            size = sum(f.stat().st_size for f in cache_path.rglob('*') if f.is_file())
            size_mb = size / (1024 * 1024)
            total_size += size_mb
            
            print(f"   ğŸ“ {cache_path}: {size_mb:.1f} MB")
        else:
            print(f"   ğŸ“ {cache_path}: ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
    
    print(f"   ğŸ’¾ ì´ ìºì‹œ í¬ê¸°: {total_size:.1f} MB")
    return total_size

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 60)
    print("ğŸš€ Red Heart AI í•„ìˆ˜ ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ")
    print("=" * 60)
    
    print("\nğŸ“‹ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ëª©ë¡:")
    print("   1. Helsinki-NLP/opus-mt-ko-en (ë²ˆì—­)")
    print("   2. jhgan/ko-sroberta-multitask (í•œêµ­ì–´ ì„ë² ë”©)")
    print("   3. beomi/KcELECTRA-base-v2022 (í•œêµ­ì–´)")
    print("   4. j-hartmann/emotion-english-distilroberta-base (ê°ì • ë¶„ì„)")
    print("   5. klue/bert-base-kor-ner (í•œêµ­ì–´ NER)")
    print("   6. sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (ë‹¤êµ­ì–´)")
    
    # ì‹œì‘ ì „ ìºì‹œ ìƒíƒœ
    print("\n" + "=" * 40)
    print("ì‹œì‘ ì „ ìºì‹œ ìƒíƒœ")
    print("=" * 40)
    initial_cache_size = check_cache_status()
    
    # ë‹¤ìš´ë¡œë“œ ì‹œì‘
    start_time = time.time()
    
    # 1. Transformers ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    transformers_success = download_transformers_models()
    
    # 2. SentenceTransformer ëª¨ë¸ ë‹¤ìš´ë¡œë“œ  
    sentence_success = download_sentence_transformers()
    
    # ìµœì¢… ê²°ê³¼
    total_time = time.time() - start_time
    
    print("\n" + "=" * 40)
    print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ìºì‹œ ìƒíƒœ")
    print("=" * 40)
    final_cache_size = check_cache_status()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    print("=" * 60)
    print(f"   â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ")
    print(f"   ğŸ“ˆ ìºì‹œ ì¦ê°€ëŸ‰: {final_cache_size - initial_cache_size:.1f} MB")
    print(f"   ğŸ”§ Transformers: {'âœ… ì„±ê³µ' if transformers_success else 'âŒ ì‹¤íŒ¨'}")
    print(f"   ğŸ”¤ SentenceTransformers: {'âœ… ì„±ê³µ' if sentence_success else 'âŒ ì‹¤íŒ¨'}")
    
    if transformers_success and sentence_success:
        print("\nâœ… ëª¨ë“  ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("   ì´ì œ unified-testë¥¼ ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ ì—†ì´ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("\nğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”:")
        print("   ./run_learning.sh unified-test --samples 3 --debug --verbose")
        return 0
    else:
        print("\nâš ï¸ ì¼ë¶€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("   ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)