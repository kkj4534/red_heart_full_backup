#!/usr/bin/env python3
"""
ë¡œì»¬ ë²ˆì—­ ëª¨ë“ˆ - OPUS-MT ê¸°ë°˜ í•œêµ­ì–´â†’ì˜ì–´ ë²ˆì—­
ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„ë¦¬í•˜ì—¬ MasterMemoryOrchestratorê°€ ê´€ë¦¬

ì´ ëª¨ë“ˆì€ ì „ì—­ ëª¨ë“ˆë¡œ ë“±ë¡ë˜ì–´:
1. ì´ˆê¸°í™” ì‹œì ì´ ì˜ˆì¸¡ ê°€ëŠ¥
2. GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ì •ì±… ì¤€ìˆ˜
3. ë‹¤ë¥¸ ëª¨ë“ˆë“¤ê³¼ ì¼ê´€ëœ ê´€ë¦¬
"""

import torch
import time
import logging
from typing import Optional, Dict
from config import get_smart_device

logger = logging.getLogger(__name__)


class LocalTranslator:
    """ë¡œì»¬ OPUS-MT ê¸°ë°˜ í•œêµ­ì–´â†’ì˜ì–´ ë²ˆì—­ê¸°
    
    íŠ¹ì§•:
    - ì™„ì „ ì˜¤í”„ë¼ì¸ ì‘ë™
    - MasterMemoryOrchestrator í†µí•©
    - ìºì‹±ì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”
    """
    
    def __init__(self, lazy_load: bool = True):
        """ë¡œì»¬ ë²ˆì—­ê¸° ì´ˆê¸°í™” (lazy loading ì§€ì›)
        
        Args:
            lazy_load: Trueë©´ ì‹¤ì œ ë²ˆì—­ì´ í•„ìš”í•  ë•Œ ëª¨ë¸ ë¡œë“œ
        """
        self.model_name = 'Helsinki-NLP/opus-mt-ko-en'
        self.tokenizer = None
        self.model = None
        self.device = None
        self.translation_cache = {}  # ë²ˆì—­ ê²°ê³¼ ìºì‹±
        self.initialized = False
        self.lazy_load = lazy_load
        
        logger.info(f"LocalTranslator ìƒì„± - {'lazy loading ëª¨ë“œ' if lazy_load else 'ì¦‰ì‹œ ë¡œë“œ ëª¨ë“œ'}")
        
        # lazy_loadê°€ Falseì¼ ë•Œë§Œ ì¦‰ì‹œ ì´ˆê¸°í™”
        if not self.lazy_load:
            self._initialize_model()
    
    def _initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™” - ì „ì—­ ëª¨ë“ˆ ë“±ë¡ ì‹œ ì¦‰ì‹œ ì‹¤í–‰"""
        if self.initialized:
            return
        
        try:
            logger.info(f"ğŸ”„ OPUS-MT ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
            start_time = time.time()
            
            from transformers import MarianMTModel, MarianTokenizer
            
            # HF ë˜í¼ëŠ” ëª¨ë¸ì—ë§Œ ì‚¬ìš© (í† í¬ë‚˜ì´ì €ëŠ” ì§ì ‘ ë¡œë“œ)
            from hf_model_wrapper import get_hf_wrapper, enable_auto_registration
            
            # HF ëª¨ë¸ ìë™ ë“±ë¡ í™œì„±í™” (ì›ë³¸ í•¨ìˆ˜ ì €ì¥)
            enable_auto_registration()
            
            hf_wrapper = get_hf_wrapper()
            
            # í† í¬ë‚˜ì´ì €ëŠ” ì§ì ‘ ë¡œë“œ (íŒ¨ì¹˜ ë¬¸ì œ ìš°íšŒ)
            self.tokenizer = MarianTokenizer.from_pretrained(
                self.model_name,
                local_files_only=True
            )
            
            # ëª¨ë¸ì€ ë˜í¼ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì¶”ì ) - CPUì—ì„œë§Œ ì‹¤í–‰
            self.model = hf_wrapper.wrapped_from_pretrained(
                MarianMTModel, self.model_name, 
                owner="translator",
                local_files_only=True,
                device_map="cpu"  # CPU ì „ìš©ìœ¼ë¡œ ëª…ì‹œ
            )
            
            # CPUì—ì„œë§Œ ì´ˆê¸°í™” (GPU ë©”ëª¨ë¦¬ ì ˆì•½)
            self.device = torch.device('cpu')  # CPU ê³ ì •
            self.model = self.model.to(self.device)  # CPUì— ìœ ì§€
            
            # í‰ê°€ ëª¨ë“œ ì„¤ì •
            self.model.eval()
            
            load_time = time.time() - start_time
            logger.info(f"âœ… OPUS-MT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {load_time:.1f}ì´ˆ, ë””ë°”ì´ìŠ¤: {self.device})")
            
            # CPU ì‚¬ìš© ë¡œê¹…
            logger.info("ğŸ“Š TranslatorëŠ” CPUì—ì„œ ì´ˆê¸°í™”ë¨ (GPU ë©”ëª¨ë¦¬ ì ˆì•½)")
            
            # DSMì— ë“±ë¡ (CPU residentë¡œ ì‹œì‘)
            try:
                from dynamic_swap_manager import get_swap_manager, SwapPriority
                import asyncio
                
                swap_manager = get_swap_manager()
                if swap_manager:
                    # ì´ë²¤íŠ¸ ë£¨í”„ í™•ì¸ ë° ìƒì„±
                    try:
                        loop = asyncio.get_running_loop()
                    except RuntimeError:
                        # ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        logger.debug("DSM ë“±ë¡ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±")
                    
                    # compression_enabledê°€ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì„ ì‹œì‘í•˜ë¯€ë¡œ ì„ì‹œ ë¹„í™œì„±í™”
                    original_compression = swap_manager.compression_enabled
                    swap_manager.compression_enabled = False
                    
                    # ë™ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ë“±ë¡
                    swap_manager.register_model(
                        "translator",
                        self.model,
                        priority=SwapPriority.HIGH  # NO FALLBACK - lazy ì œê±°
                    )
                    
                    # ì••ì¶• ì„¤ì • ë³µêµ¬
                    swap_manager.compression_enabled = original_compression
                    
                    logger.info("âœ… Translatorë¥¼ DSMì— ë“±ë¡ (HIGH priority)")
            except Exception as e:
                logger.warning(f"DSM ë“±ë¡ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
            
            self.initialized = True
            
        except Exception as e:
            logger.error(f"âŒ OPUS-MT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.initialized = False
            raise RuntimeError(f"ë¡œì»¬ ë²ˆì—­ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _is_english_text(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ ì˜ì–´ì¸ì§€ ê°ì§€"""
        if not text or len(text.strip()) == 0:
            return True
        
        # í•œêµ­ì–´ ë¬¸ì ë¹„ìœ¨ ê³„ì‚° (ìœ ë‹ˆì½”ë“œ ë²”ìœ„ í™œìš©)
        korean_chars = 0
        total_chars = 0
        
        for char in text:
            if char.isalpha():  # ì•ŒíŒŒë²³ ë¬¸ìë§Œ ê³ ë ¤
                total_chars += 1
                # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„: AC00-D7AF (ê°€-í£), 1100-11FF (ìëª¨)
                if '\uAC00' <= char <= '\uD7AF' or '\u1100' <= char <= '\u11FF':
                    korean_chars += 1
        
        if total_chars == 0:
            return True  # ì•ŒíŒŒë²³ì´ ì—†ìœ¼ë©´ ì˜ì–´ë¡œ ê°„ì£¼ (ìˆ«ì, ê¸°í˜¸ë§Œ ìˆëŠ” ê²½ìš°)
        
        korean_ratio = korean_chars / total_chars
        return korean_ratio < 0.1  # í•œêµ­ì–´ ë¹„ìœ¨ì´ 10% ë¯¸ë§Œì´ë©´ ì˜ì–´ë¡œ íŒë‹¨
    
    def load_to_gpu(self) -> bool:
        """í•„ìš”ì‹œ ëª¨ë¸ì„ GPUë¡œ ìŠ¹ê²©"""
        try:
            if not self.initialized or self.model is None:
                logger.warning("ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return False
            
            # ì´ë¯¸ GPUì— ìˆìœ¼ë©´ ìŠ¤í‚µ
            if next(self.model.parameters()).is_cuda:
                return True
            
            # GPU ê°€ìš©ì„± í™•ì¸
            if not torch.cuda.is_available():
                logger.warning("GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥")
                return False
            
            # WorkflowAwareMemoryManagerë¥¼ í†µí•´ GPU ë©”ëª¨ë¦¬ í™•ë³´
            logger.info("ğŸ”„ Translator GPU ë©”ëª¨ë¦¬ ìš”ì²­ ì¤‘...")
            from workflow_aware_memory_manager import WorkflowAwareMemoryManager
            mem_manager = WorkflowAwareMemoryManager()
            
            # DSM ì‹¤ì¸¡ì¹˜ ì‚¬ìš© (í—ˆìˆ˜ ì˜ˆì•½ì¹˜ ì œê±°)
            from dynamic_swap_manager import get_swap_manager
            swap = get_swap_manager()
            required_mb = 0.0
            if swap and "translator" in swap.models:
                required_mb = max(0.0, swap.models["translator"].size_mb)  # ì‹¤ì¸¡ì¹˜
                logger.info(f"ğŸ“Š Translator ì‹¤ì¸¡ í¬ê¸°: {required_mb:.1f}MB")
            else:
                # DSMì— ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                required_mb = 300  # ìµœì†Œê°’
                logger.info(f"ğŸ“Š Translator ê¸°ë³¸ í¬ê¸° ì‚¬ìš©: {required_mb}MB")
            
            # ë™ê¸° ë°©ì‹ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ ìš”ì²­ (DSM ì‹¤ì¸¡ì¹˜)
            mem_ok = mem_manager.request_gpu_blocking(
                module_name="translator",
                required_mb=required_mb,
                deps=[],
                target_util=0.85,
                timeout=30.0,
                is_required=False  # í•„ìˆ˜ê°€ ì•„ë‹˜
            )
            
            if not mem_ok:
                logger.error("GPU ë©”ëª¨ë¦¬ í™•ë³´ ì‹¤íŒ¨")
                raise RuntimeError("GPU space for translator not available")
            
            # GPUë¡œ ì´ë™
            logger.info("ğŸš€ Translatorë¥¼ GPUë¡œ ìŠ¹ê²© ì¤‘...")
            self.model = self.model.to(torch.device('cuda'))
            self.device = torch.device('cuda')
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
            allocated_mb = torch.cuda.memory_allocated(self.device) / 1024 / 1024
            logger.info(f"âœ… Translator GPU ìŠ¹ê²© ì™„ë£Œ (ë©”ëª¨ë¦¬: {allocated_mb:.1f}MB)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ GPU ìŠ¹ê²© ì‹¤íŒ¨: {e}")
            return False
    
    def unload_from_gpu(self) -> bool:
        """GPUì—ì„œ CPUë¡œ ì–¸ë¡œë“œ"""
        try:
            if not self.initialized or self.model is None:
                return False
            
            # ì´ë¯¸ CPUì— ìˆìœ¼ë©´ ìŠ¤í‚µ
            if not next(self.model.parameters()).is_cuda:
                return True
            
            # CPUë¡œ ì´ë™
            logger.info("â¬‡ï¸ Translatorë¥¼ CPUë¡œ ì–¸ë¡œë“œ ì¤‘...")
            self.model = self.model.to(torch.device('cpu'))
            self.device = torch.device('cpu')
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info("âœ… Translator CPU ì–¸ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ CPU ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def translate_ko_to_en(self, korean_text: str) -> str:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­
        
        Args:
            korean_text: ë²ˆì—­í•  í•œêµ­ì–´ í…ìŠ¤íŠ¸
            
        Returns:
            ë²ˆì—­ëœ ì˜ì–´ í…ìŠ¤íŠ¸
        """
        if not korean_text or len(korean_text.strip()) == 0:
            return korean_text
        
        # ì˜ì–´ í…ìŠ¤íŠ¸ ê°ì§€
        if self._is_english_text(korean_text):
            logger.debug("í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ ì˜ì–´ë¡œ íŒë‹¨ë¨, ë²ˆì—­ ìƒëµ")
            return korean_text
        
        # ìºì‹œ í™•ì¸
        cache_key = hash(korean_text.strip())
        if cache_key in self.translation_cache:
            logger.debug("ë²ˆì—­ ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜")
            return self.translation_cache[cache_key]
        
        # Lazy loading: ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì—¬ê¸°ì„œ ì´ˆê¸°í™”
        if not self.initialized:
            if self.lazy_load:
                logger.info("ğŸ”„ Lazy loading: ë²ˆì—­ì´ í•„ìš”í•´ì„œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
                self._initialize_model()
                if not self.initialized:
                    logger.error("LocalTranslator ì´ˆê¸°í™” ì‹¤íŒ¨")
                    raise RuntimeError("LocalTranslator initialization failed")
            else:
                logger.error("LocalTranslatorê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                raise RuntimeError("LocalTranslator not initialized")
        
        try:
            # ë²ˆì—­ ìˆ˜í–‰
            start_time = time.time()
            inputs = self.tokenizer([korean_text], return_tensors='pt', padding=True).to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=128,      # ì¶©ë¶„í•œ ê¸¸ì´
                    num_beams=3,         # ì ë‹¹í•œ í’ˆì§ˆ
                    early_stopping=True, # íš¨ìœ¨ì„±
                    do_sample=False      # ì¼ê´€ì„±
                )
            
            translated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            translation_time = time.time() - start_time
            
            # ìºì‹œ ì €ì¥
            self.translation_cache[cache_key] = translated_text
            
            logger.debug(f"ë²ˆì—­ ì™„ë£Œ: \"{korean_text[:30]}...\" â†’ \"{translated_text[:30]}...\" ({translation_time:.2f}ì´ˆ)")
            return translated_text
            
        except Exception as e:
            logger.error(f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise RuntimeError(f"Translation failed: {e}")
    
    def clear_cache(self):
        """ë²ˆì—­ ìºì‹œ ì´ˆê¸°í™”"""
        self.translation_cache.clear()
        logger.info("ë²ˆì—­ ìºì‹œ ì´ˆê¸°í™”ë¨")
    
    def get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        if not self.initialized or self.device is None:
            return {"allocated_mb": 0, "cached_mb": 0}
        
        if self.device.type == 'cuda':
            allocated_mb = torch.cuda.memory_allocated(self.device) / 1024 / 1024
            cached_mb = torch.cuda.memory_reserved(self.device) / 1024 / 1024
            return {
                "allocated_mb": allocated_mb,
                "cached_mb": cached_mb
            }
        else:
            return {"allocated_mb": 0, "cached_mb": 0}
    
    def to(self, device):
        """ëª¨ë¸ì„ ë‹¤ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (MasterMemoryOrchestrator í˜¸í™˜)"""
        if self.initialized and self.model is not None:
            self.device = device
            self.model = self.model.to(device)
            logger.info(f"LocalTranslator ëª¨ë¸ì„ {device}ë¡œ ì´ë™")
        return self
    
    def get_pytorch_network(self):
        """PyTorch ë„¤íŠ¸ì›Œí¬ ë°˜í™˜ (HeadAdapterì™€ì˜ í˜¸í™˜ì„±)"""
        if not self.initialized:
            logger.warning("LocalTranslator: ëª¨ë¸ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            self._initialize_model()
        
        if self.model is not None:
            logger.info("âœ… LocalTranslator: MarianMT ëª¨ë¸ ë°˜í™˜")
            return self.model
        
        # STRICT_NO_FALLBACK
        raise RuntimeError("LocalTranslator: get_pytorch_network ì‹¤íŒ¨ - ëª¨ë¸ ì—†ìŒ")
    
    async def translate_async(self, text: str) -> str:
        """ë¹„ë™ê¸° ë²ˆì—­ ë©”ì„œë“œ - claude_inference.py í˜¸í™˜ìš©
        
        Args:
            text: ë²ˆì—­í•  í…ìŠ¤íŠ¸ (í•œêµ­ì–´)
            
        Returns:
            ë²ˆì—­ëœ ì˜ì–´ í…ìŠ¤íŠ¸
        """
        import asyncio
        
        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ë˜í•‘
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.translate_ko_to_en, text)
    
    def __repr__(self):
        return f"LocalTranslator(model={self.model_name}, initialized={self.initialized}, device={self.device})"