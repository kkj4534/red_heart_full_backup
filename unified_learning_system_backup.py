"""
í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ - Week 4 í•µì‹¬ êµ¬í˜„
Unified Learning System - Week 4 Core Implementation

ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ 800M íŒŒë¼ë¯¸í„° í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ:
- ê³µìœ  ë°±ë³¸ì„ í†µí•œ ë‹¤ì¤‘ í—¤ë“œ ë™ì‹œ í•™ìŠµ
- ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ë° í˜¼í•© ì •ë°€ë„ (FP16) í›ˆë ¨
- ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ ë° ë©”ëª¨ë¦¬ ìµœì í™”
- í—¤ë“œë³„ í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ ë° ì†ì‹¤ í•¨ìˆ˜ ê· í˜•
- íš¨ìœ¨ì  ë°±í”„ë¡œíŒŒê²Œì´ì…˜ ë° ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
"""

import asyncio
import logging
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import GradScaler, autocast
from torch.utils.checkpoint import checkpoint
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque, defaultdict, OrderedDict
from enum import Enum
import numpy as np
# pathlib ì œê±° - WSL í˜¸í™˜ì„±ì„ ìœ„í•´ os.path ì‚¬ìš©
import json
import threading
from abc import ABC, abstractmethod
import math
import gc
import psutil
import os

# í•µì‹¬ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from config import ADVANCED_CONFIG, get_smart_device, get_gpu_memory_info, ModelPriority, get_priority_based_device, _force_swap_low_priority_models, _emergency_gpu_cleanup
from head_compatibility_interface import HeadType, HeadProcessingResult
from unified_red_heart_core import RedHeartUnifiedBackbone, UnifiedRepresentation
from dynamic_swap_manager import RedHeartDynamicSwapManager
from intelligent_synergy_system import IntelligentSynergySystem

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

class LearningPhase(Enum):
    """í•™ìŠµ ë‹¨ê³„"""
    WARM_UP = "warm_up"                    # ì›Œë°ì—… ë‹¨ê³„
    COLLABORATIVE = "collaborative"        # í˜‘ë ¥ í•™ìŠµ ë‹¨ê³„
    SPECIALIZED = "specialized"            # ì „ë¬¸í™” í•™ìŠµ ë‹¨ê³„
    INTEGRATION = "integration"            # í†µí•© í•™ìŠµ ë‹¨ê³„
    FINE_TUNING = "fine_tuning"           # íŒŒì¸íŠœë‹ ë‹¨ê³„

class TrainingStrategy(Enum):
    """í›ˆë ¨ ì „ëµ"""
    ROUND_ROBIN = "round_robin"           # ìˆœì°¨ì  í—¤ë“œ í›ˆë ¨
    PARALLEL = "parallel"                 # ë³‘ë ¬ í—¤ë“œ í›ˆë ¨
    PRIORITY_BASED = "priority_based"     # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ í›ˆë ¨
    ADAPTIVE = "adaptive"                 # ì ì‘ì  í›ˆë ¨

@dataclass
class TrainingMetrics:
    """í›ˆë ¨ ë©”íŠ¸ë¦­"""
    epoch: int = 0
    step: int = 0
    total_loss: float = 0.0
    head_losses: Dict[HeadType, float] = field(default_factory=dict)
    learning_rates: Dict[str, float] = field(default_factory=dict)
    memory_usage: float = 0.0
    gpu_utilization: float = 0.0
    training_time: float = 0.0
    gradient_norm: float = 0.0
    synergy_gain: float = 0.0
    
    # íš¨ìœ¨ì„± ë©”íŠ¸ë¦­
    samples_per_second: float = 0.0
    memory_efficiency: float = 0.0  # ë©”ëª¨ë¦¬ ì‚¬ìš© íš¨ìœ¨ì„±
    convergence_rate: float = 0.0   # ìˆ˜ë ´ ì†ë„

@dataclass
class HeadTrainingConfig:
    """í—¤ë“œë³„ í›ˆë ¨ ì„¤ì •"""
    head_type: HeadType
    learning_rate: float = 1e-4
    weight_decay: float = 1e-5
    gradient_clip_value: float = 1.0
    loss_weight: float = 1.0
    update_frequency: int = 1  # Në²ˆì˜ ìŠ¤í…ë§ˆë‹¤ ì—…ë°ì´íŠ¸
    freeze_until_epoch: int = 0  # ì´ ì—í¬í¬ê¹Œì§€ ë™ê²°
    
    # ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
    warmup_steps: int = 1000
    scheduler_type: str = "cosine"  # cosine, linear, exponential
    
    # ì •ê·œí™” ì„¤ì •
    dropout_rate: float = 0.1
    label_smoothing: float = 0.0

class MemoryEfficientTrainer(nn.Module):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í›ˆë ¨ê¸°"""
    
    def __init__(self, unified_backbone: RedHeartUnifiedBackbone,
                 head_configs: Dict[HeadType, HeadTrainingConfig],
                 unified_system=None):  # UnifiedLearningSystem ì°¸ì¡° ì¶”ê°€
        super().__init__()
        
        self.unified_backbone = unified_backbone
        self.head_configs = head_configs
        self.unified_system = unified_system  # cached_head_modules ì ‘ê·¼ì„ ìœ„í•´
        
        # í›ˆë ¨ ìƒíƒœ - ë©”ëª¨ë¦¬ ì•ˆì „ì„±ì„ ìœ„í•´ ROUND_ROBIN ê³ ì •
        self.current_phase = LearningPhase.WARM_UP
        self.training_strategy = TrainingStrategy.ROUND_ROBIN
        logger.info(f"í›ˆë ¨ ì „ëµ ì„¤ì •: {self.training_strategy.value} (ë©”ëª¨ë¦¬ ì•ˆì „ì„± ìš°ì„ )")
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        self.use_gradient_checkpointing = True
        self.use_mixed_precision = True
        self.gradient_accumulation_steps = 4
        self.max_batch_size = 8
        self.adaptive_batch_sizing = True
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ëŸ¬ (í˜¼í•© ì •ë°€ë„ìš©)
        self.scaler = GradScaler()
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizers = {}
        self.schedulers = {}
        self._initialize_optimizers()
        
        # í›ˆë ¨ ë©”íŠ¸ë¦­
        self.metrics_history = deque(maxlen=1000)
        self.current_metrics = TrainingMetrics()
        
        # ë™ì  ë°°ì¹˜ í¬ê¸° ê´€ë¦¬
        self.adaptive_batch_manager = AdaptiveBatchSizeManager()
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
        self.memory_monitor = MemoryMonitor()
        
        # ë°±ë³¸ì„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ì— ë“±ë¡
        self.memory_monitor.register_gpu_module("unified_backbone", self.unified_backbone)
        
        logger.info("MemoryEfficientTrainer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def forward_with_checkpointing(self, batch_data: Dict[str, Any], 
                                  active_heads: List[HeadType]) -> Dict[str, Any]:
        """ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ì„ ì‚¬ìš©í•œ ìˆœì „íŒŒ"""
        
        # 1. ë°±ë³¸ ìˆœì „íŒŒ
        backbone_output = self.unified_backbone(batch_data)
        
        # 2. í—¤ë“œë³„ ì²˜ë¦¬
        head_outputs = {}
        for head_type in active_heads:
            if self.unified_system and hasattr(self.unified_system, 'cached_head_modules'):
                head_module = self.unified_system.cached_head_modules.get(head_type)
                if head_module is not None:
                    try:
                        # ë°±ë³¸ ì¶œë ¥ì„ í—¤ë“œì— ì „ë‹¬
                        head_input = backbone_output.shared_embedding
                        logger.debug(f"í—¤ë“œ {head_type.value} - ë°±ë³¸ ì¶œë ¥ shape: {head_input.shape}")
                        
                        # í—¤ë“œ forward ì‹¤í–‰ (forward ë©”ì„œë“œê°€ ë‚´ë¶€ì ìœ¼ë¡œ input_adapter ì²˜ë¦¬)
                        if hasattr(head_module, 'forward'):
                            logger.debug(f"í—¤ë“œ {head_type.value} - forward ì‹¤í–‰")
                            try:
                                head_output = head_module.forward(head_input)
                                head_outputs[head_type] = head_output
                                logger.debug(f"í—¤ë“œ {head_type.value} - forward ì„±ê³µ, ì¶œë ¥ shape: {head_output.shape}")
                            except RuntimeError as re:
                                if "mat1 and mat2" in str(re):
                                    logger.error(f"Shape mismatch ìƒì„¸ ì •ë³´:")
                                    logger.error(f"  - í—¤ë“œ íƒ€ì…: {head_type.value}")
                                    logger.error(f"  - í—¤ë“œ í´ë˜ìŠ¤: {head_module.__class__.__name__}")
                                    logger.error(f"  - ì…ë ¥ shape: {head_input.shape}")
                                    
                                    # PyTorch ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„¸ ë¶„ì„
                                    if hasattr(head_module, 'get_pytorch_network'):
                                        pytorch_net = head_module.get_pytorch_network()
                                        if pytorch_net:
                                            logger.error(f"  - PyTorch ë„¤íŠ¸ì›Œí¬ íƒ€ì…: {type(pytorch_net).__name__}")
                                            
                                            # Sequentialì´ë‚˜ ë‹¤ë¥¸ ì»¨í…Œì´ë„ˆì¸ ê²½ìš°
                                            if hasattr(pytorch_net, 'modules'):
                                                for idx, module in enumerate(pytorch_net.modules()):
                                                    if isinstance(module, torch.nn.Linear):
                                                        logger.error(f"    - Linear ë ˆì´ì–´ {idx}: in_features={module.in_features}, out_features={module.out_features}")
                                            
                                            # ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ shape ì¶œë ¥
                                            for name, param in pytorch_net.named_parameters():
                                                if 'weight' in name:
                                                    logger.error(f"    - {name}: shape={param.shape}")
                                raise
                        else:
                            logger.warning(f"í—¤ë“œ {head_type.value}ì— forward ë©”ì„œë“œê°€ ì—†ìŒ")
                    except Exception as e:
                        logger.error(f"í—¤ë“œ {head_type.value} ìˆœì „íŒŒ ì˜¤ë¥˜: {str(e)}")
                        logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                        import traceback
                        logger.error(f"íŠ¸ë ˆì´ìŠ¤ë°±:\n{traceback.format_exc()}")
                        logger.error(f"ì…ë ¥ shape: {head_input.shape if hasattr(head_input, 'shape') else 'N/A'}")
                        
                        # ë” ìì„¸í•œ ë””ë²„ê¹… ì •ë³´
                        if hasattr(head_module, '__class__'):
                            logger.error(f"í—¤ë“œ í´ë˜ìŠ¤: {head_module.__class__.__name__}")
                        if hasattr(head_module, 'get_pytorch_network'):
                            pytorch_net = head_module.get_pytorch_network()
                            if pytorch_net:
                                logger.error(f"PyTorch ë„¤íŠ¸ì›Œí¬ íƒ€ì…: {type(pytorch_net)}")
        
        return {
            'backbone_output': backbone_output,
            'head_outputs': head_outputs
        }
    
    def _initialize_optimizers(self):
        """ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”"""
        
        # ê³µìœ  ë°±ë³¸ìš© ì˜µí‹°ë§ˆì´ì €
        backbone_params = list(self.unified_backbone.parameters())
        self.optimizers['backbone'] = torch.optim.AdamW(
            backbone_params,
            lr=1e-4,
            weight_decay=1e-5,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # ë°±ë³¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •í•˜ê³  gradient ì—°ê²° ê°•í™”
        self.unified_backbone.train()
        
        # ë°±ë³¸ íŒŒë¼ë¯¸í„°ë“¤ì˜ requires_grad í™•ì¸ ë° ê°•í™”
        backbone_grad_params = sum(1 for p in self.unified_backbone.parameters() if p.requires_grad)
        backbone_total_params = sum(1 for p in self.unified_backbone.parameters())
        
        if backbone_grad_params != backbone_total_params:
            logger.warning(f"ë°±ë³¸ íŒŒë¼ë¯¸í„° gradient ì„¤ì • ë¶ˆì¼ì¹˜: {backbone_grad_params}/{backbone_total_params}")
            for param in self.unified_backbone.parameters():
                param.requires_grad_(True)
        
        logger.info(f"ë°±ë³¸ ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”: {backbone_total_params}ê°œ íŒŒë¼ë¯¸í„° ì¤‘ {backbone_grad_params}ê°œ gradient í™œì„±í™”")
        
        # í—¤ë“œë³„ ì˜µí‹°ë§ˆì´ì € (ë‚˜ì¤‘ì— í—¤ë“œê°€ ë¡œë“œë  ë•Œ ì¶”ê°€)
        for head_type, config in self.head_configs.items():
            # í—¤ë“œë³„ íŒŒë¼ë¯¸í„°ëŠ” ì‹¤ì œ í—¤ë“œê°€ ë¡œë“œë  ë•Œ ì„¤ì •
            pass
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        self.schedulers['backbone'] = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizers['backbone'],
            T_0=1000,  # ì²« ë²ˆì§¸ ì¬ì‹œì‘ê¹Œì§€ì˜ ìŠ¤í… ìˆ˜
            T_mult=2,  # ì¬ì‹œì‘ ì£¼ê¸° ë°°ìˆ˜
            eta_min=1e-6
        )
    
    def add_head_optimizer(self, head_type: HeadType, head_module: nn.Module):
        """í—¤ë“œë³„ ì˜µí‹°ë§ˆì´ì € ì¶”ê°€"""
        config = self.head_configs[head_type]
        
        self.optimizers[head_type.value] = torch.optim.AdamW(
            head_module.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            betas=(0.9, 0.999)
        )
        
        # í—¤ë“œë¥¼ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ì— ë“±ë¡
        self.memory_monitor.register_gpu_module(f"head_{head_type.value}", head_module)
        
        if config.scheduler_type == "cosine":
            self.schedulers[head_type.value] = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizers[head_type.value],
                T_max=10000,
                eta_min=config.learning_rate * 0.01
            )
        elif config.scheduler_type == "linear":
            self.schedulers[head_type.value] = torch.optim.lr_scheduler.LinearLR(
                self.optimizers[head_type.value],
                start_factor=0.1,
                total_iters=config.warmup_steps
            )
    
    def forward_with_checkpointing(self, input_data: Dict[str, Any],
                                 active_heads: List[HeadType]) -> Dict[str, torch.Tensor]:
        """ì²´í¬í¬ì¸íŒ…ì„ ì‚¬ìš©í•œ ìˆœì „íŒŒ"""
        
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        input_text = input_data.get('text', '')
        batch_size = input_data.get('batch_size', 1)
        
        # í† í¬ë‚˜ì´ì§• (ê°€ìƒì˜ í† í¬ë‚˜ì´ì§•)
        input_ids = torch.randint(0, 30000, (batch_size, 128), device=self.unified_backbone.device)
        attention_mask = torch.ones_like(input_ids, dtype=torch.bool)
        
        # ë°±ë³¸ì´ í•™ìŠµ ëª¨ë“œì¸ì§€ í™•ì¸í•˜ê³  ì„¤ì •
        if not self.unified_backbone.training:
            logger.warning("ë°±ë³¸ì´ eval ëª¨ë“œì˜€ìŠµë‹ˆë‹¤. í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
            self.unified_backbone.train()
        
        # input_idsëŠ” ì •ìˆ˜í˜•ì´ë¯€ë¡œ requires_grad ì„¤ì •í•˜ì§€ ì•ŠìŒ (ì˜¤ë¥˜ ë°©ì§€)
        logger.debug(f"input_ids dtype: {input_ids.dtype}, shape: {input_ids.shape}")
        logger.debug(f"ë°±ë³¸ training ëª¨ë“œ: {self.unified_backbone.training}")
        
        # ë°±ë³¸ íŒŒë¼ë¯¸í„°ë“¤ì˜ gradient ìƒíƒœ í™•ì¸
        backbone_grad_params = sum(1 for p in self.unified_backbone.parameters() if p.requires_grad)
        backbone_total_params = sum(1 for p in self.unified_backbone.parameters())
        logger.debug(f"ë°±ë³¸ gradient íŒŒë¼ë¯¸í„°: {backbone_grad_params}/{backbone_total_params}")
        
        if self.use_gradient_checkpointing:
            # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš©
            def create_custom_forward(module):
                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            
            # ë°±ë³¸ ì²˜ë¦¬ (ì²´í¬í¬ì¸íŒ… ì ìš©)
            unified_repr = checkpoint(
                create_custom_forward(self.unified_backbone),
                input_ids,
                attention_mask
            )
        else:
            # ì¼ë°˜ ìˆœì „íŒŒ
            unified_repr = self.unified_backbone(input_ids, attention_mask)
        
        # ê° í—¤ë“œë³„ ì¶œë ¥ ê³„ì‚° - ë°±ë³¸ë§Œ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        head_outputs = {}
        for head_type in active_heads:
            # ìºì‹œëœ ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆ ì‚¬ìš©
            if self.unified_system is not None:
                real_head = self.unified_system.cached_head_modules.get(head_type)
            else:
                real_head = None
            
            if real_head is None:
                raise RuntimeError(f"ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆ {head_type.value}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ì´ˆê¸°í™”ê°€ ì œëŒ€ë¡œ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # ì‹¤ì œ í—¤ë“œë¥¼ í†µí•œ ìˆœì „íŒŒ - CPUì—ì„œ ê³„ì‚°í•˜ì—¬ ë©”ëª¨ë¦¬ ë¬¸ì œ í•´ê²°
            head_input = unified_repr.shared_embedding  # ë°±ë³¸ ì¶œë ¥ì„ í—¤ë“œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
            
            # 103% GPU ë©”ëª¨ë¦¬ ë¬¸ì œ í•´ê²°: gradient ì—°ê²°ì„ ìœ ì§€í•˜ë©´ì„œ CPUì—ì„œ ê³„ì‚°
            # í—¤ë“œ ëª¨ë“ˆì„ CPUë¡œ ì´ë™ (ì´ë¯¸ CPUì— ìˆì–´ì•¼ í•¨)
            if next(real_head.parameters()).device.type != 'cpu':
                logger.warning(f"í—¤ë“œ {head_type.value}ì´ GPUì— ìˆìŠµë‹ˆë‹¤. CPUë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                real_head = real_head.cpu()
            
            # í—¤ë“œ ì…ë ¥ì„ CPUë¡œ ì´ë™ (gradient ì—°ê²° ìœ ì§€)
            head_input_cpu = head_input.cpu()  # detach() ì œê±°í•˜ì—¬ gradient ì—°ê²° ìœ ì§€
            logger.debug(f"í—¤ë“œ {head_type.value}: gradient ì—°ê²° ìœ ì§€í•˜ë©° CPUë¡œ ì´ë™")
            
            # gradient ì—°ê²° ìƒíƒœ í™•ì¸ ë° ë¶„ì„
            logger.debug(f"ë°±ë³¸ ì¶œë ¥ ë¶„ì„ - requires_grad: {head_input_cpu.requires_grad}, dtype: {head_input_cpu.dtype}, shape: {head_input_cpu.shape}")
            logger.debug(f"ë°±ë³¸ ì¶œë ¥ grad_fn: {head_input_cpu.grad_fn}")
            
            if not head_input_cpu.requires_grad:
                # ë°±ë³¸ íŒŒë¼ë¯¸í„°ë“¤ì˜ requires_grad ìƒíƒœ ì¬í™•ì¸
                backbone_params_requiring_grad = [p for p in self.unified_backbone.parameters() if p.requires_grad]
                logger.warning(f"ë°±ë³¸ ì¶œë ¥ì´ requires_grad=Falseì…ë‹ˆë‹¤!")
                logger.warning(f"ë°±ë³¸ requires_grad íŒŒë¼ë¯¸í„°: {len(backbone_params_requiring_grad)}ê°œ")
                
                # ì‹¤ì œ gradient ì—°ê²° ë¬¸ì œ í•´ê²°
                if len(backbone_params_requiring_grad) > 0:
                    logger.warning("ë°±ë³¸ training ëª¨ë“œ ë° gradient ì„¤ì •ì„ ê°•ì œë¡œ ë³µêµ¬í•©ë‹ˆë‹¤.")
                    
                    # ë°±ë³¸ì„ ëª…ì‹œì ìœ¼ë¡œ training ëª¨ë“œë¡œ ì„¤ì •
                    self.unified_backbone.train()
                    
                    # ëª¨ë“  ë°±ë³¸ íŒŒë¼ë¯¸í„°ì˜ requires_grad ì¬ì„¤ì •
                    for param in self.unified_backbone.parameters():
                        param.requires_grad_(True)
                    
                    # ë°±ë³¸ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ gradientê°€ ì—°ê²°ëœ ì¶œë ¥ ìƒì„±
                    logger.warning("ë°±ë³¸ ìˆœì „íŒŒë¥¼ ì¬ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    if self.use_gradient_checkpointing:
                        unified_repr = checkpoint(
                            create_custom_forward(self.unified_backbone),
                            input_ids,
                            attention_mask
                        )
                    else:
                        unified_repr = self.unified_backbone(input_ids, attention_mask)
                    
                    head_input = unified_repr.shared_embedding
                    
                    # ì¬ì‹¤í–‰ í›„ì—ë„ gradientê°€ ì—†ìœ¼ë©´ ì¹˜ëª…ì  ì˜¤ë¥˜
                    if not head_input.requires_grad:
                        logger.error("ë°±ë³¸ ìˆœì „íŒŒ ì¬ì‹¤í–‰ í›„ì—ë„ gradientê°€ ì—°ê²°ë˜ì§€ ì•ŠìŒ")
                        raise RuntimeError("ë°±ë³¸ gradient ì—°ê²° ì‹¤íŒ¨ - ë°±ë³¸ êµ¬í˜„ ê²°í•¨")
                    else:
                        logger.info("ë°±ë³¸ gradient ì—°ê²° ë³µêµ¬ ì„±ê³µ")
                else:
                    logger.error("ë°±ë³¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ requires_grad=Falseì…ë‹ˆë‹¤.")
                    # ë°±ë³¸ íŒŒë¼ë¯¸í„°ë“¤ì„ ê°•ì œë¡œ í™œì„±í™”
                    for param in self.unified_backbone.parameters():
                        param.requires_grad_(True)
                    logger.warning("ë°±ë³¸ íŒŒë¼ë¯¸í„° requires_gradë¥¼ ê°•ì œë¡œ Trueë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
                    raise RuntimeError("ë°±ë³¸ íŒŒë¼ë¯¸í„° gradient ì„¤ì • ë¬¸ì œ - ì¬ì‹œë„ í•„ìš”")
            else:
                logger.debug(f"í—¤ë“œ {head_type.value}: gradient ì—°ê²° ì •ìƒ")
            
            # í—¤ë“œ ëª¨ë“ˆì´ í•™ìŠµ ëª¨ë“œì¸ì§€ í™•ì¸
            if hasattr(real_head, 'training'):
                if not real_head.training:
                    logger.warning(f"í—¤ë“œ {head_type.value}ì´ eval ëª¨ë“œì…ë‹ˆë‹¤. í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
                    real_head.train()
            
            # í—¤ë“œ íŒŒë¼ë¯¸í„°ì˜ requires_grad í™•ì¸
            head_params_with_grad = sum(1 for p in real_head.parameters() if p.requires_grad)
            head_params_total = sum(1 for p in real_head.parameters())
            if head_params_with_grad == 0:
                logger.error(f"í—¤ë“œ {head_type.value}ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ requires_grad=Falseì…ë‹ˆë‹¤")
                raise RuntimeError(f"í—¤ë“œ {head_type.value} íŒŒë¼ë¯¸í„°ì— gradientê°€ ì—°ê²°ë˜ì§€ ì•ŠìŒ")
            
            logger.debug(f"í—¤ë“œ {head_type.value}: {head_params_with_grad}/{head_params_total} íŒŒë¼ë¯¸í„°ê°€ requires_grad=True")
            
            # ì‹¤ì œ í—¤ë“œ ìˆœì „íŒŒ (CPUì—ì„œ ìˆ˜í–‰)
            head_output_cpu = real_head(head_input_cpu)
            
            # ê²°ê³¼ë¥¼ ë‹¤ì‹œ GPUë¡œ ì´ë™ (loss ê³„ì‚°ì„ ìœ„í•´)
            head_output = head_output_cpu.to(head_input.device)  # ë°±ë³¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            
            # ì¶œë ¥ì˜ gradient ì—°ê²° í™•ì¸
            if not head_output.requires_grad:
                logger.error(f"í—¤ë“œ {head_type.value} ì¶œë ¥ì´ requires_grad=Falseì…ë‹ˆë‹¤")
                raise RuntimeError(f"í—¤ë“œ {head_type.value} ì¶œë ¥ì— gradientê°€ ì—°ê²°ë˜ì§€ ì•ŠìŒ")
            
            logger.debug(f"í—¤ë“œ {head_type.value}: CPU ê³„ì‚° ì™„ë£Œ í›„ GPUë¡œ ì´ë™ ({head_output_cpu.device} â†’ {head_output.device})")
            head_outputs[head_type.value] = head_output
        
        return {
            'unified_representation': unified_repr,
            'head_outputs': head_outputs
        }
    
    async def train_step(self, batch_data: Dict[str, Any],
                        active_heads: List[HeadType]) -> TrainingMetrics:
        """ë‹¨ì¼ í›ˆë ¨ ìŠ¤í…"""
        
        step_start_time = time.time()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        memory_before = self.memory_monitor.get_memory_usage()
        
        # ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ
        if self.adaptive_batch_sizing:
            batch_size = self.adaptive_batch_manager.get_optimal_batch_size(
                memory_before, len(active_heads)
            )
            batch_data['batch_size'] = batch_size
        
        # í˜¼í•© ì •ë°€ë„ ì»¨í…ìŠ¤íŠ¸
        with autocast(enabled=self.use_mixed_precision):
            # ìˆœì „íŒŒ
            outputs = self.forward_with_checkpointing(batch_data, active_heads)
            
            # ì†ì‹¤ ê³„ì‚°
            losses = self._calculate_losses(outputs, batch_data, active_heads)
            total_loss = sum(losses.values()) / len(losses)
        
        # ì—­ì „íŒŒ (ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì‚¬ìš©)
        scaled_loss = total_loss / self.gradient_accumulation_steps
        
        if self.use_mixed_precision:
            self.scaler.scale(scaled_loss).backward()
        else:
            scaled_loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ (ëˆ„ì  ìŠ¤í…ë§ˆë‹¤)
        if (self.current_metrics.step + 1) % self.gradient_accumulation_steps == 0:
            await self._update_gradients(active_heads)
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        memory_after = self.memory_monitor.get_memory_usage()
        step_time = time.time() - step_start_time
        
        self._update_metrics(
            losses, total_loss, memory_after - memory_before, 
            step_time, batch_data.get('batch_size', 1)
        )
        
        return self.current_metrics
    
    async def _update_gradients(self, active_heads: List[HeadType]):
        """ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸"""
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        total_norm = 0.0
        
        if self.use_mixed_precision:
            # ë°±ë³¸ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            self.scaler.unscale_(self.optimizers['backbone'])
            backbone_norm = torch.nn.utils.clip_grad_norm_(
                self.unified_backbone.parameters(), 1.0
            )
            total_norm += backbone_norm
            
            # í—¤ë“œë³„ ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            for head_type in active_heads:
                if head_type.value in self.optimizers:
                    self.scaler.unscale_(self.optimizers[head_type.value])
                    config = self.head_configs[head_type]
                    # ì‹¤ì œ í—¤ë“œ íŒŒë¼ë¯¸í„°ê°€ ìˆë‹¤ë©´ í´ë¦¬í•‘
                    # head_norm = torch.nn.utils.clip_grad_norm_(head_params, config.gradient_clip_value)
                    # total_norm += head_norm
        
        # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
        if self.use_mixed_precision:
            self.scaler.step(self.optimizers['backbone'])
            for head_type in active_heads:
                if head_type.value in self.optimizers:
                    self.scaler.step(self.optimizers[head_type.value])
            self.scaler.update()
        else:
            self.optimizers['backbone'].step()
            for head_type in active_heads:
                if head_type.value in self.optimizers:
                    self.optimizers[head_type.value].step()
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        for scheduler in self.schedulers.values():
            scheduler.step()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        for optimizer in self.optimizers.values():
            optimizer.zero_grad()
        
        self.current_metrics.gradient_norm = total_norm
    
    def _calculate_losses(self, outputs: Dict[str, torch.Tensor],
                         batch_data: Dict[str, Any],
                         active_heads: List[HeadType]) -> Dict[str, torch.Tensor]:
        """ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°"""
        losses = {}
        
        # ê° í—¤ë“œë³„ ì†ì‹¤ ê³„ì‚°
        for head_type in active_heads:
            config = self.head_configs[head_type]
            
            # ê°€ìƒì˜ íƒ€ê²Ÿ ìƒì„± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” batch_dataì—ì„œ ì¶”ì¶œ)
            batch_size = batch_data.get('batch_size', 1)
            
            if head_type == HeadType.EMOTION_EMPATHY:
                # ê°ì • ë¶„ë¥˜ ì†ì‹¤
                target = torch.randint(0, 10, (batch_size,), device=outputs['head_outputs'][head_type.value].device)
                logits = outputs['head_outputs'][head_type.value][:, :10]  # 10ê°œ ê°ì • í´ë˜ìŠ¤
                loss = F.cross_entropy(logits, target, label_smoothing=config.label_smoothing)
                
            elif head_type == HeadType.BENTHAM_FROMM:
                # ìœ¤ë¦¬ ì ìˆ˜ íšŒê·€ ì†ì‹¤
                target = torch.rand(batch_size, 1, device=outputs['head_outputs'][head_type.value].device)
                pred = outputs['head_outputs'][head_type.value][:, :1]
                loss = F.mse_loss(pred, target)
                
            elif head_type == HeadType.SEMANTIC_SURD:
                # ì˜ë¯¸ ìœ ì‚¬ë„ ì†ì‹¤
                target = torch.rand(batch_size, 768, device=outputs['head_outputs'][head_type.value].device)
                pred = outputs['head_outputs'][head_type.value]
                loss = F.cosine_embedding_loss(pred, target, torch.ones(batch_size, device=pred.device))
                
            elif head_type == HeadType.REGRET_LEARNING:
                # í›„íšŒ ì˜ˆì¸¡ ì†ì‹¤
                target = torch.rand(batch_size, 1, device=outputs['head_outputs'][head_type.value].device)
                pred = outputs['head_outputs'][head_type.value][:, :1]
                loss = F.smooth_l1_loss(pred, target)
                
            else:
                # ê¸°ë³¸ ì†ì‹¤
                target = torch.randn_like(outputs['head_outputs'][head_type.value])
                loss = F.mse_loss(outputs['head_outputs'][head_type.value], target)
            
            # ê°€ì¤‘ì¹˜ ì ìš©
            losses[head_type.value] = loss * config.loss_weight
        
        return losses
    
    def _update_metrics(self, losses: Dict[str, torch.Tensor], total_loss: torch.Tensor,
                       memory_delta: float, step_time: float, batch_size: int):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        
        self.current_metrics.step += 1
        self.current_metrics.total_loss = float(total_loss.item())
        self.current_metrics.head_losses = {k: float(v.item()) for k, v in losses.items()}
        self.current_metrics.memory_usage = memory_delta
        self.current_metrics.training_time = step_time
        self.current_metrics.samples_per_second = batch_size / step_time if step_time > 0 else 0.0
        
        # í•™ìŠµë¥  ê¸°ë¡
        for name, optimizer in self.optimizers.items():
            self.current_metrics.learning_rates[name] = optimizer.param_groups[0]['lr']
        
        # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        metrics_copy = TrainingMetrics(
            epoch=self.current_metrics.epoch,
            step=self.current_metrics.step,
            total_loss=self.current_metrics.total_loss,
            head_losses=self.current_metrics.head_losses.copy(),
            learning_rates=self.current_metrics.learning_rates.copy(),
            memory_usage=self.current_metrics.memory_usage,
            training_time=self.current_metrics.training_time,
            samples_per_second=self.current_metrics.samples_per_second
        )
        self.metrics_history.append(metrics_copy)

class AdaptiveBatchSizeManager:
    """ì ì‘ì  ë°°ì¹˜ í¬ê¸° ê´€ë¦¬ì - 103% GPU ë©”ëª¨ë¦¬ ë¬¸ì œ ëŒ€ì‘"""
    
    def __init__(self, initial_batch_size: int = 4, max_batch_size: int = 16):
        self.current_batch_size = initial_batch_size
        self.max_batch_size = max_batch_size
        self.min_batch_size = 1
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
        self.memory_history = deque(maxlen=10)
        self.oom_count = 0
        self.successful_steps = 0
        
        # ì ì‘ íŒŒë¼ë¯¸í„° (í˜„ì‹¤ì  ì¡°ì • - GPU threshold 85% ë³µì›ì— ë§ì¶¤)
        self.memory_threshold = 0.65  # GPU ë©”ëª¨ë¦¬ 65% ì‚¬ìš©ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ (50% â†’ 65%)
        self.increase_threshold = 0.50  # GPU ë©”ëª¨ë¦¬ 50% ë¯¸ë§Œì‹œ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê³ ë ¤ (35% â†’ 50%)
        
    def get_optimal_batch_size(self, current_memory_usage: float, num_active_heads: int) -> int:
        """ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œê³¼ í†µí•©ëœ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        memory_info = get_gpu_memory_info()
        if memory_info:
            memory_utilization = memory_info.get('usage_percent', 50) / 100.0
        else:
            memory_utilization = 0.5  # ê¸°ë³¸ê°’
        
        self.memory_history.append(memory_utilization)
        
        # ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œê³¼ í†µí•©ëœ ë‹¨ê³„ë³„ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        # 70% ë¯¸ë§Œ: ì •ìƒ ìš´ì˜
        if memory_utilization < 0.70:
            target_batch_size = 4  # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°
            
        # 70-75%: ê²½ê³  ë ˆë²¨ - ë°°ì¹˜ í¬ê¸° ê°ì†Œ
        elif memory_utilization < 0.75:
            target_batch_size = 2
            logger.warning(f"GPU ë©”ëª¨ë¦¬ ê²½ê³  ë ˆë²¨({memory_utilization:.1%}) - ë°°ì¹˜ í¬ê¸° 2ë¡œ ê°ì†Œ")
            
        # 75-80%: ìœ„í—˜ ë ˆë²¨ - ìµœì†Œ ë°°ì¹˜ í¬ê¸°
        elif memory_utilization < 0.80:
            target_batch_size = 1
            logger.error(f"GPU ë©”ëª¨ë¦¬ ìœ„í—˜ ë ˆë²¨({memory_utilization:.1%}) - ë°°ì¹˜ í¬ê¸° 1ë¡œ ìµœì†Œí™”")
            
        # 80% ì´ˆê³¼: ê¸´ê¸‰ ìƒí™© - ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨
        else:
            target_batch_size = 1
            logger.critical(f"GPU ë©”ëª¨ë¦¬ ê¸´ê¸‰ ìƒí™©({memory_utilization:.1%}) - ìµœì†Œ ë°°ì¹˜ë¡œ í•™ìŠµ ê³„ì†")
        
        # í™œì„± í—¤ë“œ ìˆ˜ì— ë”°ë¥¸ ì¶”ê°€ ì¡°ì •
        if num_active_heads > 2:
            target_batch_size = max(1, target_batch_size // 2)
            logger.info(f"ë‹¤ì¤‘ í—¤ë“œ í™œì„±({num_active_heads}ê°œ) - ë°°ì¹˜ í¬ê¸° ì¶”ê°€ ê°ì†Œ: {target_batch_size}")
        
        # ì ì§„ì  ë³€ê²½ (ê°‘ì‘ìŠ¤ëŸ° ë³€í™” ë°©ì§€)
        if target_batch_size > self.current_batch_size:
            self.current_batch_size = min(target_batch_size, self.current_batch_size + 1)
        elif target_batch_size < self.current_batch_size:
            self.current_batch_size = target_batch_size  # ê°ì†ŒëŠ” ì¦‰ì‹œ ì ìš©
        
        # ë²”ìœ„ ì œí•œ
        self.current_batch_size = max(self.min_batch_size, 
                                     min(self.max_batch_size, self.current_batch_size))
        
        self.successful_steps += 1
        return self.current_batch_size
    
    def report_oom(self):
        """Out of Memory ë°œìƒ ë³´ê³ """
        self.oom_count += 1
        if self.current_batch_size > self.min_batch_size:
            self.current_batch_size = max(self.min_batch_size, self.current_batch_size // 2)
            logger.error(f"OOM ë°œìƒ, ë°°ì¹˜ í¬ê¸° ëŒ€í­ ê°ì†Œ: {self.current_batch_size}")

class MemoryMonitor:
    """ê°•í™”ëœ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„° - GPU ëª¨ë“ˆë³„ ìƒì„¸ ì¶”ì """
    
    def __init__(self):
        self.cpu_memory_history = deque(maxlen=100)
        self.gpu_memory_history = deque(maxlen=100)
        
        # GPU ëª¨ë“ˆ ì¶”ì ì„ ìœ„í•œ ì¶”ê°€ ì €ì¥ì†Œ
        self.gpu_module_registry = {}  # ëª¨ë“ˆëª… -> ëª¨ë“ˆ ê°ì²´
        self.memory_snapshots = deque(maxlen=50)  # ìƒì„¸ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·
        self.high_memory_alerts = []  # ê³ ë©”ëª¨ë¦¬ ì‚¬ìš© ì•Œë¦¼ ê¸°ë¡
        
        # ë””ë²„ê·¸ ì„¤ì •
        self.debug_threshold = 0.90  # 90% ì´ìƒì—ì„œ ìƒì„¸ ë¡œê·¸
        self.critical_threshold = 1.0  # 100% ì´ìƒì—ì„œ ê¸´ê¸‰ ëŒ€ì‘
        
    def get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ ë° ê³ ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ ìë™ ë¶„ì„"""
        
        # CPU ë©”ëª¨ë¦¬
        cpu_memory = psutil.virtual_memory()
        cpu_usage_gb = (cpu_memory.total - cpu_memory.available) / (1024**3)
        
        # GPU ë©”ëª¨ë¦¬ (ìƒì„¸ ë¶„ì„)
        gpu_info = get_gpu_memory_info()
        gpu_usage_gb = gpu_info.get('memory_used_gb', 0) if gpu_info else 0
        gpu_total_gb = gpu_info.get('memory_total_gb', 8) if gpu_info else 8
        gpu_percent = (gpu_usage_gb / gpu_total_gb * 100) if gpu_info else 0
        
        # torch.cuda ì§ì ‘ ì¡°íšŒë¡œ ë” ì •í™•í•œ ìˆ˜ì¹˜ ì–»ê¸°
        import torch
        if torch.cuda.is_available():
            allocated_gb = torch.cuda.memory_allocated(0) / (1024**3)
            reserved_gb = torch.cuda.memory_reserved(0) / (1024**3)
            free_gb = (torch.cuda.get_device_properties(0).total_memory / (1024**3)) - allocated_gb
            torch_gpu_percent = (allocated_gb / (torch.cuda.get_device_properties(0).total_memory / (1024**3))) * 100
            
            # ë” ì •í™•í•œ ìˆ˜ì¹˜ë¡œ ì—…ë°ì´íŠ¸
            gpu_usage_gb = allocated_gb
            gpu_percent = torch_gpu_percent
            
            usage = {
                'cpu_memory_gb': cpu_usage_gb,
                'gpu_memory_gb': gpu_usage_gb,
                'cpu_percent': cpu_memory.percent,
                'gpu_percent': gpu_percent,
                # ì¶”ê°€ ìƒì„¸ ì •ë³´
                'gpu_allocated_gb': allocated_gb,
                'gpu_reserved_gb': reserved_gb,
                'gpu_free_gb': free_gb,
                'gpu_total_gb': torch.cuda.get_device_properties(0).total_memory / (1024**3)
            }
        else:
            usage = {
                'cpu_memory_gb': cpu_usage_gb,
                'gpu_memory_gb': gpu_usage_gb,
                'cpu_percent': cpu_memory.percent,
                'gpu_percent': gpu_percent
            }
        
        self.cpu_memory_history.append(usage['cpu_memory_gb'])
        self.gpu_memory_history.append(usage['gpu_memory_gb'])
        
        # 90% ì´ìƒì´ë©´ ìƒì„¸ ë¶„ì„ ìˆ˜í–‰
        if usage['gpu_percent'] >= (self.debug_threshold * 100):
            self._analyze_high_memory_usage(usage)
        
        # 100% ì´ìƒì´ë©´ ê¸´ê¸‰ ëŒ€ì‘
        if usage['gpu_percent'] >= (self.critical_threshold * 100):
            self._emergency_memory_response(usage)
        
        return usage
    
    def get_memory_efficiency(self) -> float:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³„ì‚°"""
        if len(self.gpu_memory_history) < 2:
            return 1.0
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì˜ ì•ˆì •ì„±ì„ íš¨ìœ¨ì„± ì§€í‘œë¡œ ì‚¬ìš©
        recent_usage = list(self.gpu_memory_history)[-10:]
        if len(recent_usage) < 2:
            return 1.0
        
        std_dev = np.std(recent_usage)
        mean_usage = np.mean(recent_usage)
        
        # í‘œì¤€í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ íš¨ìœ¨ì  (ì•ˆì •ì )
        efficiency = 1.0 / (1.0 + std_dev / max(0.1, mean_usage))
        return min(1.0, efficiency)
    
    def register_gpu_module(self, name: str, module):
        """GPU ëª¨ë“ˆ ë“±ë¡ (ì¶”ì ìš©)"""
        self.gpu_module_registry[name] = module
        logger.debug(f"GPU ëª¨ë“ˆ ë“±ë¡: {name}")
    
    def _analyze_high_memory_usage(self, usage: Dict[str, float]):
        """ê³ ë©”ëª¨ë¦¬ ì‚¬ìš© ìƒì„¸ ë¶„ì„"""
        
        timestamp = datetime.now()
        gpu_percent = usage['gpu_percent']
        
        logger.critical(f"ğŸš¨ HIGH GPU MEMORY ALERT: {gpu_percent:.1f}% ì‚¬ìš© ì¤‘!")
        logger.critical(f"   í• ë‹¹ëœ ë©”ëª¨ë¦¬: {usage.get('gpu_allocated_gb', 0):.2f}GB")
        logger.critical(f"   ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {usage.get('gpu_reserved_gb', 0):.2f}GB")
        logger.critical(f"   ì—¬ìœ  ë©”ëª¨ë¦¬: {usage.get('gpu_free_gb', 0):.2f}GB")
        logger.critical(f"   ì „ì²´ ë©”ëª¨ë¦¬: {usage.get('gpu_total_gb', 8):.2f}GB")
        
        # torch.cuda ìƒì„¸ ì •ë³´
        import torch
        if torch.cuda.is_available():
            logger.critical("ğŸ“Š TORCH.CUDA ìƒì„¸ ì •ë³´:")
            logger.critical(f"   memory_allocated(): {torch.cuda.memory_allocated(0) / (1024**3):.3f}GB")
            logger.critical(f"   memory_reserved(): {torch.cuda.memory_reserved(0) / (1024**3):.3f}GB")
            logger.critical(f"   max_memory_allocated(): {torch.cuda.max_memory_allocated(0) / (1024**3):.3f}GB")
            logger.critical(f"   max_memory_reserved(): {torch.cuda.max_memory_reserved(0) / (1024**3):.3f}GB")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚° ë°©ì‹ë“¤ ë¹„êµ
            device_props = torch.cuda.get_device_properties(0)
            total_memory = device_props.total_memory / (1024**3)
            allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)
            reserved_memory = torch.cuda.memory_reserved(0) / (1024**3)
            
            allocated_percent = (allocated_memory / total_memory) * 100
            reserved_percent = (reserved_memory / total_memory) * 100
            
            logger.critical(f"ğŸ” ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê³„ì‚° ë¹„êµ:")
            logger.critical(f"   í• ë‹¹ ê¸°ì¤€: {allocated_percent:.2f}%")
            logger.critical(f"   ì˜ˆì•½ ê¸°ì¤€: {reserved_percent:.2f}%")
            logger.critical(f"   config.py ê¸°ì¤€: {usage.get('gpu_percent', 0):.2f}%")
        
        # ë“±ë¡ëœ GPU ëª¨ë“ˆë“¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        self._debug_gpu_modules()
        
        # ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ì €ì¥
        snapshot = {
            'timestamp': timestamp,
            'gpu_percent': gpu_percent,
            'usage_details': usage.copy(),
            'module_count': len(self.gpu_module_registry)
        }
        self.memory_snapshots.append(snapshot)
        self.high_memory_alerts.append(snapshot)
        
        # ìµœê·¼ ë©”ëª¨ë¦¬ íŒ¨í„´ ë¶„ì„
        if len(self.gpu_memory_history) >= 5:
            recent_usage = list(self.gpu_memory_history)[-5:]
            avg_usage = sum(recent_usage) / len(recent_usage)
            trend = "ì¦ê°€" if recent_usage[-1] > recent_usage[0] else "ê°ì†Œ"
            logger.critical(f"ğŸ“ˆ ìµœê·¼ ë©”ëª¨ë¦¬ íŒ¨í„´: í‰ê·  {avg_usage:.2f}GB, {trend} ì¶”ì„¸")
    
    def _emergency_memory_response(self, usage: Dict[str, float]):
        """100% ì´ìƒ ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ ê¸´ê¸‰ ëŒ€ì‘"""
        
        gpu_percent = usage['gpu_percent']
        
        logger.error(f"ğŸš¨ğŸš¨ğŸš¨ CRITICAL GPU MEMORY OVERFLOW: {gpu_percent:.1f}%!!")
        logger.error("ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘...")
        
        # torch ìºì‹œ ì •ë¦¬
        import torch
        if torch.cuda.is_available():
            logger.error("ğŸ§¹ torch.cuda.empty_cache() ì‹¤í–‰")
            torch.cuda.empty_cache()
            
            # ì •ë¦¬ í›„ ìƒíƒœ ì¬í™•ì¸
            after_allocated = torch.cuda.memory_allocated(0) / (1024**3)
            after_reserved = torch.cuda.memory_reserved(0) / (1024**3)
            after_percent = (after_allocated / (torch.cuda.get_device_properties(0).total_memory / (1024**3))) * 100
            
            logger.error(f"ğŸ”„ ì •ë¦¬ í›„ ìƒíƒœ: {after_percent:.1f}% (í• ë‹¹: {after_allocated:.2f}GB, ì˜ˆì•½: {after_reserved:.2f}GB)")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        import gc
        logger.error("ğŸ—‘ï¸  gc.collect() ì‹¤í–‰")
        gc.collect()
        
        # ìœ„í—˜ ìˆ˜ì¤€ ê¸°ë¡
        self.high_memory_alerts.append({
            'timestamp': datetime.now(),
            'type': 'CRITICAL_OVERFLOW',
            'gpu_percent': gpu_percent,
            'emergency_response': True
        })
    
    def _debug_gpu_modules(self):
        """ë“±ë¡ëœ GPU ëª¨ë“ˆë“¤ì˜ ìƒì„¸ ë©”ëª¨ë¦¬ ë¶„ì„"""
        
        if not self.gpu_module_registry:
            logger.warning("ğŸ” ë“±ë¡ëœ GPU ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. ì¶”ì  ë¶ˆê°€.")
            return
        
        logger.critical(f"ğŸ” GPU ëª¨ë“ˆë³„ ë©”ëª¨ë¦¬ ë¶„ì„ ({len(self.gpu_module_registry)}ê°œ ëª¨ë“ˆ):")
        
        import torch
        total_params = 0
        
        for name, module in self.gpu_module_registry.items():
            try:
                if hasattr(module, 'parameters'):
                    # PyTorch ëª¨ë“ˆì¸ ê²½ìš°
                    param_count = sum(p.numel() for p in module.parameters() if p.requires_grad)
                    param_memory_mb = sum(p.numel() * p.element_size() for p in module.parameters()) / (1024**2)
                    device = next(module.parameters()).device if param_count > 0 else "unknown"
                    
                    logger.critical(f"   ğŸ“¦ {name}:")
                    logger.critical(f"      íŒŒë¼ë¯¸í„°: {param_count:,}ê°œ")
                    logger.critical(f"      ë©”ëª¨ë¦¬: {param_memory_mb:.2f}MB")
                    logger.critical(f"      ë””ë°”ì´ìŠ¤: {device}")
                    logger.critical(f"      í›ˆë ¨ ëª¨ë“œ: {getattr(module, 'training', 'unknown')}")
                    
                    total_params += param_count
                
                else:
                    logger.critical(f"   â“ {name}: PyTorch ëª¨ë“ˆ ì•„ë‹˜")
            
            except Exception as e:
                logger.critical(f"   âŒ {name}: ë¶„ì„ ì‹¤íŒ¨ - {str(e)}")
        
        logger.critical(f"ğŸ”¢ ì „ì²´ ì¶”ì ëœ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
        
        # í˜„ì¬ GPU í…ì„œë“¤ ë¶„ì„ ì‹œë„
        try:
            if torch.cuda.is_available():
                logger.critical("ğŸ§  GPU í…ì„œ ë¶„ì„:")
                import gc
                gpu_tensors = []
                for obj in gc.get_objects():
                    if torch.is_tensor(obj) and obj.is_cuda:
                        gpu_tensors.append(obj)
                
                total_tensor_memory = sum(tensor.element_size() * tensor.numel() for tensor in gpu_tensors) / (1024**3)
                logger.critical(f"   GPU í…ì„œ ê°œìˆ˜: {len(gpu_tensors)}ê°œ")
                logger.critical(f"   GPU í…ì„œ ë©”ëª¨ë¦¬: {total_tensor_memory:.3f}GB")
        
        except Exception as e:
            logger.warning(f"GPU í…ì„œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
    
    def get_debug_summary(self) -> Dict[str, Any]:
        """ë””ë²„ê·¸ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        return {
            'registered_modules': len(self.gpu_module_registry),
            'high_memory_alerts': len(self.high_memory_alerts),
            'memory_snapshots': len(self.memory_snapshots),
            'latest_alert': self.high_memory_alerts[-1] if self.high_memory_alerts else None,
            'debug_threshold': self.debug_threshold,
            'critical_threshold': self.critical_threshold
        }

class UnifiedLearningScheduler:
    """í†µí•© í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, head_configs: Dict[HeadType, HeadTrainingConfig]):
        self.head_configs = head_configs
        self.current_phase = LearningPhase.WARM_UP
        self.phase_progress = 0.0
        
        # ìŠ¤ì¼€ì¤„ë§ ìƒíƒœ
        self.step_count = 0
        self.epoch_count = 0
        self.phase_step_count = 0
        
        # í•™ìŠµ ì „ëµ ë§¤ê°œë³€ìˆ˜
        self.strategy_params = {
            TrainingStrategy.ROUND_ROBIN: {'cycle_length': 4},
            TrainingStrategy.PARALLEL: {'weight_balance': 0.5},
            TrainingStrategy.PRIORITY_BASED: {'priority_weights': {}},
            TrainingStrategy.ADAPTIVE: {'adaptation_rate': 0.1}
        }
        
    def get_active_heads(self, current_strategy: TrainingStrategy, 
                        available_heads: List[HeadType]) -> List[HeadType]:
        """í˜„ì¬ ì „ëµì— ë”°ë¥¸ í™œì„± í—¤ë“œ ì„ íƒ"""
        
        if current_strategy == TrainingStrategy.ROUND_ROBIN:
            # ìˆœì°¨ì  í—¤ë“œ í›ˆë ¨
            cycle_length = self.strategy_params[TrainingStrategy.ROUND_ROBIN]['cycle_length']
            cycle_position = self.step_count % (len(available_heads) * cycle_length)
            head_index = cycle_position // cycle_length
            return [available_heads[head_index]]
        
        elif current_strategy == TrainingStrategy.PARALLEL:
            # ëª¨ë“  í—¤ë“œ ë³‘ë ¬ í›ˆë ¨
            return available_heads
        
        elif current_strategy == TrainingStrategy.PRIORITY_BASED:
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì„ íƒ
            priorities = self._calculate_head_priorities(available_heads)
            # ìƒìœ„ 50% í—¤ë“œ ì„ íƒ
            sorted_heads = sorted(priorities.items(), key=lambda x: x[1], reverse=True)
            top_heads = [head for head, _ in sorted_heads[:max(1, len(sorted_heads)//2)]]
            return top_heads
        
        elif current_strategy == TrainingStrategy.ADAPTIVE:
            # ì ì‘ì  ì„ íƒ (ì„±ëŠ¥ ê¸°ë°˜)
            return self._adaptive_head_selection(available_heads)
        
        return available_heads  # ê¸°ë³¸ê°’
    
    def _calculate_head_priorities(self, available_heads: List[HeadType]) -> Dict[HeadType, float]:
        """í—¤ë“œë³„ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        priorities = {}
        
        for head_type in available_heads:
            config = self.head_configs[head_type]
            
            # ê¸°ë³¸ ìš°ì„ ìˆœìœ„ëŠ” í•™ìŠµë¥ ê³¼ ì†ì‹¤ ê°€ì¤‘ì¹˜ì— ê¸°ë°˜
            base_priority = config.learning_rate * config.loss_weight
            
            # í˜„ì¬ ì—í¬í¬ì™€ freeze_until_epoch ê³ ë ¤
            if self.epoch_count < config.freeze_until_epoch:
                priority = 0.0  # ë™ê²°ëœ í—¤ë“œ
            else:
                # ì›Œë°ì—… ì™„ë£Œ ì •ë„ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„
                warmup_factor = min(1.0, self.step_count / max(1, config.warmup_steps))
                priority = base_priority * warmup_factor
            
            priorities[head_type] = priority
        
        return priorities
    
    def _adaptive_head_selection(self, available_heads: List[HeadType]) -> List[HeadType]:
        """ì ì‘ì  í—¤ë“œ ì„ íƒ"""
        
        # í˜„ì¬ ë‹¨ê³„ì™€ ì„±ëŠ¥ì„ ê³ ë ¤í•œ ë™ì  ì„ íƒ
        if self.current_phase == LearningPhase.WARM_UP:
            # ì›Œë°ì—… ë‹¨ê³„: ê¸°ë³¸ í—¤ë“œë“¤ë§Œ
            basic_heads = [HeadType.EMOTION_EMPATHY, HeadType.BENTHAM_FROMM]
            return [h for h in basic_heads if h in available_heads]
        
        elif self.current_phase == LearningPhase.COLLABORATIVE:
            # í˜‘ë ¥ í•™ìŠµ: ëª¨ë“  í—¤ë“œ
            return available_heads
        
        elif self.current_phase == LearningPhase.SPECIALIZED:
            # ì „ë¬¸í™” í•™ìŠµ: êµëŒ€ë¡œ ì „ë¬¸í™”
            cycle_length = 3
            head_index = (self.phase_step_count // cycle_length) % len(available_heads)
            return [available_heads[head_index]]
        
        elif self.current_phase == LearningPhase.INTEGRATION:
            # í†µí•© í•™ìŠµ: ì‹œë„ˆì§€ íš¨ê³¼ê°€ ë†’ì€ ì¡°í•©
            synergistic_combinations = [
                [HeadType.EMOTION_EMPATHY, HeadType.BENTHAM_FROMM],
                [HeadType.BENTHAM_FROMM, HeadType.REGRET_LEARNING],
                [HeadType.SEMANTIC_SURD, HeadType.META_INTEGRATION]
            ]
            combination_index = (self.phase_step_count // 5) % len(synergistic_combinations)
            selected_combination = synergistic_combinations[combination_index]
            return [h for h in selected_combination if h in available_heads]
        
        return available_heads
    
    def update_phase(self, performance_metrics: Dict[str, float]):
        """í•™ìŠµ ë‹¨ê³„ ì—…ë°ì´íŠ¸"""
        self.step_count += 1
        self.phase_step_count += 1
        
        # ë‹¨ê³„ ì „í™˜ ì¡°ê±´ í™•ì¸
        should_advance = False
        
        if self.current_phase == LearningPhase.WARM_UP:
            # ì›Œë°ì—… ì™„ë£Œ ì¡°ê±´: 1000 ìŠ¤í… ë˜ëŠ” ì†ì‹¤ ì•ˆì •í™”
            if (self.phase_step_count >= 1000 or 
                performance_metrics.get('loss_stability', 0) > 0.8):
                should_advance = True
                next_phase = LearningPhase.COLLABORATIVE
        
        elif self.current_phase == LearningPhase.COLLABORATIVE:
            # í˜‘ë ¥ í•™ìŠµ ì™„ë£Œ ì¡°ê±´: ì‹œë„ˆì§€ íš¨ê³¼ í™•ì¸
            if (self.phase_step_count >= 2000 or
                performance_metrics.get('synergy_gain', 0) > 0.15):
                should_advance = True
                next_phase = LearningPhase.SPECIALIZED
        
        elif self.current_phase == LearningPhase.SPECIALIZED:
            # ì „ë¬¸í™” ì™„ë£Œ ì¡°ê±´: ê° í—¤ë“œë³„ ì„±ëŠ¥ ì•ˆì •í™”
            if (self.phase_step_count >= 3000 or
                performance_metrics.get('head_stability', 0) > 0.9):
                should_advance = True
                next_phase = LearningPhase.INTEGRATION
        
        elif self.current_phase == LearningPhase.INTEGRATION:
            # í†µí•© í•™ìŠµ ì™„ë£Œ ì¡°ê±´: ì „ì²´ ì„±ëŠ¥ ìˆ˜ë ´
            if (self.phase_step_count >= 2000 or
                performance_metrics.get('convergence_rate', 0) > 0.95):
                should_advance = True
                next_phase = LearningPhase.FINE_TUNING
        
        if should_advance:
            logger.info(f"í•™ìŠµ ë‹¨ê³„ ì „í™˜: {self.current_phase.value} â†’ {next_phase.value}")
            self.current_phase = next_phase
            self.phase_step_count = 0
            self.phase_progress = 0.0

class RealTimeMemoryMonitor:
    """ì‹¤ì‹œê°„ GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìŠ¤ì™‘ ì œì–´"""
    
    def __init__(self):
        self.monitoring_enabled = True
        self.warning_threshold = 70  # 70% ì´ìƒì‹œ ê²½ê³ 
        self.force_swap_threshold = 75  # 75% ì´ìƒì‹œ ê°•ì œ ìŠ¤ì™‘
        self.emergency_threshold = 80  # 80% ì´ìƒì‹œ ê¸´ê¸‰ ì •ë¦¬
        self.memory_history = deque(maxlen=10)
        
    def check_memory_and_act(self, step_count: int = 0):
        """ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ë° í•„ìš”ì‹œ ìŠ¤ì™‘ ì•¡ì…˜ ìˆ˜í–‰"""
        if not self.monitoring_enabled:
            return True
            
        memory_info = get_gpu_memory_info()
        if memory_info is None:
            return True
            
        usage_percent = memory_info['usage_percent']
        self.memory_history.append(usage_percent)
        
        # 70% ë¯¸ë§Œ: ì •ìƒ ìš´ì˜
        if usage_percent < self.warning_threshold:
            if step_count % 20 == 0:  # 20ìŠ¤í…ë§ˆë‹¤ ì •ìƒ ìƒíƒœ ë¡œê·¸
                logger.debug(f"GPU ë©”ëª¨ë¦¬ ì •ìƒ: {usage_percent:.1f}% ì‚¬ìš©ì¤‘")
            return True
        
        # 70-75%: ê²½ê³  ë° ì˜ˆë°©ì  ìŠ¤ì™‘ ì¤€ë¹„
        elif usage_percent < self.force_swap_threshold:
            logger.warning(f"GPU ë©”ëª¨ë¦¬ ê²½ê³  ë ˆë²¨: {usage_percent:.1f}% - ì˜ˆë°©ì  ìŠ¤ì™‘ ì¤€ë¹„")
            self._prepare_preventive_swap()
            return True
        
        # 75-80%: ê°•ì œ ìŠ¤ì™‘ ìˆ˜í–‰
        elif usage_percent < self.emergency_threshold:
            logger.error(f"GPU ë©”ëª¨ë¦¬ ìœ„í—˜ ë ˆë²¨: {usage_percent:.1f}% - ê°•ì œ ìŠ¤ì™‘ ìˆ˜í–‰")
            self._perform_force_swap()
            return True
        
        # 80% ì´ˆê³¼: ê¸´ê¸‰ ì •ë¦¬
        else:
            logger.critical(f"GPU ë©”ëª¨ë¦¬ ê¸´ê¸‰ ìƒí™©: {usage_percent:.1f}% - ê¸´ê¸‰ ì •ë¦¬ ìˆ˜í–‰")
            self._perform_emergency_cleanup()
            return False  # í•™ìŠµ ì¼ì‹œ ì¤‘ë‹¨ ì‹ í˜¸
    
    def _prepare_preventive_swap(self):
        """ì˜ˆë°©ì  ìŠ¤ì™‘ ì¤€ë¹„"""
        # LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ CPU ì´ë™ ì¤€ë¹„
        logger.info("LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ CPU ì´ë™ ì¤€ë¹„ ì¤‘...")
    
    def _perform_force_swap(self):
        """ê°•ì œ ìŠ¤ì™‘ ìˆ˜í–‰"""
        logger.info("ë‚®ì€ ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ ê°•ì œ ìŠ¤ì™‘ ìˆ˜í–‰ ì¤‘...")
        _force_swap_low_priority_models()
        
        # ìŠ¤ì™‘ í›„ ë©”ëª¨ë¦¬ ì¬í™•ì¸
        memory_info = get_gpu_memory_info()
        if memory_info:
            logger.info(f"ê°•ì œ ìŠ¤ì™‘ ì™„ë£Œ: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {memory_info['usage_percent']:.1f}%")
    
    def _perform_emergency_cleanup(self):
        """ê¸´ê¸‰ GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        logger.critical("ê¸´ê¸‰ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ìˆ˜í–‰ ì¤‘...")
        _emergency_gpu_cleanup()
        
        # ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ ì¬í™•ì¸
        memory_info = get_gpu_memory_info()
        if memory_info:
            logger.critical(f"ê¸´ê¸‰ ì •ë¦¬ ì™„ë£Œ: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {memory_info['usage_percent']:.1f}%")
    
    def get_memory_trend(self) -> str:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¶”ì„¸ ë°˜í™˜"""
        if len(self.memory_history) < 3:
            return "insufficient_data"
        
        recent_avg = sum(self.memory_history[-3:]) / 3
        older_avg = sum(self.memory_history[:-3]) / max(1, len(self.memory_history) - 3)
        
        if recent_avg > older_avg + 5:
            return "increasing"
        elif recent_avg < older_avg - 5:
            return "decreasing"
        else:
            return "stable"

class UnifiedLearningSystem:
    """
    í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ - ë©”ì¸ í´ë˜ìŠ¤
    
    800M íŒŒë¼ë¯¸í„° ë‹¤ì¤‘ í—¤ë“œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or ADVANCED_CONFIG.get('unified_learning_config', {})
        
        # í—¤ë“œ í›ˆë ¨ ì„¤ì •
        self.head_configs = self._initialize_head_configs()
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤
        self.unified_backbone = RedHeartUnifiedBackbone()
        
        # ë°±ë³¸ training ëª¨ë“œ ë° gradient ì„¤ì • ê°•í™”
        self.unified_backbone.train()
        for param in self.unified_backbone.parameters():
            param.requires_grad_(True)
        logger.info(f"ë°±ë³¸ gradient ì„¤ì • ì™„ë£Œ: {sum(1 for p in self.unified_backbone.parameters() if p.requires_grad)}/{sum(1 for p in self.unified_backbone.parameters())} íŒŒë¼ë¯¸í„°")
        
        self.trainer = MemoryEfficientTrainer(self.unified_backbone, self.head_configs, self)
        self.scheduler = UnifiedLearningScheduler(self.head_configs)
        
        # ìŠ¤ì™‘ ë§¤ë‹ˆì € ë° ì‹œë„ˆì§€ ì‹œìŠ¤í…œ
        self.swap_manager = RedHeartDynamicSwapManager()
        self.synergy_system = IntelligentSynergySystem()
        
        # í—¤ë“œ í˜¸í™˜ì„± ë§¤ë‹ˆì € - í—¤ë“œë“¤ì„ ìŠ¤ì™‘ ë§¤ë‹ˆì €ì— ë“±ë¡
        from head_compatibility_interface import HeadCompatibilityManager
        self.head_compatibility_manager = HeadCompatibilityManager(
            self.unified_backbone, self.swap_manager
        )
        
        # ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        self.memory_monitor = RealTimeMemoryMonitor()
        self.last_memory_check = 0
        self.memory_check_interval = 5  # 5ìŠ¤í…ë§ˆë‹¤ ë©”ëª¨ë¦¬ í™•ì¸
        
        # í›ˆë ¨ ìƒíƒœ
        self.is_training = False
        self.training_thread = None
        self.training_stats = {}
        
        # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
        self.checkpoint_dir = "checkpoints/unified_learning"
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆ ìºì‹œ
        self.cached_head_modules = {}
        
        # ì´ˆê¸°í™” ì™„ë£Œ í”Œë˜ê·¸
        self.initialized = False
        
        logger.info("UnifiedLearningSystem ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def initialize_system(self):
        """ì‹œìŠ¤í…œ ë¹„ë™ê¸° ì´ˆê¸°í™” - ìŠ¤ì™‘ ë§¤ë‹ˆì € ë° í—¤ë“œë“¤ì„ ì‚¬ì „ ì´ˆê¸°í™”"""
        if self.initialized:
            return
        
        logger.info("í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹œì‘...")
        
        # 1. ìŠ¤ì™‘ ë§¤ë‹ˆì € ë¨¼ì € ì´ˆê¸°í™”
        await self.swap_manager.initialize()
        logger.info("ë™ì  ìŠ¤ì™‘ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 2. í—¤ë“œ í˜¸í™˜ì„± ë§¤ë‹ˆì € ì´ˆê¸°í™” (í—¤ë“œë“¤ì„ ìŠ¤ì™‘ ë§¤ë‹ˆì €ì— ë“±ë¡)
        logger.info("í—¤ë“œ í˜¸í™˜ì„± ë§¤ë‹ˆì € ì´ˆê¸°í™” ì¤‘...")
        await self.head_compatibility_manager.initialize_all_heads()
        logger.info("ëª¨ë“  í—¤ë“œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 3. ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆë“¤ì„ ìºì‹œ
        logger.info("ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆ ìºì‹± ì¤‘...")
        for head_type in self.head_configs.keys():
            try:
                real_head = await self._get_real_head_module(head_type)
                if real_head is not None:
                    self.cached_head_modules[head_type] = real_head
                    logger.info(f"í—¤ë“œ {head_type.value} ìºì‹± ì™„ë£Œ")
                else:
                    logger.warning(f"í—¤ë“œ {head_type.value} ìºì‹± ì‹¤íŒ¨: None ë°˜í™˜")
            except Exception as e:
                logger.error(f"í—¤ë“œ {head_type.value} ìºì‹± ì˜¤ë¥˜: {str(e)}")
        
        logger.info(f"í—¤ë“œ ëª¨ë“ˆ ìºì‹± ì™„ë£Œ: {len(self.cached_head_modules)}/{len(self.head_configs)}ê°œ")
        
        # 4. ëª¨ë“  í—¤ë“œê°€ ì„±ê³µì ìœ¼ë¡œ ìºì‹±ë˜ì—ˆëŠ”ì§€ ê²€ì¦ (fallback ì—†ìŒ)
        if len(self.cached_head_modules) != len(self.head_configs):
            missing_heads = [head_type.value for head_type in self.head_configs.keys() 
                           if head_type not in self.cached_head_modules]
            logger.error(f"í—¤ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: {missing_heads} í—¤ë“œë“¤ì„ ë¡œë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            logger.error("í”„ë¡œì íŠ¸ ê·œì¹™ì— ë”°ë¼ fallback ì²˜ë¦¬ëŠ” ê¸ˆì§€ë©ë‹ˆë‹¤. ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            raise RuntimeError(f"í•„ìˆ˜ í—¤ë“œ ì´ˆê¸°í™” ì‹¤íŒ¨: {missing_heads}. HeadCompatibilityManager í™•ì¸ í•„ìš”.")
        
        # 5. ê° í—¤ë“œì˜ PyTorch ëª¨ë“ˆ ê²€ì¦
        for head_type, head_module in self.cached_head_modules.items():
            if not isinstance(head_module, nn.Module):
                logger.error(f"í—¤ë“œ {head_type.value}ì´ ìœ íš¨í•œ PyTorch ëª¨ë“ˆì´ ì•„ë‹™ë‹ˆë‹¤: {type(head_module)}")
                raise RuntimeError(f"í—¤ë“œ {head_type.value} ëª¨ë“ˆ íƒ€ì… ì˜¤ë¥˜")
            
            # íŒŒë¼ë¯¸í„° ìˆ˜ ê²€ì¦
            param_count = sum(p.numel() for p in head_module.parameters())
            if param_count == 0:
                logger.error(f"í—¤ë“œ {head_type.value}ì— íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                raise RuntimeError(f"í—¤ë“œ {head_type.value} íŒŒë¼ë¯¸í„° ë¶€ì¬")
            
            logger.info(f"í—¤ë“œ {head_type.value} ê²€ì¦ ì™„ë£Œ: {param_count:,}ê°œ íŒŒë¼ë¯¸í„°")
        
        # 6. ì´ˆê¸°í™” ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
        self.initialized = True
        logger.info("í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë“  í—¤ë“œ ê²€ì¦ë¨")
    
    def _initialize_head_configs(self) -> Dict[HeadType, HeadTrainingConfig]:
        """í—¤ë“œë³„ í›ˆë ¨ ì„¤ì • ì´ˆê¸°í™”"""
        configs = {}
        
        # ê°ì • ê³µê° í—¤ë“œ
        configs[HeadType.EMOTION_EMPATHY] = HeadTrainingConfig(
            head_type=HeadType.EMOTION_EMPATHY,
            learning_rate=3e-4,
            weight_decay=1e-4,
            loss_weight=1.2,  # ê°ì • ì²˜ë¦¬ ì¤‘ìš”ë„ ë†’ìŒ
            warmup_steps=500,
            scheduler_type="cosine",
            dropout_rate=0.1
        )
        
        # ë²¤ë‹´-í”„ë¡¬ ìœ¤ë¦¬ í—¤ë“œ
        configs[HeadType.BENTHAM_FROMM] = HeadTrainingConfig(
            head_type=HeadType.BENTHAM_FROMM,
            learning_rate=2e-4,
            weight_decay=2e-4,
            loss_weight=1.5,  # ìœ¤ë¦¬ íŒë‹¨ ê°€ì¥ ì¤‘ìš”
            warmup_steps=800,
            scheduler_type="cosine",
            dropout_rate=0.15
        )
        
        # ì˜ë¯¸ SURD í—¤ë“œ
        configs[HeadType.SEMANTIC_SURD] = HeadTrainingConfig(
            head_type=HeadType.SEMANTIC_SURD,
            learning_rate=1e-4,
            weight_decay=1e-5,
            loss_weight=0.8,
            warmup_steps=1000,
            scheduler_type="linear",
            dropout_rate=0.05
        )
        
        # í›„íšŒ í•™ìŠµ í—¤ë“œ
        configs[HeadType.REGRET_LEARNING] = HeadTrainingConfig(
            head_type=HeadType.REGRET_LEARNING,
            learning_rate=2.5e-4,
            weight_decay=1e-4,
            loss_weight=1.0,
            warmup_steps=600,
            scheduler_type="cosine",
            dropout_rate=0.12
        )
        
        # ë©”íƒ€ í†µí•© í—¤ë“œ
        configs[HeadType.META_INTEGRATION] = HeadTrainingConfig(
            head_type=HeadType.META_INTEGRATION,
            learning_rate=1.5e-4,
            weight_decay=5e-5,
            loss_weight=1.1,
            freeze_until_epoch=2,  # ë‹¤ë¥¸ í—¤ë“œë“¤ì´ ì–´ëŠ ì •ë„ í•™ìŠµëœ í›„ ì‹œì‘
            warmup_steps=1200,
            scheduler_type="cosine",
            dropout_rate=0.08
        )
        
        return configs
    
    async def train_unified_system(self, train_data_loader, 
                                 validation_data_loader=None,
                                 num_epochs: int = 10,
                                 training_strategy: TrainingStrategy = TrainingStrategy.ROUND_ROBIN):
        """í†µí•© ì‹œìŠ¤í…œ í›ˆë ¨"""
        
        logger.info(f"í†µí•© í•™ìŠµ ì‹œì‘: {num_epochs} ì—í¬í¬, ì „ëµ: {training_strategy.value}")
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì²˜ìŒ í›ˆë ¨ì‹œì—ë§Œ)
        if not self.initialized:
            await self.initialize_system()
        
        self.is_training = True
        self.trainer.training_strategy = training_strategy
        
        try:
            for epoch in range(num_epochs):
                self.trainer.current_metrics.epoch = epoch
                self.scheduler.epoch_count = epoch
                
                # ì—í¬í¬ ì‹œì‘
                epoch_start_time = time.time()
                epoch_losses = []
                epoch_metrics = []
                
                logger.info(f"ì—í¬í¬ {epoch+1}/{num_epochs} ì‹œì‘")
                
                # ì—í¬í¬ìš© í™œì„± í—¤ë“œ ì´ˆê¸°í™”
                available_heads = list(self.head_configs.keys())
                epoch_active_heads = self.scheduler.get_active_heads(training_strategy, available_heads)
                
                # ë°°ì¹˜ë³„ í›ˆë ¨
                for batch_idx, batch_data in enumerate(train_data_loader):
                    if not self.is_training:
                        break
                    
                    # ë°°ì¹˜ë³„ í™œì„± í—¤ë“œ ì„ íƒ (ë™ì  ìŠ¤ì¼€ì¤„ë§ ì§€ì›)
                    active_heads = self.scheduler.get_active_heads(training_strategy, available_heads)
                    
                    # í—¤ë“œ ë¡œë”© (ìŠ¤ì™‘ ë§¤ë‹ˆì € ì‚¬ìš©)
                    await self._load_active_heads(active_heads)
                    
                    try:
                        # í›ˆë ¨ ìŠ¤í… ì‹¤í–‰
                        step_metrics = await self.trainer.train_step(batch_data, active_heads)
                        epoch_metrics.append(step_metrics)
                        epoch_losses.append(step_metrics.total_loss)
                        
                        # ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìŠ¤ì™‘ ì œì–´
                        total_steps = epoch * len(train_data_loader) + batch_idx
                        if total_steps % self.memory_check_interval == 0:
                            memory_ok = self.memory_monitor.check_memory_and_act(total_steps)
                            if not memory_ok:
                                logger.warning(f"ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ í•™ìŠµ ìŠ¤í… {total_steps} ì¼ì‹œ ì •ì§€")
                                # ê¸´ê¸‰ ì •ë¦¬ í›„ ì ì‹œ ëŒ€ê¸°
                                await asyncio.sleep(2)
                                continue
                        
                        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                        performance_metrics = {
                            'loss_stability': self._calculate_loss_stability(epoch_losses),
                            'synergy_gain': step_metrics.synergy_gain,
                            'convergence_rate': self._calculate_convergence_rate(epoch_metrics)
                        }
                        self.scheduler.update_phase(performance_metrics)
                        
                        # ë¡œê¹… (ë§¤ 100 ìŠ¤í…ë§ˆë‹¤)
                        if batch_idx % 100 == 0:
                            await self._log_training_progress(step_metrics, batch_idx, active_heads)
                        
                    except RuntimeError as e:
                        if "out of memory" in str(e).lower():
                            logger.error("GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - ë°°ì¹˜ í¬ê¸° ì¡°ì •")
                            self.trainer.adaptive_batch_manager.report_oom()
                            torch.cuda.empty_cache()
                            gc.collect()
                            continue
                        else:
                            raise
                
                # ì—í¬í¬ ì™„ë£Œ ì²˜ë¦¬
                epoch_time = time.time() - epoch_start_time
                await self._complete_epoch(epoch, epoch_metrics, epoch_time)
                
                # ê²€ì¦ (ì„ íƒì )
                if validation_data_loader is not None:
                    # ê²€ì¦ì„ ìœ„í•œ í—¤ë“œ ì„ íƒ (ì—í¬í¬ ì „ì²´ì—ì„œ ì‚¬ìš©ëœ í—¤ë“œë“¤)
                    await self._validate_model(validation_data_loader, epoch_active_heads)
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if (epoch + 1) % 2 == 0:  # 2 ì—í¬í¬ë§ˆë‹¤ ì €ì¥
                    await self._save_checkpoint(epoch)
        
        except Exception as e:
            logger.error(f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
        
        finally:
            self.is_training = False
            logger.info("í†µí•© í•™ìŠµ ì™„ë£Œ")
    
    async def _load_active_heads(self, active_heads: List[HeadType]):
        """í™œì„± í—¤ë“œë“¤ ë¡œë”©"""
        
        for head_type in active_heads:
            # ìŠ¤ì™‘ ë§¤ë‹ˆì €ë¥¼ í†µí•´ í—¤ë“œ ë¡œë”©
            await self.swap_manager.load_head_to_gpu(head_type.value)
            
            # ì˜µí‹°ë§ˆì´ì € ë“±ë¡ (ì²˜ìŒ ë¡œë”©ì‹œ)
            if head_type.value not in self.trainer.optimizers:
                # HeadCompatibilityManagerë¥¼ í†µí•´ ì‹¤ì œ í—¤ë“œ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
                real_head = await self._get_real_head_module(head_type)
                if real_head is not None:
                    self.trainer.add_head_optimizer(head_type, real_head)
                else:
                    logger.warning(f"í—¤ë“œ {head_type.value}ì˜ ì‹¤ì œ ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤")
    
    async def _get_real_head_module(self, head_type: HeadType) -> Optional[nn.Module]:
        """HeadCompatibilityManagerë¥¼ í†µí•´ í—¤ë“œ ì–´ëŒ‘í„° ê°€ì ¸ì˜¤ê¸°"""
        try:
            # í—¤ë“œ ì–´ëŒ‘í„° ê°€ì ¸ì˜¤ê¸°
            head_adapter = self.head_compatibility_manager.head_adapters.get(head_type)
            if head_adapter is None:
                logger.error(f"í—¤ë“œ ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {head_type.value}")
                return None
            
            # í—¤ë“œ ì–´ëŒ‘í„° ì´ˆê¸°í™” í™•ì¸
            if not head_adapter.initialized:
                await head_adapter.initialize_head()
            
            # í—¤ë“œ ì–´ëŒ‘í„°ê°€ forward ë©”ì„œë“œë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
            if hasattr(head_adapter, 'forward'):
                logger.info(f"í—¤ë“œ ì–´ëŒ‘í„° ì „ì²´ ë°˜í™˜: {head_type.value}")
                return head_adapter  # í—¤ë“œ ì–´ëŒ‘í„° ì „ì²´ë¥¼ ë°˜í™˜ (input_adapter í¬í•¨)
            else:
                # forward ë©”ì„œë“œê°€ ì—†ìœ¼ë©´ PyTorch ë„¤íŠ¸ì›Œí¬ë§Œ ë°˜í™˜
                pytorch_network = None
                if hasattr(head_adapter, 'get_pytorch_network'):
                    pytorch_network = head_adapter.get_pytorch_network()
                
                if pytorch_network is not None:
                    logger.info(f"PyTorch ë„¤íŠ¸ì›Œí¬ë§Œ ë°˜í™˜: {head_type.value}")
                    return pytorch_network
                else:
                    logger.warning(f"í—¤ë“œ {head_type.value}ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return None
                
        except Exception as e:
            logger.error(f"í—¤ë“œ ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {head_type.value}: {str(e)}")
            return None
    
    def _calculate_loss_stability(self, recent_losses: List[float], window_size: int = 50) -> float:
        """ì†ì‹¤ ì•ˆì •ì„± ê³„ì‚°"""
        if len(recent_losses) < window_size:
            return 0.0
        
        recent_window = recent_losses[-window_size:]
        std_dev = np.std(recent_window)
        mean_loss = np.mean(recent_window)
        
        # í‘œì¤€í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ì•ˆì •ì 
        stability = 1.0 / (1.0 + std_dev / max(0.001, mean_loss))
        return min(1.0, stability)
    
    def _calculate_convergence_rate(self, metrics_history: List[TrainingMetrics]) -> float:
        """ìˆ˜ë ´ ì†ë„ ê³„ì‚°"""
        if len(metrics_history) < 10:
            return 0.0
        
        recent_losses = [m.total_loss for m in metrics_history[-10:]]
        
        # ì†ì‹¤ ê°ì†Œ ì¶”ì„¸ ë¶„ì„
        if len(recent_losses) >= 2:
            loss_trend = recent_losses[-1] - recent_losses[0]
            if loss_trend < 0:  # ì†ì‹¤ ê°ì†Œ ì¤‘
                convergence = min(1.0, abs(loss_trend) / recent_losses[0])
            else:
                convergence = 0.0
        else:
            convergence = 0.0
        
        return convergence
    
    async def _log_training_progress(self, metrics: TrainingMetrics, 
                                   batch_idx: int, active_heads: List[HeadType]):
        """í›ˆë ¨ ì§„í–‰ìƒí™© ë¡œê¹…"""
        
        head_names = [h.value for h in active_heads]
        memory_info = self.trainer.memory_monitor.get_memory_usage()
        
        logger.info(
            f"ìŠ¤í… {metrics.step}, ë°°ì¹˜ {batch_idx}: "
            f"ì†ì‹¤={metrics.total_loss:.4f}, "
            f"í™œì„±í—¤ë“œ={head_names}, "
            f"GPU={memory_info['gpu_percent']:.1f}%, "
            f"ì²˜ë¦¬ì†ë„={metrics.samples_per_second:.1f} samples/s"
        )
        
        # í—¤ë“œë³„ ì†ì‹¤ ë¡œê¹…
        for head_name, loss in metrics.head_losses.items():
            logger.debug(f"  {head_name} ì†ì‹¤: {loss:.4f}")
    
    async def _complete_epoch(self, epoch: int, epoch_metrics: List[TrainingMetrics], 
                            epoch_time: float):
        """ì—í¬í¬ ì™„ë£Œ ì²˜ë¦¬"""
        
        # ì—í¬í¬ í†µê³„ ê³„ì‚°
        avg_loss = np.mean([m.total_loss for m in epoch_metrics])
        avg_memory = np.mean([m.memory_usage for m in epoch_metrics])
        total_samples = sum(m.samples_per_second * m.training_time for m in epoch_metrics)
        
        # íš¨ìœ¨ì„± ë©”íŠ¸ë¦­
        memory_efficiency = self.trainer.memory_monitor.get_memory_efficiency()
        
        epoch_stats = {
            'epoch': epoch,
            'avg_loss': avg_loss,
            'epoch_time': epoch_time,
            'total_samples': total_samples,
            'avg_memory_usage': avg_memory,
            'memory_efficiency': memory_efficiency,
            'learning_phase': self.scheduler.current_phase.value,
            'training_strategy': self.trainer.training_strategy.value
        }
        
        self.training_stats[f'epoch_{epoch}'] = epoch_stats
        
        logger.info(
            f"ì—í¬í¬ {epoch} ì™„ë£Œ: "
            f"í‰ê· ì†ì‹¤={avg_loss:.4f}, "
            f"ì‹œê°„={epoch_time:.1f}s, "
            f"ë©”ëª¨ë¦¬íš¨ìœ¨={memory_efficiency:.2%}, "
            f"í•™ìŠµë‹¨ê³„={self.scheduler.current_phase.value}"
        )
    
    async def _validate_model(self, validation_data_loader, active_heads: List[HeadType]):
        """ëª¨ë¸ ê²€ì¦"""
        
        logger.info("ëª¨ë¸ ê²€ì¦ ì‹œì‘...")
        
        self.trainer.eval()
        validation_losses = []
        
        try:
            with torch.no_grad():
                for batch_data in validation_data_loader:
                    # ê²€ì¦ ìˆœì „íŒŒ
                    outputs = self.trainer.forward_with_checkpointing(batch_data, active_heads)
                    
                    # ê²€ì¦ ì†ì‹¤ ê³„ì‚°
                    losses = self.trainer._calculate_losses(outputs, batch_data, active_heads)
                    total_loss = sum(losses.values()) / len(losses)
                    validation_losses.append(float(total_loss.item()))
        
        finally:
            self.trainer.train()  # í›ˆë ¨ ëª¨ë“œë¡œ ë³µì›
        
        avg_val_loss = np.mean(validation_losses)
        logger.info(f"ê²€ì¦ ì™„ë£Œ: í‰ê·  ê²€ì¦ ì†ì‹¤ = {avg_val_loss:.4f}")
        
        return avg_val_loss
    
    async def _save_checkpoint(self, epoch: int):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        
        checkpoint_path = os.path.join(self.checkpoint_dir, f"unified_model_epoch_{epoch}.pth")
        
        checkpoint = {
            'epoch': epoch,
            'unified_backbone_state_dict': self.unified_backbone.state_dict(),
            'optimizers_state_dict': {name: opt.state_dict() for name, opt in self.trainer.optimizers.items()},
            'schedulers_state_dict': {name: sch.state_dict() for name, sch in self.trainer.schedulers.items()},
            'training_stats': self.training_stats,
            'current_phase': self.scheduler.current_phase.value,
            'step_count': self.scheduler.step_count,
            'config': self.config
        }
        
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    def get_training_statistics(self) -> Dict[str, Any]:
        """í›ˆë ¨ í†µê³„ ë°˜í™˜"""
        
        current_metrics = self.trainer.current_metrics
        memory_stats = self.trainer.memory_monitor.get_memory_usage()
        
        stats = {
            'current_metrics': {
                'epoch': current_metrics.epoch,
                'step': current_metrics.step,
                'total_loss': current_metrics.total_loss,
                'head_losses': current_metrics.head_losses,
                'learning_rates': current_metrics.learning_rates,
                'samples_per_second': current_metrics.samples_per_second,
                'memory_efficiency': self.trainer.memory_monitor.get_memory_efficiency()
            },
            'memory_stats': memory_stats,
            'training_phase': self.scheduler.current_phase.value,
            'training_strategy': self.trainer.training_strategy.value,
            'batch_size': self.trainer.adaptive_batch_manager.current_batch_size,
            'epoch_stats': self.training_stats
        }
        
        return stats

# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
async def example_usage():
    """í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ"""
    
    # ê°€ìƒì˜ ë°ì´í„° ë¡œë” ìƒì„±
    class DummyDataLoader:
        def __init__(self, num_batches=100):
            self.num_batches = num_batches
        
        def __iter__(self):
            for i in range(self.num_batches):
                yield {
                    'text': f'ìƒ˜í”Œ í…ìŠ¤íŠ¸ {i}',
                    'batch_size': 4,
                    'labels': torch.randint(0, 10, (4,))
                }
    
    # í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ìƒì„±
    learning_system = UnifiedLearningSystem()
    
    # ë°ì´í„° ë¡œë”
    train_loader = DummyDataLoader(num_batches=500)
    val_loader = DummyDataLoader(num_batches=50)
    
    print("=== í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    # í›ˆë ¨ ì‹¤í–‰
    await learning_system.train_unified_system(
        train_data_loader=train_loader,
        validation_data_loader=val_loader,
        num_epochs=3,
        training_strategy=TrainingStrategy.ROUND_ROBIN
    )
    
    # í›ˆë ¨ í†µê³„ ì¶œë ¥
    stats = learning_system.get_training_statistics()
    print(f"\n=== í›ˆë ¨ ì™„ë£Œ í†µê³„ ===")
    print(f"ìµœì¢… ì†ì‹¤: {stats['current_metrics']['total_loss']:.4f}")
    print(f"í›ˆë ¨ ë‹¨ê³„: {stats['training_phase']}")
    print(f"ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {stats['current_metrics']['memory_efficiency']:.2%}")
    print(f"ì²˜ë¦¬ ì†ë„: {stats['current_metrics']['samples_per_second']:.1f} samples/s")
    
    # í—¤ë“œë³„ ì†ì‹¤
    print(f"\n=== í—¤ë“œë³„ ì†ì‹¤ ===")
    for head_name, loss in stats['current_metrics']['head_losses'].items():
        print(f"{head_name}: {loss:.4f}")

if __name__ == "__main__":
    asyncio.run(example_usage())