"""
ê³ ê¸‰ ë°ì´í„° ë¡œë” - Linux ì „ìš©
Advanced Data Loader for Red Heart Linux

ê¸°ì¡´ Red Heartì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê³ ê¸‰ AI ë¶„ì„ì„ ìœ„í•œ 
ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import json
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Union, Tuple
from pathlib import Path
import time
from concurrent.futures import ThreadPoolExecutor
import threading
from dataclasses import dataclass, field

# ê³ ê¸‰ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, LabelEncoder
# SentenceTransformerëŠ” sentence_transformer_singletonì„ í†µí•´ ì‚¬ìš©
import torch

from config import ADVANCED_CONFIG, DEVICE, MODELS_DIR
from data_models import EthicalScenario, DatasetSummary, ProcessingMetrics

logger = logging.getLogger('RedHeart.AdvancedDataLoader')


@dataclass
class DatasetInfo:
    """ë°ì´í„°ì…‹ ì •ë³´"""
    name: str
    file_path: str
    total_scenarios: int
    categories: List[str]
    ethical_themes: List[str]
    last_updated: str
    processing_status: str = "ready"
    error_message: Optional[str] = None


@dataclass
class LoadingProgress:
    """ë°ì´í„° ë¡œë”© ì§„í–‰ ìƒí™©"""
    total_files: int = 0
    processed_files: int = 0
    current_file: str = ""
    errors: List[str] = field(default_factory=list)
    start_time: float = 0.0
    estimated_completion: float = 0.0


class AdvancedDataLoader:
    """ê³ ê¸‰ ë°ì´í„° ë¡œë”"""
    
    def __init__(self):
        self.logger = logger
        self.device = DEVICE
        
        # ë°ì´í„° ê²½ë¡œ
        self.data_dir = Path("./data")
        self.processed_datasets_dir = Path("./processed_datasets")
        self.korean_literature_dir = Path("./korean_literature_data")
        
        # ë¡œë”©ëœ ë°ì´í„°
        self.datasets = {}
        self.dataset_info = {}
        self.embeddings_cache = {}
        
        # ê³ ê¸‰ ì²˜ë¦¬ ë„êµ¬
        self.sentence_transformer = None
        self.tfidf_vectorizer = None
        self.label_encoder = None
        self.scaler = StandardScaler()
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self.loading_lock = threading.Lock()
        self.progress = LoadingProgress()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.processing_metrics = ProcessingMetrics()
        
        self.logger.info("ê³ ê¸‰ ë°ì´í„° ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ")
        
    def initialize_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if ADVANCED_CONFIG['use_sentence_transformers']:
                self.logger.info("Sentence Transformer ëª¨ë¸ ë¡œë”© ì¤‘...")
                from sentence_transformer_singleton import get_sentence_transformer
                
                self.sentence_transformer = get_sentence_transformer(
                    'paraphrase-multilingual-mpnet-base-v2',
                    device=str(self.device),
                    cache_folder=os.path.join(MODELS_DIR, 'sentence_transformers')
                )
                self.logger.info("Sentence Transformer ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
            # TF-IDF ë²¡í„°ë¼ì´ì €
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=10000,
                ngram_range=(1, 2),
                stop_words='english'
            )
            
            # ë¼ë²¨ ì¸ì½”ë”
            self.label_encoder = LabelEncoder()
            
            self.logger.info("ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
            
    def discover_datasets(self) -> Dict[str, DatasetInfo]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ë°œê²¬"""
        self.logger.info("ë°ì´í„°ì…‹ ê²€ìƒ‰ ì‹œì‘...")
        
        datasets = {}
        
        # 1. ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ë“¤
        if self.processed_datasets_dir.exists():
            for json_file in self.processed_datasets_dir.glob("*.json"):
                try:
                    info = self._analyze_dataset_file(json_file)
                    if info:
                        datasets[info.name] = info
                        self.logger.debug(f"ë°ì´í„°ì…‹ ë°œê²¬: {info.name} ({info.total_scenarios} ì‹œë‚˜ë¦¬ì˜¤)")
                except Exception as e:
                    self.logger.error(f"ë°ì´í„°ì…‹ ë¶„ì„ ì‹¤íŒ¨ {json_file}: {e}")
                    
        # 2. í•œêµ­ ë¬¸í•™ ë°ì´í„°
        if self.korean_literature_dir.exists():
            for json_file in self.korean_literature_dir.glob("*.json"):
                try:
                    info = self._analyze_korean_literature_file(json_file)
                    if info:
                        datasets[info.name] = info
                        self.logger.debug(f"í•œêµ­ ë¬¸í•™ ë°ì´í„°ì…‹ ë°œê²¬: {info.name}")
                except Exception as e:
                    self.logger.error(f"í•œêµ­ ë¬¸í•™ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨ {json_file}: {e}")
                    
        self.dataset_info = datasets
        self.logger.info(f"ì´ {len(datasets)}ê°œ ë°ì´í„°ì…‹ ë°œê²¬")
        
        return datasets
        
    def _analyze_dataset_file(self, file_path: Path) -> Optional[DatasetInfo]:
        """ë°ì´í„°ì…‹ íŒŒì¼ ë¶„ì„"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # íŒŒì¼ êµ¬ì¡°ì— ë”°ë¥¸ ë¶„ì„
            if isinstance(data, dict):
                if 'integrated_scenarios' in data:
                    # í†µí•© ë°ì´í„°ì…‹
                    scenarios = data['integrated_scenarios']
                    total_scenarios = len(scenarios)
                    
                    categories = set()
                    ethical_themes = set()
                    
                    for scenario in scenarios[:100]:  # ìƒ˜í”Œë§ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
                        if 'category' in scenario:
                            categories.add(scenario['category'])
                        if 'ethical_themes' in scenario:
                            ethical_themes.update(scenario['ethical_themes'])
                            
                    return DatasetInfo(
                        name=file_path.stem,
                        file_path=str(file_path),
                        total_scenarios=total_scenarios,
                        categories=list(categories),
                        ethical_themes=list(ethical_themes),
                        last_updated=data.get('integration_date', 'unknown')
                    )
                    
                elif 'scenarios' in data:
                    # ì¼ë°˜ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°ì…‹
                    scenarios = data['scenarios']
                    return DatasetInfo(
                        name=file_path.stem,
                        file_path=str(file_path),
                        total_scenarios=len(scenarios),
                        categories=[],
                        ethical_themes=[],
                        last_updated='unknown'
                    )
                    
            elif isinstance(data, list):
                # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ë°ì´í„°ì…‹
                return DatasetInfo(
                    name=file_path.stem,
                    file_path=str(file_path),
                    total_scenarios=len(data),
                    categories=[],
                    ethical_themes=[],
                    last_updated='unknown'
                )
                
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ì…‹ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            return None
            
        return None
        
    def _analyze_korean_literature_file(self, file_path: Path) -> Optional[DatasetInfo]:
        """í•œêµ­ ë¬¸í•™ ë°ì´í„° íŒŒì¼ ë¶„ì„"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            if isinstance(data, dict):
                total_items = len(data)
            elif isinstance(data, list):
                total_items = len(data)
            else:
                total_items = 1
                
            return DatasetInfo(
                name=f"korean_literature_{file_path.stem}",
                file_path=str(file_path),
                total_scenarios=total_items,
                categories=['korean_literature'],
                ethical_themes=['cultural_context'],
                last_updated='unknown'
            )
            
        except Exception as e:
            self.logger.error(f"í•œêµ­ ë¬¸í•™ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            return None
            
    def load_dataset(self, dataset_name: str, 
                    include_embeddings: bool = True,
                    sample_size: Optional[int] = None) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ë°ì´í„°ì…‹ ë¡œë“œ"""
        
        if dataset_name not in self.dataset_info:
            self.logger.error(f"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_name}")
            return None
            
        dataset_info = self.dataset_info[dataset_name]
        
        try:
            self.logger.info(f"ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘: {dataset_name}")
            start_time = time.time()
            
            # JSON íŒŒì¼ ë¡œë“œ
            with open(dataset_info.file_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
                
            # ë°ì´í„° êµ¬ì¡° ì •ê·œí™”
            normalized_data = self._normalize_data_structure(raw_data, dataset_name)
            
            # ìƒ˜í”Œë§ (í•„ìš”í•œ ê²½ìš°)
            if sample_size and len(normalized_data) > sample_size:
                normalized_data = np.random.choice(
                    normalized_data, sample_size, replace=False
                ).tolist()
                self.logger.info(f"ë°ì´í„° ìƒ˜í”Œë§: {len(normalized_data)} í•­ëª©")
                
            # ê³ ê¸‰ ì „ì²˜ë¦¬
            processed_data = self._advanced_preprocessing(normalized_data, dataset_name)
            
            # ì„ë² ë”© ìƒì„± (ì„ íƒì )
            if include_embeddings and self.sentence_transformer:
                embeddings = self._generate_embeddings(processed_data)
                processed_data['embeddings'] = embeddings
                
            # ìºì‹œì— ì €ì¥
            self.datasets[dataset_name] = processed_data
            
            loading_time = time.time() - start_time
            self.logger.info(f"ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ: {dataset_name} ({loading_time:.2f}ì´ˆ)")
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.processing_metrics.update_loading_stats(dataset_name, loading_time, len(normalized_data))
            
            return processed_data
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨ {dataset_name}: {e}")
            return None
            
    def _normalize_data_structure(self, raw_data: Any, dataset_name: str) -> List[Dict[str, Any]]:
        """ë°ì´í„° êµ¬ì¡° ì •ê·œí™”"""
        normalized = []
        
        try:
            if isinstance(raw_data, dict):
                if 'integrated_scenarios' in raw_data:
                    # í†µí•© ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°
                    scenarios = raw_data['integrated_scenarios']
                    for scenario in scenarios:
                        normalized_scenario = {
                            'id': scenario.get('id', ''),
                            'title': scenario.get('title', ''),
                            'description': scenario.get('description', ''),
                            'category': scenario.get('category', 'unknown'),
                            'ethical_themes': scenario.get('ethical_themes', []),
                            'stakeholders': scenario.get('stakeholders', []),
                            'options': scenario.get('options', []),
                            'source_type': scenario.get('source_type', 'unknown'),
                            'source_file': scenario.get('source_file', ''),
                            'metadata': {
                                'dataset': dataset_name,
                                'original_structure': 'integrated_scenarios'
                            }
                        }
                        normalized.append(normalized_scenario)
                        
                elif 'scenarios' in raw_data:
                    # ì¼ë°˜ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°
                    scenarios = raw_data['scenarios']
                    for scenario in scenarios:
                        normalized_scenario = {
                            'id': scenario.get('id', ''),
                            'title': scenario.get('title', ''),
                            'description': scenario.get('description', scenario.get('text', '')),
                            'category': scenario.get('category', 'unknown'),
                            'ethical_themes': scenario.get('themes', scenario.get('ethical_themes', [])),
                            'stakeholders': scenario.get('stakeholders', []),
                            'options': scenario.get('options', []),
                            'source_type': 'scenario_collection',
                            'metadata': {
                                'dataset': dataset_name,
                                'original_structure': 'scenarios'
                            }
                        }
                        normalized.append(normalized_scenario)
                        
                else:
                    # í•œêµ­ ë¬¸í•™ì´ë‚˜ ê¸°íƒ€ êµ¬ì¡°
                    for key, value in raw_data.items():
                        if isinstance(value, dict):
                            normalized_item = {
                                'id': key,
                                'title': value.get('title', key),
                                'description': value.get('content', value.get('text', str(value))),
                                'category': 'korean_literature',
                                'ethical_themes': value.get('themes', []),
                                'stakeholders': [],
                                'options': [],
                                'source_type': 'korean_literature',
                                'metadata': {
                                    'dataset': dataset_name,
                                    'original_structure': 'key_value'
                                }
                            }
                            normalized.append(normalized_item)
                            
            elif isinstance(raw_data, list):
                # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ë°ì´í„°
                for i, item in enumerate(raw_data):
                    if isinstance(item, dict):
                        normalized_item = {
                            'id': item.get('id', f"{dataset_name}_{i}"),
                            'title': item.get('title', f"Item {i+1}"),
                            'description': item.get('description', item.get('text', str(item))),
                            'category': item.get('category', 'unknown'),
                            'ethical_themes': item.get('ethical_themes', []),
                            'stakeholders': item.get('stakeholders', []),
                            'options': item.get('options', []),
                            'source_type': 'list_item',
                            'metadata': {
                                'dataset': dataset_name,
                                'original_structure': 'list',
                                'index': i
                            }
                        }
                        normalized.append(normalized_item)
                        
        except Exception as e:
            self.logger.error(f"ë°ì´í„° êµ¬ì¡° ì •ê·œí™” ì‹¤íŒ¨: {e}")
            
        return normalized
        
    def _advanced_preprocessing(self, data: List[Dict[str, Any]], 
                              dataset_name: str) -> Dict[str, Any]:
        """ê³ ê¸‰ ì „ì²˜ë¦¬"""
        try:
            # í…ìŠ¤íŠ¸ ì •ë¦¬ ë° í†µê³„ ìƒì„±
            texts = []
            categories = []
            ethical_themes_flat = []
            
            for item in data:
                # í…ìŠ¤íŠ¸ ê²°í•© ë° ì •ë¦¬
                combined_text = f"{item.get('title', '')} {item.get('description', '')}"
                cleaned_text = self._clean_text(combined_text)
                texts.append(cleaned_text)
                
                # ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘
                categories.append(item.get('category', 'unknown'))
                
                # ìœ¤ë¦¬ì  í…Œë§ˆ ìˆ˜ì§‘
                themes = item.get('ethical_themes', [])
                if isinstance(themes, list):
                    ethical_themes_flat.extend(themes)
                    
            # í†µê³„ ìƒì„±
            category_counts = pd.Series(categories).value_counts().to_dict()
            theme_counts = pd.Series(ethical_themes_flat).value_counts().to_dict()
            
            # TF-IDF ë²¡í„°í™” (ìƒ˜í”Œë§ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ)
            sample_texts = texts[:min(1000, len(texts))]
            try:
                tfidf_matrix = self.tfidf_vectorizer.fit_transform(sample_texts)
                feature_names = self.tfidf_vectorizer.get_feature_names_out()
                
                # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
                mean_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()
                top_indices = np.argsort(mean_scores)[::-1][:50]
                top_keywords = [feature_names[i] for i in top_indices]
            except Exception as e:
                self.logger.warning(f"TF-IDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                top_keywords = []
                
            return {
                'scenarios': data,
                'texts': texts,
                'statistics': {
                    'total_scenarios': len(data),
                    'category_distribution': category_counts,
                    'theme_distribution': theme_counts,
                    'top_keywords': top_keywords,
                    'avg_text_length': np.mean([len(text) for text in texts]),
                    'total_unique_categories': len(category_counts),
                    'total_unique_themes': len(theme_counts)
                },
                'metadata': {
                    'dataset_name': dataset_name,
                    'preprocessing_version': '2.0',
                    'processed_at': time.time()
                }
            }
            
        except Exception as e:
            self.logger.error(f"ê³ ê¸‰ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'scenarios': data,
                'texts': [item.get('description', '') for item in data],
                'statistics': {'total_scenarios': len(data)},
                'metadata': {'dataset_name': dataset_name, 'error': str(e)}
            }
            
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
            
        # ê¸°ë³¸ ì •ë¦¬
        cleaned = text.strip()
        
        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        import re
        cleaned = re.sub(r'\s+', ' ', cleaned)  # ê³µë°± ì •ë¦¬
        cleaned = re.sub(r'[^\w\sê°€-í£.,!?]', '', cleaned)  # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€ ìœ ì§€)
        
        return cleaned
        
    def _generate_embeddings(self, processed_data: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """ì„ë² ë”© ìƒì„±"""
        try:
            texts = processed_data['texts']
            self.logger.info(f"ì„ë² ë”© ìƒì„± ì‹œì‘: {len(texts)} í…ìŠ¤íŠ¸")
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
            batch_size = 32
            embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                try:
                    batch_embeddings = self.sentence_transformer.encode(
                        batch_texts,
                        convert_to_tensor=False,
                        show_progress_bar=False
                    )
                    embeddings.extend(batch_embeddings)
                except Exception as e:
                    self.logger.error(f"ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨: {e}")
                    # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” 0ìœ¼ë¡œ ì±„ì›€
                    embeddings.extend([np.zeros(384) for _ in batch_texts])
                    
            embeddings_array = np.array(embeddings)
            self.logger.info(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings_array.shape}")
            
            return {
                'text_embeddings': embeddings_array,
                'embedding_model': 'paraphrase-multilingual-mpnet-base-v2',
                'embedding_dim': embeddings_array.shape[1] if len(embeddings_array) > 0 else 0
            }
            
        except Exception as e:
            self.logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
            
    def load_all_datasets(self, include_embeddings: bool = False,
                         max_scenarios_per_dataset: int = 1000) -> Dict[str, Any]:
        """ëª¨ë“  ë°ì´í„°ì…‹ ë¡œë“œ"""
        
        self.logger.info("ëª¨ë“  ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘...")
        start_time = time.time()
        
        # ë°ì´í„°ì…‹ ë°œê²¬
        datasets_info = self.discover_datasets()
        
        self.progress.total_files = len(datasets_info)
        self.progress.start_time = start_time
        
        loaded_datasets = {}
        total_scenarios = 0
        
        for dataset_name, dataset_info in datasets_info.items():
            try:
                self.progress.current_file = dataset_name
                
                dataset = self.load_dataset(
                    dataset_name,
                    include_embeddings=include_embeddings,
                    sample_size=max_scenarios_per_dataset
                )
                
                if dataset:
                    loaded_datasets[dataset_name] = dataset
                    scenario_count = dataset['statistics']['total_scenarios']
                    total_scenarios += scenario_count
                    
                    self.logger.info(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {dataset_name} ({scenario_count} ì‹œë‚˜ë¦¬ì˜¤)")
                    
                self.progress.processed_files += 1
                
            except Exception as e:
                self.logger.error(f"ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨ {dataset_name}: {e}")
                self.progress.errors.append(f"{dataset_name}: {str(e)}")
                
        total_time = time.time() - start_time
        
        # ì „ì²´ í†µê³„ ìƒì„±
        overall_stats = self._generate_overall_statistics(loaded_datasets)
        
        result = {
            'datasets': loaded_datasets,
            'overall_statistics': overall_stats,
            'loading_summary': {
                'total_datasets': len(datasets_info),
                'loaded_datasets': len(loaded_datasets),
                'failed_datasets': len(datasets_info) - len(loaded_datasets),
                'total_scenarios': total_scenarios,
                'loading_time': total_time,
                'errors': self.progress.errors
            },
            'metadata': {
                'loaded_at': time.time(),
                'loader_version': '2.0',
                'include_embeddings': include_embeddings
            }
        }
        
        self.logger.info(f"ì „ì²´ ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ: {len(loaded_datasets)}ê°œ ë°ì´í„°ì…‹, "
                        f"{total_scenarios}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ({total_time:.2f}ì´ˆ)")
        
        return result
        
    def _generate_overall_statistics(self, loaded_datasets: Dict[str, Any]) -> Dict[str, Any]:
        """ì „ì²´ í†µê³„ ìƒì„±"""
        
        all_categories = []
        all_themes = []
        all_text_lengths = []
        dataset_sizes = {}
        
        for dataset_name, dataset in loaded_datasets.items():
            stats = dataset.get('statistics', {})
            
            # ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘
            if 'category_distribution' in stats:
                all_categories.extend(stats['category_distribution'].keys())
                
            # í…Œë§ˆ ìˆ˜ì§‘
            if 'theme_distribution' in stats:
                all_themes.extend(stats['theme_distribution'].keys())
                
            # í…ìŠ¤íŠ¸ ê¸¸ì´
            if 'avg_text_length' in stats:
                all_text_lengths.append(stats['avg_text_length'])
                
            # ë°ì´í„°ì…‹ í¬ê¸°
            dataset_sizes[dataset_name] = stats.get('total_scenarios', 0)
            
        # í†µê³„ ê³„ì‚°
        unique_categories = list(set(all_categories))
        unique_themes = list(set(all_themes))
        
        return {
            'total_unique_categories': len(unique_categories),
            'total_unique_themes': len(unique_themes),
            'categories': unique_categories,
            'themes': unique_themes,
            'dataset_sizes': dataset_sizes,
            'avg_text_length_overall': np.mean(all_text_lengths) if all_text_lengths else 0,
            'largest_dataset': max(dataset_sizes.items(), key=lambda x: x[1]) if dataset_sizes else None,
            'smallest_dataset': min(dataset_sizes.items(), key=lambda x: x[1]) if dataset_sizes else None
        }
        
    def get_dataset_summary(self) -> DatasetSummary:
        """ë°ì´í„°ì…‹ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        
        summary = DatasetSummary(
            total_datasets=len(self.dataset_info),
            loaded_datasets=len(self.datasets),
            total_scenarios=sum(info.total_scenarios for info in self.dataset_info.values()),
            available_categories=list(set().union(*[info.categories for info in self.dataset_info.values()])),
            available_themes=list(set().union(*[info.ethical_themes for info in self.dataset_info.values()])),
            processing_metrics=self.processing_metrics
        )
        
        return summary
        
    def clear_cache(self):
        """ìºì‹œ í´ë¦¬ì–´"""
        self.datasets.clear()
        self.embeddings_cache.clear()
        self.logger.info("ë°ì´í„° ìºì‹œê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤.")


def test_advanced_data_loader():
    """ê³ ê¸‰ ë°ì´í„° ë¡œë” í…ŒìŠ¤íŠ¸"""
    
    logging.basicConfig(level=logging.INFO)
    
    try:
        # ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
        loader = AdvancedDataLoader()
        loader.initialize_models()
        
        print("=== ê³ ê¸‰ ë°ì´í„° ë¡œë” í…ŒìŠ¤íŠ¸ ===")
        
        # 1. ë°ì´í„°ì…‹ ë°œê²¬
        print("\nğŸ“‚ ë°ì´í„°ì…‹ ë°œê²¬ ì¤‘...")
        datasets_info = loader.discover_datasets()
        
        print(f"ë°œê²¬ëœ ë°ì´í„°ì…‹: {len(datasets_info)}ê°œ")
        for name, info in datasets_info.items():
            print(f"  â€¢ {name}: {info.total_scenarios} ì‹œë‚˜ë¦¬ì˜¤")
            print(f"    ì¹´í…Œê³ ë¦¬: {len(info.categories)}ê°œ")
            print(f"    ìœ¤ë¦¬ í…Œë§ˆ: {len(info.ethical_themes)}ê°œ")
            
        # 2. íŠ¹ì • ë°ì´í„°ì…‹ ë¡œë“œ í…ŒìŠ¤íŠ¸
        if datasets_info:
            first_dataset_name = list(datasets_info.keys())[0]
            print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ í…ŒìŠ¤íŠ¸: {first_dataset_name}")
            
            dataset = loader.load_dataset(
                first_dataset_name, 
                include_embeddings=True,
                sample_size=100
            )
            
            if dataset:
                stats = dataset['statistics']
                print(f"  ë¡œë“œëœ ì‹œë‚˜ë¦¬ì˜¤: {stats['total_scenarios']}ê°œ")
                print(f"  í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {stats['avg_text_length']:.1f}ì")
                print(f"  ì¹´í…Œê³ ë¦¬: {stats['total_unique_categories']}ê°œ")
                print(f"  í…Œë§ˆ: {stats['total_unique_themes']}ê°œ")
                
                if 'embeddings' in dataset:
                    embeddings = dataset['embeddings']
                    if 'text_embeddings' in embeddings:
                        embed_shape = embeddings['text_embeddings'].shape
                        print(f"  ì„ë² ë”©: {embed_shape}")
                        
        # 3. ì „ì²´ ë°ì´í„°ì…‹ ë¡œë”© (ì œí•œì )
        print(f"\nğŸ”„ ì „ì²´ ë°ì´í„°ì…‹ ë¡œë”© (ìƒ˜í”Œë§)...")
        all_data = loader.load_all_datasets(
            include_embeddings=False,
            max_scenarios_per_dataset=50
        )
        
        summary = all_data['loading_summary']
        print(f"  ë¡œë“œëœ ë°ì´í„°ì…‹: {summary['loaded_datasets']}/{summary['total_datasets']}")
        print(f"  ì´ ì‹œë‚˜ë¦¬ì˜¤: {summary['total_scenarios']}ê°œ")
        print(f"  ë¡œë”© ì‹œê°„: {summary['loading_time']:.2f}ì´ˆ")
        
        if summary['errors']:
            print(f"  ì˜¤ë¥˜: {len(summary['errors'])}ê°œ")
            
        # 4. í†µê³„ ì •ë³´
        overall_stats = all_data['overall_statistics']
        print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
        print(f"  ê³ ìœ  ì¹´í…Œê³ ë¦¬: {overall_stats['total_unique_categories']}ê°œ")
        print(f"  ê³ ìœ  í…Œë§ˆ: {overall_stats['total_unique_themes']}ê°œ")
        
        if overall_stats['largest_dataset']:
            largest = overall_stats['largest_dataset']
            print(f"  ìµœëŒ€ ë°ì´í„°ì…‹: {largest[0]} ({largest[1]} ì‹œë‚˜ë¦¬ì˜¤)")
            
        print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        return loader
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    test_advanced_data_loader()