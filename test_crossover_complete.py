#!/usr/bin/env python3
"""
í¬ë¡œìŠ¤ì˜¤ë²„ ì‹œìŠ¤í…œ ì™„ì „ì„± ê²€ì¦
- ëª¨ë“  ëª¨ë“ˆì´ ì €ì¥ë˜ëŠ”ì§€ í™•ì¸
- 50 ì—í­ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
"""

import sys
import torch
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from training.unified_training_final import UnifiedModel, UnifiedTrainingConfig
from training.enhanced_checkpoint_manager import EnhancedCheckpointManager

def test_complete_save():
    """ëª¨ë“  ëª¨ë“ˆì´ ì €ì¥ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
    
    print("=" * 70)
    print("í¬ë¡œìŠ¤ì˜¤ë²„ ì™„ì „ì„± í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # ì„¤ì • ë° ëª¨ë¸ ìƒì„±
    config = UnifiedTrainingConfig()
    model = UnifiedModel(config)
    
    # Checkpoint Manager ìƒì„±
    manager = EnhancedCheckpointManager(
        checkpoint_dir="test_checkpoints",
        max_checkpoints=50,
        save_interval=1
    )
    
    # ë”ë¯¸ optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    print("\nğŸ“Š ëª¨ë¸ êµ¬ì¡° í™•ì¸:")
    expected_modules = [
        'backbone', 'emotion_head', 'bentham_head', 'regret_head', 'surd_head',
        'neural_analyzers', 'phase0_net', 'phase2_net', 'hierarchical_integrator',
        'dsp_simulator', 'kalman_filter', 'advanced_wrappers', 'system'
    ]
    
    for module_name in expected_modules:
        if module_name == 'system':
            # systemì€ ë©”íƒ€ë°ì´í„°ë¡œ _extract_modular_statesì—ì„œ ì¶”ê°€ë¨
            print(f"  âœ… {module_name}: ë©”íƒ€ë°ì´í„° (ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì‹œ ì¶”ê°€)")
        elif hasattr(model, module_name):
            module = getattr(model, module_name)
            if module is not None:
                if hasattr(module, 'parameters'):
                    param_count = sum(p.numel() for p in module.parameters()) / 1e6
                    print(f"  âœ… {module_name}: {param_count:.2f}M íŒŒë¼ë¯¸í„°")
                else:
                    print(f"  âœ… {module_name}: dict í˜•íƒœ")
            else:
                print(f"  âŒ {module_name}: None")
        else:
            print(f"  âŒ {module_name}: ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
    
    print("\nğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í…ŒìŠ¤íŠ¸:")
    
    # í…ŒìŠ¤íŠ¸ ì—í­ë“¤
    test_epochs = [1, 10, 20, 30, 40, 50]
    
    for epoch in test_epochs:
        metrics = {'loss': 1.0 / epoch, 'accuracy': epoch / 50.0}
        
        # optimizer ì €ì¥ ì—¬ë¶€ í™•ì¸
        keep_optimizer = manager.should_keep_optimizer(epoch)
        print(f"\nì—í­ {epoch:2d}:")
        print(f"  - Optimizer ì €ì¥: {'âœ… Yes' if keep_optimizer else 'âŒ No'}")
        
        # ëª¨ë“ˆë³„ state ì¶”ì¶œ
        modular_states = manager._extract_modular_states(model)
        
        # ì €ì¥ëœ ëª¨ë“ˆ í™•ì¸
        saved_modules = list(modular_states.keys())
        print(f"  - ì €ì¥ëœ ëª¨ë“ˆ: {len(saved_modules)}ê°œ")
        
        # ëˆ„ë½ëœ ëª¨ë“ˆ í™•ì¸
        missing = []
        for expected in expected_modules:
            if expected not in saved_modules:
                # advanced_wrappersëŠ” Noneì¼ ìˆ˜ ìˆìŒ
                if expected == 'advanced_wrappers' and model.advanced_wrappers is None:
                    continue
                # systemì€ _extract_modular_statesì—ì„œ ìë™ ì¶”ê°€ë¨
                if expected == 'system':
                    continue
                missing.append(expected)
        
        if missing:
            print(f"  âš ï¸ ëˆ„ë½ëœ ëª¨ë“ˆ: {missing}")
        else:
            print(f"  âœ… ëª¨ë“  ëª¨ë“ˆ ì €ì¥ ì™„ë£Œ")
    
    print("\n" + "=" * 70)
    print("âœ… í¬ë¡œìŠ¤ì˜¤ë²„ ì™„ì „ì„± ê²€ì¦ ì™„ë£Œ!")
    print("=" * 70)
    
    # í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì •ë¦¬
    import shutil
    if Path("test_checkpoints").exists():
        shutil.rmtree("test_checkpoints")

if __name__ == "__main__":
    test_complete_save()