"""
ê°ì •-ìœ¤ë¦¬-í›„íšŒ ì‚¼ê° íšŒë¡œ (Emotion-Ethics-Regret Triangle Circuit)
ì¸ê°„ì  ìœ¤ë¦¬ íŒë‹¨ ê³¼ì •ì„ ëª¨ë¸ë§í•œ ìœ ê¸°ì  ìƒí˜¸ì‘ìš© ì‹œìŠ¤í…œ

í•µì‹¬ ì›ì¹™:
1. ê°ì • ìš°ì„ ìˆœìœ„: ê³µë™ì²´ > íƒ€ì > ìì•„ (ì¹˜ëª…ì  ì†ì‹¤ ì‹œ ìš°ì„ ìˆœìœ„ ì—­ì „)
2. ìœ¤ë¦¬ì  ì¶”ë¡ : ê°ì •ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ê°€ì¹˜ íŒë‹¨
3. í›„íšŒëŠ” í•™ìŠµ: ì§ì ‘ ê°œì… ì•„ë‹Œ ë¯¸ë¬˜í•œ í¸í–¥ìœ¼ë¡œ ì‘ìš©
4. ì†ì‹¤ ì–µì œ ìš°ì„ : ê¸°ì¨ë³´ë‹¤ ìŠ¬í””ì„ ìš°ì„ ì‹œ (ì˜êµ¬ ì†ì‹¤ ì›ë¦¬)
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
import time
from pathlib import Path

# ê³ ê¸‰ ëª¨ë“ˆ í•„ìˆ˜ ë¡œë“œ (ì˜ì¡´ì„± ë¬¸ì œ í•´ê²° í›„)
from advanced_bentham_calculator import AdvancedBenthamCalculator
from advanced_emotion_analyzer import AdvancedEmotionAnalyzer  
from advanced_regret_analyzer import AdvancedRegretAnalyzer
ADVANCED_MODULES_AVAILABLE = True
from data_models import EmotionData, EmotionState, EmotionIntensity

logger = logging.getLogger('RedHeart.EmotionEthicsRegretCircuit')

@dataclass
class CircuitDecisionContext:
    """íšŒë¡œ ì˜ì‚¬ê²°ì • ë§¥ë½"""
    scenario_text: str
    proposed_action: str
    
    # ë‹¤ì¸µ ê°ì • ì…ë ¥
    community_emotion: Optional[EmotionData] = None
    other_emotion: Optional[EmotionData] = None
    self_emotion: Optional[EmotionData] = None
    
    # ë§¥ë½ ì •ë³´
    stakeholders: List[str] = None
    social_context: Dict[str, Any] = None
    temporal_urgency: float = 0.5
    
    # ê³¼ê±° ê²½í—˜
    past_regret_memory: Optional[Dict[str, float]] = None
    similar_decisions_history: List[Dict] = None

@dataclass
class CircuitDecisionResult:
    """íšŒë¡œ ì˜ì‚¬ê²°ì • ê²°ê³¼"""
    final_ethical_score: float
    confidence: float
    
    # ë‹¨ê³„ë³„ ê²°ê³¼
    integrated_emotion: EmotionData
    ethical_values: Dict[str, float]
    bentham_result: Any  # EnhancedHedonicResult
    predicted_regret: Dict[str, float]
    
    # ë©”íƒ€ ì •ë³´
    critical_loss_detected: bool
    emotion_conflict_resolved: str
    reasoning_trace: List[str]
    processing_time: float

class EmotionEthicsRegretCircuit:
    """ê°ì •-ìœ¤ë¦¬-í›„íšŒ ì‚¼ê° íšŒë¡œ ê´€ë¦¬ì"""
    
    def __init__(self):
        """íšŒë¡œ ì´ˆê¸°í™”"""
        self.logger = logger
        
        # í•µì‹¬ ëª¨ë“ˆë“¤ (ê³ ê¸‰ ëª¨ë“ˆ ìš°ì„ )
        if ADVANCED_MODULES_AVAILABLE:
            self.emotion_analyzer = AdvancedEmotionAnalyzer()
            self.bentham_calculator = AdvancedBenthamCalculator()
            self.regret_analyzer = AdvancedRegretAnalyzer()
            print("âœ… ê³ ê¸‰ ëª¨ë“ˆë“¤ë¡œ íšŒë¡œ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            raise RuntimeError("ê³ ê¸‰ ëª¨ë“ˆë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë“ˆë¡œëŠ” ì‹œìŠ¤í…œì´ ë¶ˆì™„ì „í•©ë‹ˆë‹¤.")
        
        # ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            from advanced_experience_database import AdvancedExperienceDatabase
            self.experience_db = AdvancedExperienceDatabase()
            self.experience_enabled = True
            print("âœ… ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except ImportError as e:
            print(f"âš ï¸ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ ì—†ìŒ: {e}")
            self.experience_db = None
            self.experience_enabled = False
        
        # íšŒë¡œ ìƒíƒœ
        self.decision_history = []
        self.learning_memory = {
            'regret_patterns': {},
            'successful_decisions': {},
            'emotion_adaptations': {}
        }
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_metrics = {
            'total_decisions': 0,
            'average_processing_time': 0.0,
            'emotion_conflict_rate': 0.0,
            'critical_loss_rate': 0.0
        }
        
        self.logger.info("ê°ì •-ìœ¤ë¦¬-í›„íšŒ ì‚¼ê° íšŒë¡œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def _try_experience_based_decision(self, 
                                           context: CircuitDecisionContext,
                                           reasoning_trace: List[str]) -> Optional[CircuitDecisionResult]:
        """ê²½í—˜ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì‹œë„"""
        
        if not self.experience_enabled:
            reasoning_trace.append("ê²½í—˜ ì‹œìŠ¤í…œ ë¹„í™œì„±í™”ë¨")
            return None
        
        try:
            # í˜„ì¬ ìƒí™©ì„ ê²½í—˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜
            from advanced_experience_database import ExperienceQuery
            
            query = ExperienceQuery(
                query_text=f"{context.scenario_text} {context.proposed_action}",
                category_filter="ethical_decision",
                similarity_threshold=0.75,  # ë†’ì€ ìœ ì‚¬ë„ ìš”êµ¬
                max_results=5,
                boost_recent=True
            )
            
            # ìœ ì‚¬ ê²½í—˜ ê²€ìƒ‰
            similar_experiences = await self.experience_db.search_experiences(query)
            
            if not similar_experiences or len(similar_experiences) == 0:
                reasoning_trace.append("ìœ ì‚¬ ê²½í—˜ ì—†ìŒ - ê²€ìƒ‰ ê²°ê³¼ 0ê±´")
                return None
            
            # ìµœê³  ìœ ì‚¬ë„ ê²½í—˜ í™•ì¸
            best_experience = similar_experiences[0]
            if best_experience['similarity'] < 0.8:
                reasoning_trace.append(f"ìœ ì‚¬ë„ ë¶€ì¡± ({best_experience['similarity']:.3f} < 0.8)")
                return None
            
            reasoning_trace.append(
                f"ìœ ì‚¬ ê²½í—˜ ë°œê²¬: {len(similar_experiences)}ê±´, "
                f"ìµœê³  ìœ ì‚¬ë„: {best_experience['similarity']:.3f}"
            )
            
            # ê²½í—˜ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì‹¤í–‰
            return await self._make_experience_based_decision(
                context, similar_experiences, reasoning_trace
            )
            
        except Exception as e:
            reasoning_trace.append(f"ê²½í—˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    async def _make_experience_based_decision(self,
                                           context: CircuitDecisionContext,
                                           similar_experiences: List[Dict],
                                           reasoning_trace: List[str]) -> CircuitDecisionResult:
        """ìœ ì‚¬ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì˜ì‚¬ê²°ì •"""
        
        start_time = time.time()
        
        # ê²½í—˜ë“¤ë¡œë¶€í„° íŒ¨í„´ ì¶”ì¶œ
        ethical_patterns = []
        regret_patterns = []
        confidence_scores = []
        
        for exp in similar_experiences:
            if 'ethical_score' in exp['metadata']:
                ethical_patterns.append(exp['metadata']['ethical_score'])
            if 'regret_score' in exp['metadata']:
                regret_patterns.append(exp['metadata']['regret_score'])
            confidence_scores.append(exp['similarity'])
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚° (ìœ ì‚¬ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜)
        if ethical_patterns:
            weighted_ethical_score = np.average(ethical_patterns, weights=confidence_scores[:len(ethical_patterns)])
        else:
            # ê²½í—˜ì— ìœ¤ë¦¬ ì ìˆ˜ê°€ ì—†ìœ¼ë©´ ì¤‘ê°„ê°’
            weighted_ethical_score = 0.5
        
        if regret_patterns:
            weighted_regret_score = np.average(regret_patterns, weights=confidence_scores[:len(regret_patterns)])
        else:
            weighted_regret_score = 0.3  # ê¸°ë³¸ í›„íšŒ ìˆ˜ì¤€
        
        # ê²½í—˜ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
        experience_confidence = np.mean(confidence_scores)
        
        # ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (ê²½í—˜ ê¸°ë°˜ì´ë¯€ë¡œ ë¹ ë¥¸ ì²˜ë¦¬)
        basic_emotion = self.emotion_analyzer.analyze_emotion(
            f"{context.scenario_text} {context.proposed_action}", 
            language='ko'
        )
        
        reasoning_trace.append(
            f"ê²½í—˜ ê¸°ë°˜ ì ìˆ˜: ìœ¤ë¦¬={weighted_ethical_score:.3f}, "
            f"í›„íšŒ={weighted_regret_score:.3f}, ì‹ ë¢°ë„={experience_confidence:.3f}"
        )
        
        # ê²½í—˜ ê¸°ë°˜ ê²°ê³¼ ìƒì„±
        return CircuitDecisionResult(
            final_ethical_score=weighted_ethical_score,
            confidence=experience_confidence,
            integrated_emotion=basic_emotion,
            ethical_values={
                'care_harm': weighted_ethical_score,
                'fairness': weighted_ethical_score * 0.9,
                'loyalty': weighted_ethical_score * 0.8
            },
            bentham_result=None,  # ê²½í—˜ ê¸°ë°˜ì—ì„œëŠ” ê°„ì†Œí™”
            predicted_regret={
                'anticipated_regret': weighted_regret_score,
                'regret_intensity': weighted_regret_score * 0.8,
                'confidence': experience_confidence
            },
            critical_loss_detected=False,
            emotion_conflict_resolved='experience_based',
            reasoning_trace=reasoning_trace,
            processing_time=time.time() - start_time
        )

    async def process_ethical_decision(self, 
                                     context: CircuitDecisionContext) -> CircuitDecisionResult:
        """ì¸ê°„ì  ìœ¤ë¦¬ íŒë‹¨ ê³¼ì •ì„ í†µí•œ ì˜ì‚¬ê²°ì •"""
        
        start_time = time.time()
        reasoning_trace = []
        
        try:
            # 0ë‹¨ê³„: ê²½í—˜ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì‹œë„
            reasoning_trace.append("0ë‹¨ê³„: ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì‹œì‘")
            experience_result = await self._try_experience_based_decision(context, reasoning_trace)
            
            if experience_result is not None:
                # ìœ ì‚¬ ê²½í—˜ ë°œê²¬ - ê²½í—˜ ê¸°ë°˜ ì˜ì‚¬ê²°ì •
                reasoning_trace.append("âœ… ìœ ì‚¬ ê²½í—˜ ë°œê²¬ - ê²½í—˜ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì ìš©")
                return experience_result
            else:
                # ê²½í—˜ ì—†ìŒ - ì‚¬ê³ ì‹¤í—˜ ëª¨ë“œë¡œ ì „í™˜
                reasoning_trace.append("ğŸ’­ ìœ ì‚¬ ê²½í—˜ ì—†ìŒ - ì‚¬ê³ ì‹¤í—˜ ëª¨ë“œ í™œì„±í™”")
            
            # 1ë‹¨ê³„: ê¹Šì´ ìˆëŠ” ì‚¬ê³ ì‹¤í—˜ ëª¨ë“œ - ë‹¤ê°ë„ ê´€ì  ë¶„ì„
            reasoning_trace.append("1ë‹¨ê³„: ì‹¬ì¸µ ì‚¬ê³ ì‹¤í—˜ ì‹œì‘ - ë‹¤ê°ë„ ê´€ì  ë¶„ì„")
            stakeholder_perspectives = await self._analyze_stakeholder_perspectives(context, reasoning_trace)
            
            # 2ë‹¨ê³„: ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ íƒêµ¬
            reasoning_trace.append("2ë‹¨ê³„: ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ì‹¬ì¸µ íƒêµ¬")
            counterfactual_scenarios = await self._explore_counterfactual_scenarios(context, reasoning_trace)
            
            # 3ë‹¨ê³„: ë‹¤ì¸µ ê°ì • ë¶„ì„ ë° í†µí•©
            reasoning_trace.append("3ë‹¨ê³„: ì´í•´ê´€ê³„ìë³„ ê°ì • ë¶„ì„ ë° í†µí•©")
            integrated_emotion, emotion_meta = await self._analyze_and_integrate_emotions(
                context, reasoning_trace, stakeholder_perspectives
            )
            
            # 4ë‹¨ê³„: ìœ¤ë¦¬ì  ê°€ì¹˜ ì¶”ë¡  (ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ ë°˜ì˜)
            reasoning_trace.append("4ë‹¨ê³„: ìœ¤ë¦¬ì  ê°€ì¹˜ ì¶”ë¡  (ë°˜ì‚¬ì‹¤ì  ë¶„ì„ ë°˜ì˜)")
            ethical_values = await self._perform_ethical_reasoning(
                integrated_emotion, context, reasoning_trace, counterfactual_scenarios
            )
            
            # 5ë‹¨ê³„: ë²¤ë‹´ ê³„ì‚° (ìœ¤ë¦¬ì  ê°€ì¹˜ ë°˜ì˜)
            reasoning_trace.append("5ë‹¨ê³„: ìœ¤ë¦¬ì  ë²¤ë‹´ ê³„ì‚° ì‹œì‘")
            bentham_result = await self._calculate_ethical_bentham(
                integrated_emotion, ethical_values, context, reasoning_trace
            )
            
            # 6ë‹¨ê³„: í›„íšŒ ì˜ˆì¸¡ ë° í•™ìŠµ í¸í–¥ ì¶”ì¶œ
            reasoning_trace.append("6ë‹¨ê³„: í›„íšŒ ì˜ˆì¸¡ ë° í•™ìŠµ í¸í–¥ ì‹œì‘")
            predicted_regret, learning_insights = await self._predict_regret_and_learning(
                context, bentham_result, reasoning_trace, counterfactual_scenarios
            )
            
            # 7ë‹¨ê³„: ê²°ê³¼ í†µí•© ë° ì‹ ë¢°ë„ ê³„ì‚°
            reasoning_trace.append("7ë‹¨ê³„: ê²°ê³¼ í†µí•© ë° í‰ê°€")
            final_result = self._integrate_final_result(
                integrated_emotion, ethical_values, bentham_result, 
                predicted_regret, emotion_meta, reasoning_trace, start_time
            )
            
            # 6ë‹¨ê³„: í•™ìŠµ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ (ê²°ê³¼ ì €ì¥ ë° ê²½í—˜ ì¶•ì )
            await self._update_learning_memory(context, final_result)
            
            # 7ë‹¨ê³„: ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ë¯¸ë˜ ì°¸ì¡°ìš©)
            await self._store_experience_for_future(context, final_result)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_performance_metrics(final_result, emotion_meta)
            
            return final_result
            
        except Exception as e:
            self.logger.error(f"ìœ¤ë¦¬ì  ì˜ì‚¬ê²°ì • ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ìœ¤ë¦¬ì  ì˜ì‚¬ê²°ì • ì‹œìŠ¤í…œ ì‹¤íŒ¨: {e}. í´ë°± ì—†ì´ ëª…í™•í•œ ì‹¤íŒ¨.")
    
    async def _analyze_and_integrate_emotions(self, 
                                            context: CircuitDecisionContext,
                                            reasoning_trace: List[str]) -> Tuple[EmotionData, Dict]:
        """ë‹¤ì¸µ ê°ì • ë¶„ì„ ë° í†µí•©"""
        
        emotion_meta = {
            'critical_loss_detected': False,
            'emotion_conflict_type': 'none',
            'emotion_sources_used': []
        }
        
        # í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°ì • ë¶„ì„ (ìì•„ ê°ì •ìœ¼ë¡œ ì‚¬ìš©)
        if not context.self_emotion:
            combined_text = f"{context.scenario_text} {context.proposed_action}"
            context.self_emotion = self.emotion_analyzer.analyze_emotion(
                combined_text, language='ko'
            )
            reasoning_trace.append(f"ìì•„ ê°ì • ë¶„ì„ ì™„ë£Œ: {context.self_emotion.primary_emotion.value}")
        
        # ê³µë™ì²´ ê°ì • ì¶”ë¡  (ì‚¬íšŒì  ë§¥ë½ ê¸°ë°˜)
        if not context.community_emotion and context.social_context:
            context.community_emotion = await self._infer_community_emotion(
                context, reasoning_trace
            )
        
        # íƒ€ì ê°ì • ì¶”ë¡  (ì´í•´ê´€ê³„ì ê¸°ë°˜)
        if not context.other_emotion and context.stakeholders:
            context.other_emotion = await self._infer_other_emotion(
                context, reasoning_trace
            )
        
        # ê³ ê¸‰ ê°ì • í†µí•© (ë²¤ë‹´ ê³„ì‚°ê¸°ì˜ ë©”ì„œë“œ í™œìš©)
        integrated_emotion = self.bentham_calculator._integrate_emotion_hierarchy(
            context.community_emotion, context.other_emotion, context.self_emotion
        )
        
        # ë©”íƒ€ ì •ë³´ ìˆ˜ì§‘
        emotion_meta['emotion_sources_used'] = [
            'community' if context.community_emotion else None,
            'other' if context.other_emotion else None,
            'self' if context.self_emotion else None
        ]
        emotion_meta['emotion_sources_used'] = [x for x in emotion_meta['emotion_sources_used'] if x]
        
        # ì¹˜ëª…ì  ì†ì‹¤ íƒì§€
        critical_loss = self.bentham_calculator._detect_critical_emotional_loss(
            context.community_emotion, context.other_emotion, context.self_emotion
        )
        emotion_meta['critical_loss_detected'] = critical_loss['any_critical']
        
        if emotion_meta['critical_loss_detected']:
            reasoning_trace.append("âš ï¸ ì¹˜ëª…ì  ê°ì • ì†ì‹¤ íƒì§€ë¨ - ì†ì‹¤ ì–µì œ ëª¨ë“œ í™œì„±í™”")
            
        reasoning_trace.append(
            f"ê°ì • í†µí•© ì™„ë£Œ: {integrated_emotion.primary_emotion.value} "
            f"(ì¶œì²˜: {', '.join(emotion_meta['emotion_sources_used'])})"
        )
        
        return integrated_emotion, emotion_meta
    
    async def _infer_community_emotion(self, 
                                     context: CircuitDecisionContext,
                                     reasoning_trace: List[str]) -> EmotionData:
        """ì‚¬íšŒì  ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ ê³µë™ì²´ ê°ì • ì¶”ë¡ """
        
        # ì‚¬íšŒì  ë§¥ë½ í‚¤ì›Œë“œ ë¶„ì„
        social_keywords = context.social_context.get('keywords', [])
        impact_scope = context.social_context.get('impact_scope', 'individual')
        
        # ê³µë™ì²´ ê´€ì‹¬ì‚¬ ë§¤í•‘
        community_concern_mapping = {
            'safety': EmotionState.FEAR,
            'injustice': EmotionState.ANGER, 
            'loss': EmotionState.SADNESS,
            'celebration': EmotionState.JOY,
            'uncertainty': EmotionState.FEAR,
            'achievement': EmotionState.JOY
        }
        
        # ê¸°ë³¸ ê³µë™ì²´ ê°ì •
        community_emotion = EmotionState.NEUTRAL
        intensity = EmotionIntensity.MODERATE
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ì¶”ë¡ 
        for keyword in social_keywords:
            if keyword in community_concern_mapping:
                community_emotion = community_concern_mapping[keyword]
                break
        
        # ì˜í–¥ ë²”ìœ„ì— ë”°ë¥¸ ê°•ë„ ì¡°ì •
        if impact_scope in ['society', 'national']:
            if community_emotion in [EmotionState.FEAR, EmotionState.SADNESS]:
                intensity = EmotionIntensity.VERY_STRONG
            elif community_emotion == EmotionState.JOY:
                intensity = EmotionIntensity.STRONG
        
        reasoning_trace.append(f"ê³µë™ì²´ ê°ì • ì¶”ë¡ : {community_emotion.value} (ë²”ìœ„: {impact_scope})")
        
        return EmotionData(
            primary_emotion=community_emotion,
            intensity=intensity,
            confidence=0.7,
            language='ko',
            processing_method='community_inference',
            dominance=0.5
        )
    
    async def _infer_other_emotion(self, 
                                 context: CircuitDecisionContext,
                                 reasoning_trace: List[str]) -> EmotionData:
        """ì´í•´ê´€ê³„ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ íƒ€ì ê°ì • ì¶”ë¡ """
        
        # ì´í•´ê´€ê³„ì ìœ í˜•ë³„ ê°ì • íŒ¨í„´
        stakeholder_emotion_patterns = {
            'vulnerable': EmotionState.FEAR,      # ì·¨ì•½ê³„ì¸µ
            'affected': EmotionState.SADNESS,     # ì§ì ‘ ì˜í–¥ë°›ëŠ” ì‚¬ëŒë“¤
            'beneficiary': EmotionState.JOY,      # ìˆ˜í˜œì
            'competitor': EmotionState.ANGER,     # ê²½ìŸì
            'observer': EmotionState.NEUTRAL      # ê´€ì°°ì
        }
        
        # ì£¼ìš” ì´í•´ê´€ê³„ì ì‹ë³„
        primary_stakeholders = context.stakeholders[:3]  # ìƒìœ„ 3ê°œë§Œ
        
        # ê°€ì¥ ì·¨ì•½í•˜ê±°ë‚˜ ì˜í–¥ì„ ë§ì´ ë°›ëŠ” ì§‘ë‹¨ì˜ ê°ì •ì„ ìš°ì„ 
        other_emotion = EmotionState.NEUTRAL
        intensity = EmotionIntensity.MODERATE
        
        for stakeholder in primary_stakeholders:
            if 'vulnerable' in stakeholder.lower() or 'victim' in stakeholder.lower():
                other_emotion = EmotionState.FEAR
                intensity = EmotionIntensity.STRONG
                break
            elif 'affected' in stakeholder.lower() or 'impact' in stakeholder.lower():
                other_emotion = EmotionState.SADNESS
                intensity = EmotionIntensity.MODERATE
            elif 'benefit' in stakeholder.lower():
                other_emotion = EmotionState.JOY
                intensity = EmotionIntensity.MODERATE
        
        reasoning_trace.append(f"íƒ€ì ê°ì • ì¶”ë¡ : {other_emotion.value} (ì£¼ìš” ì´í•´ê´€ê³„ì: {primary_stakeholders[0] if primary_stakeholders else 'None'})")
        
        return EmotionData(
            primary_emotion=other_emotion,
            intensity=intensity,
            confidence=0.6,
            language='ko',
            processing_method='stakeholder_inference',
            dominance=0.5
        )
    
    async def _perform_ethical_reasoning(self, 
                                       integrated_emotion: EmotionData,
                                       context: CircuitDecisionContext,
                                       reasoning_trace: List[str]) -> Dict[str, float]:
        """ê°ì •ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ìœ¤ë¦¬ì  ê°€ì¹˜ ì¶”ë¡ """
        
        # ë²¤ë‹´ ê³„ì‚°ê¸°ì˜ ìœ¤ë¦¬ì  ì¶”ë¡  í™œìš©
        ethical_values = self.bentham_calculator._perform_ethical_reasoning(
            integrated_emotion, {'text_description': context.scenario_text}
        )
        
        # ì‹œê¸‰ì„±ì— ë”°ë¥¸ ì¡°ì •
        if context.temporal_urgency > 0.8:
            ethical_values['care_harm'] += 0.1  # ê¸´ê¸‰í•  ë•Œ ì•ˆì „ ìš°ì„ 
            ethical_values['authority'] += 0.05  # ì‹ ì†í•œ ê²°ì • í•„ìš”
        
        # ì´í•´ê´€ê³„ì ìˆ˜ì— ë”°ë¥¸ ê³µì •ì„± ì¡°ì •
        if context.stakeholders and len(context.stakeholders) > 5:
            ethical_values['fairness'] += 0.1  # ë§ì€ ì´í•´ê´€ê³„ì ì‹œ ê³µì •ì„± ì¤‘ì‹œ
        
        # ê·¹ë‹¨ê°’ ë°©ì§€
        for key in ethical_values:
            ethical_values[key] = max(0.1, min(0.9, ethical_values[key]))
        
        reasoning_trace.append(
            f"ìœ¤ë¦¬ì  ê°€ì¹˜ ì¶”ë¡  ì™„ë£Œ: "
            f"ëŒë´„={ethical_values['care_harm']:.2f}, "
            f"ê³µì •ì„±={ethical_values['fairness']:.2f}, "
            f"ì¶©ì„±={ethical_values['loyalty']:.2f}"
        )
        
        return ethical_values
    
    async def _calculate_ethical_bentham(self, 
                                       integrated_emotion: EmotionData,
                                       ethical_values: Dict[str, float],
                                       context: CircuitDecisionContext,
                                       reasoning_trace: List[str]) -> Any:
        """ìœ¤ë¦¬ì  ê°€ì¹˜ë¥¼ ë°˜ì˜í•œ ë²¤ë‹´ ê³„ì‚°"""
        
        # ê³¼ê±° í›„íšŒ ë©”ëª¨ë¦¬ì—ì„œ í•™ìŠµ í¸í–¥ ì¶”ì¶œ
        past_regret_memory = context.past_regret_memory or {}
        
        # ë²¤ë‹´ ê³„ì‚° ì…ë ¥ ë°ì´í„° êµ¬ì„±
        bentham_input = {
            'input_values': {
                'intensity': 0.7,  # ê¸°ë³¸ê°’ë“¤
                'duration': 0.6,
                'certainty': 0.8,
                'propinquity': 0.7,
                'fecundity': 0.5,
                'purity': 0.6,
                'extent': min(1.0, len(context.stakeholders) / 10.0) if context.stakeholders else 0.5
            },
            'text_description': f"{context.scenario_text} {context.proposed_action}",
            'language': 'ko',
            'affected_count': len(context.stakeholders) if context.stakeholders else 1,
            'duration_seconds': 3600 * (1 + context.temporal_urgency),  # ì‹œê¸‰ì„± ë°˜ì˜
            'information_quality': 0.8,
            'uncertainty_level': 1.0 - integrated_emotion.confidence,
            'social_context': context.social_context or {}
        }
        
        # ìœ¤ë¦¬ì  ë²¤ë‹´ ê³„ì‚° ì‹¤í–‰
        bentham_result = self.bentham_calculator.calculate_with_ethical_reasoning(
            bentham_input,
            community_emotion=context.community_emotion,
            other_emotion=context.other_emotion,
            self_emotion=context.self_emotion,
            past_regret_memory=past_regret_memory
        )
        
        reasoning_trace.append(
            f"ë²¤ë‹´ ê³„ì‚° ì™„ë£Œ: ìµœì¢…ì ìˆ˜={bentham_result.final_score:.3f}, "
            f"ê¸°ë³¸ì ìˆ˜={bentham_result.base_score:.3f}, "
            f"ì‹ ë¢°ë„={bentham_result.confidence_score:.3f}"
        )
        
        return bentham_result
    
    async def _predict_regret_and_learning(self, 
                                         context: CircuitDecisionContext,
                                         bentham_result: Any,
                                         reasoning_trace: List[str]) -> Tuple[Dict[str, float], Dict[str, Any]]:
        """í›„íšŒ ì˜ˆì¸¡ ë° í•™ìŠµ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        
        # í›„íšŒ ë¶„ì„ ì…ë ¥ êµ¬ì„±
        regret_input = {
            'id': f"decision_{int(time.time())}",
            'scenario': context.scenario_text,
            'action': context.proposed_action,
            'context': {
                'bentham_score': bentham_result.final_score,
                'confidence': bentham_result.confidence_score,
                'stakeholders': context.stakeholders or [],
                'temporal_urgency': context.temporal_urgency
            }
        }
        
        # outcome_data êµ¬ì„± (ë²¤ë‹´ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœí™”)
        outcome_data = {
            'utility_score': bentham_result.final_score,  # ë²¤ë‹´ ì ìˆ˜ë¥¼ ìœ í‹¸ë¦¬í‹°ë¡œ ì‚¬ìš©
            'satisfaction': bentham_result.confidence_score,  # ì‹ ë¢°ë„ë¥¼ ë§Œì¡±ë„ë¡œ ì‚¬ìš©
            'success_rating': bentham_result.confidence_score,  # ë²¤ë‹´ ì‹ ë¢°ë„ë¥¼ ì„±ê³µë„ë¡œ ì‚¬ìš©
            'emotional_impact': 0.5,  # ê¸°ë³¸ê°’
            'decision_timestamp': time.time(),
            'stakeholder_count': len(context.stakeholders) if context.stakeholders else 1
        }
        
        # í›„íšŒ ë¶„ì„ ì‹¤í–‰ (outcome_dataì™€ í•¨ê»˜)
        regret_metrics = await self.regret_analyzer.analyze_regret(regret_input, outcome_data)
        
        # ì˜ˆì¸¡ëœ í›„íšŒ ì •ë³´
        predicted_regret = {
            'anticipated_regret': regret_metrics.anticipated_regret,
            'regret_intensity': regret_metrics.regret_intensity,
            'regret_duration': regret_metrics.regret_duration,
            'confidence': regret_metrics.model_confidence
        }
        
        # í•™ìŠµ ì¸ì‚¬ì´íŠ¸
        learning_insights = {
            'risk_aversion_tendency': regret_metrics.anticipated_regret * 0.5,
            'decision_pattern_match': self._find_similar_decisions(context),
            'improvement_suggestions': self._generate_improvement_suggestions(regret_metrics)
        }
        
        reasoning_trace.append(
            f"í›„íšŒ ì˜ˆì¸¡ ì™„ë£Œ: ì˜ˆìƒí›„íšŒ={predicted_regret['anticipated_regret']:.3f}, "
            f"í›„íšŒê°•ë„={predicted_regret['regret_intensity']:.3f}"
        )
        
        return predicted_regret, learning_insights
    
    def _integrate_final_result(self, 
                              integrated_emotion: EmotionData,
                              ethical_values: Dict[str, float],
                              bentham_result: Any,
                              predicted_regret: Dict[str, float],
                              emotion_meta: Dict,
                              reasoning_trace: List[str],
                              start_time: float) -> CircuitDecisionResult:
        """ìµœì¢… ê²°ê³¼ í†µí•©"""
        
        # ìµœì¢… ìœ¤ë¦¬ì  ì ìˆ˜ ê³„ì‚°
        final_ethical_score = bentham_result.final_score
        
        # í›„íšŒë¥¼ ê³ ë ¤í•œ ì‹ ë¢°ë„ ì¡°ì •
        regret_adjusted_confidence = bentham_result.confidence_score * (1 - predicted_regret['anticipated_regret'] * 0.3)
        
        # ê°ì • ì¶©ëŒ ìœ í˜• ê²°ì •
        emotion_conflict_type = 'none'
        if len(emotion_meta['emotion_sources_used']) > 1:
            if emotion_meta['critical_loss_detected']:
                emotion_conflict_type = 'critical_loss_resolved'
            else:
                emotion_conflict_type = 'standard_integration'
        
        processing_time = time.time() - start_time
        
        reasoning_trace.append(f"ìµœì¢… í†µí•© ì™„ë£Œ: ì²˜ë¦¬ì‹œê°„={processing_time:.3f}ì´ˆ")
        
        return CircuitDecisionResult(
            final_ethical_score=final_ethical_score,
            confidence=regret_adjusted_confidence,
            integrated_emotion=integrated_emotion,
            ethical_values=ethical_values,
            bentham_result=bentham_result,
            predicted_regret=predicted_regret,
            critical_loss_detected=emotion_meta['critical_loss_detected'],
            emotion_conflict_resolved=emotion_conflict_type,
            reasoning_trace=reasoning_trace,
            processing_time=processing_time
        )
    
    def _find_similar_decisions(self, context: CircuitDecisionContext) -> float:
        """ìœ ì‚¬í•œ ê³¼ê±° ê²°ì • ì°¾ê¸°"""
        # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ìœ ì‚¬ë„ ê³„ì‚° í•„ìš”
        if context.similar_decisions_history:
            return len(context.similar_decisions_history) / 10.0
        return 0.0
    
    def _generate_improvement_suggestions(self, regret_metrics) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        if regret_metrics.uncertainty_estimate > 0.7:
            suggestions.append("ë” ë§ì€ ì •ë³´ ìˆ˜ì§‘ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        if regret_metrics.anticipated_regret > 0.6:
            suggestions.append("ëŒ€ì•ˆ ì˜µì…˜ì„ ë” íƒìƒ‰í•´ë³´ì„¸ìš”")
        
        if regret_metrics.model_confidence < 0.5:
            suggestions.append("ì „ë¬¸ê°€ ì˜ê²¬ì„ êµ¬í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”")
        
        return suggestions
    
    async def _update_learning_memory(self, 
                                    context: CircuitDecisionContext,
                                    result: CircuitDecisionResult):
        """í•™ìŠµ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸"""
        
        # ì˜ì‚¬ê²°ì • ê¸°ë¡ ì €ì¥
        decision_record = {
            'timestamp': time.time(),
            'context': context,
            'result': result,
            'ethical_score': result.final_ethical_score,
            'predicted_regret': result.predicted_regret['anticipated_regret']
        }
        
        self.decision_history.append(decision_record)
        
        # ìµœê·¼ 100ê°œ ê²°ì •ë§Œ ìœ ì§€
        if len(self.decision_history) > 100:
            self.decision_history = self.decision_history[-100:]
    
    async def _store_experience_for_future(self, 
                                         context: CircuitDecisionContext,
                                         result: CircuitDecisionResult):
        """ë¯¸ë˜ ì°¸ì¡°ìš© ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
        
        if not self.experience_enabled:
            return
        
        try:
            from advanced_experience_database import AdvancedExperience
            
            # ê²½í—˜ ë°ì´í„° êµ¬ì„±
            experience_text = f"{context.scenario_text} {context.proposed_action}"
            
            experience = AdvancedExperience(
                text=experience_text,
                category="ethical_decision",
                metadata={
                    'ethical_score': result.final_ethical_score,
                    'regret_score': result.predicted_regret.get('anticipated_regret', 0.0),
                    'confidence': result.confidence,
                    'stakeholder_count': len(context.stakeholders) if context.stakeholders else 1,
                    'temporal_urgency': context.temporal_urgency,
                    'emotion_type': result.integrated_emotion.primary_emotion.value,
                    'processing_time': result.processing_time,
                    'reasoning_steps': len(result.reasoning_trace)
                },
                importance_score=result.final_ethical_score * result.confidence  # ìœ¤ë¦¬ì ìˆ˜ Ã— ì‹ ë¢°ë„
            )
            
            # ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            await self.experience_db.add_experience(experience)
            
            self.logger.debug(f"ê²½í—˜ ì €ì¥ ì™„ë£Œ: ìœ¤ë¦¬ì ìˆ˜={result.final_ethical_score:.3f}")
            
        except Exception as e:
            self.logger.warning(f"ê²½í—˜ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # íŒ¨í„´ í•™ìŠµ (ê°„ë‹¨í•œ ë²„ì „)
        if result.final_ethical_score > 0.7:
            emotion_key = result.integrated_emotion.primary_emotion.value
            if emotion_key not in self.learning_memory['successful_decisions']:
                self.learning_memory['successful_decisions'][emotion_key] = []
            self.learning_memory['successful_decisions'][emotion_key].append(result.final_ethical_score)
    
    def _update_performance_metrics(self, 
                                  result: CircuitDecisionResult,
                                  emotion_meta: Dict):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        
        self.performance_metrics['total_decisions'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total = self.performance_metrics['total_decisions']
        old_avg = self.performance_metrics['average_processing_time']
        new_avg = (old_avg * (total - 1) + result.processing_time) / total
        self.performance_metrics['average_processing_time'] = new_avg
        
        # ê°ì • ì¶©ëŒë¥  ì—…ë°ì´íŠ¸
        if result.emotion_conflict_resolved != 'none':
            conflict_count = self.performance_metrics['emotion_conflict_rate'] * (total - 1) + 1
            self.performance_metrics['emotion_conflict_rate'] = conflict_count / total
        
        # ì¹˜ëª…ì  ì†ì‹¤ë¥  ì—…ë°ì´íŠ¸
        if result.critical_loss_detected:
            loss_count = self.performance_metrics['critical_loss_rate'] * (total - 1) + 1
            self.performance_metrics['critical_loss_rate'] = loss_count / total
    
    

    def get_circuit_status(self) -> Dict[str, Any]:
        """íšŒë¡œ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            'performance_metrics': self.performance_metrics,
            'learning_memory_size': {
                'decision_history': len(self.decision_history),
                'successful_patterns': len(self.learning_memory['successful_decisions']),
                'regret_patterns': len(self.learning_memory['regret_patterns'])
            },
            'recent_decisions': len([d for d in self.decision_history if time.time() - d['timestamp'] < 3600])
        }


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_emotion_ethics_regret_circuit():
    """ê°ì •-ìœ¤ë¦¬-í›„íšŒ íšŒë¡œ í…ŒìŠ¤íŠ¸"""
    
    circuit = EmotionEthicsRegretCircuit()
    
    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
    test_context = CircuitDecisionContext(
        scenario_text="ì§€ì—­ ê³µì›ì„ ê°œë°œí•´ì„œ ìƒì—…ì‹œì„¤ì„ ì§“ëŠ” ê²ƒì„ ê³ ë ¤í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ê²½ì œì  ì´ìµì„ ê°€ì ¸ë‹¤ì£¼ì§€ë§Œ í™˜ê²½ê³¼ ì£¼ë¯¼ë“¤ì˜ íœ´ì‹ê³µê°„ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        proposed_action="ê³µì› ì¼ë¶€ë¥¼ ìƒì—…ì‹œì„¤ë¡œ ê°œë°œí•œë‹¤",
        stakeholders=["ì§€ì—­ì£¼ë¯¼", "í™˜ê²½ë³´í˜¸ë‹¨ì²´", "ê°œë°œì—…ì²´", "ì§€ë°©ì •ë¶€", "ë¯¸ë˜ì„¸ëŒ€"],
        social_context={
            'impact_scope': 'community',
            'keywords': ['development', 'environment', 'economic']
        },
        temporal_urgency=0.6,
        past_regret_memory={'average_regret': 0.4}
    )
    
    print("=== ê°ì •-ìœ¤ë¦¬-í›„íšŒ ì‚¼ê° íšŒë¡œ í…ŒìŠ¤íŠ¸ ===")
    
    # íšŒë¡œ ì²˜ë¦¬ ì‹¤í–‰
    result = await circuit.process_ethical_decision(test_context)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"- ìœ¤ë¦¬ì  ì ìˆ˜: {result.final_ethical_score:.3f}")
    print(f"- ì‹ ë¢°ë„: {result.confidence:.3f}")
    print(f"- ì²˜ë¦¬ ì‹œê°„: {result.processing_time:.3f}ì´ˆ")
    print(f"- ì¹˜ëª…ì  ì†ì‹¤ íƒì§€: {'ì˜ˆ' if result.critical_loss_detected else 'ì•„ë‹ˆì˜¤'}")
    
    print(f"\nğŸ­ í†µí•©ëœ ê°ì •:")
    print(f"- ì£¼ìš” ê°ì •: {result.integrated_emotion.primary_emotion.value}")
    print(f"- ê°•ë„: {result.integrated_emotion.intensity.value}")
    print(f"- ì‹ ë¢°ë„: {result.integrated_emotion.confidence:.3f}")
    
    print(f"\nâš–ï¸ ìœ¤ë¦¬ì  ê°€ì¹˜:")
    for key, value in result.ethical_values.items():
        print(f"- {key}: {value:.3f}")
    
    print(f"\nğŸ˜” ì˜ˆì¸¡ëœ í›„íšŒ:")
    for key, value in result.predicted_regret.items():
        print(f"- {key}: {value:.3f}")
    
    print(f"\nğŸ” ì¶”ë¡  ê³¼ì •:")
    for i, trace in enumerate(result.reasoning_trace, 1):
        print(f"{i}. {trace}")
    
    # íšŒë¡œ ìƒíƒœ í™•ì¸
    status = circuit.get_circuit_status()
    print(f"\nğŸ“ˆ íšŒë¡œ ìƒíƒœ:")
    print(f"- ì´ ê²°ì • ìˆ˜: {status['performance_metrics']['total_decisions']}")
    print(f"- í‰ê·  ì²˜ë¦¬ ì‹œê°„: {status['performance_metrics']['average_processing_time']:.3f}ì´ˆ")
    
    return result

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_emotion_ethics_regret_circuit())
