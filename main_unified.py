#!/usr/bin/env python3
"""
Red Heart AI - í†µí•© ì¶”ë¡  ì‹œìŠ¤í…œ (Unified Inference System)
50 epochìœ¼ë¡œ í•™ìŠµëœ 730M ëª¨ë¸ ì „ì²´ í™œìš©
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
import asyncio
import argparse
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
import glob
import warnings
warnings.filterwarnings('ignore')

# ì •ë°€ ê°ì •-ë²¤ë‹´ ë§¤í•‘ ì‹œìŠ¤í…œ - í•„ìˆ˜ ì˜ì¡´ì„±
from semantic_emotion_bentham_mapper import (
    SemanticEmotionBenthamMapper,
    NeuralEmotionBenthamAdapter,
    create_precision_mapper
)

# 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ
from three_view_scenario_system import (
    ThreeViewScenarioSystem,
    ScenarioType,
    ThreeViewAnalysisResult
)

# ë‹¤ì›ì  ìœ¤ë¦¬ ì²´ê³„
from deep_multi_dimensional_ethics_system import (
    DeepMultiDimensionalEthicsSystem,
    EthicsSchool,
    EthicalDilemma,
    StakeholderPerspective,
    UtilitarianEngine,
    DeontologicalEngine,
    VirtueEthicsEngine,
    CareEthicsEngine,
    JusticeTheoryEngine  # MD ë¬¸ì„œ Bì•ˆ: 5ë²ˆì§¸ ìœ¤ë¦¬ ì‹œìŠ¤í…œ
)

# ë©”ëª¨ë¦¬ ìŠ¤ì™‘ ë§¤ë‹ˆì €
from memory_swap_manager import SystemSwapManager, SystemType

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'training'))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('RedHeart.MainUnified')

# GPU/CPU ë””ë°”ì´ìŠ¤ ì„¤ì •
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
logger.info(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {DEVICE}")
if DEVICE.type == 'cuda':
    logger.info(f"   GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")


class MemoryMode(Enum):
    """ë©”ëª¨ë¦¬ ëª¨ë“œ - MD ë¬¸ì„œ ì‚¬ì–‘ ì¤€ìˆ˜"""
    LIGHT = "light"        # 230M - ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
    MEDIUM = "medium"      # 600M - ê· í˜•ì¡íŒ ì¼ë°˜ ì‚¬ìš© (ì¬ì„¤ê³„ë¨)
    HEAVY = "heavy"        # 970M - ì‹¬ì¸µ ë¶„ì„ (ë™ì  ìŠ¤ì™‘)
    MCP = "mcp"           # MCP ì„œë²„ ëª¨ë“œ (HEAVY ê¸°ë°˜)
    
    # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
    MINIMAL = "minimal"    # 90M (êµ¬ë²„ì „)
    NORMAL = "normal"      # 400M (êµ¬ë²„ì „)
    ULTRA = "ultra"        # 842M (êµ¬ë²„ì „)
    EXTREME = "extreme"    # 922M (êµ¬ë²„ì „)


def auto_select_memory_mode(gpu_memory_mb: int = None, batch_size: int = 1) -> MemoryMode:
    """GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìë™ ëª¨ë“œ ì„ íƒ"""
    if gpu_memory_mb is None and torch.cuda.is_available():
        gpu_memory_mb = torch.cuda.get_device_properties(0).total_memory // (1024**2)
    
    effective_memory = gpu_memory_mb - (batch_size * 500) if gpu_memory_mb else 4000
    
    if effective_memory < 3000:
        return MemoryMode.MINIMAL
    elif effective_memory < 4000:
        return MemoryMode.LIGHT
    elif effective_memory < 5000:
        return MemoryMode.NORMAL
    elif effective_memory < 6000:
        return MemoryMode.HEAVY
    elif effective_memory < 7000:
        return MemoryMode.ULTRA
    else:
        return MemoryMode.EXTREME


@dataclass
class InferenceConfig:
    """í†µí•© ì¶”ë¡  ì„¤ì •"""
    # ì²´í¬í¬ì¸íŠ¸ ì„¤ì • - ì—í­ ë²ˆí˜¸ë¡œ ìë™ ê²€ìƒ‰
    checkpoint_epoch: int = 50  # ìµœì  ì—í­ (sweet spot ë¶„ì„ ê²°ê³¼)
    checkpoint_path: Optional[str] = None  # ì§ì ‘ ê²½ë¡œ ì§€ì • (ìš°ì„ ìˆœìœ„)
    checkpoint_dir: str = "training/checkpoints_final"
    
    # ëª¨ë¸ ì„¤ì •
    device: str = str(DEVICE)
    batch_size: int = 4
    max_seq_length: int = 512
    
    # ë©”ëª¨ë¦¬ ëª¨ë“œ ì„¤ì •
    memory_mode: MemoryMode = MemoryMode.NORMAL  # ê¸°ë³¸ê°’
    auto_memory_mode: bool = True  # ìë™ ëª¨ë“œ ì„ íƒ
    
    # ëª¨ë“ˆ í™œì„±í™” í”Œë˜ê·¸ (ë©”ëª¨ë¦¬ ëª¨ë“œì— ë”°ë¼ ìë™ ì¡°ì •)
    use_neural_analyzers: bool = True  # 368M (HEAVY+)
    use_advanced_wrappers: bool = True  # 112M (ULTRA+)
    use_dsp_simulator: bool = True  # 14M (NORMAL+)
    use_kalman_filter: bool = True  # 2.3M (NORMAL+)
    use_phase_networks: bool = True  # 4.3M (NORMAL+)
    use_regret_circuit: bool = True  # Regret Head í™œì„±í™”
    
    # ìƒˆë¡œìš´ ëª¨ë“ˆ í”Œë˜ê·¸ (EXTREME ëª¨ë“œì—ì„œ í™œì„±í™”)
    use_meta_integration: bool = True  # 40M - ë©”íƒ€ í†µí•© ì‹œìŠ¤í…œ
    use_counterfactual_reasoning: bool = True  # 15M - ë°˜ì‚¬ì‹¤ ì¶”ë¡ 
    use_advanced_regret_learning: bool = True  # 20M - ê³ ê¸‰ í›„íšŒ í•™ìŠµ
    use_workflow_memory_manager: bool = True  # 5M - ì›Œí¬í”Œë¡œìš° ë©”ëª¨ë¦¬ ê´€ë¦¬
    use_temporal_propagation: bool = True  # ì‹œê³„ì—´ ì „íŒŒ ë¶„ì„
    use_experience_database: bool = True  # ê²½í—˜ DB ì—°ë™
    use_emotion_hierarchy: bool = True  # ê³„ì¸µì  ê°ì • ì²˜ë¦¬ (ê³µë™ì²´>íƒ€ì>ìì•„)
    use_three_view_scenario: bool = True  # 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ (ë‚™ê´€/ì¤‘ë„/ë¹„ê´€)
    use_multi_ethics_system: bool = True  # ë‹¤ì›ì  ìœ¤ë¦¬ ì²´ê³„ (5ê°œ í•™íŒŒ)
    
    # LLM í†µí•© ì˜µì…˜
    llm_mode: str = "none"  # "none", "local", "claude", "mcp"
    llm_model_path: str = "llm_module/HelpingAI2-9B.Q4_K_M.gguf"
    
    # ì„ë² ë”© ì„¤ì •
    embedding_model: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"  # ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©
    use_cached_embeddings: bool = True
    
    # ë²ˆì—­ ì„¤ì •
    use_translator: bool = False  # í•œêµ­ì–´ ê°ì§€ ì‹œ í™œì„±í™”
    translator_model: str = "facebook/m2m100_418M"
    
    # ì„±ëŠ¥ ì„¤ì •
    enable_xai: bool = False
    enable_monitoring: bool = True
    cache_size: int = 100
    
    # ë¡œê¹…
    verbose: bool = True
    debug: bool = False


class UnifiedInferenceSystem:
    """í†µí•© ì¶”ë¡  ì‹œìŠ¤í…œ - 730M~922M ëª¨ë¸ ì „ì²´ í™œìš©"""
    
    def __init__(self, config: InferenceConfig):
        self.config = config
        self.logger = logger
        
        # ë©”ëª¨ë¦¬ ëª¨ë“œ ìë™ ì„¤ì •
        if config.auto_memory_mode:
            self.config.memory_mode = auto_select_memory_mode(batch_size=config.batch_size)
            self.logger.info(f"ğŸ›ï¸ ìë™ ë©”ëª¨ë¦¬ ëª¨ë“œ ì„ íƒ: {self.config.memory_mode.value}")
            self._adjust_modules_by_memory_mode()
        
        # ê¸°ë³¸ ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ë“¤ (730M)
        self.unified_model = None
        self.neural_analyzers = None
        self.advanced_wrappers = None
        self.dsp_simulator = None
        self.kalman_filter = None
        self.phase_networks = None
        
        # ìƒˆë¡œìš´ í†µí•© ëª¨ë“ˆë“¤ (192M) - EXTREME ëª¨ë“œ
        self.meta_integration = None  # 40M
        self.counterfactual_reasoning = None  # 15M
        self.advanced_regret_learning = None  # 20M
        self.workflow_memory_manager = None  # 5M
        self.temporal_propagator = None  # ì‹œê³„ì—´ ì „íŒŒ
        self.experience_database = None  # ê²½í—˜ DB
        self.emotion_hierarchy_processor = None  # ê³„ì¸µì  ê°ì •
        
        # ì •ë°€ ê°ì •â†’ë²¤ë‹´ ë§¤í¼
        self.emotion_bentham_mapper = None  # ì˜ë¯¸ë¡ ì  ë§¤í¼
        self.neural_emotion_adapter = None  # ì‹ ê²½ë§ ì–´ëŒ‘í„° (EXTREME ëª¨ë“œ)
        
        # 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ
        self.three_view_system = None
        
        # ë‹¤ì›ì  ìœ¤ë¦¬ ì²´ê³„
        self.multi_ethics_system = None
        self.ethics_engines = {}  # ê°œë³„ ìœ¤ë¦¬ ì—”ì§„ë“¤
        
        # ìœ íœ´ í•™ìŠµ ì‹œìŠ¤í…œ
        self.idle_learner = None
        
        # LLM ê´€ë ¨
        self.llm_engine = None
        self.translator = None
        
        # ë©”ëª¨ë¦¬ ìŠ¤ì™‘ ë§¤ë‹ˆì €
        self.swap_manager = None
        
        # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì €
        self.checkpoint_manager = None
        
        # ìºì‹œ
        self.cache = {}
        
        # í†µê³„
        self.stats = {
            'total_requests': 0,
            'successful': 0,
            'failed': 0,
            'avg_time': 0.0
        }
        
        self.logger.info("âœ¨ Red Heart AI í†µí•© ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    
    def _adjust_modules_by_memory_mode(self):
        """ë©”ëª¨ë¦¬ ëª¨ë“œì— ë”°ë¥¸ ëª¨ë“ˆ í™œì„±í™” ì¡°ì • - MD ë¬¸ì„œ ì‚¬ì–‘ ì¤€ìˆ˜"""
        mode = self.config.memory_mode
        
        if mode == MemoryMode.LIGHT:
            # LIGHT ëª¨ë“œ (230M) - MD ë¬¸ì„œ ì‚¬ì–‘
            # ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ë§Œ
            self.config.use_neural_analyzers = False
            self.config.use_advanced_wrappers = False
            self.config.use_dsp_simulator = False
            self.config.use_kalman_filter = False
            self.config.use_phase_networks = False
            self.config.use_meta_integration = False
            self.config.use_counterfactual_reasoning = False
            self.config.use_advanced_regret_learning = False
            self.config.use_workflow_memory_manager = True  # ë©”ëª¨ë¦¬ ê´€ë¦¬ëŠ” LIGHTì—ì„œë„ ì¤‘ìš”
            self.config.use_temporal_propagation = False
            self.config.use_experience_database = False
            self.config.use_emotion_hierarchy = False
            self.config.use_three_view_scenario = False
            self.config.use_multi_ethics_system = False  # ê³µë¦¬ì£¼ì˜ë§Œ
            
        elif mode == MemoryMode.MEDIUM:
            # MEDIUM ëª¨ë“œ (600M) - MD ë¬¸ì„œ ì¬ì„¤ê³„ ì‚¬ì–‘
            # Neural Analyzers ì„ ë³„ (194M)
            self.config.use_neural_analyzers = True  # emotion/benthamë§Œ
            self.config.neural_analyzers_subset = ['emotion', 'bentham']  # ë¶€ë¶„ ë¡œë“œ
            # Advanced Wrappers ì„ ë³„ (56M)
            self.config.use_advanced_wrappers = True  # emotion/benthamë§Œ
            self.config.advanced_wrappers_subset = ['advanced_emotion', 'advanced_bentham']  # ë¶€ë¶„ ë¡œë“œ - ì •í™•í•œ í‚¤ ì‚¬ìš©
            # DSP/Kalman (14M)
            self.config.use_dsp_simulator = True
            self.config.use_kalman_filter = True
            # LLMì„ ì´ˆê¸°ì— RAMìœ¼ë¡œ ë³´ë‚´ê¸°
            self.config.llm_start_in_ram = True
            # í•µì‹¬ í†µí•© ëª¨ë“ˆ (80M)
            self.config.use_three_view_scenario = True  # 20M
            self.config.use_multi_ethics_system = True  # 30M (3ê°œ í•™íŒŒë§Œ)
            self.config.use_temporal_propagation = True  # 15M
            self.config.use_meta_integration = True  # 15M (ê¸°ë³¸ ë²„ì „)
            # ì„ íƒì§€ ìƒì„±/í‰ê°€ë¥¼ ìœ„í•œ ëª¨ë“ˆ í™œì„±í™”
            self.config.use_phase_networks = True  # Phase Networks í™œì„±í™” (íƒ€ì-ìì•„-ê³µë™ì²´ ê°ì •)
            self.config.use_counterfactual_reasoning = True  # ë°˜ì‚¬ì‹¤ ì¶”ë¡  í™œì„±í™” (ì„ íƒì§€ ìƒì„±)
            self.config.use_advanced_regret_learning = True  # ëŒ€ì•ˆ ì œì‹œ í™œì„±í™” (suggest_alternatives)
            self.config.use_workflow_memory_manager = True  # ë©”ëª¨ë¦¬ ê´€ë¦¬ í™œì„±í™”
            self.config.use_experience_database = False  # ê²½í—˜ DB ë¹„í™œì„±í™” (MVP í…ŒìŠ¤íŠ¸ìš©)
            self.config.use_emotion_hierarchy = True  # ê³„ì¸µì  ê°ì • ì²˜ë¦¬
            # API ëª¨ë“œì—ì„œëŠ” ë²ˆì—­ê¸° ë¶ˆí•„ìš” (APIê°€ í•œêµ­ì–´ ì§ì ‘ ì²˜ë¦¬)
            self.config.use_translator = False  # ì´ˆê¸°ì—” ë¹„í™œì„±í™”, ë¡œì»¬ ëª¨ë“œì—ì„œë§Œ í™œì„±í™”
            
        elif mode == MemoryMode.HEAVY or mode == MemoryMode.MCP:
            # HEAVY ëª¨ë“œ (970M) - MD ë¬¸ì„œ ì‚¬ì–‘
            # ëª¨ë“  ëª¨ë“ˆ í™œì„±í™”
            self.config.use_neural_analyzers = True
            self.config.use_advanced_wrappers = True
            self.config.use_dsp_simulator = True
            self.config.use_kalman_filter = True
            self.config.use_phase_networks = True
            self.config.use_meta_integration = True
            self.config.use_counterfactual_reasoning = True
            self.config.use_advanced_regret_learning = True
            self.config.use_three_view_scenario = True
            self.config.use_multi_ethics_system = True  # 5ê°œ í•™íŒŒ
            self.config.use_temporal_propagation = True
            self.config.use_workflow_memory_manager = True
            self.config.use_experience_database = True
            self.config.use_emotion_hierarchy = True
            
        # êµ¬ë²„ì „ í˜¸í™˜ì„±
        elif mode == MemoryMode.MINIMAL:
            # ìµœì†Œ ëª¨ë“œ (90M)
            self._set_all_modules_false()
        elif mode == MemoryMode.NORMAL:
            # êµ¬ë²„ì „ NORMAL (400M)
            self.config.use_dsp_simulator = True
            self.config.use_kalman_filter = True
        elif mode == MemoryMode.ULTRA:
            # êµ¬ë²„ì „ ULTRA (842M)
            self.config.use_neural_analyzers = True
            self.config.use_advanced_wrappers = True
        elif mode == MemoryMode.EXTREME:
            # êµ¬ë²„ì „ EXTREME (922M)
            self._set_all_modules_true()
    
    def _set_all_modules_false(self):
        """ëª¨ë“  ëª¨ë“ˆ ë¹„í™œì„±í™”"""
        for key in self.config.__dict__:
            if key.startswith('use_'):
                setattr(self.config, key, False)
    
    def _set_all_modules_true(self):
        """ëª¨ë“  ëª¨ë“ˆ í™œì„±í™”"""
        for key in self.config.__dict__:
            if key.startswith('use_'):
                setattr(self.config, key, True)
    
    def _detect_memory_mode(self):
        """GPU ë©”ëª¨ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ ëª¨ë“œ ì„ íƒ"""
        try:
            if torch.cuda.is_available():
                # GPU ë©”ëª¨ë¦¬ ì²´í¬
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
                
                if gpu_memory >= 24:
                    self.config.memory_mode = MemoryMode.HEAVY
                    self.logger.info(f"ğŸ›ï¸ ìë™ ë©”ëª¨ë¦¬ ëª¨ë“œ ì„ íƒ: heavy (GPU: {gpu_memory:.1f}GB)")
                elif gpu_memory >= 12:
                    self.config.memory_mode = MemoryMode.MEDIUM
                    self.logger.info(f"ğŸ›ï¸ ìë™ ë©”ëª¨ë¦¬ ëª¨ë“œ ì„ íƒ: medium (GPU: {gpu_memory:.1f}GB)")
                elif gpu_memory >= 8:
                    # 8GB GPUëŠ” MEDIUM ëª¨ë“œë¡œ ì„¤ì • (ë™ì  ìŠ¤ì™‘ í™œìš©)
                    self.config.memory_mode = MemoryMode.MEDIUM
                    self.logger.info(f"ğŸ›ï¸ ìë™ ë©”ëª¨ë¦¬ ëª¨ë“œ ì„ íƒ: medium (GPU: {gpu_memory:.1f}GB, ë™ì  ìŠ¤ì™‘ í™œìš©)")
                else:
                    self.config.memory_mode = MemoryMode.LIGHT
                    self.logger.info(f"ğŸ›ï¸ ìë™ ë©”ëª¨ë¦¬ ëª¨ë“œ ì„ íƒ: light (GPU: {gpu_memory:.1f}GB)")
            else:
                # CPU ì „ìš©
                self.config.memory_mode = MemoryMode.LIGHT
                self.logger.info("ğŸ›ï¸ ìë™ ë©”ëª¨ë¦¬ ëª¨ë“œ ì„ íƒ: light (CPU ì „ìš©)")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ëª¨ë“œ ìë™ ê°ì§€ ì‹¤íŒ¨: {e}")
            self.config.memory_mode = MemoryMode.LIGHT
            self.logger.info("ğŸ›ï¸ ê¸°ë³¸ ë©”ëª¨ë¦¬ ëª¨ë“œ ì„ íƒ: light")
        
    async def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            self.logger.info("=" * 70)
            self.logger.info("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
            self.logger.info("=" * 70)
            
            # 0. ë©”ëª¨ë¦¬ ëª¨ë“œ ì„¤ì •
            if self.config.auto_memory_mode:
                # ìë™ ëª¨ë“œ ê°ì§€
                self._detect_memory_mode()
            else:
                # ì§€ì •ëœ ëª¨ë“œ ì‚¬ìš©
                self.logger.info(f"ğŸ“¦ ì§€ì •ëœ ë©”ëª¨ë¦¬ ëª¨ë“œ: {self.config.memory_mode.value}")
            
            # ëª¨ë“œì— ë”°ë¥¸ ëª¨ë“ˆ í”Œë˜ê·¸ ì¡°ì •
            self._adjust_modules_by_memory_mode()
            
            # DSM ì´ˆê¸°í™” (UnifiedModel ë¡œë“œ ì „ì—! ClaudeëŠ” ì œì™¸)
            if self.config.llm_mode != "none" and self.config.llm_mode != "claude":
                await self._init_dsm_early()
            
            # 1. UnifiedModel ë¡œë“œ
            await self._load_unified_model()
            
            # 2. ë²ˆì—­ê¸° ë¡œë“œ (Advanced Wrappersê°€ í•„ìˆ˜ë¡œ ìš”êµ¬í•¨)
            # API ëª¨ë“œì—ì„œë„ translator ê°ì²´ëŠ” í•„ìš” (ì‹¤ì œ ë²ˆì—­ì€ use_translatorë¡œ ì œì–´)
            await self._load_translator()
            
            # 3. Neural Analyzers ë¡œë“œ (368M)
            if self.config.use_neural_analyzers:
                await self._load_neural_analyzers()
            
            # 4. Advanced Wrappers ë¡œë“œ (112M) - translator í•„ìˆ˜
            if self.config.use_advanced_wrappers:
                await self._load_advanced_wrappers()
            
            # 5. DSP & Kalman Filter ë¡œë“œ
            if self.config.use_dsp_simulator:
                await self._load_dsp_components()
            
            # 6. Phase Networks ë¡œë“œ
            if self.config.use_phase_networks:
                await self._load_phase_networks()
            
            # 7. ìƒˆë¡œìš´ í†µí•© ëª¨ë“ˆë“¤ (EXTREME ëª¨ë“œ)
            if self.config.use_workflow_memory_manager:
                await self._load_workflow_memory_manager()
            
            if self.config.use_meta_integration:
                await self._load_meta_integration()
            
            if self.config.use_counterfactual_reasoning:
                await self._load_counterfactual_reasoning()
            
            if self.config.use_advanced_regret_learning:
                await self._load_advanced_regret_learning()
            
            if self.config.use_temporal_propagation:
                await self._load_temporal_propagation()
            
            if self.config.use_experience_database:
                await self._load_experience_database()
            
            if self.config.use_emotion_hierarchy:
                await self._load_emotion_hierarchy()
            
            # 8. ì •ë°€ ê°ì •â†’ë²¤ë‹´ ë§¤í¼ ë¡œë“œ
            await self._load_precision_mapper()
            
            # 9. 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ ë¡œë“œ
            if self.config.use_three_view_scenario:
                await self._load_three_view_scenario_system()
            
            # 10. ë‹¤ì›ì  ìœ¤ë¦¬ ì²´ê³„ ë¡œë“œ
            if self.config.use_multi_ethics_system:
                await self._load_multi_ethics_system()
            
            # 11. LLM í†µí•© (ì„ íƒì )
            if self.config.llm_mode != "none":
                await self._load_llm_integration()
            
            # 12. ìœ íœ´ í•™ìŠµ ì‹œìŠ¤í…œ (MD ë¬¸ì„œ: í”„ë¡œë•ì…˜ ë ˆë²¨ê¹Œì§€ ì£¼ì„ ì²˜ë¦¬)
            # TODO: í”„ë¡œë•ì…˜ ë ˆë²¨ì—ì„œ í™œì„±í™”
            # if self.config.memory_mode.value in ['heavy', 'ultra', 'extreme']:
            #     await self._load_idle_learner()
            
            self.logger.info("=" * 70)
            self.logger.info("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
            self._print_system_status()
            self.logger.info("=" * 70)
            
        except Exception as e:
            self.logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _load_unified_model(self):
        """UnifiedModel ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        self.logger.info("ğŸ“¦ UnifiedModel ë¡œë“œ ì¤‘...")
        
        try:
            # UnifiedModel ì„í¬íŠ¸
            from training.unified_training_final import UnifiedModel, UnifiedTrainingConfig
            from training.enhanced_checkpoint_manager import EnhancedCheckpointManager
            
            # ì²´í¬í¬ì¸íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
            self.checkpoint_manager = EnhancedCheckpointManager(
                checkpoint_dir=self.config.checkpoint_dir
            )
            
            # ì„¤ì • ìƒì„±
            train_config = UnifiedTrainingConfig()
            train_config.device = self.config.device
            
            # UnifiedModel ìƒì„±
            self.unified_model = UnifiedModel(config=train_config)
            
            # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ê²°ì •
            if self.config.checkpoint_path:
                # ì§ì ‘ ê²½ë¡œê°€ ì§€ì •ëœ ê²½ìš°
                checkpoint_path = Path(self.config.checkpoint_path)
            else:
                # ì—í­ ë²ˆí˜¸ë¡œ ìë™ ê²€ìƒ‰
                pattern = f"{self.config.checkpoint_dir}/checkpoint_epoch_{self.config.checkpoint_epoch:04d}_*.pt"
                matches = glob.glob(pattern)
                if matches:
                    checkpoint_path = Path(matches[0])  # í•´ë‹¹ ì—í­ì˜ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©
                    self.logger.info(f"   ì—í­ {self.config.checkpoint_epoch} ì²´í¬í¬ì¸íŠ¸ ìë™ ê²€ìƒ‰: {checkpoint_path.name}")
                else:
                    # ëª» ì°¾ìœ¼ë©´ ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©
                    all_checkpoints = sorted(glob.glob(f"{self.config.checkpoint_dir}/checkpoint_epoch_*.pt"))
                    if all_checkpoints:
                        checkpoint_path = Path(all_checkpoints[-1])
                        self.logger.warning(f"   ì—í­ {self.config.checkpoint_epoch} ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ. ìµœì‹  ì‚¬ìš©: {checkpoint_path.name}")
                    else:
                        checkpoint_path = None
                        self.logger.warning(f"   ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ. ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ë¡œ ì‹œì‘...")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            if checkpoint_path and checkpoint_path.exists():
                self.logger.info(f"   ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path.name}")
                
                checkpoint = torch.load(
                    checkpoint_path,
                    map_location=self.config.device,
                    weights_only=False  # ì „ì²´ ëª¨ë¸ êµ¬ì¡° í¬í•¨
                )
                
                # ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‚˜ì¤‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
                self.loaded_checkpoint = checkpoint
                
                # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
                if 'model_state' in checkpoint:
                    self.unified_model.load_state_dict(
                        checkpoint['model_state'],
                        strict=False  # ì¼ë¶€ ëˆ„ë½ í—ˆìš©
                    )
                    self.logger.info(f"   âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
                    self.logger.info(f"   - Epoch: {checkpoint.get('epoch', 'unknown')}")
                    best_loss = checkpoint.get('best_loss', 'unknown')
                    if isinstance(best_loss, (int, float)):
                        self.logger.info(f"   - Loss: {best_loss:.4f}")
                    else:
                        self.logger.info(f"   - Loss: {best_loss}")
                else:
                    self.logger.warning("   âš ï¸ ì²´í¬í¬ì¸íŠ¸ì— model_stateê°€ ì—†ìŒ")
            else:
                self.logger.warning(f"   âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_path}")
                self.logger.info("   ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ë¡œ ì‹œì‘...")
            
            # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
            self.unified_model.eval()
            
            # MEDIUM ëª¨ë“œì—ì„œëŠ” ë°±ë³¸ë§Œ GPUë¡œ, ë‚˜ë¨¸ì§€ëŠ” CPUì— ìœ ì§€
            if self.config.memory_mode == MemoryMode.MEDIUM:
                # ë°±ë³¸ê³¼ í—¤ë“œë§Œ GPUë¡œ
                self.unified_model.backbone.to(self.config.device)
                self.unified_model.emotion_head.to(self.config.device)
                self.unified_model.bentham_head.to(self.config.device)
                self.unified_model.regret_head.to(self.config.device)
                self.unified_model.surd_head.to(self.config.device)
                # neural_analyzersëŠ” ì´ë¯¸ CPUì— ìˆìŒ (MEDIUM ëª¨ë“œ ì„¤ì •)
                self.logger.info("   ğŸ“Œ MEDIUM ëª¨ë“œ: ë°±ë³¸/í—¤ë“œë§Œ GPU, analyzersëŠ” CPU ìœ ì§€")
            else:
                # ë‹¤ë¥¸ ëª¨ë“œì—ì„œëŠ” ì „ì²´ë¥¼ GPUë¡œ
                self.unified_model.to(self.config.device)
            
            # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ë³´
            total_params = sum(p.numel() for p in self.unified_model.parameters())
            trainable_params = sum(p.numel() for p in self.unified_model.parameters() if p.requires_grad)
            self.logger.info(f"   ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {total_params/1e6:.1f}M")
            self.logger.info(f"   ğŸ“Š í•™ìŠµê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params/1e6:.1f}M")
            
            # DSMì— UnifiedModel ë“±ë¡ (DSMì´ ì—†ìœ¼ë©´ ì´ˆê¸°í™”)
            if not hasattr(self, 'swap_manager') or self.swap_manager is None:
                # DSMì´ ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ ì¦‰ì‹œ ì´ˆê¸°í™”
                if self.config.llm_mode != "none":
                    self.logger.warning("   âš ï¸ DSMì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ. ì§€ê¸ˆ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
                    try:
                        from dynamic_swap_manager import DynamicSwapManager, set_swap_manager
                        self.swap_manager = DynamicSwapManager.get_instance()
                        set_swap_manager(self.swap_manager)
                        self.logger.info(f"   âœ… DSM ê¸´ê¸‰ ì´ˆê¸°í™” ì™„ë£Œ (ID: {id(self.swap_manager)})")
                    except Exception as e:
                        self.logger.error(f"   âŒ DSM ê¸´ê¸‰ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # DSM ë“±ë¡ ì‹œë„
            if self.swap_manager:
                try:
                    from dynamic_swap_manager import SwapPriority
                    
                    # ë°±ë³¸ê³¼ í—¤ë“œë“¤ì„ ê°œë³„ ë“±ë¡ (ì„¸ë°€í•œ ê´€ë¦¬) - owner ì •ë³´ í¬í•¨
                    self.swap_manager.register_model(
                        'unified_backbone', 
                        self.unified_model.backbone,
                        priority=SwapPriority.CRITICAL,
                        owner_obj=self.unified_model,
                        owner_attr='backbone'
                    )
                    self.swap_manager.register_model(
                        'emotion_head', 
                        self.unified_model.emotion_head,
                        priority=SwapPriority.HIGH,
                        owner_obj=self.unified_model,
                        owner_attr='emotion_head'
                    )
                    self.swap_manager.register_model(
                        'bentham_head', 
                        self.unified_model.bentham_head,
                        priority=SwapPriority.HIGH,
                        owner_obj=self.unified_model,
                        owner_attr='bentham_head'
                    )
                    self.swap_manager.register_model(
                        'regret_head', 
                        self.unified_model.regret_head,
                        priority=SwapPriority.HIGH,
                        owner_obj=self.unified_model,
                        owner_attr='regret_head'
                    )
                    self.swap_manager.register_model(
                        'surd_head', 
                        self.unified_model.surd_head,
                        priority=SwapPriority.HIGH,
                        owner_obj=self.unified_model,
                        owner_attr='surd_head'
                    )
                    self.logger.info("   ğŸ“Œ UnifiedModel DSM ë“±ë¡ ì™„ë£Œ")
                except Exception as dsm_error:
                    import traceback
                    self.logger.error(f"   âŒ DSM ë“±ë¡ ì‹¤íŒ¨: {dsm_error}")
                    self.logger.error(f"   Traceback:\n{traceback.format_exc()}")
            else:
                self.logger.warning("   âš ï¸ swap_managerê°€ None - DSM ë“±ë¡ ìŠ¤í‚µ")
            
        except Exception as e:
            self.logger.error(f"   âŒ UnifiedModel ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    async def _load_neural_analyzers(self):
        """Neural Analyzers ë¡œë“œ (368M ë˜ëŠ” ë¶€ë¶„)"""
        subset = getattr(self.config, 'neural_analyzers_subset', None)
        if subset:
            self.logger.info(f"ğŸ§  Neural Analyzers ë¶€ë¶„ ë¡œë“œ ì¤‘ ({subset})...")
        else:
            self.logger.info("ğŸ§  Neural Analyzers ì „ì²´ ë¡œë“œ ì¤‘ (368M)...")
        
        try:
            from analyzer_neural_modules import create_neural_analyzers
            
            # Neural Analyzers ìƒì„± - 768ì°¨ì› (BERT/ì„ë² ë”© ì°¨ì›)
            all_analyzers = create_neural_analyzers(input_dim=768)
            
            # ë¶€ë¶„ ë¡œë“œ ëª¨ë“œì¸ ê²½ìš° í•„ìš”í•œ ê²ƒë§Œ ì„ íƒ
            if subset:
                self.neural_analyzers = {k: v for k, v in all_analyzers.items() if k in subset}
                self.logger.info(f"   ë¶€ë¶„ ë¡œë“œ ì™„ë£Œ: {list(self.neural_analyzers.keys())}")
            else:
                self.neural_analyzers = all_analyzers
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë³µì› ì‹œë„ (ì°¨ì› í˜¸í™˜ì„± ì²´í¬)
            if hasattr(self.unified_model, 'neural_analyzers'):
                # ê¸°ì¡´ neural_analyzersì˜ ì°¨ì› í™•ì¸
                old_analyzers = self.unified_model.neural_analyzers
                if old_analyzers and 'emotion' in old_analyzers:
                    # ì²« ë²ˆì§¸ ë ˆì´ì–´ì˜ ì…ë ¥ ì°¨ì› í™•ì¸
                    first_layer = next(old_analyzers['emotion'].parameters())
                    old_dim = first_layer.shape[-1] if len(first_layer.shape) > 1 else first_layer.shape[0]
                    
                    if old_dim == 768:
                        self.logger.info("   ì²´í¬í¬ì¸íŠ¸ì—ì„œ Neural Analyzers ê°€ì¤‘ì¹˜ ë³µì› (768ì°¨ì› í˜¸í™˜)...")
                        self.neural_analyzers = old_analyzers
                    elif old_dim == 896:
                        # 896ì°¨ì› ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ë˜, 768->896 í”„ë¡œì ì…˜ ì–´ëŒ‘í„° ì¶”ê°€
                        self.logger.info(f"   ì²´í¬í¬ì¸íŠ¸ Neural Analyzers 896ì°¨ì› ê°€ì¤‘ì¹˜ ë³µì› ì¤‘...")
                        self.neural_analyzers = old_analyzers
                        
                        # ê° analyzerì— ëŒ€í•´ í”„ë¡œì ì…˜ ì–´ëŒ‘í„° ì¶”ê°€
                        self.neural_projection_adapters = {}
                        for name in self.neural_analyzers.keys():
                            # 768 -> 896 í”„ë¡œì ì…˜ ë ˆì´ì–´
                            adapter = nn.Sequential(
                                nn.Linear(768, 896),
                                nn.LayerNorm(896),
                                nn.GELU()
                            ).to(self.config.device)
                            self.neural_projection_adapters[name] = adapter
                            self.logger.info(f"   - {name}: 768â†’896 í”„ë¡œì ì…˜ ì–´ëŒ‘í„° ì¶”ê°€")
                        
                        self.logger.info("   âœ… 896ì°¨ì› Neural Analyzers ë³µì› ì™„ë£Œ (í”„ë¡œì ì…˜ ì–´ëŒ‘í„° ì‚¬ìš©)")
                    else:
                        self.logger.warning(f"   ì²´í¬í¬ì¸íŠ¸ Neural Analyzers ì°¨ì› ë¶ˆì¼ì¹˜ ({old_dim}ì°¨ì›), ìƒˆë¡œ ìƒì„±ëœ 768ì°¨ì› ì‚¬ìš©")
                        # ìƒˆë¡œ ìƒì„±ëœ 768ì°¨ì› neural_analyzers ìœ ì§€
            
            # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ ë° ë””ë°”ì´ìŠ¤ í• ë‹¹ (ë©”ëª¨ë¦¬ ëª¨ë“œë³„ ì „ëµ)
            for name, module in self.neural_analyzers.items():
                module.eval()
                # MEDIUM ëª¨ë“œì—ì„œëŠ” CPUë¡œ ì´ˆê¸°í™” (ì‹¤í–‰ ì‹œ ë™ì  ìŠ¤ì™‘)
                if self.config.memory_mode == MemoryMode.MEDIUM:
                    target_device = torch.device('cpu')
                    self.logger.info(f"   - {name}: CPU ì´ˆê¸°í™” (ë™ì  ìŠ¤ì™‘ ëŒ€ê¸°)")
                else:
                    target_device = self.config.device
                module.to(target_device)
                params = sum(p.numel() for p in module.parameters())
                self.logger.info(f"   - {name}: {params/1e6:.1f}M params")
            
            self.logger.info("   âœ… Neural Analyzers ë¡œë“œ ì™„ë£Œ")
            
            # UnifiedModelì— neural_analyzers ì „ë‹¬
            if hasattr(self, 'unified_model') and self.unified_model is not None:
                # UnifiedModelì˜ ê¸°ì¡´ neural_analyzersë¥¼ ë®ì–´ì“°ê¸°
                self.unified_model.neural_analyzers = nn.ModuleDict(self.neural_analyzers)
                self.logger.info("   ğŸ“Œ UnifiedModelì— Neural Analyzers ì „ë‹¬ ì™„ë£Œ")
            
            # DSMì— Neural Analyzers ë“±ë¡
            if hasattr(self, 'swap_manager') and self.swap_manager and self.neural_analyzers:
                try:
                    from dynamic_swap_manager import SwapPriority
                    
                    for name, analyzer in self.neural_analyzers.items():
                        # Neural AnalyzersëŠ” MEDIUM ìš°ì„ ìˆœìœ„
                        self.swap_manager.register_model(
                            f'neural_{name}', 
                            analyzer,
                            priority=SwapPriority.MEDIUM
                        )
                    self.logger.info("   ğŸ“Œ Neural Analyzers DSM ë“±ë¡ ì™„ë£Œ")
                except Exception as dsm_error:
                    self.logger.warning(f"   âš ï¸ DSM ë“±ë¡ ì‹¤íŒ¨: {dsm_error}")
            
        except Exception as e:
            self.logger.error(f"   âŒ Neural Analyzers ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"Neural Analyzers ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_advanced_wrappers(self):
        """Advanced Analyzer Wrappers ë¡œë“œ (112M ë˜ëŠ” ë¶€ë¶„)"""
        subset = getattr(self.config, 'advanced_wrappers_subset', None)
        if subset:
            self.logger.info(f"ğŸ¯ Advanced Analyzer Wrappers ë¶€ë¶„ ë¡œë“œ ì¤‘ ({subset})...")
        else:
            self.logger.info("ğŸ¯ Advanced Analyzer Wrappers ì „ì²´ ë¡œë“œ ì¤‘ (112M)...")
        
        try:
            from advanced_analyzer_wrappers import create_advanced_analyzer_wrappers
            
            # MEDIUM ëª¨ë“œì—ì„œëŠ” CPU ì´ˆê¸°í™” ê°•ì œ
            if self.config.memory_mode == MemoryMode.MEDIUM:
                import os
                os.environ['FORCE_CPU_INIT'] = '1'
                self.logger.info("   ğŸ“Œ MEDIUM ëª¨ë“œ: Advanced Wrappers CPU ì´ˆê¸°í™” ì„¤ì •")
            
            # Advanced Wrappers ìƒì„±
            all_wrappers = create_advanced_analyzer_wrappers()
            
            # í™˜ê²½ë³€ìˆ˜ ì •ë¦¬
            if 'FORCE_CPU_INIT' in os.environ:
                del os.environ['FORCE_CPU_INIT']
            
            # ë¶€ë¶„ ë¡œë“œ ëª¨ë“œì¸ ê²½ìš° í•„ìš”í•œ ê²ƒë§Œ ì„ íƒ
            if subset:
                self.advanced_wrappers = {k: v for k, v in all_wrappers.items() if k in subset}
                self.logger.info(f"   ë¶€ë¶„ ë¡œë“œ ì™„ë£Œ: {list(self.advanced_wrappers.keys())}")
            else:
                self.advanced_wrappers = all_wrappers
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë³µì› ì‹œë„
            if hasattr(self, 'loaded_checkpoint') and self.loaded_checkpoint is not None:
                if 'model_state' in self.loaded_checkpoint and 'advanced_wrappers' in self.loaded_checkpoint['model_state']:
                    self.logger.info("   ì²´í¬í¬ì¸íŠ¸ì—ì„œ Advanced Wrappers ê°€ì¤‘ì¹˜ ë³µì›...")
                    saved_wrappers = self.loaded_checkpoint['model_state']['advanced_wrappers']
                    
                    # ì²´í¬í¬ì¸íŠ¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ í˜„ì¬ wrapperì— ë¡œë“œ
                    for name in self.advanced_wrappers.keys():
                        if name in saved_wrappers:
                            try:
                                # í‚¤ ë¦¬ë§¤í•‘ ì²˜ë¦¬ - ì²´í¬í¬ì¸íŠ¸ì™€ í˜„ì¬ ì½”ë“œì˜ í‚¤ ì´ë¦„ ì°¨ì´ í•´ê²°
                                saved_state = saved_wrappers[name]
                                current_state = self.advanced_wrappers[name].state_dict()
                                remapped_state = {}
                                
                                # í˜„ì¬ ëª¨ë¸ì˜ í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë§¤ì¹­ë˜ëŠ” í‚¤ ì°¾ê¸°
                                for current_key in current_state.keys():
                                    # ì§ì ‘ ë§¤ì¹­ ì‹œë„
                                    if current_key in saved_state:
                                        saved_tensor = saved_state[current_key]
                                        current_tensor = current_state[current_key]
                                        
                                        # Shape ë¶ˆì¼ì¹˜ ì²˜ë¦¬
                                        if saved_tensor.shape != current_tensor.shape:
                                            self.logger.info(f"       Shape ë¶ˆì¼ì¹˜ ê°ì§€: {current_key}")
                                            self.logger.info(f"         ì²´í¬í¬ì¸íŠ¸: {list(saved_tensor.shape)}")
                                            self.logger.info(f"         í˜„ì¬ ëª¨ë¸: {list(current_tensor.shape)}")
                                            
                                            # diversity_layer.weight íŠ¹ë³„ ì²˜ë¦¬
                                            if 'diversity_layer.weight' in current_key:
                                                # [4, 192] â†’ [4, 768] ë˜ëŠ” [4, 128] â†’ [4, 512]
                                                if len(saved_tensor.shape) == 2 and len(current_tensor.shape) == 2:
                                                    if saved_tensor.shape[0] == current_tensor.shape[0]:
                                                        # í”„ë¡œì ì…˜: ì‘ì€ ì°¨ì›ì„ í° ì°¨ì›ìœ¼ë¡œ í™•ì¥
                                                        expanded_tensor = torch.zeros_like(current_tensor)
                                                        min_dim = min(saved_tensor.shape[1], current_tensor.shape[1])
                                                        expanded_tensor[:, :min_dim] = saved_tensor[:, :min_dim]
                                                        
                                                        # Xavier ì´ˆê¸°í™”ë¡œ ë‚˜ë¨¸ì§€ ë¶€ë¶„ ì±„ìš°ê¸°
                                                        if min_dim < current_tensor.shape[1]:
                                                            nn.init.xavier_uniform_(expanded_tensor[:, min_dim:])
                                                        
                                                        remapped_state[current_key] = expanded_tensor
                                                        self.logger.info(f"         âœ… í”„ë¡œì ì…˜ ì ìš©: {saved_tensor.shape} â†’ {current_tensor.shape}")
                                                    else:
                                                        self.logger.warning(f"         âŒ ì²« ë²ˆì§¸ ì°¨ì› ë¶ˆì¼ì¹˜, ìŠ¤í‚µ")
                                                else:
                                                    self.logger.warning(f"         âŒ ì°¨ì› ìˆ˜ ë¶ˆì¼ì¹˜, ìŠ¤í‚µ")
                                            else:
                                                # ë‹¤ë¥¸ ë ˆì´ì–´ë“¤ì€ ìŠ¤í‚µ (ìƒˆë¡œ ì´ˆê¸°í™”)
                                                self.logger.warning(f"         âš ï¸ Shape ë¶ˆì¼ì¹˜ë¡œ ìŠ¤í‚µ")
                                        else:
                                            remapped_state[current_key] = saved_state[current_key]
                                    # gating_network ë¦¬ë§¤í•‘
                                    elif 'gate' in current_key:
                                        # gate â†’ gating_network.gating_network ë§¤í•‘
                                        checkpoint_key = current_key.replace('gate', 'gating_network.gating_network')
                                        if checkpoint_key in saved_state:
                                            remapped_state[current_key] = saved_state[checkpoint_key]
                                            self.logger.debug(f"       í‚¤ ë¦¬ë§¤í•‘: {checkpoint_key} â†’ {current_key}")
                                    # emotion_moe.ga â†’ emotion_moe.gating_network.gating_network ë§¤í•‘
                                    elif 'emotion_moe.ga' in current_key:
                                        checkpoint_key = current_key.replace('emotion_moe.ga', 'emotion_moe.gating_network.gating_network')
                                        if checkpoint_key in saved_state:
                                            remapped_state[current_key] = saved_state[checkpoint_key]
                                            self.logger.debug(f"       í‚¤ ë¦¬ë§¤í•‘: {checkpoint_key} â†’ {current_key}")
                                    # bentham_ethi â†’ bentham_deep_ethics ë§¤í•‘
                                    elif 'bentham_ethi' in current_key:
                                        checkpoint_key = current_key.replace('bentham_ethi', 'bentham_deep_ethics')
                                        if checkpoint_key in saved_state:
                                            remapped_state[current_key] = saved_state[checkpoint_key]
                                            self.logger.debug(f"       í‚¤ ë¦¬ë§¤í•‘: {checkpoint_key} â†’ {current_key}")
                                
                                # ë¦¬ë§¤í•‘ëœ state_dict ë¡œë“œ
                                if remapped_state:
                                    incompatible = self.advanced_wrappers[name].load_state_dict(
                                        remapped_state, 
                                        strict=False
                                    )
                                    if incompatible.missing_keys or incompatible.unexpected_keys:
                                        self.logger.warning(f"     âš ï¸ {name} ë¶€ë¶„ ë³µì› - ëˆ„ë½: {len(incompatible.missing_keys)}ê°œ, ì˜ˆìƒì™¸: {len(incompatible.unexpected_keys)}ê°œ")
                                    else:
                                        self.logger.info(f"     âœ… {name} ê°€ì¤‘ì¹˜ ì™„ì „ ë³µì› ì„±ê³µ")
                                    self.logger.info(f"     ğŸ“Š {name} ë¦¬ë§¤í•‘ ì„±ê³µ: {len(remapped_state)}/{len(current_state)}ê°œ í‚¤ ë³µì›")
                                else:
                                    self.logger.warning(f"     âš ï¸ {name} ë¦¬ë§¤í•‘ ì‹¤íŒ¨ - ë§¤ì¹­ë˜ëŠ” í‚¤ ì—†ìŒ")
                            except Exception as e:
                                # shape mismatch ë“±ì˜ ê²½ìš° ìƒˆë¡œ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©
                                error_msg = str(e)
                                self.logger.warning(f"     âš ï¸ {name} ê°€ì¤‘ì¹˜ ë³µì› ì‹¤íŒ¨ - ìƒˆë¡œ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©: {error_msg}")
                                
                                # size mismatch ì—ëŸ¬ì¸ ê²½ìš° ìƒì„¸ ì •ë³´ ì¶œë ¥
                                if "size mismatch" in error_msg:
                                    # ì–´ë–¤ í‚¤ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆëŠ”ì§€ íŒŒì•…
                                    import re
                                    match = re.search(r"size mismatch for (.+?):", error_msg)
                                    if match:
                                        problem_key = match.group(1)
                                        if problem_key in remapped_state:
                                            checkpoint_shape = list(remapped_state[problem_key].shape)
                                            current_shape = list(current_state[problem_key].shape) if problem_key in current_state else "N/A"
                                            self.logger.warning(f"       Shape ë¶ˆì¼ì¹˜: {problem_key}")
                                            self.logger.warning(f"         ì²´í¬í¬ì¸íŠ¸: {checkpoint_shape}")
                                            self.logger.warning(f"         í˜„ì¬ ëª¨ë¸: {current_shape}")
                        else:
                            self.logger.warning(f"     âš ï¸ {name}ì´ ì²´í¬í¬ì¸íŠ¸ì— ì—†ìŒ")
                else:
                    self.logger.info("   âš ï¸ ì²´í¬í¬ì¸íŠ¸ì— Advanced Wrappers ê°€ì¤‘ì¹˜ ì—†ìŒ - ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
            else:
                self.logger.info("   âš ï¸ ì²´í¬í¬ì¸íŠ¸ê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ - ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
            
            # None ì²´í¬ ì¶”ê°€
            if self.advanced_wrappers is None:
                self.logger.error("   âŒ Advanced Wrappersê°€ Noneì…ë‹ˆë‹¤")
                raise ValueError("Advanced Wrappers ìƒì„± ì‹¤íŒ¨")
            
            # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
            for name, wrapper in self.advanced_wrappers.items():
                if hasattr(wrapper, 'eval'):
                    wrapper.eval()
                    # MEDIUM ëª¨ë“œì—ì„œëŠ” CPUë¡œ
                    if self.config.memory_mode == MemoryMode.MEDIUM:
                        wrapper.to(torch.device('cpu'))
                    else:
                        wrapper.to(self.config.device)
                params = sum(p.numel() for p in wrapper.parameters() if hasattr(wrapper, 'parameters'))
                self.logger.info(f"   - {name}: {params/1e6:.1f}M params")
            
            self.logger.info("   âœ… Advanced Wrappers ë¡œë“œ ì™„ë£Œ")
            
            # UnifiedModelì— advanced_wrappers ì „ë‹¬
            if hasattr(self, 'unified_model') and self.unified_model is not None:
                self.unified_model.advanced_wrappers = nn.ModuleDict(self.advanced_wrappers)
                self.logger.info("   ğŸ“Œ UnifiedModelì— Advanced Wrappers ì „ë‹¬ ì™„ë£Œ")
            
            # DSMì— Advanced Wrappers ë“±ë¡
            if hasattr(self, 'swap_manager') and self.swap_manager and self.advanced_wrappers:
                try:
                    from dynamic_swap_manager import SwapPriority
                    
                    for name, wrapper in self.advanced_wrappers.items():
                        # Advanced WrappersëŠ” MEDIUM ìš°ì„ ìˆœìœ„
                        self.swap_manager.register_model(
                            f'wrapper_{name}', 
                            wrapper,
                            priority=SwapPriority.MEDIUM
                        )
                    self.logger.info("   ğŸ“Œ Advanced Wrappers DSM ë“±ë¡ ì™„ë£Œ")
                except Exception as dsm_error:
                    self.logger.warning(f"   âš ï¸ DSM ë“±ë¡ ì‹¤íŒ¨: {dsm_error}")
            
        except Exception as e:
            self.logger.error(f"   âŒ Advanced Wrappers ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"Advanced Wrappers ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_dsp_components(self):
        """DSP ì‹œë®¬ë ˆì´í„° & Kalman Filter ë¡œë“œ"""
        self.logger.info("ğŸ“¡ DSP ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì¤‘...")
        
        try:
            from emotion_dsp_simulator import EmotionDSPSimulator
            
            # DSP ì‹œë®¬ë ˆì´í„° ìƒì„±
            self.dsp_simulator = EmotionDSPSimulator()
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›
            if hasattr(self.unified_model, 'dsp_simulator'):
                self.logger.info("   ì²´í¬í¬ì¸íŠ¸ì—ì„œ DSP ì‹œë®¬ë ˆì´í„° ë³µì›...")
                self.dsp_simulator = self.unified_model.dsp_simulator
            
            # MEDIUM ëª¨ë“œì—ì„œëŠ” CPUë¡œ ì´ˆê¸°í™”, ì•„ë‹ˆë©´ GPUë¡œ
            dsp_device = torch.device('cpu') if self.config.memory_mode == MemoryMode.MEDIUM else self.config.device
            self.dsp_simulator = self.dsp_simulator.to(dsp_device)
            self.dsp_simulator.eval()
            if self.config.memory_mode == MemoryMode.MEDIUM:
                self.logger.info("   ğŸ“Œ DSP ì‹œë®¬ë ˆì´í„° CPU ì´ˆê¸°í™” (ë™ì  ìŠ¤ì™‘ ëŒ€ê¸°)")
            
            # Kalman FilterëŠ” DSP ë‚´ë¶€ì— í¬í•¨
            if hasattr(self.dsp_simulator, 'kalman_filter'):
                self.kalman_filter = self.dsp_simulator.kalman_filter
                self.logger.info("   âœ… Kalman Filter í™œì„±í™”")
            
            params = sum(p.numel() for p in self.dsp_simulator.parameters())
            self.logger.info(f"   ğŸ“Š DSP íŒŒë¼ë¯¸í„°: {params/1e6:.1f}M")
            self.logger.info("   âœ… DSP ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"   âŒ DSP ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"DSP ì»´í¬ë„ŒíŠ¸ ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_phase_networks(self):
        """Phase Networks ë¡œë“œ"""
        self.logger.info("ğŸ”„ Phase Networks ë¡œë“œ ì¤‘...")
        
        try:
            from phase_neural_networks import (
                Phase0ProjectionNet,
                Phase2CommunityNet,
                HierarchicalEmotionIntegrator
            )
            
            self.phase_networks = {
                'phase0': Phase0ProjectionNet(),
                'phase2': Phase2CommunityNet(),
                'hierarchical': HierarchicalEmotionIntegrator()
            }
            
            # MEDIUM ëª¨ë“œì—ì„œëŠ” CPUë¡œ ì´ˆê¸°í™”, ì•„ë‹ˆë©´ GPUë¡œ
            phase_device = torch.device('cpu') if self.config.memory_mode == MemoryMode.MEDIUM else self.config.device
            for name, network in self.phase_networks.items():
                self.phase_networks[name] = network.to(phase_device)
            
            if self.config.memory_mode == MemoryMode.MEDIUM:
                self.logger.info("   ğŸ“Œ Phase Networks CPU ì´ˆê¸°í™” (ë™ì  ìŠ¤ì™‘ ëŒ€ê¸°)")
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë³µì› ì‹œë„
            if hasattr(self, 'unified_model') and hasattr(self.unified_model, 'phase_networks'):
                self.logger.info("   ì²´í¬í¬ì¸íŠ¸ì—ì„œ Phase Networks ê°€ì¤‘ì¹˜ ë³µì› ì¤‘...")
                checkpoint_phase_nets = self.unified_model.phase_networks
                
                for name in self.phase_networks.keys():
                    if name in checkpoint_phase_nets:
                        try:
                            # ê°€ì¤‘ì¹˜ ë³µì‚¬
                            self.phase_networks[name].load_state_dict(
                                checkpoint_phase_nets[name].state_dict()
                            )
                            self.logger.info(f"   - {name}: ê°€ì¤‘ì¹˜ ë³µì› ì™„ë£Œ")
                        except Exception as e:
                            self.logger.warning(f"   - {name}: ê°€ì¤‘ì¹˜ ë³µì› ì‹¤íŒ¨ ({e}), ìƒˆë¡œ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©")
                    else:
                        self.logger.info(f"   - {name}: ì²´í¬í¬ì¸íŠ¸ì— ì—†ìŒ, ìƒˆë¡œ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©")
            else:
                self.logger.info("   ì²´í¬í¬ì¸íŠ¸ì— Phase Networks ì—†ìŒ, ìƒˆë¡œ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©")
            
            # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ ë° ë””ë°”ì´ìŠ¤ í• ë‹¹ (ë©”ëª¨ë¦¬ ëª¨ë“œë³„ ì „ëµ)
            for name, net in self.phase_networks.items():
                net.eval()
                # MEDIUM ëª¨ë“œì—ì„œëŠ” CPUë¡œ ì´ˆê¸°í™” (ì‹¤í–‰ ì‹œ ë™ì  ìŠ¤ì™‘)
                if self.config.memory_mode == MemoryMode.MEDIUM:
                    target_device = torch.device('cpu')
                    self.logger.info(f"   - {name}: CPU ì´ˆê¸°í™” (ë™ì  ìŠ¤ì™‘ ëŒ€ê¸°)")
                else:
                    target_device = self.config.device
                net.to(target_device)
                params = sum(p.numel() for p in net.parameters())
                self.logger.info(f"   - {name}: {params/1e6:.2f}M params")
            
            # Phase ì¶œë ¥ í”„ë¡œì í„° ì´ˆê¸°í™” (896 â†’ 768)
            # MEDIUM ëª¨ë“œì—ì„œë„ í”„ë¡œì í„°ëŠ” CPUë¡œ
            projector_device = torch.device('cpu') if self.config.memory_mode == MemoryMode.MEDIUM else self.config.device
            self.phase_output_projector = nn.Linear(896, 768).to(projector_device)
            self.logger.info("   - output_projector: 896â†’768 ì°¨ì› ë³€í™˜ê¸° ì´ˆê¸°í™”")
            
            self.logger.info("   âœ… Phase Networks ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"   âŒ Phase Networks ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"Phase Networks ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _init_dsm_early(self):
        """DSM ì´ˆê¸° ì´ˆê¸°í™” (UnifiedModel ë¡œë“œ ì „)"""
        self.logger.info("ğŸ”„ Dynamic Swap Manager ì´ˆê¸° ì´ˆê¸°í™”...")
        
        try:
            from dynamic_swap_manager import DynamicSwapManager, set_swap_manager
            
            # DSM ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± 
            self.swap_manager = DynamicSwapManager.get_instance()
            set_swap_manager(self.swap_manager)
            
            self.logger.info(f"   âœ… DSM ì´ˆê¸°í™” ì™„ë£Œ (ID: {id(self.swap_manager)})")
            
        except Exception as e:
            self.logger.warning(f"   âš ï¸ DSM ì´ˆê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # DSM ì—†ì´ë„ ê³„ì† ì§„í–‰
    
    async def _load_llm_integration(self):
        """LLM í†µí•© ë¡œë“œ ë° ìŠ¤ì™‘ ë§¤ë‹ˆì € ì„¤ì •"""
        self.logger.info(f"ğŸ¤– LLM í†µí•© ë¡œë“œ ì¤‘ (ëª¨ë“œ: {self.config.llm_mode})...")
        
        try:
            # API ëª¨ë“œ ëª©ë¡ ì •ì˜ (claudeëŠ” ë³„ë„ ì²˜ë¦¬)
            api_modes = ['gpt', 'perplexity', 'deepseek']
            
            # API ëª¨ë“œ ì²´í¬ë¥¼ ë¨¼ì € ìˆ˜í–‰
            if self.config.llm_mode in api_modes:
                # API ëª¨ë“œ (GPT, Perplexity, DeepSeek)
                self.logger.info(f"   ğŸŒ API ëª¨ë“œ í™œì„±í™”: {self.config.llm_mode}")
                
                # Dynamic Swap Manager ì´ˆê¸°í™” (ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ)
                if not hasattr(self, 'swap_manager') or self.swap_manager is None:
                    self.logger.info("   ğŸ”„ Dynamic Swap Manager ì´ˆê¸°í™”...")
                    from dynamic_swap_manager import DynamicSwapManager, set_swap_manager
                    
                    # DSM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì „ì—­ ì„¤ì •
                    self.swap_manager = DynamicSwapManager.get_instance()
                    set_swap_manager(self.swap_manager)  # dynamic_swap_manager.pyì˜ ì „ì—­ ì„¤ì •
                    
                    self.logger.info(f"   âœ… DSM ì´ˆê¸°í™” ì™„ë£Œ (ID: {id(self.swap_manager)})")
                else:
                    self.logger.info(f"   ğŸ“Œ DSM ì´ë¯¸ ì´ˆê¸°í™”ë¨ (ID: {id(self.swap_manager)})")
                
                # LLM ì—”ì§„ ì´ˆê¸°í™”
                from llm_module.advanced_llm_engine import AdvancedLLMEngine, set_llm_engine
                
                self.llm_engine = AdvancedLLMEngine(use_api=self.config.llm_mode)
                
                # ì „ì—­ LLM ì—”ì§„ ì„¤ì • (ë‹¤ë¥¸ ëª¨ë“ˆë“¤ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡)
                set_llm_engine(self.llm_engine)
                
                # Advanced Wrappersê°€ ì´ë¯¸ ë¡œë“œëœ ê²½ìš° LLM ì—”ì§„ ì—…ë°ì´íŠ¸
                if hasattr(self, 'advanced_wrappers') and self.advanced_wrappers:
                    for wrapper_name, wrapper in self.advanced_wrappers.items():
                        if hasattr(wrapper, 'llm_engine'):
                            wrapper.llm_engine = self.llm_engine
                            self.logger.info(f"   ğŸ“Œ {wrapper_name} LLM ì—”ì§„ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                
                self.logger.info(f"   âœ… {self.config.llm_mode.upper()} API ì—”ì§„ ì´ˆê¸°í™” ë° ì „ì—­ ì„¤ì • ì™„ë£Œ")
                
            elif self.config.llm_mode == "local":
                # ë¡œì»¬ LLM (Dolphin Llama3 8B) - ìŠ¤ì™‘ ë§¤ë‹ˆì € ì‚¬ìš©
                self.logger.info("   ë©”ëª¨ë¦¬ ìŠ¤ì™‘ ë§¤ë‹ˆì € ì´ˆê¸°í™”...")
                
                # ìŠ¤ì™‘ ë§¤ë‹ˆì € ì„¤ì •
                swap_config = {
                    'gpu_threshold': 7000,  # 8GB GPU ê¸°ì¤€
                    'ram_threshold': 16000,
                    'llm_model_path': self.config.llm_model_path,
                    'generate_explanation': True,
                    'enable_optimization': True
                }
                
                self.swap_manager = SystemSwapManager(swap_config)
                
                # Red Heartë¥¼ RAMì— ëŒ€ê¸° (MD ë¬¸ì„œ: ì´ˆê¸° ìƒíƒœ)
                await self.swap_manager.initialize(
                    red_heart_system=self,  # í˜„ì¬ ì‹œìŠ¤í…œ ì „ë‹¬
                    llm_model=None  # LLMì€ ì•„ì§ ë¡œë“œí•˜ì§€ ì•ŠìŒ
                )
                
                self.logger.info("   âœ… ë©”ëª¨ë¦¬ ìŠ¤ì™‘ ë§¤ë‹ˆì € ì„¤ì • ì™„ë£Œ")
                self.logger.info("   ğŸ“Œ Red HeartëŠ” RAMì—, LLMì€ í•„ìš”ì‹œ ë¡œë“œ")
                
                # ë¡œì»¬ LLM ì—”ì§„ ì´ˆê¸°í™”
                from llm_module.advanced_llm_engine import AdvancedLLMEngine, set_llm_engine
                
                self.llm_engine = AdvancedLLMEngine()
                
                # ì „ì—­ LLM ì—”ì§„ ì„¤ì •
                set_llm_engine(self.llm_engine)
                
                # Advanced Wrappersê°€ ì´ë¯¸ ë¡œë“œëœ ê²½ìš° LLM ì—”ì§„ ì—…ë°ì´íŠ¸
                if hasattr(self, 'advanced_wrappers') and self.advanced_wrappers:
                    for wrapper_name, wrapper in self.advanced_wrappers.items():
                        if hasattr(wrapper, 'llm_engine'):
                            wrapper.llm_engine = self.llm_engine
                            self.logger.info(f"   ğŸ“Œ {wrapper_name} LLM ì—”ì§„ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                
                self.logger.info("   âœ… Local LLM ì—”ì§„ ì´ˆê¸°í™” ë° ì „ì—­ ì„¤ì • ì™„ë£Œ (CPU ëŒ€ê¸°)")
                self.logger.info(f"   ğŸ“Œ ëª¨ë¸: Dolphin Llama3 8B")
                
            elif self.config.llm_mode == "claude":
                # Claude API í†µí•© (DSM ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, ì§ì ‘ GPU ê´€ë¦¬)
                self.logger.info("   ğŸŒ Claude API ëª¨ë“œ í™œì„±í™” (DSM ë¹„í™œì„±í™”)")
                self.logger.info("   ğŸ¯ GPU ì§ì ‘ ê´€ë¦¬ ëª¨ë“œë¡œ ì „í™˜")
                
                # GPU ì§ì ‘ ê´€ë¦¬ë¥¼ ìœ„í•œ í—¬í¼ í´ë˜ìŠ¤
                import gc
                class DirectGPUManager:
                    def __init__(self, logger):
                        self.logger = logger
                        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                        self.models_on_gpu = {}
                        
                    def clear_gpu_cache(self):
                        """GPU ìºì‹œ ì •ë¦¬"""
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                        gc.collect()
                        self.logger.info("   ğŸ§¹ GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                        
                    def get_gpu_memory(self):
                        """GPU ë©”ëª¨ë¦¬ í˜„í™© í™•ì¸"""
                        if torch.cuda.is_available():
                            allocated = torch.cuda.memory_allocated() / 1024**3
                            reserved = torch.cuda.memory_reserved() / 1024**3
                            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                            return allocated, reserved, total
                        return 0, 0, 0
                        
                    def move_to_gpu(self, model, name):
                        """ëª¨ë¸ì„ GPUë¡œ ì´ë™"""
                        allocated, reserved, total = self.get_gpu_memory()
                        self.logger.info(f"   ğŸ“Š GPU ë©”ëª¨ë¦¬: {allocated:.2f}/{total:.2f}GB ì‚¬ìš©ì¤‘")
                        
                        # ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ìºì‹œ ì •ë¦¬
                        if allocated > total * 0.8:
                            self.clear_gpu_cache()
                            
                        model = model.to(self.device)
                        self.models_on_gpu[name] = model
                        self.logger.info(f"   âœ… {name} GPU ë¡œë“œ ì™„ë£Œ")
                        return model
                        
                    def move_to_cpu(self, model, name):
                        """ëª¨ë¸ì„ CPUë¡œ ì´ë™"""
                        model = model.cpu()
                        if name in self.models_on_gpu:
                            del self.models_on_gpu[name]
                        self.clear_gpu_cache()
                        self.logger.info(f"   âœ… {name} CPUë¡œ ì–¸ë¡œë“œ ì™„ë£Œ")
                        return model
                
                # GPU ë§¤ë‹ˆì € ìƒì„± (DSM ëŒ€ì‹ )
                self.gpu_manager = DirectGPUManager(self.logger)
                
                # Claude API ì—”ì§„ ì´ˆê¸°í™”
                from llm_module.advanced_llm_engine import AdvancedLLMEngine, set_llm_engine
                self.llm_engine = AdvancedLLMEngine(use_api='claude')
                
                # ì „ì—­ LLM ì—”ì§„ ì„¤ì •
                set_llm_engine(self.llm_engine)
                
                # Advanced Wrappersê°€ ì´ë¯¸ ë¡œë“œëœ ê²½ìš° LLM ì—”ì§„ ì—…ë°ì´íŠ¸
                if hasattr(self, 'advanced_wrappers') and self.advanced_wrappers:
                    for wrapper_name, wrapper in self.advanced_wrappers.items():
                        if hasattr(wrapper, 'llm_engine'):
                            wrapper.llm_engine = self.llm_engine
                            self.logger.info(f"   ğŸ“Œ {wrapper_name} LLM ì—”ì§„ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                
                # DSM ë¹„í™œì„±í™” í”Œë˜ê·¸ ì„¤ì •
                self.use_dsm = False
                self.swap_manager = None
                
                self.logger.info("   âœ… Claude API ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (ì§ì ‘ GPU ê´€ë¦¬)")
                
            elif self.config.llm_mode == "mcp":
                # MCP í”„ë¡œí† ì½œ
                self.logger.info("   ğŸŒ MCP í”„ë¡œí† ì½œ ëª¨ë“œ í™œì„±í™”")
                
                from llm_module.mcp_client import MCPClient, get_mcp_client
                from llm_module.advanced_llm_engine import set_llm_engine
                
                # MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ì—°ê²°
                self.llm_engine = MCPClient()
                connected = await self.llm_engine.connect()
                
                if connected:
                    # ì „ì—­ ì„¤ì •
                    set_llm_engine(self.llm_engine)
                    
                    # Advanced Wrappersê°€ ì´ë¯¸ ë¡œë“œëœ ê²½ìš° LLM ì—”ì§„ ì—…ë°ì´íŠ¸
                    if hasattr(self, 'advanced_wrappers') and self.advanced_wrappers:
                        for wrapper_name, wrapper in self.advanced_wrappers.items():
                            if hasattr(wrapper, 'llm_engine'):
                                wrapper.llm_engine = self.llm_engine
                                self.logger.info(f"   ğŸ“Œ {wrapper_name} LLM ì—”ì§„ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    
                    self.logger.info("   âœ… MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì—°ê²° ì™„ë£Œ")
                    self.logger.info("   ğŸ“Œ Red Heart Ethics ì„œë²„ì™€ í†µì‹  ì¤‘")
                else:
                    self.logger.warning("   âš ï¸ MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨")
                    self.logger.info("   ğŸ’¡ MCP ì„œë²„ ì‹œì‘: python mcp_server.py")
                    raise RuntimeError("MCP ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤. mcp_server.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                
        except Exception as e:
            self.logger.warning(f"   âš ï¸ LLM í†µí•© ì‹¤íŒ¨: {e}")
            # llm_modeëŠ” ìœ ì§€í•˜ê³  DSM ë“±ë¡ì€ ê³„ì† ì§„í–‰
        
        # DynamicSwapManagerê°€ ì´ˆê¸°í™”ë˜ì—ˆìœ¼ë©´ í—¤ë“œë“¤ì„ ë“±ë¡ (SystemSwapManagerëŠ” ì œì™¸)
        if hasattr(self, 'swap_manager') and self.swap_manager and hasattr(self.swap_manager, 'register_model'):
            try:
                self.logger.info("   ğŸ“Œ UnifiedModel í—¤ë“œë“¤ì„ DSMì— ë“±ë¡...")
                
                # ë°±ë³¸ê³¼ í—¤ë“œë“¤ì„ ê°œë³„ ë“±ë¡
                if hasattr(self, 'unified_model') and self.unified_model:
                    from dynamic_swap_manager import SwapPriority
                    
                    # ë°±ë³¸ì€ CRITICAL ìš°ì„ ìˆœìœ„
                    self.swap_manager.register_model(
                        'unified_backbone', 
                        self.unified_model.backbone, 
                        priority=SwapPriority.CRITICAL,
                        owner_obj=self.unified_model,
                        owner_attr='backbone'
                    )
                    
                    # í—¤ë“œë“¤ì€ HIGH ìš°ì„ ìˆœìœ„ - owner ì •ë³´ í¬í•¨
                    self.swap_manager.register_model(
                        'emotion_head', 
                        self.unified_model.emotion_head,
                        priority=SwapPriority.HIGH,
                        owner_obj=self.unified_model,
                        owner_attr='emotion_head'
                    )
                    self.swap_manager.register_model(
                        'bentham_head', 
                        self.unified_model.bentham_head,
                        priority=SwapPriority.HIGH,
                        owner_obj=self.unified_model,
                        owner_attr='bentham_head'
                    )
                    self.swap_manager.register_model(
                        'regret_head', 
                        self.unified_model.regret_head,
                        priority=SwapPriority.HIGH,
                        owner_obj=self.unified_model,
                        owner_attr='regret_head'
                    )
                    self.swap_manager.register_model(
                        'surd_head', 
                        self.unified_model.surd_head,
                        priority=SwapPriority.HIGH,
                        owner_obj=self.unified_model,
                        owner_attr='surd_head'
                    )
                    
                    self.logger.info("   âœ… UnifiedModel DSM ë“±ë¡ ì™„ë£Œ (API ëª¨ë“œ)")
                
                # Neural Analyzersë„ ë“±ë¡ (ì´ë¯¸ ë¡œë“œëœ ê²½ìš°)
                if hasattr(self, 'neural_analyzers') and self.neural_analyzers:
                    for name, analyzer in self.neural_analyzers.items():
                        self.swap_manager.register_model(
                            f'neural_{name}', 
                            analyzer,
                            priority=SwapPriority.MEDIUM
                        )
                    self.logger.info("   âœ… Neural Analyzers DSM ë“±ë¡ ì™„ë£Œ")
                
                # Advanced Wrappersë„ ë“±ë¡ (ì´ë¯¸ ë¡œë“œëœ ê²½ìš°)
                if hasattr(self, 'advanced_wrappers') and self.advanced_wrappers:
                    for name, wrapper in self.advanced_wrappers.items():
                        self.swap_manager.register_model(
                            f'wrapper_{name}', 
                            wrapper,
                            priority=SwapPriority.MEDIUM
                        )
                    self.logger.info("   âœ… Advanced Wrappers DSM ë“±ë¡ ì™„ë£Œ")
                    
            except Exception as dsm_error:
                self.logger.warning(f"   âš ï¸ DSM í—¤ë“œ ë“±ë¡ ì‹¤íŒ¨: {dsm_error}")
    
    async def _load_translator(self):
        """ë²ˆì—­ê¸° ë¡œë“œ (ì„ íƒì )"""
        self.logger.info("ğŸŒ ë²ˆì—­ê¸° ë¡œë“œ ì¤‘...")
        
        try:
            from local_translator import LocalTranslator
            from config import register_system_module
            
            self.translator = LocalTranslator()
            
            # initialize() ë©”ì„œë“œê°€ ìˆìœ¼ë©´ í˜¸ì¶œ
            if hasattr(self.translator, 'initialize'):
                await self.translator.initialize()
            
            # ì‹œìŠ¤í…œ ëª¨ë“ˆë¡œ ë“±ë¡ (Advanced Emotion Wrapperê°€ ì°¾ì„ ìˆ˜ ìˆë„ë¡)
            register_system_module('translator', self.translator)
            
            self.logger.info("   âœ… ë²ˆì—­ê¸° ë¡œë“œ ë° ë“±ë¡ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"   âŒ ë²ˆì—­ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ë²ˆì—­ê¸° ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_workflow_memory_manager(self):
        """ì›Œí¬í”Œë¡œìš° ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë¡œë“œ (5M)"""
        self.logger.info("ğŸ§  ì›Œí¬í”Œë¡œìš° ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë¡œë“œ ì¤‘...")
        try:
            from workflow_aware_memory_manager import WorkflowAwareMemoryManager
            
            self.workflow_memory_manager = WorkflowAwareMemoryManager()
            # initialize ë©”ì„œë“œ ì—†ìŒ - ìƒì„±ìë¡œ ì¶©ë¶„
            self.logger.info("   âœ… ì›Œí¬í”Œë¡œìš° ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"   âŒ ì›Œí¬í”Œë¡œìš° ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ì›Œí¬í”Œë¡œìš° ë©”ëª¨ë¦¬ ê´€ë¦¬ì ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_meta_integration(self):
        """ë©”íƒ€ í†µí•© ì‹œìŠ¤í…œ ë¡œë“œ (40M)"""
        self.logger.info("ğŸ”® ë©”íƒ€ í†µí•© ì‹œìŠ¤í…œ ë¡œë“œ ì¤‘ (40M)...")
        try:
            from advanced_meta_integration_system import AdvancedMetaIntegrationSystem
            
            self.meta_integration = AdvancedMetaIntegrationSystem()
            # AdvancedMetaIntegrationSystemì€ ë‚´ë¶€ì— integration_networkë¥¼ ê°€ì§€ê³  ìˆìŒ
            # ì´ë¯¸ ìƒì„±ìì—ì„œ .to(device)ì™€ .eval() ì²˜ë¦¬ë¨
            
            # ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
            if hasattr(self.meta_integration, 'integration_network'):
                params = sum(p.numel() for p in self.meta_integration.integration_network.parameters())
                self.logger.info(f"   ğŸ“Š ë©”íƒ€ í†µí•© íŒŒë¼ë¯¸í„°: {params/1e6:.1f}M")
            
            self.logger.info("   âœ… ë©”íƒ€ í†µí•© ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"   âŒ ë©”íƒ€ í†µí•© ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ë©”íƒ€ í†µí•© ì‹œìŠ¤í…œ ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_counterfactual_reasoning(self):
        """ë°˜ì‚¬ì‹¤ ì¶”ë¡  ì‹œìŠ¤í…œ ë¡œë“œ (15M)"""
        self.logger.info("ğŸ’­ ë°˜ì‚¬ì‹¤ ì¶”ë¡  ì‹œìŠ¤í…œ ë¡œë“œ ì¤‘...")
        try:
            from advanced_counterfactual_reasoning import AdvancedCounterfactualReasoning
            
            self.counterfactual_reasoning = AdvancedCounterfactualReasoning()
            self.logger.info("   âœ… ë°˜ì‚¬ì‹¤ ì¶”ë¡  ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"   âŒ ë°˜ì‚¬ì‹¤ ì¶”ë¡  ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ë°˜ì‚¬ì‹¤ ì¶”ë¡  ì‹œìŠ¤í…œ ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_advanced_regret_learning(self):
        """ê³ ê¸‰ í›„íšŒ í•™ìŠµ ì‹œìŠ¤í…œ ë¡œë“œ (20M)"""
        self.logger.info("ğŸ˜” ê³ ê¸‰ í›„íšŒ í•™ìŠµ ì‹œìŠ¤í…œ ë¡œë“œ ì¤‘...")
        try:
            from advanced_regret_learning_system import AdvancedRegretLearningSystem
            
            self.advanced_regret_learning = AdvancedRegretLearningSystem()
            self.logger.info("   âœ… ê³ ê¸‰ í›„íšŒ í•™ìŠµ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"   âŒ ê³ ê¸‰ í›„íšŒ í•™ìŠµ ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ê³ ê¸‰ í›„íšŒ í•™ìŠµ ì‹œìŠ¤í…œ ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_temporal_propagation(self):
        """ì‹œê³„ì—´ ì „íŒŒ ë¶„ì„ê¸° ë¡œë“œ"""
        self.logger.info("â° ì‹œê³„ì—´ ì „íŒŒ ë¶„ì„ê¸° ë¡œë“œ ì¤‘...")
        try:
            from temporal_event_propagation_analyzer import TemporalEventPropagationAnalyzer
            
            self.temporal_propagator = TemporalEventPropagationAnalyzer()
            self.logger.info("   âœ… ì‹œê³„ì—´ ì „íŒŒ ë¶„ì„ê¸° ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"   âŒ ì‹œê³„ì—´ ì „íŒŒ ë¶„ì„ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ì‹œê³„ì—´ ì „íŒŒ ë¶„ì„ê¸° ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_experience_database(self):
        """ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"""
        self.logger.info("ğŸ’¾ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...")
        try:
            from advanced_experience_database import AdvancedExperienceDatabase
            
            # AdvancedExperienceDatabaseëŠ” __init__ì—ì„œ ëª¨ë“  ì´ˆê¸°í™” ì™„ë£Œ
            self.experience_database = AdvancedExperienceDatabase()
            self.logger.info("   âœ… ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"   âŒ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_emotion_hierarchy(self):
        """ê³„ì¸µì  ê°ì • ì²˜ë¦¬ê¸° ë¡œë“œ"""
        self.logger.info("ğŸ­ ê³„ì¸µì  ê°ì • ì²˜ë¦¬ê¸° ë¡œë“œ ì¤‘...")
        try:
            from emotion_ethics_regret_circuit import EmotionEthicsRegretCircuit
            
            # EmotionEthicsRegretCircuitì—ì„œ ê³„ì¸µì  ë¡œì§ ì¶”ì¶œ
            self.emotion_hierarchy_processor = EmotionEthicsRegretCircuit()
            self.logger.info("   âœ… ê³„ì¸µì  ê°ì • ì²˜ë¦¬ê¸° ë¡œë“œ ì™„ë£Œ (ê³µë™ì²´>íƒ€ì>ìì•„)")
        except Exception as e:
            self.logger.error(f"   âŒ ê³„ì¸µì  ê°ì • ì²˜ë¦¬ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ê³„ì¸µì  ê°ì • ì²˜ë¦¬ê¸° ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_precision_mapper(self):
        """ì •ë°€ ê°ì •â†’ë²¤ë‹´ ë§¤í¼ ë¡œë“œ - í•„ìˆ˜ êµ¬ì„± ìš”ì†Œ"""
        self.logger.info("ğŸ¯ ì •ë°€ ê°ì •â†’ë²¤ë‹´ ë§¤í¼ ì´ˆê¸°í™”...")
        
        # ì˜ë¯¸ë¡ ì  ë§¤í¼ëŠ” í•­ìƒ ë¡œë“œ (í•„ìˆ˜) - LIGHT ëª¨ë“œ í¬í•¨ ëª¨ë“  ëª¨ë“œ
        if self.config.memory_mode.value in ['light', 'medium', 'heavy', 'mcp', 'normal', 'ultra', 'extreme']:
            self.emotion_bentham_mapper = SemanticEmotionBenthamMapper()
            self.logger.info("   âœ… ì˜ë¯¸ë¡ ì  ë§¤í¼ í™œì„±í™”")
            
            # EXTREME ëª¨ë“œì—ì„œëŠ” ì‹ ê²½ë§ ì–´ëŒ‘í„°ë„ ë¡œë“œ
            if self.config.memory_mode == MemoryMode.EXTREME:
                self.neural_emotion_adapter = NeuralEmotionBenthamAdapter()
                self.neural_emotion_adapter.eval()
                self.neural_emotion_adapter.to(self.config.device)
                self.logger.info("   âœ… ì‹ ê²½ë§ ì–´ëŒ‘í„° í™œì„±í™”")
        else:
            # MINIMAL ë“± ë¯¸ì •ì˜ ëª¨ë“œë„ ê¸°ë³¸ ë§¤í¼ ì‚¬ìš©
            self.emotion_bentham_mapper = SemanticEmotionBenthamMapper()
            self.logger.info("   âœ… ê¸°ë³¸ ì˜ë¯¸ë¡ ì  ë§¤í¼ í™œì„±í™”")
    
    async def _load_three_view_scenario_system(self):
        """3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ ë¡œë“œ (20M)"""
        self.logger.info("ğŸ”º 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ ë¡œë“œ ì¤‘...")
        try:
            # 3ë·° ì‹œìŠ¤í…œì€ ë°˜ì‚¬ì‹¤ ì¶”ë¡ ê³¼ ë³‘í•©í•˜ì—¬ ì‚¬ìš©
            self.three_view_system = ThreeViewScenarioSystem(device=self.config.device)
            self.logger.info("   âœ… 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ (ë‚™ê´€/ì¤‘ë„/ë¹„ê´€)")
        except Exception as e:
            self.logger.error(f"   âŒ 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"3ë·° ì‹œë‚˜ë¦¬ì˜¤ ì‹œìŠ¤í…œ ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    async def _load_multi_ethics_system(self):
        """ë‹¤ì›ì  ìœ¤ë¦¬ ì²´ê³„ ë¡œë“œ (30M) - 5ê°œ ìœ¤ë¦¬í•™íŒŒ"""
        self.logger.info("âš–ï¸ ë‹¤ì›ì  ìœ¤ë¦¬ ì²´ê³„ ë¡œë“œ ì¤‘...")
        try:
            # ì „ì²´ ì‹œìŠ¤í…œ ë¡œë“œ
            self.multi_ethics_system = DeepMultiDimensionalEthicsSystem()
            
            # ê°œë³„ ì—”ì§„ë“¤ë„ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ì €ì¥ (MD ë¬¸ì„œ Bì•ˆ: 5ê°œ í•™íŒŒ)
            self.ethics_engines = {
                'utilitarianism': UtilitarianEngine(),      # ê³µë¦¬ì£¼ì˜
                'deontological': DeontologicalEngine(),     # ì˜ë¬´ë¡ 
                'virtue_ethics': VirtueEthicsEngine(),      # ë•ìœ¤ë¦¬
                'care_ethics': CareEthicsEngine(),          # ëŒë´„ìœ¤ë¦¬
                'justice_theory': JusticeTheoryEngine()     # ì •ì˜ë¡  (MD ë¬¸ì„œ Bì•ˆ)
            }
            
            # ë©”ëª¨ë¦¬ ëª¨ë“œì— ë”°ë¥¸ ì„ íƒì  ë¡œë“œ
            if self.config.memory_mode in [MemoryMode.HEAVY, MemoryMode.ULTRA, MemoryMode.EXTREME]:
                # HEAVY ì´ìƒì—ì„œëŠ” ëª¨ë“  ì—”ì§„ í™œì„±í™”
                self.logger.info("   âœ… ì „ì²´ ìœ¤ë¦¬ ì—”ì§„ í™œì„±í™” (5ê°œ í•™íŒŒ - MD ë¬¸ì„œ Bì•ˆ)")
            elif self.config.memory_mode in [MemoryMode.MEDIUM, MemoryMode.NORMAL]:
                # MEDIUM/NORMAL ëª¨ë“œì—ì„œëŠ” í•µì‹¬ 3ê°œë§Œ
                limited_engines = ['utilitarianism', 'deontological', 'virtue_ethics']
                self.ethics_engines = {k: v for k, v in self.ethics_engines.items() if k in limited_engines}
                self.logger.info("   âœ… í•µì‹¬ ìœ¤ë¦¬ ì—”ì§„ í™œì„±í™” (3ê°œ í•™íŒŒ)")
            else:
                # LIGHT/MINIMALì—ì„œëŠ” ê³µë¦¬ì£¼ì˜ë§Œ
                self.ethics_engines = {'utilitarianism': self.ethics_engines['utilitarianism']}
                self.logger.info("   âœ… ê¸°ë³¸ ìœ¤ë¦¬ ì—”ì§„ í™œì„±í™” (ê³µë¦¬ì£¼ì˜ë§Œ)")
                
        except Exception as e:
            self.logger.error(f"   âŒ ë‹¤ì›ì  ìœ¤ë¦¬ ì²´ê³„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ë‹¤ì›ì  ìœ¤ë¦¬ ì²´ê³„ ë¡œë“œ í•„ìˆ˜ - ì‹¤íŒ¨: {e}")
    
    def emotion_to_bentham_converter(self, emotion_data: Dict) -> Dict:
        """ì •ë°€ ì˜ë¯¸ë¡ ì  ë§¤í•‘ ê¸°ë°˜ ê°ì •â†’ë²¤ë‹´ ë³€í™˜ v2
        
        ê°œì„ ì‚¬í•­:
        - 6ì°¨ì› ê°ì •ê³¼ 10ì°¨ì› ë²¤ë‹´ì˜ ì˜ë¯¸ë¡ ì  ì—°ê²°
        - ê³„ì¸µì  ì²˜ë¦¬ ì§€ì› (ê³µë™ì²´>íƒ€ì>ìì•„)
        - ì‹ ê²½ë§ ì–´ëŒ‘í„° ì˜µì…˜ (EXTREME ëª¨ë“œ)
        """
        
        # ì •ë°€ ë§¤í¼ í•„ìˆ˜ - fallback ì—†ìŒ
        if self.emotion_bentham_mapper is None:
            raise RuntimeError("SemanticEmotionBenthamMapperê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - ì •ë°€ ë§¤í•‘ ì‹œìŠ¤í…œ í•„ìˆ˜")
            
        # ê³„ì¸µ ë ˆë²¨ í™•ì¸
        hierarchy_level = 'self'
        if 'hierarchy' in emotion_data:
            if emotion_data['hierarchy'].get('community'):
                hierarchy_level = 'community'
            elif emotion_data['hierarchy'].get('other'):
                hierarchy_level = 'other'
        
        # ì˜ë¯¸ë¡ ì  ë§¤í•‘ ìˆ˜í–‰
        bentham_params = self.emotion_bentham_mapper.map_with_hierarchy(
            emotion_data, 
            hierarchy_level
        )
        
        # EXTREME ëª¨ë“œì—ì„œ ì‹ ê²½ë§ ì–´ëŒ‘í„°ë¡œ ì¶”ê°€ ì •ì œ
        if self.neural_emotion_adapter is not None and 'scores' in emotion_data:
            scores = emotion_data['scores']
            if isinstance(scores, list) and len(scores) >= 6:
                emotion_tensor = torch.tensor(scores[:6], dtype=torch.float32)
                emotion_tensor = emotion_tensor.unsqueeze(0).to(self.config.device)
                
                with torch.no_grad():
                    neural_output = self.neural_emotion_adapter(emotion_tensor)
                    neural_bentham = neural_output[0].cpu().numpy()
                
                # ì˜ë¯¸ë¡ ì  ê²°ê³¼ì™€ ì‹ ê²½ë§ ê²°ê³¼ í˜¼í•© (7:3 ë¹„ìœ¨)
                for idx, key in enumerate(bentham_params.keys()):
                    if idx < len(neural_bentham):
                        bentham_params[key] = bentham_params[key] * 0.7 + neural_bentham[idx] * 0.3
                        
                self.logger.debug("   ì‹ ê²½ë§ ì–´ëŒ‘í„°ë¡œ ë²¤ë‹´ íŒŒë¼ë¯¸í„° ì •ì œ ì™„ë£Œ")
        
        # ì‹œê³„ì—´ ì „íŒŒê°€ ì´ë¯¸ ì ìš©ëœ ê²½ìš° ë³´ì¡´
        if 'temporal_duration' in emotion_data:
            bentham_params['duration'] = emotion_data['temporal_duration']
        if 'temporal_fecundity' in emotion_data:
            bentham_params['fecundity'] = emotion_data['temporal_fecundity']
        
        return bentham_params
    
    async def _load_idle_learner(self):
        """ìœ íœ´ ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ ë¡œë“œ - MD ë¬¸ì„œ: í”„ë¡œë•ì…˜ ë ˆë²¨ì—ì„œ í™œì„±í™”"""
        # MD ë¬¸ì„œ ì‚¬ì–‘: ìœ íœ´ í•™ìŠµì€ ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ í”„ë¡œë•ì…˜ ë ˆë²¨ì—ì„œ í™œì„±í™”
        self.logger.info("ğŸŒ™ ìœ íœ´ ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ - í˜„ì¬ ë¹„í™œì„±í™” (í”„ë¡œë•ì…˜ì—ì„œ í™œì„±í™”)")
        self.idle_learner = None  # ëª…ì‹œì ìœ¼ë¡œ None ì„¤ì •
        
        # TODO: í”„ë¡œë•ì…˜ ë ˆë²¨ì—ì„œ ì•„ë˜ ì½”ë“œ í™œì„±í™”
        """
        try:
            from idle_time_learner import HierarchicalIdleLearner
            
            # ìœ íœ´ í•™ìŠµê¸° ìƒì„±
            self.idle_learner = HierarchicalIdleLearner(
                model=self.unified_model,
                config=self.config
            )
            
            # í•™ìŠµ ë°ì´í„° ì†ŒìŠ¤ ë“±ë¡
            if self.experience_database:
                self.idle_learner.register_data_source(self.experience_database)
            
            # ìœ íœ´ í•™ìŠµ ì‹œì‘
            await self.idle_learner.start()
            
            self.logger.info("   âœ… ìœ íœ´ í•™ìŠµ ì‹œìŠ¤í…œ í™œì„±í™”")
            self.logger.info(f"   - ì¦‰ì‹œ í•™ìŠµ: 60ì´ˆ ìœ íœ´ ì‹œ")
            self.logger.info(f"   - ë‹¨ê¸° í•™ìŠµ: 10ë¶„ ìœ íœ´ ì‹œ")
            self.logger.info(f"   - ì¤‘ê¸° í•™ìŠµ: 30ë¶„ ìœ íœ´ ì‹œ")
            self.logger.info(f"   - ì¥ê¸° í•™ìŠµ: 1ì‹œê°„ ìœ íœ´ ì‹œ")
            self.logger.info(f"   - ì•¼ê°„ í•™ìŠµ: 8ì‹œê°„ ìœ íœ´ ì‹œ")
            
        except Exception as e:
            self.logger.warning(f"   âš ï¸ ìœ íœ´ í•™ìŠµ ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.idle_learner = None
        """
    
    def _print_system_status(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¶œë ¥"""
        self.logger.info("\nğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ:")
        self.logger.info(f"   ë””ë°”ì´ìŠ¤: {self.config.device}")
        self.logger.info(f"   ë©”ëª¨ë¦¬ ëª¨ë“œ: {self.config.memory_mode.value}")
        self.logger.info(f"   UnifiedModel: {'âœ…' if self.unified_model else 'âŒ'}")
        self.logger.info(f"   Neural Analyzers (368M): {'âœ…' if self.config.use_neural_analyzers else 'âŒ'}")
        self.logger.info(f"   Advanced Wrappers (112M): {'âœ…' if self.config.use_advanced_wrappers else 'âŒ'}")
        self.logger.info(f"   DSP Simulator (14M): {'âœ…' if self.config.use_dsp_simulator else 'âŒ'}")
        self.logger.info(f"   Kalman Filter: {'âœ…' if self.config.use_kalman_filter else 'âŒ'}")
        self.logger.info(f"   Phase Networks: {'âœ…' if self.config.use_phase_networks else 'âŒ'}")
        self.logger.info(f"   Regret Circuit: {'âœ…' if self.config.use_regret_circuit else 'âŒ'}")
        self.logger.info(f"   ë©”íƒ€ í†µí•© (40M): {'âœ…' if self.config.use_meta_integration else 'âŒ'}")
        self.logger.info(f"   ë°˜ì‚¬ì‹¤ ì¶”ë¡  (15M): {'âœ…' if self.config.use_counterfactual_reasoning else 'âŒ'}")
        self.logger.info(f"   ê³ ê¸‰ í›„íšŒ í•™ìŠµ (20M): {'âœ…' if self.config.use_advanced_regret_learning else 'âŒ'}")
        self.logger.info(f"   ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ì: {'âœ…' if self.config.use_workflow_memory_manager else 'âŒ'}")
        self.logger.info(f"   ì‹œê³„ì—´ ì „íŒŒ: {'âœ…' if self.config.use_temporal_propagation else 'âŒ'}")
        self.logger.info(f"   ê²½í—˜ DB: {'âœ…' if self.config.use_experience_database else 'âŒ'}")
        self.logger.info(f"   ê³„ì¸µì  ê°ì •: {'âœ…' if self.config.use_emotion_hierarchy else 'âŒ'}")
        self.logger.info(f"   ìœ íœ´ í•™ìŠµ: {'âœ…' if self.idle_learner else 'âŒ'}")
        self.logger.info(f"   LLM ëª¨ë“œ: {self.config.llm_mode}")
        self.logger.info(f"   ë²ˆì—­ê¸°: {'âœ…' if self.config.use_translator else 'âŒ'}")
    
    async def analyze(self, text: str, **kwargs) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ë¶„ì„ (ëª¨ë“  ëª¨ë“ˆ í™œìš©)"""
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        try:
            # ìºì‹œ í™•ì¸ - kwargsë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•´ í•´ì‹œ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦
            import json
            kwargs_str = json.dumps(kwargs, sort_keys=True, default=str)
            cache_key = f"{text[:50]}_{hash(kwargs_str)}"
            if cache_key in self.cache:
                self.logger.info("   ğŸ“¦ ìºì‹œ íˆíŠ¸")
                return self.cache[cache_key]
            
            # í•œêµ­ì–´ ê°ì§€ ë° ë²ˆì—­
            original_text = text
            if self.config.use_translator and self._is_korean(text):
                self.logger.info("   ğŸŒ í•œêµ­ì–´ ê°ì§€ - ë²ˆì—­ ì¤‘...")
                text = self.translator.translate_ko_to_en(text)
                self.logger.info(f"   ë²ˆì—­ ê²°ê³¼: {text}")
            
            # ========== Phase 0: LLM ì´ˆê¸° ë¶„ì„ (NEW) ==========
            # LLMì´ ë¨¼ì € ê¸°ì´ˆ ì¶”ë¡ ê³¼ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œê³µ
            llm_initial_analysis = None
            llm_scenarios = []
            
            if self.config.llm_mode != "none" and hasattr(self, 'advanced_wrappers'):
                self.logger.info("\n   ğŸ¤– ========== Phase 0: LLM ì´ˆê¸° ë¶„ì„ ==========")
                self.logger.info(f"   ì…ë ¥: {text}")
                
                # Advanced Emotion Wrapperì—ì„œ LLM ì‚¬ìš©
                if 'advanced_emotion' in self.advanced_wrappers:
                    try:
                        emotion_wrapper = self.advanced_wrappers['advanced_emotion']
                        # API ëª¨ë“œì¼ ë•ŒëŠ” self.llm_engine ì‚¬ìš©
                        llm_engine_to_use = None
                        if self.config.llm_mode in ['gpt', 'claude', 'perplexity', 'deepseek', 'mcp'] and hasattr(self, 'llm_engine') and self.llm_engine:
                            llm_engine_to_use = self.llm_engine
                        elif hasattr(emotion_wrapper, 'llm_engine') and emotion_wrapper.llm_engine:
                            llm_engine_to_use = emotion_wrapper.llm_engine
                        
                        if llm_engine_to_use:
                            self.logger.info("   ğŸ“ LLMì—ê²Œ ì´ˆê¸° ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ìš”ì²­...")
                            
                            llm_prompt = f"""
Analyze the following situation and provide initial analysis:

Text: "{text}"

Provide:
1. Emotional state analysis (joy, sadness, anger, fear, surprise, disgust, neutral - scores 0-1)
2. Three possible action scenarios the person might take
3. Ethical considerations for each scenario
4. Potential regret factors

Respond in JSON format with keys:
- "emotions": dict of emotion scores
- "scenarios": list of 3 scenarios with "action", "ethical_score", "regret_potential"
- "context": brief context understanding
                            """.strip()
                            
                            # LLM í˜¸ì¶œ
                            from llm_module.advanced_llm_engine import LLMRequest, TaskComplexity
                            llm_request = LLMRequest(
                                prompt=llm_prompt,
                                task_type="initial_analysis",
                                complexity=TaskComplexity.MODERATE,
                                max_tokens=1000,
                                temperature=0.3
                            )
                            llm_response_obj = await llm_engine_to_use.generate_async(llm_request)
                            
                            # LLMResponse ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            if llm_response_obj and llm_response_obj.success:
                                llm_response = {'text': llm_response_obj.generated_text}
                            else:
                                llm_response = None
                            
                            if llm_response and 'text' in llm_response:
                                import json
                                try:
                                    llm_initial_analysis = json.loads(llm_response['text'])
                                    self.logger.info("   âœ… LLM ì´ˆê¸° ë¶„ì„ ì™„ë£Œ")
                                    
                                    # ê°ì • ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
                                    if 'emotions' in llm_initial_analysis:
                                        self.logger.info(f"   - ê°ì • ìƒíƒœ: {llm_initial_analysis['emotions']}")
                                    
                                    # ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ
                                    if 'scenarios' in llm_initial_analysis:
                                        llm_scenarios = llm_initial_analysis['scenarios']
                                        self.logger.info(f"   - ìƒì„±ëœ ì‹œë‚˜ë¦¬ì˜¤: {len(llm_scenarios)}ê°œ")
                                        for i, scenario in enumerate(llm_scenarios[:3]):
                                            self.logger.info(f"     ì‹œë‚˜ë¦¬ì˜¤ {i+1}: {scenario.get('action', 'N/A')}")
                                    
                                    # ê²°ê³¼ ì €ì¥
                                    results['llm_initial'] = llm_initial_analysis
                                    
                                except json.JSONDecodeError:
                                    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë¶„ì„
                                    self.logger.warning("   âš ï¸ LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨")
                                    llm_initial_analysis = {'raw_response': llm_response['text']}
                                    # í…ìŠ¤íŠ¸ì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ ì‹œë„
                                    if 'scenario' in llm_response['text'].lower():
                                        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ íŒŒì‹±ìœ¼ë¡œ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ
                                        lines = llm_response['text'].split('\n')
                                        for line in lines:
                                            if 'scenario' in line.lower() or 'action' in line.lower():
                                                llm_scenarios.append({'action': line.strip()})
                                
                        else:
                            self.logger.info("   âš ï¸ LLM ì—”ì§„ì´ ì—†ìŒ, ê±´ë„ˆëœ€")
                            
                    except Exception as e:
                        self.logger.warning(f"   âš ï¸ LLM ì´ˆê¸° ë¶„ì„ ì‹¤íŒ¨: {e}")
                        # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                else:
                    self.logger.info("   âš ï¸ Emotion Wrapperê°€ ì—†ìŒ, LLM ì´ˆê¸° ë¶„ì„ ê±´ë„ˆëœ€")
            else:
                self.logger.info("   â„¹ï¸ LLM ëª¨ë“œ ë¹„í™œì„±í™”, ì´ˆê¸° ë¶„ì„ ê±´ë„ˆëœ€")
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self._tokenize(text)
            
            results = {}
            
            # LLM ì´ˆê¸° ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ resultsì— í¬í•¨
            if llm_initial_analysis:
                results['llm_initial'] = llm_initial_analysis
                results['llm_scenarios'] = llm_scenarios
            
            # ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ì ì‹œì‘
            if self.config.use_workflow_memory_manager and self.workflow_memory_manager:
                from workflow_aware_memory_manager import WorkflowStage
                await self.workflow_memory_manager.prepare_for_workflow(
                    "inference", WorkflowStage.EVALUATION, set()
                )
            
            with torch.no_grad():
                # ========== Phase 1: Red Heart ì‹¬ì¸µ ë¶„ì„ ==========
                self.logger.info("\n   ğŸ§  ========== Phase 1: Red Heart ì‹¬ì¸µ ë¶„ì„ ==========")
                
                # LLM ì´ˆê¸° ë¶„ì„ ê²°ê³¼ë¥¼ í™œìš©í•œ Red Heart ì¶”ë¡ 
                if llm_initial_analysis:
                    self.logger.info("   ğŸ“Œ LLM ì´ˆê¸° ë¶„ì„ ê²°ê³¼ë¥¼ Red Heartì— í†µí•©")
                    
                    # LLM ê°ì • ë¶„ì„ì„ íŒíŠ¸ë¡œ ì‚¬ìš©
                    if 'emotions' in llm_initial_analysis:
                        results['llm_emotion_hint'] = llm_initial_analysis['emotions']
                        self.logger.info(f"   - LLM ê°ì • íŒíŠ¸: {llm_initial_analysis['emotions']}")
                    
                    # LLM ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë°˜ì‚¬ì‹¤ ì¶”ë¡ ì— í™œìš© ì˜ˆì •
                    if llm_scenarios:
                        results['llm_scenarios_for_counterfactual'] = llm_scenarios
                        self.logger.info(f"   - ë°˜ì‚¬ì‹¤ ì¶”ë¡ ìš© LLM ì‹œë‚˜ë¦¬ì˜¤: {len(llm_scenarios)}ê°œ")
                
                # 1. UnifiedModel ë°±ë³¸ ì¶”ë¡ 
                if self.unified_model:
                    self.logger.info("   ğŸ§  UnifiedModel ë°±ë³¸ ì²˜ë¦¬...")
                    # UnifiedModelì€ ì„ë² ë”© í…ì„œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
                    
                    # ë””ë°”ì´ìŠ¤ í™•ì¸ ë° ì¡°ì •
                    model_device = next(self.unified_model.parameters()).device
                    if inputs['embeddings'].device != model_device:
                        self.logger.debug(f"   ğŸ“ ì„ë² ë”© ë””ë°”ì´ìŠ¤ ì¡°ì •: {inputs['embeddings'].device} â†’ {model_device}")
                        inputs['embeddings'] = inputs['embeddings'].to(model_device)
                    
                    # 1-1. Emotion íƒœìŠ¤í¬
                    emotion_outputs = self.unified_model(
                        x=inputs['embeddings'],  # ì„ë² ë”© í…ì„œ
                        task='emotion',  # ê°ì • íƒœìŠ¤í¬
                        return_all=True  # ëª¨ë“  ì¶œë ¥ ë°˜í™˜
                    )
                    results['unified'] = self._process_unified_outputs(emotion_outputs, task='emotion')
                    
                    # 1-2. Bentham íƒœìŠ¤í¬ - í•™ìŠµëœ bentham_head ì‚¬ìš©
                    self.logger.info("   âš–ï¸ Bentham ìœ¤ë¦¬ ê³„ì‚° (í•™ìŠµëœ 27M ëª¨ë¸)...")
                    bentham_outputs = self.unified_model(
                        x=inputs['embeddings'],
                        task='bentham',  # bentham íƒœìŠ¤í¬
                        return_all=True
                    )
                    bentham_results = self._process_unified_outputs(bentham_outputs, task='bentham')
                    results['bentham'] = bentham_results.get('bentham', {})
                
                # ========== Phase 2: ê°ì • ì²˜ë¦¬ (ê³„ì¸µì ) ==========
                emotion_data = results.get('unified', {}).get('emotion', {})
                
                # 2-1. EmotionEthicsRegretCircuit í†µí•© ì²˜ë¦¬
                circuit_result = None
                circuit_context_saved = None  # Circuit ì¬ì‹¤í–‰ì„ ìœ„í•´ ì»¨í…ìŠ¤íŠ¸ ì €ì¥
                if self.config.use_emotion_hierarchy and self.emotion_hierarchy_processor:
                    self.logger.info("   ğŸ­ ê°ì •-ìœ¤ë¦¬-í›„íšŒ í†µí•© íšŒë¡œ ì²˜ë¦¬ (ì´ˆê¸° ì‹œë„)...")
                    # CircuitDecisionContext ìƒì„± - í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
                    from emotion_ethics_regret_circuit import CircuitDecisionContext
                    
                    # ì´í•´ê´€ê³„ì ì¶”ì¶œ (í…ìŠ¤íŠ¸ì—ì„œ ì–¸ê¸‰ëœ ëŒ€ìƒë“¤)
                    stakeholders = []
                    if "ì¹œêµ¬" in text:
                        stakeholders.append("ì¹œêµ¬")
                    if "ê°€ì¡±" in text:
                        stakeholders.append("ê°€ì¡±")
                    if "ë™ë£Œ" in text or "íšŒì‚¬" in text:
                        stakeholders.append("ë™ë£Œ")
                    if not stakeholders:
                        stakeholders = ["íƒ€ì¸", "ì‚¬íšŒ"]  # ê¸°ë³¸ ì´í•´ê´€ê³„ì
                    
                    # emotion_dataê°€ dictì¸ ê²½ìš° EmotionDataë¡œ ë³€í™˜
                    from data_models import EmotionData, EmotionState, EmotionIntensity
                    self_emotion = None
                    if emotion_data:
                        if isinstance(emotion_data, dict):
                            emotion_id = emotion_data.get('emotion', 0)
                            primary_emotion = EmotionState(emotion_id) if emotion_id in [e.value for e in EmotionState] else EmotionState.NEUTRAL
                            intensity_val = emotion_data.get('intensity', 3)
                            intensity = EmotionIntensity(intensity_val) if intensity_val in [i.value for i in EmotionIntensity] else EmotionIntensity.MODERATE
                            
                            self_emotion = EmotionData(
                                primary_emotion=primary_emotion,
                                intensity=intensity,
                                arousal=emotion_data.get('arousal', 0.0),
                                valence=emotion_data.get('valence', 0.0),
                                dominance=emotion_data.get('dominance', 0.0),
                                confidence=emotion_data.get('confidence', 0.5),
                                language='ko'
                            )
                        else:
                            self_emotion = emotion_data
                    
                    circuit_context = CircuitDecisionContext(
                        scenario_text=text,
                        proposed_action="ìƒí™© ë¶„ì„ ë° ìµœì  ì‘ë‹µ ìƒì„±",
                        stakeholders=stakeholders,
                        social_context={
                            'impact_scope': 'personal' if len(stakeholders) < 3 else 'community',
                            'keywords': text.split()[:5],  # ì£¼ìš” í‚¤ì›Œë“œ
                            'urgency': 0.5  # ê¸°ë³¸ ê¸´ê¸‰ë„
                        },
                        temporal_urgency=0.5,
                        self_emotion=self_emotion
                    )
                    
                    # Circuit ì»¨í…ìŠ¤íŠ¸ë¥¼ ì €ì¥ (í›„ë°˜ ì¬ì‹¤í–‰ìš©)
                    circuit_context_saved = circuit_context
                    
                    # GPU ë©”ëª¨ë¦¬ ì²´í¬ - ë¶€ì¡±í•˜ë©´ ë¹ ë¥¸ fallback
                    if torch.cuda.is_available():
                        gpu_free = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)
                        gpu_free_gb = gpu_free / (1024**3)
                        if gpu_free_gb < 2.0:  # 2GB ë¯¸ë§Œì´ë©´ Circuit skip
                            self.logger.warning(f"   âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ({gpu_free_gb:.1f}GB), Circuit í›„ë°˜ ì‹¤í–‰ ì˜ˆì•½ (ì˜ë„ì  fallback)")
                            circuit_result = None
                        else:
                            try:
                                # ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì‹œë„ (5ì´ˆ)
                                import asyncio
                                circuit_result = await asyncio.wait_for(
                                    self.emotion_hierarchy_processor.process_ethical_decision(circuit_context),
                                    timeout=5.0
                                )
                            except asyncio.TimeoutError:
                                self.logger.warning("   â±ï¸ Circuit ì´ˆê¸° ì‹œë„ íƒ€ì„ì•„ì›ƒ, í›„ë°˜ ì¬ì‹¤í–‰ ì˜ˆì•½ (ì˜ë„ì  fallback)")
                                circuit_result = None
                    else:
                        try:
                            circuit_result = await self.emotion_hierarchy_processor.process_ethical_decision(circuit_context)
                            
                            if circuit_result:
                                # Circuit ê²°ê³¼ë¥¼ ì›Œí¬í”Œë¡œìš°ì— í†µí•©
                                # 1. ê°ì • ë°ì´í„° í†µí•©
                                if hasattr(circuit_result, 'integrated_emotion'):
                                    emotion_data['circuit_integrated'] = {
                                        'primary': circuit_result.integrated_emotion.primary_emotion.value,
                                        'intensity': circuit_result.integrated_emotion.intensity.value,
                                        'confidence': circuit_result.integrated_emotion.confidence
                                    }
                                
                                # 2. ìœ¤ë¦¬ì  ê°€ì¹˜ í†µí•©
                                if hasattr(circuit_result, 'ethical_values'):
                                    results['circuit_ethics'] = circuit_result.ethical_values
                                
                                # 3. ì˜ˆì¸¡ëœ í›„íšŒ í†µí•©
                                if hasattr(circuit_result, 'predicted_regret'):
                                    results['circuit_regret'] = circuit_result.predicted_regret
                                
                                # 4. ì¶”ë¡  ê³¼ì • ì €ì¥
                                if hasattr(circuit_result, 'reasoning_trace'):
                                    results['circuit_reasoning'] = circuit_result.reasoning_trace
                                    
                                self.logger.info(f"   âœ… Circuit ì²˜ë¦¬ ì™„ë£Œ (ì‹ ë¢°ë„: {getattr(circuit_result, 'confidence', 0):.2f})")
                        except Exception as e:
                            self.logger.warning(f"   âš ï¸ Circuit ì´ˆê¸° ì²˜ë¦¬ ì‹¤íŒ¨, í›„ë°˜ ì¬ì‹¤í–‰ ì˜ˆì•½ (ì˜ë„ì  fallback): {e}")
                            circuit_result = None
                
                # 2-2. DSPë¡œ ê°ì • ì‹ í˜¸ ì²˜ë¦¬
                if self.config.use_dsp_simulator and self.dsp_simulator:
                    self.logger.info("   ğŸ“¡ DSP ê°ì • ì‹ í˜¸ ì²˜ë¦¬...")
                    # DSP SimulatorëŠ” 384ì°¨ì› ì…ë ¥ì„ ë°›ìŒ - embeddingì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    unified_result = results.get('unified', {})
                    if 'embedding' in unified_result and unified_result['embedding'] is not None:
                        # embeddingì´ ìˆìœ¼ë©´ ì‚¬ìš© (ì´ë¯¸ 384ì°¨ì›)
                        dsp_input = unified_result['embedding'].unsqueeze(0) if unified_result['embedding'].dim() == 1 else unified_result['embedding']
                    else:
                        # embeddingì´ ì—†ìœ¼ë©´ ê°ì • ë°ì´í„°ë¥¼ 384ì°¨ì›ìœ¼ë¡œ í™•ì¥
                        emotion_tensor = torch.zeros(1, 7).to(self.config.device)
                        if isinstance(emotion_data, dict):
                            emotion_keys = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'neutral']
                            for i, key in enumerate(emotion_keys):
                                if key in emotion_data:
                                    emotion_tensor[0, i] = float(emotion_data[key]) if isinstance(emotion_data[key], (int, float)) else 0.5
                        
                        # 7ì°¨ì›ì„ 384ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜ (ì„ í˜• ë³€í™˜)
                        # DSP simulatorì˜ device í™•ì¸ ë° ì¼ì¹˜
                        dsp_device = next(self.dsp_simulator.parameters()).device
                        if not hasattr(self, 'emotion_to_dsp_projection'):
                            self.emotion_to_dsp_projection = nn.Linear(7, 384).to(dsp_device)
                        else:
                            self.emotion_to_dsp_projection = self.emotion_to_dsp_projection.to(dsp_device)
                        
                        # emotion_tensorë¥¼ DSP simulatorì™€ ê°™ì€ deviceë¡œ ì´ë™
                        emotion_tensor = emotion_tensor.to(dsp_device)
                        dsp_input = self.emotion_to_dsp_projection(emotion_tensor)
                    
                    dsp_result = self.dsp_simulator.forward(dsp_input)
                    emotion_data['dsp_processed'] = dsp_result
                    
                    if self.config.use_kalman_filter and self.kalman_filter:
                        self.logger.info("   ğŸ”„ Kalman í•„í„°ë§...")
                        emotion_data['kalman_filtered'] = self.kalman_filter.update(dsp_result)
                
                results['emotion'] = emotion_data
                
                # ========== Phase 3: ê°ì • â†’ ë²¤ë‹´ ì§ì ‘ ë³€í™˜ ==========
                self.logger.info("   ğŸ”€ ê°ì • â†’ ë²¤ë‹´ ì§ì ‘ ë³€í™˜...")
                bentham_params = self.emotion_to_bentham_converter(emotion_data)
                
                # 3-1. ì‹œê³„ì—´ ì „íŒŒ â†’ ë²¤ë‹´ ì§€ì†ì„± í†µí•©
                if self.config.use_temporal_propagation and self.temporal_propagator:
                    self.logger.info("   â° ì‹œê³„ì—´ ì „íŒŒ ë¶„ì„...")
                    # analyze_temporal_patterns ë©”ì„œë“œ ì‚¬ìš©
                    temporal_patterns = self.temporal_propagator.analyze_temporal_patterns()
                    # ì‹œê³„ì—´ ì˜í–¥ ì¶”ì¶œ
                    temporal_impact = {
                        'long_term_effect': temporal_patterns.get('LONG_TERM', {}).get('event_frequency', {}).get('average', 1.0),
                        'cascade_potential': temporal_patterns.get('cross_scale', {}).get('cascade_probability', 0.5),
                        'patterns': temporal_patterns
                    }
                    # ì‹œê³„ì—´ ì˜í–¥ì„ ë²¤ë‹´ íŒŒë¼ë¯¸í„°ì— ì§ì ‘ ë°˜ì˜
                    bentham_params['duration'] = temporal_impact.get('long_term_effect', bentham_params['duration'])
                    bentham_params['fecundity'] = temporal_impact.get('cascade_potential', bentham_params['fecundity'])
                    results['temporal_impact'] = temporal_impact
                
                # 3-2. ë²¤ë‹´ ê³„ì‚°
                if 'bentham' in results.get('unified', {}):
                    # UnifiedModelì˜ ë²¤ë‹´ í—¤ë“œ ê²°ê³¼ì™€ ë³‘í•©
                    unified_bentham = results['unified']['bentham']
                    for key in bentham_params:
                        if key in unified_bentham:
                            # ê°€ì¤‘ í‰ê· 
                            bentham_params[key] = (bentham_params[key] + unified_bentham[key]) / 2
                
                results['bentham'] = bentham_params
                
                # ========== Phase 4: ë°˜ì‚¬ì‹¤ ì¶”ë¡  ==========
                if self.config.use_counterfactual_reasoning and self.counterfactual_reasoning:
                    self.logger.info("\n   ğŸ’­ ========== Phase 4: ë°˜ì‚¬ì‹¤ ì¶”ë¡  ==========")
                    
                    # LLM ì‹œë‚˜ë¦¬ì˜¤ê°€ ìˆìœ¼ë©´ ë°˜ì‚¬ì‹¤ ì¶”ë¡ ì— í™œìš©
                    if 'llm_scenarios_for_counterfactual' in results:
                        self.logger.info("   ğŸ“Œ LLM ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë°˜ì‚¬ì‹¤ ì¶”ë¡ ì— í†µí•©")
                        
                    # AdvancedCounterfactualReasoningì˜ ì‹¤ì œ ë©”ì„œë“œ ì‚¬ìš©
                    base_situation = {
                        'text': text,
                        'emotion_results': emotion_data,  # emotion_data ì‚¬ìš©
                        'bentham_params': bentham_params,
                        'circuit_results': circuit_result,  # circuit_result ì‚¬ìš©
                        'llm_scenarios': results.get('llm_scenarios_for_counterfactual', [])  # LLM ì‹œë‚˜ë¦¬ì˜¤ ì¶”ê°€
                    }
                    counterfactuals = await self.counterfactual_reasoning.analyze_counterfactual_scenarios(
                        base_situation=base_situation,
                        options={'num_hypotheses': 3, 'max_actions_per_hypothesis': 3}
                    )
                    results['counterfactuals'] = counterfactuals
                    
                    # LLM ì‹œë‚˜ë¦¬ì˜¤ì™€ ë°˜ì‚¬ì‹¤ ì¶”ë¡  ê²°ê³¼ í†µí•©
                    if counterfactuals and 'llm_scenarios_for_counterfactual' in results:
                        self.logger.info("   ğŸ”€ LLM ì‹œë‚˜ë¦¬ì˜¤ì™€ ë°˜ì‚¬ì‹¤ ì¶”ë¡  ê²°ê³¼ í†µí•©")
                        # ë‘ ê²°ê³¼ë¥¼ í•©ì³ì„œ ë” í’ë¶€í•œ ëŒ€ì•ˆ ìƒì„±
                else:
                    counterfactuals = None  # LIGHT ëª¨ë“œì—ì„œëŠ” ë°˜ì‚¬ì‹¤ ì¶”ë¡  ìŠ¤í‚µ
                
                # ========== Phase 5: í›„íšŒ ê³„ì‚° (ì´ì¤‘ ì‹œìŠ¤í…œ) ==========
                regret_results = {}
                
                # 5-1. UnifiedModel RegretHead
                if 'regret' in results.get('unified', {}):
                    regret_results['unified'] = results['unified']['regret']
                
                # 5-2. ê³ ê¸‰ í›„íšŒ í•™ìŠµ ì‹œìŠ¤í…œ
                if self.config.use_advanced_regret_learning and self.advanced_regret_learning:
                    self.logger.info("   ğŸ˜” ê³ ê¸‰ í›„íšŒ í•™ìŠµ...")
                    advanced_regret = await self.advanced_regret_learning.analyze(
                        counterfactuals=counterfactuals,
                        bentham_score=results.get('bentham', {})
                    )
                    regret_results['advanced'] = advanced_regret
                
                # 5-3. ê²½í—˜ DB ê²€ìƒ‰
                if self.config.use_experience_database and self.experience_database:
                    self.logger.info("   ğŸ’¾ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰...")
                    # ExperienceQuery ìƒì„±
                    from advanced_experience_database import ExperienceQuery
                    query = ExperienceQuery(
                        query_text=text,
                        emotion_state=emotion_data if isinstance(emotion_data, dict) else None,
                        max_results=5
                    )
                    similar_experiences = await self.experience_database.search_experiences(query)
                    regret_results['experience_based'] = similar_experiences
                
                results['regret'] = regret_results
                
                # ========== Phase 6: ì¶”ê°€ ë¶„ì„ ==========
                # 6-1. Neural Analyzers (368M)
                if self.config.use_neural_analyzers and self.neural_analyzers:
                    self.logger.info("   ğŸ§  Neural Analyzers ë¶„ì„...")
                    neural_results = {}
                    # UnifiedModel ì¶œë ¥ì—ì„œ hidden_states ì¶”ì¶œ
                    hidden_states = None
                    if 'unified' in results and 'hidden_states' in results['unified']:
                        hidden_states = results['unified']['hidden_states']
                    elif 'unified' in results and 'embedding' in results['unified']:
                        # embeddingì„ hidden_statesë¡œ ì‚¬ìš©
                        hidden_states = results['unified']['embedding']
                    elif 'embeddings' in inputs:
                        # ì…ë ¥ ì„ë² ë”© ì‚¬ìš©
                        hidden_states = inputs['embeddings']
                    
                    if hidden_states is not None:
                        # ì°¨ì› í™•ì¸ ë° ì¡°ì •
                        if isinstance(hidden_states, torch.Tensor):
                            # 4D tensorë¥¼ 2D ë˜ëŠ” 3Dë¡œ ë³€í™˜
                            if hidden_states.dim() == 4:
                                # [batch, seq, heads, dim] -> [batch, seq*heads*dim]
                                batch_size = hidden_states.shape[0]
                                hidden_states = hidden_states.view(batch_size, -1)
                                self.logger.info(f"   ğŸ“ 4D tensorë¥¼ 2Dë¡œ ë³€í™˜: {hidden_states.shape}")
                            elif hidden_states.dim() == 3:
                                # [batch, seq, dim] -> [batch, seq*dim] 
                                batch_size = hidden_states.shape[0]
                                hidden_states = hidden_states.view(batch_size, -1)
                                self.logger.info(f"   ğŸ“ 3D tensorë¥¼ 2Dë¡œ ë³€í™˜: {hidden_states.shape}")
                            elif hidden_states.dim() == 1:
                                # [dim] -> [1, dim]
                                hidden_states = hidden_states.unsqueeze(0)
                                self.logger.info(f"   ğŸ“ 1D tensorë¥¼ 2Dë¡œ ë³€í™˜: {hidden_states.shape}")
                            
                            # 768ì°¨ì›ìœ¼ë¡œ ë§ì¶”ê¸° ìœ„í•œ í”„ë¡œì ì…˜
                            if hidden_states.shape[-1] != 768:
                                # ì„ í˜• í”„ë¡œì ì…˜ìœ¼ë¡œ ì°¨ì› ë§ì¶”ê¸°
                                projection = nn.Linear(hidden_states.shape[-1], 768).to(hidden_states.device)
                                hidden_states = projection(hidden_states)
                                self.logger.info(f"   ğŸ“ ì°¨ì› í”„ë¡œì ì…˜: {hidden_states.shape[-1]} -> 768")
                        
                        for name, analyzer in self.neural_analyzers.items():
                            try:
                                # analyzerì˜ device í™•ì¸
                                analyzer_device = next(analyzer.parameters()).device
                                
                                # í”„ë¡œì ì…˜ ì–´ëŒ‘í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš© (896ì°¨ì› ê°€ì¤‘ì¹˜ ë³µì›ëœ ê²½ìš°)
                                if hasattr(self, 'neural_projection_adapters') and name in self.neural_projection_adapters:
                                    # 768 -> 896 í”„ë¡œì ì…˜
                                    # hidden_statesë¥¼ analyzer deviceë¡œ ì´ë™
                                    hidden_states_on_device = hidden_states.to(analyzer_device)
                                    # í”„ë¡œì ì…˜ ì–´ëŒ‘í„°ë„ ê°™ì€ deviceë¡œ ì´ë™
                                    self.neural_projection_adapters[name] = self.neural_projection_adapters[name].to(analyzer_device)
                                    projected_hidden = self.neural_projection_adapters[name](hidden_states_on_device)
                                    neural_results[name] = analyzer(projected_hidden)
                                else:
                                    # 768ì°¨ì› ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                    # hidden_statesë¥¼ analyzer deviceë¡œ ì´ë™
                                    hidden_states_on_device = hidden_states.to(analyzer_device)
                                    neural_results[name] = analyzer(hidden_states_on_device)
                            except Exception as e:
                                self.logger.warning(f"   âš ï¸ {name} analyzer ì‹¤íŒ¨: {e}")
                        
                        if neural_results:
                            results['neural_analysis'] = neural_results
                    else:
                        self.logger.warning("   âš ï¸ Neural Analyzers: hidden_states ì—†ìŒ, ìŠ¤í‚µ")
                
                # 6-2. Advanced Wrappers (112M)
                if self.config.use_advanced_wrappers and self.advanced_wrappers:
                    self.logger.info("   ğŸ¯ Advanced Wrappers ë¶„ì„...")
                    wrapper_results = {}
                    
                    # Advanced WrappersëŠ” í…ì„œ ì…ë ¥ì„ ê¸°ëŒ€í•¨
                    # hidden_statesê°€ ì´ë¯¸ 768ì°¨ì›ìœ¼ë¡œ í”„ë¡œì ì…˜ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©
                    wrapper_input = None
                    if hidden_states is not None and isinstance(hidden_states, torch.Tensor):
                        wrapper_input = hidden_states
                    elif 'embeddings' in inputs:
                        # embeddingsë¥¼ ì‚¬ìš© (384ì°¨ì› -> 768ì°¨ì› í”„ë¡œì ì…˜ í•„ìš”)
                        wrapper_input = inputs['embeddings']
                        if wrapper_input.dim() == 1:
                            wrapper_input = wrapper_input.unsqueeze(0)
                        if wrapper_input.shape[-1] != 768:
                            if not hasattr(self, 'wrapper_projection'):
                                self.wrapper_projection = nn.Linear(wrapper_input.shape[-1], 768).to(self.config.device)
                            wrapper_input = self.wrapper_projection(wrapper_input)
                    
                    if wrapper_input is not None:
                        for name, wrapper in self.advanced_wrappers.items():
                            try:
                                if hasattr(wrapper, 'forward'):
                                    wrapper_results[name] = wrapper(wrapper_input)
                            except Exception as e:
                                self.logger.warning(f"   âš ï¸ {name} wrapper ì‹¤íŒ¨: {e}")
                        results['wrapper_analysis'] = wrapper_results
                    else:
                        self.logger.warning("   âš ï¸ Advanced Wrappers: ì ì ˆí•œ ì…ë ¥ ì—†ìŒ, ìŠ¤í‚µ")
                
                # 6-3. Phase Networks (íƒ€ì-ìì•„-ê³µë™ì²´ ê³„ì¸µì  ê°ì • ì²˜ë¦¬)
                if self.config.use_phase_networks and self.phase_networks:
                    self.logger.info("   ğŸ”„ Phase Networks ì²˜ë¦¬...")
                    phase_results = {}
                    
                    # inputsëŠ” dictì´ë¯€ë¡œ embeddings ì¶”ì¶œ
                    input_embeddings = inputs['embeddings']
                    # [1, seq_len, hidden_dim] -> [1, hidden_dim] (ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤ë§Œ ì‚¬ìš©)
                    if input_embeddings.dim() == 3:
                        input_embeddings = input_embeddings[:, 0, :]  # [1, hidden_dim]
                    
                    # Phase0: íƒ€ìâ†’ìì‹  ê°ì • íˆ¬ì˜ (í›„íšŒë¥¼ í†µí•œ í•™ìŠµ)
                    if 'phase0' in self.phase_networks:
                        # ê°ì • ë°ì´í„°ê°€ ìˆìœ¼ë©´ íƒ€ì ê´€ì ìœ¼ë¡œ ë³€í™˜
                        if 'emotion' in results and isinstance(results['emotion'], dict):
                            # ê°ì • scoresë¥¼ íƒ€ì ê°ì •ìœ¼ë¡œ ì‚¬ìš©
                            if 'scores' in results['emotion']:
                                emotion_scores = results['emotion']['scores']
                                # scoresê°€ dictì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                                if isinstance(emotion_scores, dict):
                                    emotion_keys = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'neutral']
                                    emotion_scores = [emotion_scores.get(k, 0.0) for k in emotion_keys]
                                other_emotion = torch.tensor(emotion_scores, dtype=torch.float32).unsqueeze(0).to(self.config.device)
                            else:
                                # 7ì°¨ì› ê°ì • ë²¡í„° êµ¬ì„±
                                emotion_keys = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'neutral']
                                emotion_values = []
                                for k in emotion_keys:
                                    val = results['emotion'].get(k, 0.0)
                                    # dictë‚˜ ë‹¤ë¥¸ íƒ€ì…ì´ë©´ floatë¡œ ë³€í™˜
                                    if isinstance(val, dict):
                                        val = val.get('score', 0.0) if 'score' in val else 0.0
                                    emotion_values.append(float(val))
                                other_emotion = torch.tensor(
                                    emotion_values,
                                    dtype=torch.float32
                                ).unsqueeze(0).to(self.config.device)
                            
                            # Phase0ë¡œ íƒ€ì ê°ì •ì„ ìì‹  ê°ì •ìœ¼ë¡œ íˆ¬ì˜ (embeddings ì „ë‹¬)
                            # Phase Networksì˜ deviceì™€ ì¼ì¹˜ì‹œí‚´
                            phase_device = next(self.phase_networks['phase0'].parameters()).device
                            other_emotion = other_emotion.to(phase_device)
                            input_embeddings_phase = input_embeddings.to(phase_device)
                            phase_results['phase0_projection'] = self.phase_networks['phase0'](other_emotion, input_embeddings_phase)
                            self.logger.info(f"      Phase0: íƒ€ìâ†’ìì‹  íˆ¬ì˜ ì™„ë£Œ {phase_results['phase0_projection'].shape}")
                        else:
                            # ê°ì • ë°ì´í„° ì—†ìœ¼ë©´ ì„ë² ë”© ì§ì ‘ ì‚¬ìš©
                            phase_device = next(self.phase_networks['phase0'].parameters()).device
                            input_embeddings_phase = input_embeddings.to(phase_device)
                            phase_results['phase0_projection'] = self.phase_networks['phase0'](input_embeddings_phase)
                    
                    # Phase2: ê°œì¸â†’ê³µë™ì²´ ê°ì • íŒ¨í„´
                    if 'phase2' in self.phase_networks:
                        # ë°°ì¹˜ ë‚´ ì—¬ëŸ¬ ìƒ˜í”Œì„ ê°œì¸ë“¤ë¡œ ê°„ì£¼
                        # input_embeddings: [batch_size, 768] â†’ [1, batch_size, 768] (batchë¥¼ individualsë¡œ)
                        phase_device = next(self.phase_networks['phase2'].parameters()).device
                        if input_embeddings.dim() == 2 and input_embeddings.shape[0] > 1:
                            community_input = input_embeddings.unsqueeze(0)  # [1, batch_size, 768]
                        else:
                            # ë‹¨ì¼ ìƒ˜í”Œì¸ ê²½ìš° ë³µì œí•˜ì—¬ ê°€ìƒì˜ ê°œì¸ë“¤ ìƒì„±
                            community_input = input_embeddings.repeat(5, 1).unsqueeze(0)  # [1, 5, 768]
                        
                        community_input = community_input.to(phase_device)
                        phase_results['phase2_community'] = self.phase_networks['phase2'](
                            community_input,
                            cultural_context='korean'  # í•œêµ­ ë¬¸í™” ë§¥ë½
                        )
                        self.logger.info(f"      Phase2: ê³µë™ì²´ íŒ¨í„´ ì¶”ì¶œ ì™„ë£Œ {phase_results['phase2_community'].shape}")
                    
                    # Hierarchical Integration: ê³„ì¸µì  í†µí•©
                    if 'hierarchical' in self.phase_networks:
                        # Phase Networksì˜ device í™•ì¸
                        phase_device = next(self.phase_networks['hierarchical'].parameters()).device
                        
                        # input_embeddingsê°€ 768ì°¨ì›ì¸ë° HierarchicalEmotionIntegratorëŠ” 896ì°¨ì› ê¸°ëŒ€
                        # 768 â†’ 896 íŒ¨ë”© ë˜ëŠ” í”„ë¡œì ì…˜
                        if input_embeddings.shape[-1] == 768:
                            # 128ì°¨ì› íŒ¨ë”© ì¶”ê°€ (í›„íšŒ/ë©”íƒ€ ì •ë³´ìš© ê³µê°„)
                            padded_features = F.pad(input_embeddings, (0, 128), mode='constant', value=0)
                        else:
                            padded_features = input_embeddings
                        
                        # padded_featuresë¥¼ deviceë¡œ ì´ë™
                        padded_features = padded_features.to(phase_device)
                        
                        # Phase0, Phase2 ì¶œë ¥ ì „ë‹¬ (ì´ë¯¸ phase_deviceì— ìˆìŒ)
                        phase0_out = phase_results.get('phase0_projection')
                        phase2_out = phase_results.get('phase2_community')
                        
                        integrated = self.phase_networks['hierarchical'](
                            padded_features,
                            phase0_out=phase0_out,
                            phase2_out=phase2_out
                        )
                        phase_results['hierarchical_integration'] = integrated
                        self.logger.info(f"      Hierarchical: ê³„ì¸µì  í†µí•© ì™„ë£Œ {integrated.shape}")
                        
                        # í†µí•©ëœ íŠ¹ì§•ì„ ë‹¤ì‹œ 768ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ (ë‹¤ë¥¸ ëª¨ë“ˆê³¼ í˜¸í™˜ì„±)
                        if integrated.shape[-1] == 896:
                            # 896 â†’ 768 í”„ë¡œì ì…˜
                            if not hasattr(self, 'phase_output_projector'):
                                self.phase_output_projector = nn.Linear(896, 768).to(self.config.device)
                            phase_results['integrated_768'] = self.phase_output_projector(integrated)
                    
                    results['phase_analysis'] = phase_results
                
                # ========== Phase 7: ë©”íƒ€ í†µí•© ==========
                if self.config.use_meta_integration and self.meta_integration:
                    self.logger.info("   ğŸ”® ë©”íƒ€ í†µí•© ì‹œìŠ¤í…œ...")
                    
                    # ê° ëª¨ë“ˆì˜ ì¶œë ¥ì„ í…ì„œë¡œ ë³€í™˜
                    head_tensors = {}
                    
                    # 1. Emotion tensor ì¶”ì¶œ
                    if 'emotion' in results and isinstance(results['emotion'], dict):
                        emotion_values = []
                        
                        # scoresê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                        if 'scores' in results['emotion']:
                            emotion_scores = results['emotion']['scores']
                            if isinstance(emotion_scores, list):
                                # [7] â†’ [1,7]ë¡œ ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (o3 ì œì•ˆ)
                                head_tensors['emotion'] = torch.tensor(emotion_scores, dtype=torch.float32).unsqueeze(0).to(self.config.device)
                            elif isinstance(emotion_scores, torch.Tensor):
                                # í…ì„œë„ 1Dë©´ ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                                if emotion_scores.dim() == 1:
                                    head_tensors['emotion'] = emotion_scores.unsqueeze(0).to(self.config.device)
                                else:
                                    head_tensors['emotion'] = emotion_scores.to(self.config.device)
                        # scoresê°€ ì—†ìœ¼ë©´ ê°ì • í‚¤ë“¤ì—ì„œ ê°’ ì¶”ì¶œ
                        else:
                            emotion_keys = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'neutral']
                            for key in emotion_keys:
                                if key in results['emotion']:
                                    value = results['emotion'][key]
                                    if isinstance(value, (int, float)):
                                        emotion_values.append(float(value))
                                    else:
                                        emotion_values.append(0.0)
                            
                            if emotion_values:
                                # [7] â†’ [1,7]ë¡œ ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (o3 ì œì•ˆ)
                                head_tensors['emotion'] = torch.tensor(emotion_values, dtype=torch.float32).unsqueeze(0).to(self.config.device)
                    
                    # 2. Bentham tensor ì¶”ì¶œ  
                    if 'bentham' in results and isinstance(results['bentham'], dict):
                        bentham_values = []
                        # ë²¤ë‹´ì˜ 7ê°œ ì¾Œë½ ë³€ìˆ˜ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ
                        bentham_keys = ['intensity', 'duration', 'certainty', 'propinquity', 
                                      'fecundity', 'purity', 'extent']
                        for key in bentham_keys:
                            if key in results['bentham']:
                                bentham_values.append(float(results['bentham'][key]))
                            else:
                                # í•„ìˆ˜ í‚¤ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ë¡œê¹…
                                self.logger.error(f"   âŒ bentham ê²°ê³¼ì— í•„ìˆ˜ í‚¤ ëˆ„ë½: {key}")
                        
                        if len(bentham_values) == 7:  # 7ê°œ ëª¨ë‘ ìˆì–´ì•¼ ìœ íš¨
                            head_tensors['bentham'] = torch.tensor(bentham_values, dtype=torch.float32).unsqueeze(0).to(self.config.device)
                        else:
                            self.logger.error(f"   âŒ bentham í…ì„œ ìƒì„± ì‹¤íŒ¨: {len(bentham_values)}/7ê°œ ê°’ë§Œ ìˆ˜ì§‘ë¨")
                    
                    # 3. Neural analysis tensor ì¶”ì¶œ
                    if 'neural_analysis' in results and results['neural_analysis']:
                        # neural_analysisê°€ dictì´ë©´ ì²« ë²ˆì§¸ analyzerì˜ ì¶œë ¥ ì‚¬ìš©
                        if isinstance(results['neural_analysis'], dict):
                            for analyzer_name, analyzer_output in results['neural_analysis'].items():
                                if isinstance(analyzer_output, torch.Tensor):
                                    head_tensors['neural'] = analyzer_output.to(self.config.device)
                                    break
                    
                    # 4. Hidden states ë˜ëŠ” embedding ì¶”ì¶œ
                    if 'unified' in results:
                        if 'hidden_states' in results['unified'] and isinstance(results['unified']['hidden_states'], torch.Tensor):
                            head_tensors['hidden'] = results['unified']['hidden_states'].to(self.config.device)
                        elif 'embedding' in results['unified'] and isinstance(results['unified']['embedding'], torch.Tensor):
                            head_tensors['embedding'] = results['unified']['embedding'].to(self.config.device)
                    
                    # 5. í…ì„œ ìˆ˜ì§‘ ìƒíƒœ ë¡œê¹…
                    self.logger.info(f"   ğŸ“Š ìˆ˜ì§‘ëœ í…ì„œ: {list(head_tensors.keys())}")
                    for key, tensor in head_tensors.items():
                        if isinstance(tensor, torch.Tensor):
                            is_valid = tensor.abs().sum().item() > 0
                            self.logger.info(f"      - {key}: shape={tensor.shape}, sum={tensor.abs().sum().item():.3f}, valid={is_valid}")
                    
                    # 6. í•„ìˆ˜ í…ì„œ ì²´í¬ - GPT ì œì•ˆëŒ€ë¡œ ëª…ì‹œì  ì˜ˆì™¸ ë°œìƒ
                    required = {'emotion', 'bentham'}
                    missing = [k for k in required if k not in head_tensors or head_tensors[k] is None]
                    
                    if missing:
                        # ê·¼ë³¸ ì›ì¸ ë¡œê¹…
                        self.logger.error(f"   âŒ ë©”íƒ€ í†µí•© ì°¨ë‹¨: í•„ìˆ˜ í…ì„œ ëˆ„ë½: {missing}")
                        if 'emotion' in missing:
                            self.logger.error("      emotion í…ì„œ ìƒì„± ì‹¤íŒ¨ - Advanced Emotion Analyzer ì¶œë ¥ í™•ì¸ í•„ìš”")
                        if 'bentham' in missing:
                            self.logger.error("      bentham í…ì„œ ìƒì„± ì‹¤íŒ¨ - Advanced Bentham Calculator ì¶œë ¥ í™•ì¸ í•„ìš”")
                        raise RuntimeError(f"Meta-Integration blocked. Missing required tensors: {missing}")
                    
                    # í…ì„œê°€ ìˆì–´ë„ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´(ëª¨ë‘ 0) ì—ëŸ¬
                    invalid = [k for k in required if head_tensors[k].abs().sum().item() == 0]
                    if invalid:
                        self.logger.error(f"   âŒ ë©”íƒ€ í†µí•© ì°¨ë‹¨: ë¬´íš¨í•œ í…ì„œ(ëª¨ë‘ 0): {invalid}")
                        raise RuntimeError(f"Meta-Integration blocked. Invalid tensors (all zeros): {invalid}")
                    
                    # ëª¨ë“  ê²€ì¦ í†µê³¼ ì‹œì—ë§Œ í†µí•© ì‹¤í–‰
                    try:
                        integrated_result = await self.meta_integration.integrate_head_outputs(head_tensors)
                        results['meta_integrated'] = integrated_result
                        self.logger.info("   âœ… ë©”íƒ€ í†µí•© ì™„ë£Œ")
                    except Exception as e:
                        self.logger.error(f"   âŒ ë©”íƒ€ í†µí•© ì‹¤í–‰ ì¤‘ ì‹¤íŒ¨: {e}")
                        raise
                    
                
                # ========== Phase 8: LLM ë³´ê°• (ì„ íƒì ) ==========
                if self.config.llm_mode != "none" and self.llm_engine:
                    api_modes = ['gpt', 'claude', 'perplexity', 'deepseek']
                    
                    if self.config.llm_mode in api_modes:
                        # API ëª¨ë“œ - GPU ìŠ¤ì™‘ ë¶ˆí•„ìš”
                        self.logger.info(f"   ğŸŒ LLM API ë³´ê°• ({self.config.llm_mode})...")
                    else:
                        # ë¡œì»¬ ëª¨ë“œ - GPU ìŠ¤ì™‘ í•„ìš”
                        self.logger.info(f"   ğŸ¤– LLM ë³´ê°• ({self.config.llm_mode})...")
                        
                        # LLM ì‹¤í–‰ ì „ Red Heart ëª¨ë“ˆë“¤ì„ RAMìœ¼ë¡œ ìŠ¤ì™‘í•˜ì—¬ GPU ë©”ëª¨ë¦¬ í™•ë³´
                        self.logger.info("   ğŸ”„ LLMì„ ìœ„í•´ Red Heart ëª¨ë“ˆë“¤ì„ RAMìœ¼ë¡œ ìŠ¤ì™‘...")
                    
                    # 1. ë°±ë³¸ê³¼ í—¤ë“œë¥¼ RAMìœ¼ë¡œ ì´ë™
                    if self.unified_model:
                        self.unified_model.to('cpu')
                        self.logger.info("      - UnifiedModel â†’ RAM")
                    
                    # 2. Neural Analyzersë¥¼ RAMìœ¼ë¡œ ì´ë™
                    if self.neural_analyzers:
                        for name, module in self.neural_analyzers.items():
                            module.to('cpu')
                        self.logger.info("      - Neural Analyzers â†’ RAM")
                    
                    # 3. Advanced Wrappersë¥¼ RAMìœ¼ë¡œ ì´ë™
                    if self.advanced_wrappers:
                        for name, wrapper in self.advanced_wrappers.items():
                            if hasattr(wrapper, 'to'):
                                wrapper.to('cpu')
                        self.logger.info("      - Advanced Wrappers â†’ RAM")
                    
                    # 4. Phase Networksë¥¼ RAMìœ¼ë¡œ ì´ë™
                    if self.phase_networks:
                        for name, network in self.phase_networks.items():
                            network.to('cpu')
                        self.logger.info("      - Phase Networks â†’ RAM")
                    
                    # 5. DSPì™€ ê¸°íƒ€ ëª¨ë“ˆë“¤ì„ RAMìœ¼ë¡œ ì´ë™
                    if self.dsp_simulator:
                        self.dsp_simulator.to('cpu')
                        self.logger.info("      - DSP Simulator â†’ RAM")
                    
                    if self.kalman_filter:
                        self.kalman_filter.to('cpu')
                        self.logger.info("      - Kalman Filter â†’ RAM")
                    
                    # 6. GPU ìºì‹œ ì •ë¦¬
                    torch.cuda.empty_cache()
                    if torch.cuda.is_available():
                        gpu_free = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)
                        self.logger.info(f"   âœ… GPU ë©”ëª¨ë¦¬ í™•ë³´ ì™„ë£Œ: {gpu_free/1024**3:.1f}GB ì‚¬ìš© ê°€ëŠ¥")
                    
                    # ========== Circuit ì¬ì‹¤í–‰ (GPU ì—¬ìœ  ìƒíƒœ) ==========
                    if circuit_result is None and circuit_context_saved and self.emotion_hierarchy_processor:
                        self.logger.info("   ğŸ”„ Circuit ì¬ì‹¤í–‰ (GPU ë©”ëª¨ë¦¬ í™•ë³´ë¨)...")
                        try:
                            # GPUì— ë²¤ë‹´ ê³„ì‚°ê¸° ë“± ë¡œë“œ
                            circuit_result = await self.emotion_hierarchy_processor.process_ethical_decision(circuit_context_saved)
                            
                            if circuit_result:
                                # Circuit ê²°ê³¼ë¥¼ ì›Œí¬í”Œë¡œìš°ì— í†µí•©
                                # 1. ê°ì • ë°ì´í„° í†µí•©
                                if hasattr(circuit_result, 'integrated_emotion'):
                                    results['circuit_integrated'] = {
                                        'primary': circuit_result.integrated_emotion.primary_emotion.value,
                                        'intensity': circuit_result.integrated_emotion.intensity.value,
                                        'confidence': circuit_result.integrated_emotion.confidence
                                    }
                                
                                # 2. ìœ¤ë¦¬ì  ê°€ì¹˜ í†µí•©
                                if hasattr(circuit_result, 'ethical_values'):
                                    results['circuit_ethics'] = circuit_result.ethical_values
                                
                                # 3. í›„íšŒ í•™ìŠµ ë°ì´í„° í†µí•©
                                if hasattr(circuit_result, 'regret_metrics'):
                                    results['circuit_regret'] = circuit_result.regret_metrics
                                
                                self.logger.info(f"   âœ… Circuit ì¬ì‹¤í–‰ ì„±ê³µ (ì‹ ë¢°ë„: {getattr(circuit_result, 'confidence', 0):.2f})")
                        except Exception as e:
                            self.logger.warning(f"   âš ï¸ Circuit ì¬ì‹¤í–‰ë„ ì‹¤íŒ¨, ìŠ¤í‚µ: {e}")
                    
                    # LLMRequest ìƒì„±
                    from llm_module.advanced_llm_engine import LLMRequest, TaskComplexity
                    
                    # ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
                    context_summary = []
                    if 'emotion' in results:
                        context_summary.append(f"ê°ì • ë¶„ì„: {results['emotion']}")
                    if 'bentham' in results:
                        context_summary.append(f"ë²¤ë‹´ ì ìˆ˜: {results['bentham']}")
                    if 'counterfactual' in results:
                        context_summary.append(f"ë°˜ì‚¬ì‹¤ì  ì¶”ë¡  ì™„ë£Œ")
                    
                    enhance_prompt = f"""í…ìŠ¤íŠ¸: {text}
                    
ë¶„ì„ ê²°ê³¼:
{chr(10).join(context_summary)}

ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì‹¬ì¸µ ìœ¤ë¦¬ì  í‰ê°€ë¥¼ ì œê³µí•˜ì„¸ìš”."""
                    
                    llm_request = LLMRequest(
                        prompt=enhance_prompt,
                        task_type="enhancement",
                        complexity=TaskComplexity.MODERATE,
                        context={'analysis_results': results}
                    )
                    
                    # generate_async í˜¸ì¶œ
                    llm_response = await self.llm_engine.generate_async(llm_request)
                    results['llm_enhanced'] = {
                        'text': llm_response.generated_text,  # textê°€ ì•„ë‹ˆë¼ generated_text
                        'confidence': llm_response.confidence
                    }
            
            # í†µí•© ì ìˆ˜ ê³„ì‚°
            results['integrated_score'] = self._calculate_integrated_score(results)
            results['confidence'] = self._calculate_confidence(results)
            results['processing_time'] = time.time() - start_time
            
            # ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ì ì¢…ë£Œ
            if self.config.use_workflow_memory_manager and self.workflow_memory_manager:
                self.workflow_memory_manager.complete_workflow("inference")
            
            # ê²½í—˜ DB ì €ì¥ (MD ë¬¸ì„œ ì‚¬ì–‘: ë¶„ì„ ê²°ê³¼ ì €ì¥)
            if self.config.use_experience_database and self.experience_database:
                try:
                    # ê²½í—˜ ë°ì´í„° êµ¬ì„± (dictë§Œ í—ˆìš©, ë‹¤ë¥¸ íƒ€ì…ì€ ë³€í™˜)
                    import json
                    
                    def to_serializable(obj):
                        """ê°ì²´ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
                        if isinstance(obj, dict):
                            # dict ì•ˆì˜ ê° ê°’ë„ ì¬ê·€ì ìœ¼ë¡œ ë³€í™˜
                            result = {}
                            for k, v in obj.items():
                                # keyë„ ë¬¸ìì—´ë¡œ ë³€í™˜
                                key = str(k) if not isinstance(k, str) else k
                                result[key] = to_serializable(v)
                            return result
                        elif isinstance(obj, (list, tuple)):
                            return [to_serializable(item) for item in obj]
                        elif hasattr(obj, '__dict__'):
                            return to_serializable(obj.__dict__)
                        elif isinstance(obj, (str, int, float, bool, type(None))):
                            return obj
                        else:
                            return str(obj)
                    
                    experience_data = {
                        'timestamp': time.time(),
                        'text': text,
                        'emotion': to_serializable(results.get('emotion', {})),
                        'bentham': to_serializable(results.get('bentham', {})),
                        'regret': to_serializable(results.get('regret', {})),
                        'integrated_score': float(results.get('integrated_score', 0)),
                        'confidence': float(results.get('confidence', 0)),
                        'meta_integrated': to_serializable(results.get('meta_integrated', {}))
                    }
                    
                    # ê²½í—˜ ì €ì¥
                    await self.experience_database.store_experience(
                        experience_text=text,
                        metadata=experience_data,  # experience_data ìì²´ê°€ ë©”íƒ€ë°ì´í„°
                        category='general',
                        importance_score=experience_data.get('confidence', 0.5)
                    )
                    self.logger.info("   ğŸ’¾ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ")
                    
                except Exception as e:
                    self.logger.warning(f"   âš ï¸ ê²½í—˜ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # ìºì‹±
            if len(self.cache) < self.config.cache_size:
                self.cache[cache_key] = results
            
            self.stats['successful'] += 1
            self.logger.info(f"   âœ… ë¶„ì„ ì™„ë£Œ ({results['processing_time']:.2f}ì´ˆ)")
            
            return results
            
        except Exception as e:
            self.stats['failed'] += 1
            self.logger.error(f"   âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ì •í™•í•œ ì—ëŸ¬ ìœ„ì¹˜ ì¶”ì ì„ ìœ„í•œ traceback ì¶”ê°€
            import traceback
            self.logger.error(f"   ğŸ“ ì—ëŸ¬ ë°œìƒ ìœ„ì¹˜:\n{traceback.format_exc()}")
            return {
                'error': str(e),
                'processing_time': time.time() - start_time,
                'traceback': traceback.format_exc()
            }
    
    async def analyze_ethical_dilemma(self, llm_scenarios: List[str]) -> Dict[str, Any]:
        """ë¹„ì„ í˜• ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ ì›Œí¬í”Œë¡œìš°
        
        MD ë¬¸ì„œ ì‚¬ì–‘ì— ë”°ë¥¸ êµ¬í˜„:
        1. LLMì´ ì œì‹œí•œ nê°œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ 3ë·°ë¡œ í™•ì¥ (n Ã— 3)
        2. ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°ì •/ìœ¤ë¦¬ í‰ê°€
        3. í›„íšŒ ë¶„ì„ìœ¼ë¡œ ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        4. ì •í•©ì„± íŒë‹¨ (ì‹œìŠ¤í…œ + LLM)
        5. ìƒìœ„ 2ê°œ ì‹œë‚˜ë¦¬ì˜¤ ì„ ì •
        """
        start_time = time.time()
        all_results = []
        
        self.logger.info(f"ğŸ¯ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ ì‹œì‘ - {len(llm_scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
        
        try:
            # Phase 1: 3ë·° ì‹œìŠ¤í…œ ì¦‰ì‹œ ì ìš©
            self.logger.info("   Phase 1: 3ë·° ì‹œìŠ¤í…œ ì ìš© (ë‚™ê´€/ì¤‘ë„/ë¹„ê´€)")
            for idx, scenario in enumerate(llm_scenarios):
                # 3ë·° ìƒì„±ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                scenario_context = {
                    'text': scenario,
                    'scenario_id': f'original_{idx}',
                    'urgency': 0.5,  # ê¸°ë³¸ê°’
                    'complexity': 0.7,  # ê¸°ë³¸ê°’
                    'reversibility': 0.3  # ê¸°ë³¸ê°’
                }
                
                # 3ë·° ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
                if self.three_view_system:
                    three_view_result = await self.three_view_system.analyze_three_view_scenarios(scenario_context)
                    
                    # ê° ë·°ì— ëŒ€í•´ ì „ì²´ ë¶„ì„ ìˆ˜í–‰
                    for view_type in [ScenarioType.OPTIMISTIC, ScenarioType.NEUTRAL, ScenarioType.PESSIMISTIC]:
                        view_scenario = self._create_view_scenario(scenario, three_view_result, view_type)
                        
                        # ê°ì •/ìœ¤ë¦¬ í‰ê°€
                        analysis_result = await self.analyze(view_scenario['text'])
                        
                        # ë‹¤ì›ì  ìœ¤ë¦¬ ì²´ê³„ í‰ê°€ ì¶”ê°€
                        ethics_analysis = {}
                        if self.multi_ethics_system:
                            # ìœ¤ë¦¬ì  ë”œë ˆë§ˆ êµ¬ì„±
                            dilemma = EthicalDilemma(
                                dilemma_id=f"{idx}_{view_type.value}",
                                scenario=view_scenario['text'],
                                context="ìœ¤ë¦¬ì  ì˜ì‚¬ê²°ì • ìƒí™©",
                                complexity_level=0.7,
                                urgency_level=0.5,
                                reversibility=0.3,
                                available_options=[scenario]
                            )
                            
                            # ê° ìœ¤ë¦¬ ì—”ì§„ìœ¼ë¡œ í‰ê°€
                            for ethics_name, engine in self.ethics_engines.items():
                                try:
                                    ethics_reasoning = engine.reason(dilemma)
                                    ethics_analysis[ethics_name] = {
                                        'recommendation': ethics_reasoning.final_recommendation,
                                        'confidence': ethics_reasoning.confidence_level,
                                        'reasoning': ethics_reasoning.reasoning_process[:100]  # ì²« 100ìë§Œ
                                    }
                                except Exception as e:
                                    self.logger.warning(f"   âš ï¸ {ethics_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                        
                        all_results.append({
                            'original_scenario': scenario,
                            'view_type': view_type.value,
                            'view_scenario': view_scenario,
                            'analysis': analysis_result,
                            'ethics_analysis': ethics_analysis,  # ìœ¤ë¦¬ ë¶„ì„ ì¶”ê°€
                            'utility_score': three_view_result.consensus_utility if view_type == ScenarioType.NEUTRAL 
                                           else getattr(getattr(three_view_result, f"{view_type.value}_scenario"), 'utility_score'),
                            'regret_potential': getattr(getattr(three_view_result, f"{view_type.value}_scenario"), 'regret_potential'),
                            'timestamp': time.time()
                        })
                else:
                    # 3ë·° ì‹œìŠ¤í…œ ì—†ìœ¼ë©´ ì›ë³¸ë§Œ ë¶„ì„
                    analysis_result = await self.analyze(scenario)
                    all_results.append({
                        'original_scenario': scenario,
                        'view_type': 'original',
                        'view_scenario': {'text': scenario},
                        'analysis': analysis_result,
                        'utility_score': analysis_result.get('integrated_score', 0),
                        'regret_potential': 0.5,
                        'timestamp': time.time()
                    })
            
            self.logger.info(f"   Phase 1 ì™„ë£Œ: {len(all_results)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±")
            
            # Phase 2: í›„íšŒ ë¶„ì„ìœ¼ë¡œ ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤ ì œì•ˆ
            self.logger.info("   Phase 2: í›„íšŒ ê¸°ë°˜ ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±")
            additional_scenarios = []
            
            if self.advanced_regret_learning:
                # ë†’ì€ í›„íšŒ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ë“¤ì„ ì°¾ì•„ ëŒ€ì•ˆ ìƒì„±
                high_regret_scenarios = sorted(
                    all_results, 
                    key=lambda x: x['regret_potential'], 
                    reverse=True
                )[:3]  # ìƒìœ„ 3ê°œ
                
                for scenario_data in high_regret_scenarios:
                    # í›„íšŒ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì•ˆ ìƒì„±
                    alternatives = await self.advanced_regret_learning.suggest_alternatives(
                        scenario_data['analysis']
                    )
                    
                    if alternatives and isinstance(alternatives, list):
                        additional_scenarios.extend(alternatives[:2])  # ê° ì‹œë‚˜ë¦¬ì˜¤ë‹¹ ìµœëŒ€ 2ê°œ ëŒ€ì•ˆ
            
            self.logger.info(f"   Phase 2 ì™„ë£Œ: {len(additional_scenarios)}ê°œ ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±")
            
            # Phase 3: ì •í•©ì„± íŒë‹¨ (ë‘˜ ë‹¤ ë³‘í–‰)
            self.logger.info("   Phase 3: ì •í•©ì„± íŒë‹¨")
            plausible_scenarios = []
            
            for scenario in additional_scenarios:
                # ì‹œìŠ¤í…œ ë‚´ë¶€ ì •í•©ì„± ì ìˆ˜ ê³„ì‚°
                system_score = self._calculate_plausibility(scenario, context=all_results)
                
                # ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ LLM ì¶”ê°€ ê²€ì¦ (í–¥í›„ êµ¬í˜„)
                if system_score < 0.7:
                    # TODO: LLM ì •í•©ì„± ê²€ì¦ (LLM í†µí•© í›„)
                    if self.config.llm_mode != "none" and self.llm_engine:
                        llm_plausible = await self.llm_engine.check_plausibility(scenario)
                        if llm_plausible:
                            plausible_scenarios.append(scenario)
                    else:
                        # LLM ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ ì ìˆ˜ê°€ 0.5 ì´ìƒì´ë©´ í†µê³¼
                        if system_score >= 0.5:
                            plausible_scenarios.append(scenario)
                else:
                    plausible_scenarios.append(scenario)
            
            self.logger.info(f"   Phase 3 ì™„ë£Œ: {len(plausible_scenarios)}ê°œ ì •í•©ì„± í†µê³¼")
            
            # Phase 4: ì •í•©ì„± ìˆëŠ” ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤ í‰ê°€
            if plausible_scenarios:
                self.logger.info("   Phase 4: ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤ í‰ê°€")
                for scenario in plausible_scenarios:
                    analysis_result = await self.analyze(scenario)
                    all_results.append({
                        'original_scenario': 'regret_generated',
                        'view_type': 'additional',
                        'view_scenario': {'text': scenario},
                        'analysis': analysis_result,
                        'utility_score': analysis_result.get('integrated_score', 0),
                        'regret_potential': 0.3,  # ëŒ€ì•ˆì€ í›„íšŒ ê°€ëŠ¥ì„± ë‚®ìŒ
                        'timestamp': time.time()
                    })
            
            # Phase 5: ìƒìœ„ 2ê°œ ì‹œë‚˜ë¦¬ì˜¤ ì„ ì •
            self.logger.info("   Phase 5: ìµœì¢… ì‹œë‚˜ë¦¬ì˜¤ ì„ ì •")
            
            # í†µí•© ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            sorted_results = sorted(
                all_results,
                key=lambda x: (
                    x['analysis'].get('integrated_score', 0) * 0.4 +
                    x['utility_score'] * 0.3 +
                    (1 - x['regret_potential']) * 0.3
                ),
                reverse=True
            )
            
            top_two = sorted_results[:2]
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                'selected_scenarios': top_two,
                'all_evaluations': all_results,
                'total_evaluated': len(all_results),
                'processing_time': time.time() - start_time,
                'recommendation': self._generate_recommendation(top_two),
                'metadata': {
                    'original_scenarios': len(llm_scenarios),
                    'three_view_expanded': len(llm_scenarios) * 3 if self.three_view_system else len(llm_scenarios),
                    'additional_generated': len(additional_scenarios),
                    'plausible_filtered': len(plausible_scenarios)
                }
            }
            
            self.logger.info(f"   âœ… ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ ì™„ë£Œ ({result['processing_time']:.2f}ì´ˆ)")
            self.logger.info(f"      í‰ê°€ëœ ì‹œë‚˜ë¦¬ì˜¤: {result['total_evaluated']}ê°œ")
            self.logger.info(f"      ìµœì¢… ì„ íƒ: 2ê°œ")
            
            return result
            
        except Exception as e:
            self.logger.error(f"   âŒ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def _create_view_scenario(self, original: str, three_view_result: ThreeViewAnalysisResult, 
                            view_type: ScenarioType) -> Dict[str, Any]:
        """3ë·° ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œë‚˜ë¦¬ì˜¤ í…ìŠ¤íŠ¸ ìƒì„±"""
        
        # ë·° íƒ€ì…ì— ë”°ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
        if view_type == ScenarioType.OPTIMISTIC:
            metrics = three_view_result.optimistic_scenario
            modifier = "ìµœì„ ì˜ ê²½ìš°: "
        elif view_type == ScenarioType.PESSIMISTIC:
            metrics = three_view_result.pessimistic_scenario
            modifier = "ìµœì•…ì˜ ê²½ìš°: "
        else:  # NEUTRAL
            metrics = three_view_result.neutral_scenario
            modifier = "ì¼ë°˜ì ì¸ ê²½ìš°: "
        
        # ì‹œë‚˜ë¦¬ì˜¤ í…ìŠ¤íŠ¸ ìˆ˜ì •
        scenario_text = f"{modifier}{original}"
        
        # ë¦¬ìŠ¤í¬/ê¸°íšŒ ìš”ì†Œ ì¶”ê°€
        if metrics.risk_factors:
            scenario_text += f" [ìœ„í—˜: {', '.join(metrics.risk_factors[:2])}]"
        if metrics.opportunity_factors:
            scenario_text += f" [ê¸°íšŒ: {', '.join(metrics.opportunity_factors[:2])}]"
        
        return {
            'text': scenario_text,
            'metrics': metrics,
            'confidence': metrics.confidence_level,
            'ethical_implications': metrics.ethical_implications
        }
    
    def _calculate_plausibility(self, scenario: str, context: List[Dict]) -> float:
        """ì‹œë‚˜ë¦¬ì˜¤ ì •í•©ì„± ì ìˆ˜ ê³„ì‚°"""
        
        # ê¸°ë³¸ ì ìˆ˜
        score = 0.5
        
        # ì»¨í…ìŠ¤íŠ¸ì™€ì˜ ì¼ê´€ì„± ê²€ì‚¬
        if context:
            # ê¸°ì¡´ ì‹œë‚˜ë¦¬ì˜¤ë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for existing in context:
                # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ (ì‹¤ì œë¡œëŠ” ì„ë² ë”© ì‚¬ìš©)
                if isinstance(scenario, str) and isinstance(existing.get('original_scenario'), str):
                    common_words = set(scenario.lower().split()) & set(existing['original_scenario'].lower().split())
                    similarity = len(common_words) / max(len(scenario.split()), len(existing['original_scenario'].split()))
                    similarities.append(similarity)
            
            if similarities:
                avg_similarity = sum(similarities) / len(similarities)
                # ë„ˆë¬´ ìœ ì‚¬í•˜ë©´ ì ìˆ˜ ê°ì†Œ (ì¤‘ë³µ), ë„ˆë¬´ ë‹¤ë¥´ë©´ ì ìˆ˜ ê°ì†Œ (ë¹„ì¼ê´€ì„±)
                if 0.2 < avg_similarity < 0.8:
                    score += 0.2
                elif avg_similarity > 0.9:
                    score -= 0.2  # ê±°ì˜ ë™ì¼í•œ ì‹œë‚˜ë¦¬ì˜¤
                else:
                    score -= 0.1  # ë„ˆë¬´ ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤
        
        # ê¸¸ì´ ì²´í¬
        if isinstance(scenario, str):
            word_count = len(scenario.split())
            if 10 < word_count < 200:
                score += 0.1
            else:
                score -= 0.1
        
        # ìµœì¢… ì ìˆ˜ ì •ê·œí™”
        return max(0.0, min(1.0, score))
    
    def _generate_recommendation(self, top_scenarios: List[Dict]) -> str:
        """ìƒìœ„ ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì¶”ì²œ ìƒì„±"""
        
        if not top_scenarios:
            return "ì‹œë‚˜ë¦¬ì˜¤ í‰ê°€ ì‹¤íŒ¨ - ì¶”ê°€ ë¶„ì„ í•„ìš”"
        
        # ì²« ë²ˆì§¸ ì‹œë‚˜ë¦¬ì˜¤ì˜ ì ìˆ˜ë“¤
        first = top_scenarios[0]
        utility = first.get('utility_score', 0)
        regret = first.get('regret_potential', 0)
        integrated = first['analysis'].get('integrated_score', 0)
        
        # ìœ¤ë¦¬ ë¶„ì„ ê²°ê³¼ ì¢…í•©
        ethics_consensus = self._calculate_ethics_consensus(first.get('ethics_analysis', {}))
        
        # ì¶”ì²œ ê²°ì • ë¡œì§
        if utility > 0.7 and regret < 0.3 and integrated > 0.7:
            recommendation = "ì ê·¹ ì¶”ì§„ ê¶Œì¥ - ë†’ì€ íš¨ìš©ê³¼ ë‚®ì€ í›„íšŒ ê°€ëŠ¥ì„±"
        elif utility > 0.5 and regret < 0.5:
            recommendation = "ì‹ ì¤‘í•œ ì¶”ì§„ ê¶Œì¥ - ì ì ˆí•œ íš¨ìš©, ìœ„í—˜ ê´€ë¦¬ í•„ìš”"
        elif utility > 0.3 or regret > 0.7:
            recommendation = "ì¬ê²€í†  ê¶Œì¥ - ë†’ì€ í›„íšŒ ê°€ëŠ¥ì„± ë˜ëŠ” ë‚®ì€ íš¨ìš©"
        else:
            recommendation = "ì¶”ì§„ ë¹„ê¶Œì¥ - ìœ„í—˜ì´ íš¨ìš©ì„ ì´ˆê³¼"
        
        # ë‘ ì‹œë‚˜ë¦¬ì˜¤ ê°„ ì°¨ì´ê°€ ì‘ìœ¼ë©´ ì¶”ê°€ ì •ë³´
        if len(top_scenarios) > 1:
            second = top_scenarios[1]
            diff = abs(integrated - second['analysis'].get('integrated_score', 0))
            if diff < 0.1:
                recommendation += " (ìƒìœ„ 2ê°œ ì‹œë‚˜ë¦¬ì˜¤ê°€ ìœ ì‚¬í•œ í‰ê°€)"
        
        return recommendation
    
    def _calculate_ethics_consensus(self, ethics_analysis: Dict[str, Dict]) -> Dict[str, Any]:
        """ìœ¤ë¦¬ ë¶„ì„ ê²°ê³¼ ì¢…í•©"""
        if not ethics_analysis:
            return {'consensus': 0.5, 'agreement': False}
        
        recommendations = []
        confidences = []
        
        for ethics_name, analysis in ethics_analysis.items():
            if 'recommendation' in analysis and 'confidence' in analysis:
                recommendations.append(analysis['recommendation'])
                confidences.append(analysis['confidence'])
        
        if not confidences:
            return {'consensus': 0.5, 'agreement': False}
        
        # í‰ê·  ì‹ ë¢°ë„
        avg_confidence = sum(confidences) / len(confidences)
        
        # ì¶”ì²œ ì¼ì¹˜ë„ (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¹„êµ)
        agreement = False
        if recommendations:
            # ëª¨ë“  ì¶”ì²œì´ 'ê¶Œì¥' ë˜ëŠ” 'ì¶”ì§„'ì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
            positive_count = sum(1 for r in recommendations if 'ê¶Œì¥' in r or 'ì¶”ì§„' in r)
            negative_count = sum(1 for r in recommendations if 'ë¹„ê¶Œì¥' in r or 'ì¤‘ë‹¨' in r)
            
            if positive_count > len(recommendations) * 0.7:
                agreement = True
            elif negative_count > len(recommendations) * 0.7:
                agreement = True  # ë¶€ì •ì  í•©ì˜ë„ í•©ì˜
        
        return {
            'consensus': avg_confidence,
            'agreement': agreement,
            'positive_ratio': positive_count / len(recommendations) if recommendations else 0
        }
    
    def _tokenize(self, text: str) -> Dict[str, torch.Tensor]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        # sentence_transformerë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±
        from sentence_transformer_singleton import get_sentence_transformer
        
        # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        model = get_sentence_transformer(
            model_name=self.config.embedding_model,
            device=str(self.config.device)
        )
        
        # í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        self.logger.debug(f"ì…ë ¥ í…ìŠ¤íŠ¸: {text[:50]}...")
        self.logger.debug(f"í…ìŠ¤íŠ¸ ë‹¨ì–´ ìˆ˜: {len(text.split())}")
        
        embeddings = model.encode([text])  # List[str] ì…ë ¥
        self.logger.debug(f"encode ë°˜í™˜ íƒ€ì…: {type(embeddings)}")
        self.logger.debug(f"embeddings ê¸¸ì´: {len(embeddings) if hasattr(embeddings, '__len__') else 'N/A'}")
        
        if isinstance(embeddings, list) and embeddings:
            self.logger.debug(f"embeddings[0] íƒ€ì…: {type(embeddings[0])}")
            if isinstance(embeddings[0], list):
                self.logger.debug(f"embeddings[0] ê¸¸ì´: {len(embeddings[0])}")
                self.logger.debug(f"ì²˜ìŒ 5ê°œ ê°’: {embeddings[0][:5]}")
        
        # í…ì„œë¡œ ë³€í™˜ (UnifiedModelì€ 768ì°¨ì› ê¸°ëŒ€)
        embedding_tensor = torch.tensor(embeddings[0], dtype=torch.float32)
        embedding_tensor = embedding_tensor.to(self.config.device)
        self.logger.debug(f"ë³€í™˜ í›„ í…ì„œ shape: {embedding_tensor.shape}")
        
        # ì°¨ì› ì¡°ì • (UnifiedModelì€ [batch, seq_len, hidden_dim] í˜•íƒœ ê¸°ëŒ€)
        if len(embedding_tensor.shape) == 1:
            # [hidden_dim] -> [1, 1, hidden_dim]
            embedding_tensor = embedding_tensor.unsqueeze(0).unsqueeze(0)
            self.logger.debug(f"unsqueeze í›„ shape: {embedding_tensor.shape}")
            
            # íŒ¨ë”©ì„ ìœ„í•´ max_seq_lengthë¡œ í™•ì¥
            # ë‹¨ìˆœíˆ ì²« ë²ˆì§¸ ìœ„ì¹˜ë§Œ ì‹¤ì œ ì„ë² ë”©, ë‚˜ë¨¸ì§€ëŠ” 0 íŒ¨ë”©
            padded_tensor = torch.zeros(
                1, self.config.max_seq_length, embedding_tensor.shape[-1],
                device=self.config.device, dtype=torch.float32
            )
            padded_tensor[:, 0, :] = embedding_tensor[0, 0, :]
            embedding_tensor = padded_tensor
            self.logger.debug(f"ìµœì¢… íŒ¨ë”© í›„ shape: {embedding_tensor.shape}")
        
        return {'embeddings': embedding_tensor}
    
    def _process_unified_outputs(self, outputs: Dict, task: str = 'emotion') -> Dict:
        """UnifiedModel ì¶œë ¥ ì²˜ë¦¬"""
        processed = {}
        
        # UnifiedModelì€ return_all=Trueì¼ ë•Œ 'head' í‚¤ë¡œ ë°˜í™˜
        if 'head' in outputs and outputs['head'] is not None:
            head_output = outputs['head']
            
            if task == 'emotion':
                # emotion íƒœìŠ¤í¬ ì²˜ë¦¬
                if isinstance(head_output, torch.Tensor):
                    emotion_names = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'love']
                    scores = head_output.softmax(dim=-1)[0].tolist() if head_output.dim() > 1 else head_output.softmax(dim=-1).tolist()
                    
                    # ê°ì • ì´ë¦„ê³¼ ì ìˆ˜ ë§¤í•‘
                    emotion_dict = {}
                    for i, name in enumerate(emotion_names[:len(scores)]):
                        emotion_dict[name] = scores[i]
                    
                    processed['emotion'] = emotion_dict
                    # ë©”íƒ€ í†µí•©ì„ ìœ„í•´ scoresë„ ì¶”ê°€
                    processed['emotion']['scores'] = scores
                    
            elif task == 'bentham':
                # bentham íƒœìŠ¤í¬ ì²˜ë¦¬ - í•™ìŠµëœ bentham_head ì¶œë ¥
                if isinstance(head_output, torch.Tensor):
                    # bentham_headëŠ” 10ê°œ ìš”ì†Œ ì¶œë ¥
                    bentham_elements = [
                        'intensity', 'duration', 'certainty', 'propinquity',
                        'fecundity', 'purity', 'extent',
                        'pleasure_total', 'pain_total', 'net_pleasure'
                    ]
                    
                    # í…ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    if head_output.dim() > 1:
                        scores = head_output[0].tolist()
                    else:
                        scores = head_output.tolist()
                    
                    # ë²¤ë‹´ ìš”ì†Œì™€ ì ìˆ˜ ë§¤í•‘
                    bentham_dict = {}
                    for i, name in enumerate(bentham_elements[:len(scores)]):
                        bentham_dict[name] = scores[i]
                    
                    processed['bentham'] = bentham_dict
                    
                    # ìµœì¢… ì¾Œë½ ì ìˆ˜ ê³„ì‚°
                    if len(scores) >= 10:
                        processed['bentham']['final_score'] = scores[9]  # net_pleasure
                    else:
                        # 7ê°€ì§€ ê¸°ë³¸ ìš”ì†Œì˜ í‰ê· 
                        processed['bentham']['final_score'] = sum(scores[:min(7, len(scores))]) / min(7, len(scores))
        
        # ë‹¤ë¥¸ ì¶œë ¥ë“¤ ì²˜ë¦¬ (ìˆë‹¤ë©´)
        if 'dsp' in outputs and outputs['dsp'] is not None:
            # ì •í™•í•œ êµ¬ì¡° íŒŒì•…ì„ ìœ„í•œ ìƒì„¸ ë””ë²„ê¹…
            def analyze_structure(obj, name="object", depth=0, max_depth=5):
                """ê°ì²´ì˜ ì •í™•í•œ êµ¬ì¡°ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ë¶„ì„"""
                indent = "  " * depth
                if depth > max_depth:
                    return f"{indent}{name}: <max depth reached>"
                
                result = []
                obj_type = type(obj).__name__
                
                if isinstance(obj, dict):
                    result.append(f"{indent}{name}: dict[{len(obj)} keys]")
                    for k, v in list(obj.items())[:5]:  # ìµœëŒ€ 5ê°œ í‚¤ë§Œ
                        result.append(analyze_structure(v, f"['{k}']", depth + 1, max_depth))
                elif isinstance(obj, (list, tuple)):
                    type_name = 'list' if isinstance(obj, list) else 'tuple'
                    result.append(f"{indent}{name}: {type_name}[{len(obj)} items]")
                    for i, item in enumerate(obj[:3]):  # ìµœëŒ€ 3ê°œ ì•„ì´í…œë§Œ
                        result.append(analyze_structure(item, f"[{i}]", depth + 1, max_depth))
                elif isinstance(obj, torch.Tensor):
                    result.append(f"{indent}{name}: Tensor(shape={list(obj.shape)}, dtype={obj.dtype}, device={obj.device})")
                elif hasattr(obj, '__dict__'):
                    attrs = list(obj.__dict__.keys())[:5]
                    result.append(f"{indent}{name}: {obj_type}(attrs={attrs})")
                else:
                    result.append(f"{indent}{name}: {obj_type}({str(obj)[:50]}...)" if len(str(obj)) > 50 else f"{indent}{name}: {obj_type}({obj})")
                
                return "\n".join(result)
            
            # DSP ì¶œë ¥ êµ¬ì¡° ë¶„ì„
            self.logger.debug(f"DSP ì¶œë ¥ êµ¬ì¡° ë¶„ì„:\n{analyze_structure(outputs['dsp'], 'outputs[dsp]')}")
            
            if isinstance(outputs['dsp'], dict) and 'final_emotions' in outputs['dsp']:
                final_emotions = outputs['dsp']['final_emotions']
                
                # final_emotions íƒ€ì… ìƒì„¸ ë¶„ì„
                self.logger.debug(f"final_emotions íƒ€ì… ì²´ì¸: {type(final_emotions)} â†’ {type(final_emotions).__bases__ if hasattr(type(final_emotions), '__bases__') else 'no bases'}")
                
                if isinstance(final_emotions, torch.Tensor):
                    processed['dsp_emotions'] = final_emotions.tolist()
                elif isinstance(final_emotions, (list, tuple)):
                    processed['dsp_emotions'] = list(final_emotions)
                elif isinstance(final_emotions, dict):
                    self.logger.warning(f"final_emotionsê°€ dictì…ë‹ˆë‹¤. êµ¬ì¡°: {analyze_structure(final_emotions, 'final_emotions')}")
                    # dictì¸ ê²½ìš° valuesë¥¼ ì¶”ì¶œ ì‹œë„
                    if final_emotions:
                        first_value = next(iter(final_emotions.values()))
                        if isinstance(first_value, torch.Tensor):
                            processed['dsp_emotions'] = first_value.tolist()
                else:
                    self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ final_emotions íƒ€ì…: {type(final_emotions)}")
        
        if 'neural' in outputs and outputs['neural'] is not None:
            processed['neural_analysis'] = outputs['neural']
        
        if 'wrapper' in outputs and outputs['wrapper'] is not None:
            processed['wrapper_analysis'] = outputs['wrapper']
        
        return processed
    
    def _is_korean(self, text: str) -> bool:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê°ì§€"""
        import re
        korean_pattern = re.compile('[ã„±-ã…ã…-ã…£ê°€-í£]+')
        return bool(korean_pattern.search(text))
    
    def _calculate_integrated_score(self, results: Dict) -> float:
        """í†µí•© ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        weights = {
            'unified': 0.3,  # Circuit ì‚¬ìš©ì‹œ ë¹„ì¤‘ ê°ì†Œ
            'neural_analysis': 0.25,
            'wrapper_analysis': 0.15,
            'dsp_analysis': 0.1,
            'phase_analysis': 0.05,
            'circuit_ethics': 0.15  # Circuit ìœ¤ë¦¬ ì ìˆ˜ ì¶”ê°€
        }
        
        for key, weight in weights.items():
            if key in results and results[key]:
                # ê° ëª¨ë“ˆì˜ ì ìˆ˜ ì¶”ì¶œ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
                if isinstance(results[key], dict):
                    module_score = sum(
                        v if isinstance(v, (int, float)) else 0.5
                        for v in results[key].values()
                    ) / max(len(results[key]), 1)
                    score += weight * min(max(module_score, 0), 1)
        
        return score
    
    def _calculate_confidence(self, results: Dict) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        # í™œì„±í™”ëœ ëª¨ë“ˆ ìˆ˜ì— ë”°ë¥¸ ì‹ ë¢°ë„
        active_modules = sum(1 for k in [
            'unified', 'neural_analysis', 'wrapper_analysis',
            'dsp_analysis', 'phase_analysis'
        ] if k in results and results[k])
        
        return min(active_modules / 5.0, 1.0)
    
    def get_stats(self) -> Dict:
        """í†µê³„ ë°˜í™˜"""
        if self.stats['successful'] > 0:
            self.stats['avg_time'] = self.stats['avg_time'] / self.stats['successful']
        return self.stats
    
    async def cleanup(self):
        """ì‹œìŠ¤í…œ ì •ë¦¬ ë° ì¢…ë£Œ"""
        self.logger.info("ğŸ§¹ ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘...")
        
        # ìœ íœ´ í•™ìŠµ ì‹œìŠ¤í…œ ì •ì§€
        if self.idle_learner:
            try:
                await self.idle_learner.stop()
                self.logger.info("   âœ… ìœ íœ´ í•™ìŠµ ì‹œìŠ¤í…œ ì •ì§€")
            except Exception as e:
                self.logger.warning(f"   âš ï¸ ìœ íœ´ í•™ìŠµ ì •ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (í•„ìš”ì‹œ)
        if self.checkpoint_manager:
            try:
                # í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì €ì¥
                self.logger.info("   ğŸ’¾ ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥...")
            except Exception as e:
                self.logger.warning(f"   âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        self.logger.info("   âœ… ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")


def to_jsonable(x):
    """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    import torch
    import numpy as np
    from enum import Enum
    from dataclasses import is_dataclass, asdict
    from datetime import datetime, date, timedelta
    from pathlib import Path
    import uuid
    
    # None ì²˜ë¦¬
    if x is None:
        return None
    
    # ê¸°ë³¸ íƒ€ì…ë“¤
    if isinstance(x, (str, int, float, bool)):
        return x
    
    # Tensor ì²˜ë¦¬
    if isinstance(x, torch.Tensor):
        return x.detach().cpu().tolist() if x.dim() > 0 else x.detach().cpu().item()
    
    # NumPy ë°°ì—´/ìŠ¤ì¹¼ë¼ ì²˜ë¦¬
    if isinstance(x, np.ndarray):
        return x.tolist()
    if isinstance(x, (np.floating, np.integer)):
        return x.item()
    
    # Enum ì²˜ë¦¬
    if isinstance(x, Enum):
        return x.name
    
    # UUID ì²˜ë¦¬
    if isinstance(x, uuid.UUID):
        return str(x)
    
    # Path ì²˜ë¦¬
    if isinstance(x, Path):
        return str(x)
    
    # datetime ê´€ë ¨ ì²˜ë¦¬
    if isinstance(x, (datetime, date)):
        return x.isoformat()
    if isinstance(x, timedelta):
        return x.total_seconds()
    
    # dataclass ì²˜ë¦¬ (ì¤‘ìš”!)
    if is_dataclass(x) and not isinstance(x, type):
        # dataclassë¥¼ dictë¡œ ë³€í™˜í•œ í›„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
        try:
            return {k: to_jsonable(v) for k, v in asdict(x).items()}
        except Exception as e:
            # asdict ì‹¤íŒ¨ ì‹œ __dict__ ì‚¬ìš©
            if hasattr(x, '__dict__'):
                return {k: to_jsonable(v) for k, v in x.__dict__.items() 
                       if not k.startswith('_')}
            else:
                return str(x)
    
    # dict ì²˜ë¦¬
    if isinstance(x, dict):
        return {str(k): to_jsonable(v) for k, v in x.items()}
    
    # list, tuple, set ì²˜ë¦¬
    if isinstance(x, (list, tuple, set)):
        converted = [to_jsonable(v) for v in x]
        if isinstance(x, tuple):
            return converted  # JSONì€ tupleì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ listë¡œ
        elif isinstance(x, set):
            return converted  # setë„ listë¡œ
        else:
            return converted
    
    # ê¸°íƒ€ ê°ì²´ë“¤ - __dict__ ì†ì„±ì´ ìˆìœ¼ë©´ dictë¡œ ë³€í™˜
    if hasattr(x, '__dict__'):
        return {k: to_jsonable(v) for k, v in x.__dict__.items() 
               if not k.startswith('_')}
    
    # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¬¸ìì—´ë¡œ ë³€í™˜
    try:
        return str(x)
    except:
        return None

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Red Heart AI í†µí•© ì¶”ë¡  ì‹œìŠ¤í…œ')
    
    # ê¸°ë³¸ ì¸ì
    parser.add_argument('--text', type=str, help='ë¶„ì„í•  í…ìŠ¤íŠ¸')
    parser.add_argument('--mode', default='inference',
                       choices=['inference', 'test', 'demo', 'production'],
                       help='ì‹¤í–‰ ëª¨ë“œ')
    
    # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
    parser.add_argument('--checkpoint', type=str, default=None,
                       help='ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ê²½ë¡œ (ì—†ìœ¼ë©´ --epoch ì‚¬ìš©)')
    parser.add_argument('--epoch', type=int, default=50,
                       help='ë¡œë“œí•  ì²´í¬í¬ì¸íŠ¸ ì—í­ ë²ˆí˜¸ (ê¸°ë³¸: 50 - sweet spot)')
    
    # ëª¨ë“ˆ í™œì„±í™” í”Œë˜ê·¸
    parser.add_argument('--no-neural', action='store_true', help='Neural Analyzers ë¹„í™œì„±í™”')
    parser.add_argument('--no-wrappers', action='store_true', help='Advanced Wrappers ë¹„í™œì„±í™”')
    parser.add_argument('--no-dsp', action='store_true', help='DSP ì‹œë®¬ë ˆì´í„° ë¹„í™œì„±í™”')
    parser.add_argument('--no-phase', action='store_true', help='Phase Networks ë¹„í™œì„±í™”')
    
    # LLM ì˜µì…˜
    parser.add_argument('--llm', 
                       choices=['none', 'local', 'claude', 'mcp', 'gpt', 'perplexity', 'deepseek'],
                       default='none', help='LLM í†µí•© ëª¨ë“œ (none/local/API ì´ë¦„)')
    
    # ê¸°íƒ€
    parser.add_argument('--batch-size', type=int, default=4, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--device', type=str, help='ë””ë°”ì´ìŠ¤ (cuda/cpu)')
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ë¡œê·¸')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
    
    # ë©”ëª¨ë¦¬ ëª¨ë“œ ì§ì ‘ ì„ íƒ ì˜µì…˜ ì¶”ê°€
    parser.add_argument('--memory-mode', 
                       choices=['light', 'medium', 'heavy'],
                       help='ë©”ëª¨ë¦¬ ëª¨ë“œ ì„ íƒ (ê¸°ë³¸: ìë™)')
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = InferenceConfig(
        checkpoint_path=args.checkpoint,  # ì§ì ‘ ê²½ë¡œ (ìˆìœ¼ë©´ ìš°ì„ )
        checkpoint_epoch=args.epoch,  # ì—í­ ë²ˆí˜¸ë¡œ ìë™ ê²€ìƒ‰
        batch_size=args.batch_size,
        device=args.device or str(DEVICE),
        use_neural_analyzers=not args.no_neural,
        use_advanced_wrappers=not args.no_wrappers,
        use_dsp_simulator=not args.no_dsp,
        use_phase_networks=not args.no_phase,
        llm_mode=args.llm,
        verbose=args.verbose,
        debug=args.debug
    )
    
    # ë©”ëª¨ë¦¬ ëª¨ë“œ ì„¤ì •
    if args.memory_mode:
        config.memory_mode = MemoryMode[args.memory_mode.upper()]
        config.auto_memory_mode = False  # ìˆ˜ë™ ì„ íƒì‹œ ìë™ ëª¨ë“œ ë¹„í™œì„±í™”
    
    # ì‹œìŠ¤í…œ ìƒì„± ë° ì´ˆê¸°í™”
    system = UnifiedInferenceSystem(config)
    await system.initialize()
    
    # ëª¨ë“œë³„ ì‹¤í–‰
    if args.mode == 'test':
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        logger.info("\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰...")
        test_texts = [
            "ì´ ê²°ì •ì€ ë§ì€ ì‚¬ëŒë“¤ì˜ ìƒëª…ê³¼ ì•ˆì „ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.",
            "ê¸°ìˆ  ë°œì „ê³¼ ì¼ìë¦¬ ë³´í˜¸ ì‚¬ì´ì˜ ê· í˜•ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.",
            "ê°œì¸ì •ë³´ ë³´í˜¸ì™€ ê³µìµ ì‚¬ì´ì˜ ê°ˆë“± ìƒí™©ì…ë‹ˆë‹¤."
        ]
        
        for i, text in enumerate(test_texts, 1):
            logger.info(f"\ní…ŒìŠ¤íŠ¸ {i}: {text}")
            result = await system.analyze(text)
            logger.info(f"ê²°ê³¼: {json.dumps(to_jsonable(result), indent=2, ensure_ascii=False)[:500]}...")
    
    elif args.mode == 'demo':
        # ë°ëª¨ ëª¨ë“œ
        logger.info("\nğŸ® ë°ëª¨ ëª¨ë“œ - ëŒ€í™”í˜• ë¶„ì„")
        while True:
            try:
                text = input("\ní…ìŠ¤íŠ¸ ì…ë ¥ (ì¢…ë£Œ: quit): ")
                if text.lower() == 'quit':
                    break
                
                result = await system.analyze(text)
                print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
                print(f"   í†µí•© ì ìˆ˜: {result.get('integrated_score', 0):.3f}")
                print(f"   ì‹ ë¢°ë„: {result.get('confidence', 0):.3f}")
                print(f"   ì²˜ë¦¬ ì‹œê°„: {result.get('processing_time', 0):.2f}ì´ˆ")
                
                if 'unified' in result:
                    print(f"\n   ê°ì • ë¶„ì„: {result['unified'].get('emotion', {})}")
                    print(f"   ë²¤ë‹´ ì ìˆ˜: {result['unified'].get('bentham', {})}")
                    print(f"   í›„íšŒ ì ìˆ˜: {result['unified'].get('regret', {})}")
                    print(f"   SURD ë¶„ì„: {result['unified'].get('surd', {})}")
                
            except KeyboardInterrupt:
                break
    
    elif args.mode == 'production':
        # ìš´ìš© ëª¨ë“œ
        logger.info("\nğŸš€ ìš´ìš© ëª¨ë“œ í™œì„±í™”")
        if args.text:
            result = await system.analyze(args.text)
            print(json.dumps(to_jsonable(result), indent=2, ensure_ascii=False))
        else:
            logger.info("í…ìŠ¤íŠ¸ë¥¼ --text ì¸ìë¡œ ì œê³µí•˜ì„¸ìš”")
    
    else:
        # ì¶”ë¡  ëª¨ë“œ (ê¸°ë³¸)
        if args.text:
            logger.info(f"\në¶„ì„ í…ìŠ¤íŠ¸: {args.text}")
            result = await system.analyze(args.text)
            print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
            print(json.dumps(to_jsonable(result), indent=2, ensure_ascii=False))
        else:
            logger.info("í…ìŠ¤íŠ¸ë¥¼ --text ì¸ìë¡œ ì œê³µí•˜ê±°ë‚˜ --modeë¥¼ ì„ íƒí•˜ì„¸ìš”")
    
    # ì‹œìŠ¤í…œ ì •ë¦¬
    await system.cleanup()
    
    # í†µê³„ ì¶œë ¥
    stats = system.get_stats()
    logger.info(f"\nğŸ“ˆ ì„¸ì…˜ í†µê³„:")
    logger.info(f"   ì´ ìš”ì²­: {stats['total_requests']}")
    logger.info(f"   ì„±ê³µ: {stats['successful']}")
    logger.info(f"   ì‹¤íŒ¨: {stats['failed']}")
    logger.info(f"   í‰ê·  ì‹œê°„: {stats['avg_time']:.2f}ì´ˆ")


if __name__ == "__main__":
    # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
    asyncio.run(main())