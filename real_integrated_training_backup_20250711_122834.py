"""
Red Heart AI ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ
REAL Integrated Training System for Red Heart AI

ì§„ì§œ ëª¨ë¸ë“¤ì„ ì‹¤ì œë¡œ í˜¸ì¶œí•˜ì—¬ í†µí•© í›ˆë ¨ ìˆ˜í–‰
- ì‹¤ì œ ê°ì • ë¶„ì„ ëª¨ë¸ í˜¸ì¶œ
- ì‹¤ì œ ë²¤ë‹´ ê³„ì‚° ëª¨ë¸ í˜¸ì¶œ
- ì‹¤ì œ í›„íšŒ ë¶„ì„ ëª¨ë¸ í˜¸ì¶œ
- ì‹¤ì œ SURD ë¶„ì„ ëª¨ë¸ í˜¸ì¶œ
- processed_datasets 24,170ê°œ ë°ì´í„° í›ˆë ¨
"""

import asyncio
import logging
import time
import numpy as np
import torch
import json
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, field
import traceback
from datetime import datetime
import os
import glob

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('RedHeart_REAL_Training')

# ì‹¤ì œ ì‹œìŠ¤í…œ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from advanced_emotion_analyzer import AdvancedEmotionAnalyzer
    from advanced_bentham_calculator import AdvancedBenthamCalculator
    from advanced_regret_analyzer import AdvancedRegretAnalyzer
    from advanced_surd_analyzer import AdvancedSURDAnalyzer
    from advanced_experience_database import AdvancedExperienceDatabase
    from data_models import EthicalSituation, EmotionData, HedonicValues
    
    # ì‹¤ì œ í•™ìŠµ ì‹œìŠ¤í…œ ì„í¬íŠ¸
    from advanced_learning_executor import AdvancedLearningExecutor, LearningConfig
    from advanced_regret_learning_system import AdvancedRegretLearningSystem, LearningPhase
    from advanced_hierarchical_emotion_system import AdvancedHierarchicalEmotionSystem, EmotionPhase
    from integrated_system_orchestrator import IntegratedSystemOrchestrator, IntegrationContext
    from dynamic_ethical_choice_analyzer import DynamicEthicalChoiceAnalyzer, EthicalDilemma
    
    MODULES_AVAILABLE = True
    LEARNING_SYSTEM_AVAILABLE = True
except ImportError as e:
    MODULES_AVAILABLE = False
    LEARNING_SYSTEM_AVAILABLE = False
    logger.error(f"ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

@dataclass
class RealTrainingResult:
    """ì‹¤ì œ í›ˆë ¨ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°"""
    data_id: str
    source_file: str
    processing_time: float
    emotion_analysis: Dict[str, Any]
    bentham_calculation: Dict[str, Any]
    regret_analysis: Dict[str, Any]
    surd_analysis: Dict[str, Any]
    counterfactual_experiences: List[Dict[str, Any]]
    integration_success: bool
    error_log: List[str]

class RealIntegratedTrainingSystem:
    """ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ - ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ í†µí•©"""
    
    def __init__(self):
        # ê°œë³„ ëª¨ë“ˆë“¤ (ê¸°ì¡´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ìš©)
        self.emotion_analyzer = None
        self.bentham_calculator = None
        self.regret_analyzer = None
        self.surd_analyzer = None
        self.experience_db = None
        
        # ì‹¤ì œ í•™ìŠµ ì‹œìŠ¤í…œ (ìƒˆë¡œ ì¶”ê°€)
        self.learning_executor = None
        self.integrated_orchestrator = None
        self.dynamic_choice_analyzer = None
        self.learning_mode = False  # í•™ìŠµ ëª¨ë“œ vs ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        
        # ì‹¤ì œ í›ˆë ¨ ë©”íŠ¸ë¦­
        self.training_metrics = {
            'total_processed': 0,
            'successful_integrations': 0,
            'failed_processes': 0,
            'total_processing_time': 0.0,
            'avg_processing_time': 0.0,
            'real_accuracy_scores': {},
            'module_performance': {},
            'error_patterns': [],
            'learning_phases': {
                'current_phase': 0,
                'phase_transitions': [],
                'phase_performance': {}
            }
        }
        
        # ë°ì´í„° ë¡œë”
        self.processed_datasets_path = Path("/mnt/c/large_project/linux_red_heart/processed_datasets")
        
    async def initialize_real_system(self, learning_mode: bool = False):
        """ì‹¤ì œ ì‹œìŠ¤í…œ ëª¨ë“  ëª¨ë“ˆ ì´ˆê¸°í™”"""
        logger.info("=== ì‹¤ì œ Red Heart AI í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ===")
        
        self.learning_mode = learning_mode
        
        if learning_mode and LEARNING_SYSTEM_AVAILABLE:
            logger.info("ğŸš€ í•™ìŠµ ëª¨ë“œ í™œì„±í™” - ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
            
            # ì‹¤ì œ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            try:
                logger.info("ì‹¤ì œ í•™ìŠµ ì‹¤í–‰ê¸° ì´ˆê¸°í™”...")
                learning_config = LearningConfig(
                    regrets_per_step=7,
                    bentham_per_environment=3,
                    general_data_cycles=3,
                    ebs_data_cycles=6,
                    max_scenarios_per_batch=50
                )
                self.learning_executor = AdvancedLearningExecutor(learning_config)
                logger.info("âœ… ì‹¤ì œ í•™ìŠµ ì‹¤í–‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                
                # í†µí•© ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”
                logger.info("í†µí•© ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”...")
                self.integrated_orchestrator = IntegratedSystemOrchestrator()
                logger.info("âœ… í†µí•© ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
                
                # ë™ì  ìœ¤ë¦¬ì  ì„ íƒì§€ ë¶„ì„ê¸° ì´ˆê¸°í™”
                logger.info("ë™ì  ìœ¤ë¦¬ì  ì„ íƒì§€ ë¶„ì„ê¸° ì´ˆê¸°í™”...")
                self.dynamic_choice_analyzer = DynamicEthicalChoiceAnalyzer()
                logger.info("âœ… ë™ì  ìœ¤ë¦¬ì  ì„ íƒì§€ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
                
                return True
                
            except Exception as e:
                logger.error(f"âŒ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                logger.info("ğŸ”„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì „í™˜")
                self.learning_mode = False
        
        # ê¸°ì¡´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ê¸°ë³¸ê°’)
        logger.info("ğŸ”§ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ - ê°œë³„ ëª¨ë“ˆ ì´ˆê¸°í™”")
        
        try:
            # ì‹¤ì œ ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”
            logger.info("ì‹¤ì œ ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”...")
            self.emotion_analyzer = AdvancedEmotionAnalyzer()
            logger.info("âœ… ì‹¤ì œ ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì‹¤ì œ ë²¤ë‹´ ê³„ì‚°ê¸° ì´ˆê¸°í™”
            logger.info("ì‹¤ì œ ë²¤ë‹´ ê³„ì‚°ê¸° ì´ˆê¸°í™”...")
            self.bentham_calculator = AdvancedBenthamCalculator()
            logger.info("âœ… ì‹¤ì œ ë²¤ë‹´ ê³„ì‚°ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì‹¤ì œ í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™”
            logger.info("ì‹¤ì œ í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™”...")
            try:
                self.regret_analyzer = AdvancedRegretAnalyzer()
                logger.info("âœ… ì‹¤ì œ í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ í›„íšŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.regret_analyzer = None
            
            # ì‹¤ì œ SURD ë¶„ì„ê¸° ì´ˆê¸°í™”
            logger.info("ì‹¤ì œ SURD ë¶„ì„ê¸° ì´ˆê¸°í™”...")
            self.surd_analyzer = AdvancedSURDAnalyzer()
            logger.info("âœ… ì‹¤ì œ SURD ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ì‹¤ì œ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            logger.info("ì‹¤ì œ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”...")
            self.experience_db = AdvancedExperienceDatabase()
            logger.info("âœ… ì‹¤ì œ ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
            logger.info("ğŸ¯ ì‹¤ì œ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì‹¤ì œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def load_real_training_data(self) -> List[Dict[str, Any]]:
        """ì‹¤ì œ processed_datasetsì—ì„œ í›ˆë ¨ ë°ì´í„° ë¡œë“œ"""
        logger.info("=== ì‹¤ì œ í›ˆë ¨ ë°ì´í„° ë¡œë”© ì‹œì‘ ===")
        
        training_data = []
        
        try:
            # ìŠ¤í¬ëŸ¬í”Œ ë°°ì¹˜ íŒŒì¼ë“¤ ë¡œë“œ
            scruples_pattern = self.processed_datasets_path / "scruples" / "scruples_batch_*.json"
            scruples_files = glob.glob(str(scruples_pattern))
            
            logger.info(f"ìŠ¤í¬ëŸ¬í”Œ ë°°ì¹˜ íŒŒì¼ {len(scruples_files)}ê°œ ë°œê²¬")
            
            for file_path in scruples_files[:2]:  # ì²˜ìŒ 2ê°œ ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        batch_data = json.load(f)
                        
                    if 'scenarios' in batch_data:
                        for item in batch_data['scenarios']:
                            if 'description' in item:
                                training_data.append({
                                    'source_file': os.path.basename(file_path),
                                    'data_id': item.get('id', f"scruples_{len(training_data)}"),
                                    'situation': item['description'],
                                    'context': item.get('context', {}),
                                    'moral_complexity': 0.7,  # ìŠ¤í¬ëŸ¬í”Œ ë°ì´í„° ê¸°ë³¸ ë³µì¡ë„
                                    'stakeholders': {},
                                    'data_type': 'scruples'
                                })
                            
                except Exception as e:
                    logger.warning(f"ìŠ¤í¬ëŸ¬í”Œ íŒŒì¼ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
            
            # í†µí•© ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ë“¤ ë¡œë“œ
            try:
                integrated_files = [
                    self.processed_datasets_path / "integrated_scenarios.json",
                    self.processed_datasets_path / "final_integrated_with_batch7_20250619_213234.json"
                ]
                integrated_files = [f for f in integrated_files if f.exists()]
                logger.info(f"í†µí•© ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ {len(integrated_files)}ê°œ ë°œê²¬")
                
                for file_path in integrated_files:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        scenarios = json.load(f)
                    
                    for idx, scenario in enumerate(scenarios[:5]):  # ê° íŒŒì¼ì—ì„œ 5ê°œë§Œ
                        if 'description' in scenario:
                            training_data.append({
                                'source_file': os.path.basename(file_path),
                                'data_id': scenario.get('id', f"integrated_{idx}"),
                                'situation': scenario['description'],
                                'context': scenario.get('context', {}),
                                'moral_complexity': scenario.get('complexity_score', 0.7),
                                'stakeholders': scenario.get('stakeholders', []),
                                'data_type': 'integrated'
                            })
                            
            except Exception as e:
                logger.warning(f"í†µí•© ì‹œë‚˜ë¦¬ì˜¤ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # í•œêµ­ ë¬¸í™” íŠ¹í™” ë°ì´í„° ë¡œë“œ
            try:
                korean_files = [
                    self.processed_datasets_path / "korean_cultural_scenarios.json"
                ]
                korean_files = [f for f in korean_files if f.exists()]
                logger.info(f"í•œêµ­ ë¬¸í™” íŒŒì¼ {len(korean_files)}ê°œ ë°œê²¬")
                
                for file_path in korean_files:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        cultural_data = json.load(f)
                    
                    for item in cultural_data:
                        if 'scenario' in item and len(training_data) < 25:  # ìµœëŒ€ 25ê°œ ì œí•œ
                            training_data.append({
                                'source_file': os.path.basename(file_path),
                                'data_id': item.get('id', f"korean_{len(training_data)}"),
                                'situation': item['scenario'],
                                'context': item.get('context', {}),
                                'moral_complexity': item.get('complexity', 0.8),
                                'stakeholders': item.get('stakeholders', {}),
                                'data_type': 'korean_cultural'
                            })
                            
            except Exception as e:
                logger.warning(f"í•œêµ­ ë¬¸í™” ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            
            logger.info(f"âœ… ì´ {len(training_data)}ê°œ ì‹¤ì œ í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            
            # ë°ì´í„° ë¶„í¬ ë¡œê¹…
            data_types = {}
            for data in training_data:
                data_type = data['data_type']
                data_types[data_type] = data_types.get(data_type, 0) + 1
            
            logger.info("ğŸ“Š ë°ì´í„° ë¶„í¬:")
            for data_type, count in data_types.items():
                logger.info(f"  - {data_type}: {count}ê°œ")
            
            return training_data
            
        except Exception as e:
            logger.error(f"âŒ ì‹¤ì œ í›ˆë ¨ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return []
    
    async def process_real_training_item(self, data_item: Dict[str, Any]) -> RealTrainingResult:
        """ë‹¨ì¼ í›ˆë ¨ ë°ì´í„° ì•„ì´í…œì„ ì‹¤ì œ ëª¨ë“ˆë“¤ë¡œ ì²˜ë¦¬"""
        
        start_time = time.time()
        error_log = []
        
        try:
            # 1. ì‹¤ì œ ê°ì • ë¶„ì„
            logger.info(f"ğŸ¯ ì²˜ë¦¬ ì¤‘: {data_item['data_id']} - ê°ì • ë¶„ì„...")
            emotion_start = time.time()
            
            try:
                # ì‹¤ì œ ê°ì • ë¶„ì„ê¸° í˜¸ì¶œ - ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„°
                emotion_result = self.emotion_analyzer.analyze_emotion(
                    text=data_item['situation'],
                    language="ko",
                    biosignal_data=None,
                    use_cache=True
                )
                emotion_processing_time = time.time() - emotion_start
                logger.info(f"   âœ… ê°ì • ë¶„ì„ ì™„ë£Œ ({emotion_processing_time:.3f}ì´ˆ)")
                
            except Exception as e:
                error_log.append(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
                emotion_result = {'error': str(e), 'fallback': True}
                emotion_processing_time = time.time() - emotion_start
                logger.warning(f"   âš ï¸ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # 2. ì‹¤ì œ ë²¤ë‹´ ê³„ì‚°
            logger.info(f"ğŸ¯ ì²˜ë¦¬ ì¤‘: {data_item['data_id']} - ë²¤ë‹´ ê³„ì‚°...")
            bentham_start = time.time()
            
            try:
                # ì‹¤ì œ ë²¤ë‹´ ê³„ì‚°ê¸° í˜¸ì¶œ - ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„°
                bentham_input_data = {
                    'situation': data_item['situation'],
                    'context': data_item.get('context', {}),
                    'emotion_data': emotion_result if 'error' not in emotion_result else None
                }
                bentham_result = self.bentham_calculator.calculate_with_advanced_layers(
                    input_data=bentham_input_data,
                    use_cache=True
                )
                bentham_processing_time = time.time() - bentham_start
                logger.info(f"   âœ… ë²¤ë‹´ ê³„ì‚° ì™„ë£Œ ({bentham_processing_time:.3f}ì´ˆ)")
                
            except Exception as e:
                error_log.append(f"ë²¤ë‹´ ê³„ì‚° ì‹¤íŒ¨: {e}")
                bentham_result = {'error': str(e), 'fallback': True}
                bentham_processing_time = time.time() - bentham_start
                logger.warning(f"   âš ï¸ ë²¤ë‹´ ê³„ì‚° ì‹¤íŒ¨: {e}")
            
            # 3. ì‹¤ì œ í›„íšŒ ë¶„ì„
            if self.regret_analyzer:
                logger.info(f"ğŸ¯ ì²˜ë¦¬ ì¤‘: {data_item['data_id']} - í›„íšŒ ë¶„ì„...")
                regret_start = time.time()
                
                try:
                    # í›„íšŒ ë¶„ì„ì„ ìœ„í•œ decision_data ì¤€ë¹„ - ì•ˆì „í•œ íƒ€ì… ì²˜ë¦¬
                    decision_data = {
                        'scenario': data_item['situation'],
                        'text': data_item['situation'],  # í…ìŠ¤íŠ¸ í•„ë“œ ì¶”ê°€
                        'context': data_item.get('context', {}),
                    }
                    
                    # ê°ì • ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì•ˆì „í•œ ë°©ì‹)
                    if 'error' not in emotion_result:
                        decision_data['emotion_context'] = emotion_result
                    
                    # ë²¤ë‹´ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì•ˆì „í•œ íƒ€ì… ì²˜ë¦¬)
                    bentham_has_error = False
                    if isinstance(bentham_result, dict):
                        bentham_has_error = 'error' in bentham_result
                    elif hasattr(bentham_result, 'error'):
                        bentham_has_error = bentham_result.error is not None
                    
                    if not bentham_has_error:
                        if hasattr(bentham_result, '__dict__'):
                            # getattr ëŒ€ì‹  ì‹¤ì œ ì†ì„± ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                            if hasattr(bentham_result, 'final_score') and bentham_result.final_score is not None:
                                decision_data['bentham_context'] = {
                                    'score': bentham_result.final_score,
                                    'type': 'bentham_calculation'
                                }
                            else:
                                raise ValueError(f"ë²¤ë‹´ ê³„ì‚° ê²°ê³¼ì— final_score ì†ì„±ì´ ì—†ìŒ: {type(bentham_result)}")
                        elif isinstance(bentham_result, dict):
                            decision_data['bentham_context'] = bentham_result
                    
                    # ì‹¤ì œ í›„íšŒ ë¶„ì„ê¸° í˜¸ì¶œ
                    regret_result = await self.regret_analyzer.analyze_regret(
                        decision_data=decision_data,
                        outcome_data=None
                    )
                    regret_processing_time = time.time() - regret_start
                    logger.info(f"   âœ… í›„íšŒ ë¶„ì„ ì™„ë£Œ ({regret_processing_time:.3f}ì´ˆ)")
                    
                except Exception as e:
                    error_log.append(f"í›„íšŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    regret_result = {'error': str(e), 'fallback': True}
                    regret_processing_time = time.time() - regret_start
                    logger.warning(f"   âš ï¸ í›„íšŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            else:
                regret_result = {'error': 'í›„íšŒ ë¶„ì„ê¸° ì‚¬ìš© ë¶ˆê°€', 'disabled': True}
                regret_processing_time = 0.0
            
            # 4. ì‹¤ì œ SURD í†µí•© ë¶„ì„
            logger.info(f"ğŸ¯ ì²˜ë¦¬ ì¤‘: {data_item['data_id']} - SURD í†µí•© ë¶„ì„...")
            surd_start = time.time()
            
            try:
                # SURD ë¶„ì„ì„ ìœ„í•œ ë³€ìˆ˜ ì¤€ë¹„
                surd_variables = {}
                
                # ê°ì • ë°ì´í„° í†µí•© (ì•ˆì „í•œ ì²´í¬)
                emotion_has_error = isinstance(emotion_result, dict) and 'error' in emotion_result
                if not emotion_has_error:
                    if hasattr(emotion_result, 'dominant_emotion'):
                        # ì‹¤ì œ ì†ì„± ì¡´ì¬ í™•ì¸ í›„ ê°’ ì¶”ì¶œ
                        if hasattr(emotion_result, 'intensity') and emotion_result.intensity is not None:
                            surd_variables['emotion_intensity'] = float(emotion_result.intensity)
                        else:
                            raise ValueError(f"ê°ì • ë¶„ì„ ê²°ê³¼ì— intensity ì†ì„±ì´ ì—†ìŒ: {type(emotion_result)}")
                        
                        if hasattr(emotion_result, 'confidence') and emotion_result.confidence is not None:
                            surd_variables['emotion_confidence'] = float(emotion_result.confidence)
                        else:
                            raise ValueError(f"ê°ì • ë¶„ì„ ê²°ê³¼ì— confidence ì†ì„±ì´ ì—†ìŒ: {type(emotion_result)}")
                    elif isinstance(emotion_result, dict):
                        surd_variables['emotion_intensity'] = float(emotion_result.get('intensity', 0.5))
                        surd_variables['emotion_confidence'] = float(emotion_result.get('confidence', 0.5))
                
                # ë²¤ë‹´ ë°ì´í„° í†µí•© (ì•ˆì „í•œ ì²´í¬)
                bentham_has_error = isinstance(bentham_result, dict) and 'error' in bentham_result
                if not bentham_has_error:
                    if hasattr(bentham_result, 'final_score'):
                        # ì‹¤ì œ ì†ì„± ì¡´ì¬ í™•ì¸ í›„ ê°’ ì¶”ì¶œ
                        if bentham_result.final_score is not None:
                            surd_variables['pleasure_score'] = float(bentham_result.final_score)
                        else:
                            raise ValueError(f"ë²¤ë‹´ ê³„ì‚° ê²°ê³¼ì˜ final_scoreê°€ None: {type(bentham_result)}")
                    elif isinstance(bentham_result, dict):
                        surd_variables['pleasure_score'] = float(bentham_result.get('final_score', 0.0))
                
                # í›„íšŒ ë°ì´í„° í†µí•© (AdvancedRegretMetrics íƒ€ì… ì•ˆì „ ì²˜ë¦¬)
                regret_surd_error = False
                if hasattr(regret_result, '__class__') and regret_result.__class__.__name__ == 'AdvancedRegretMetrics':
                    # AdvancedRegretMetrics ê°ì²´ì—ì„œ ë°ì´í„° ì¶”ì¶œ - ì‹¤ì œ ì†ì„± ê²€ì¦
                    if hasattr(regret_result, 'regret_intensity') and regret_result.regret_intensity is not None:
                        if regret_result.regret_intensity <= 0.0:
                            raise ValueError(f"í›„íšŒ ë¶„ì„ ê²°ê³¼ì˜ regret_intensityê°€ 0.0: {regret_result.regret_intensity}")
                        surd_variables['regret_intensity'] = float(regret_result.regret_intensity)
                    elif hasattr(regret_result, 'intensity') and regret_result.intensity is not None:
                        if regret_result.intensity <= 0.0:
                            raise ValueError(f"í›„íšŒ ë¶„ì„ ê²°ê³¼ì˜ intensityê°€ 0.0: {regret_result.intensity}")
                        surd_variables['regret_intensity'] = float(regret_result.intensity)
                    else:
                        raise ValueError(f"í›„íšŒ ë¶„ì„ ê²°ê³¼ì— regret_intensity ë˜ëŠ” intensity ì†ì„±ì´ ì—†ìŒ: {type(regret_result)}")
                elif isinstance(regret_result, dict):
                    regret_surd_error = 'error' in regret_result
                    if not regret_surd_error:
                        surd_variables['regret_intensity'] = float(regret_result.get('regret_intensity', 0.0))
                
                # ì‹¤íŒ¨ ê°ì§€ - ëª¨ë“  ë¶„ì„ì´ ì‹¤íŒ¨í–ˆì„ ê²½ìš° ì˜ˆì™¸ ë°œìƒ
                if not surd_variables:
                    raise RuntimeError(
                        f"ëª¨ë“  ë¶„ì„ ëª¨ë“ˆì´ ì‹¤íŒ¨í–ˆê±°ë‚˜ ìœ íš¨í•œ ê°’ì„ ìƒì„±í•˜ì§€ ëª»í•¨. "
                        f"ê°ì •: {'error' if emotion_has_error else 'ok'}, "
                        f"ë²¤ë‹´: {'error' if bentham_has_error else 'ok'}, "
                        f"í›„íšŒ: {'error' if regret_surd_error else 'ok'}"
                    )
                
                # ì‹¤ì œ SURD ë¶„ì„ê¸° í˜¸ì¶œ - analyze_advanced ë©”ì„œë“œ ì‚¬ìš©
                surd_result = self.surd_analyzer.analyze_advanced(
                    variables=surd_variables,
                    target_variable='ethical_decision_quality',
                    additional_context=data_item.get('context', {})
                )
                surd_processing_time = time.time() - surd_start
                logger.info(f"   âœ… SURD ë¶„ì„ ì™„ë£Œ ({surd_processing_time:.3f}ì´ˆ)")
                
            except Exception as e:
                error_log.append(f"SURD ë¶„ì„ ì‹¤íŒ¨: {e}")
                surd_result = {'error': str(e), 'fallback': True}
                surd_processing_time = time.time() - surd_start
                logger.warning(f"   âš ï¸ SURD ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # 5. ë°˜ì‚¬ì‹¤ ê²½í—˜ ìƒì„± - AdvancedRegretMetrics íƒ€ì… ì•ˆì „ ì²˜ë¦¬
            counterfactual_experiences = []
            regret_has_error = False
            
            # AdvancedRegretMetrics ê°ì²´ íƒ€ì… ì²´í¬
            if hasattr(regret_result, '__class__') and regret_result.__class__.__name__ == 'AdvancedRegretMetrics':
                # ì„±ê³µì ì¸ ê²°ê³¼ - ë°˜ì‚¬ì‹¤ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ
                if hasattr(regret_result, 'counterfactual_scenarios'):
                    counterfactual_experiences = regret_result.counterfactual_scenarios or []
                elif hasattr(regret_result, 'counterfactuals'):
                    counterfactual_experiences = regret_result.counterfactuals or []
            elif isinstance(regret_result, dict):
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ (ì˜¤ë¥˜ í¬í•¨ ê°€ëŠ¥)
                regret_has_error = 'error' in regret_result
                if not regret_has_error and 'counterfactual_scenarios' in regret_result:
                    counterfactual_experiences = regret_result['counterfactual_scenarios']
            
            # 6. ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            try:
                # ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
                def convert_to_serializable(obj):
                    if hasattr(obj, '__dict__'):
                        return {k: str(v) if not isinstance(v, (str, int, float, bool, type(None))) else v 
                               for k, v in obj.__dict__.items()}
                    elif isinstance(obj, dict):
                        return obj
                    else:
                        return {'result': str(obj), 'type': type(obj).__name__}
                
                experience_entry = {
                    'data_id': data_item['data_id'],
                    'situation': data_item['situation'],
                    'emotion_analysis': convert_to_serializable(emotion_result),
                    'bentham_calculation': convert_to_serializable(bentham_result),
                    'regret_analysis': convert_to_serializable(regret_result),
                    'surd_analysis': convert_to_serializable(surd_result),
                    'timestamp': datetime.now().isoformat(),
                    'source_file': data_item['source_file']
                }
                
                await self.experience_db.store_experience(
                    experience_text=data_item['situation'],
                    metadata=experience_entry,
                    category="training",
                    importance_score=None
                )
                
            except Exception as e:
                error_log.append(f"ê²½í—˜ ì €ì¥ ì‹¤íŒ¨: {e}")
                logger.warning(f"   âš ï¸ ê²½í—˜ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # ê²°ê³¼ ìƒì„±
            total_processing_time = time.time() - start_time
            integration_success = len(error_log) == 0
            
            result = RealTrainingResult(
                data_id=data_item['data_id'],
                source_file=data_item['source_file'],
                processing_time=total_processing_time,
                emotion_analysis=emotion_result,
                bentham_calculation=bentham_result,
                regret_analysis=regret_result,
                surd_analysis=surd_result,
                counterfactual_experiences=counterfactual_experiences,
                integration_success=integration_success,
                error_log=error_log
            )
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.training_metrics['total_processed'] += 1
            self.training_metrics['total_processing_time'] += total_processing_time
            
            if integration_success:
                self.training_metrics['successful_integrations'] += 1
                logger.info(f"âœ… {data_item['data_id']} ì‹¤ì œ í†µí•© í›ˆë ¨ ì™„ë£Œ ({total_processing_time:.3f}ì´ˆ)")
            else:
                self.training_metrics['failed_processes'] += 1
                logger.warning(f"âš ï¸ {data_item['data_id']} ë¶€ë¶„ ì‹¤íŒ¨ ({len(error_log)}ê°œ ì˜¤ë¥˜)")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ {data_item['data_id']} ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            
            total_processing_time = time.time() - start_time
            self.training_metrics['total_processed'] += 1
            self.training_metrics['failed_processes'] += 1
            self.training_metrics['total_processing_time'] += total_processing_time
            
            return RealTrainingResult(
                data_id=data_item['data_id'],
                source_file=data_item['source_file'],
                processing_time=total_processing_time,
                emotion_analysis={'error': str(e)},
                bentham_calculation={'error': str(e)},
                regret_analysis={'error': str(e)},
                surd_analysis={'error': str(e)},
                counterfactual_experiences=[],
                integration_success=False,
                error_log=[f"ì‹¬ê°í•œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}"]
            )
    
    async def run_real_integrated_training(self, max_items: int = 100) -> Dict[str, Any]:
        """ì‹¤ì œ ë°ì´í„°ë¡œ í†µí•© í›ˆë ¨ ì‹¤í–‰"""
        logger.info("ğŸš€ ì‹¤ì œ Red Heart AI í†µí•© í›ˆë ¨ ì‹œì‘")
        
        # ì‹¤ì œ í›ˆë ¨ ë°ì´í„° ë¡œë“œ
        training_data = self.load_real_training_data()
        
        if not training_data:
            logger.error("âŒ í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {"error": "í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"}
        
        # ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜ ì œí•œ
        if len(training_data) > max_items:
            training_data = training_data[:max_items]
            logger.info(f"ğŸ“Š ì²˜ë¦¬í•  ë°ì´í„°ë¥¼ {max_items}ê°œë¡œ ì œí•œ")
        
        logger.info(f"ğŸ“‹ ì´ {len(training_data)}ê°œ ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        
        training_results = []
        
        # ê° ë°ì´í„° ì•„ì´í…œ ìˆœì°¨ ì²˜ë¦¬
        for i, data_item in enumerate(training_data, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ¯ [{i}/{len(training_data)}] ì‹¤ì œ í›ˆë ¨ ì¤‘...")
            logger.info(f"ë°ì´í„° ID: {data_item['data_id']}")
            logger.info(f"ì†ŒìŠ¤ íŒŒì¼: {data_item['source_file']}")
            logger.info(f"{'='*60}")
            
            result = await self.process_real_training_item(data_item)
            training_results.append(result)
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            if i % 10 == 0 or i == len(training_data):
                success_rate = self.training_metrics['successful_integrations'] / self.training_metrics['total_processed'] * 100
                avg_time = self.training_metrics['total_processing_time'] / self.training_metrics['total_processed']
                logger.info(f"\nğŸ“Š ì¤‘ê°„ ì§„í–‰ ìƒí™© [{i}/{len(training_data)}]:")
                logger.info(f"  - ì„±ê³µë¥ : {success_rate:.1f}%")
                logger.info(f"  - í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_time:.3f}ì´ˆ")
                logger.info(f"  - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {avg_time * (len(training_data) - i):.1f}ì´ˆ")
        
        # ìµœì¢… ë¶„ì„
        return self._analyze_real_training_results(training_results)
    
    def _analyze_real_training_results(self, results: List[RealTrainingResult]) -> Dict[str, Any]:
        """ì‹¤ì œ í›ˆë ¨ ê²°ê³¼ ì¢…í•© ë¶„ì„"""
        logger.info(f"\nğŸ“Š ì‹¤ì œ í›ˆë ¨ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        if not results:
            return {"error": "ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ì „ì²´ ë©”íŠ¸ë¦­
        total_items = len(results)
        successful_items = len([r for r in results if r.integration_success])
        success_rate = successful_items / total_items * 100
        
        total_time = sum(r.processing_time for r in results)
        avg_time = total_time / total_items
        
        # ëª¨ë“ˆë³„ ì„±ëŠ¥ ë¶„ì„ - AdvancedRegretMetrics í¬í•¨ ì•ˆì „í•œ íƒ€ì… ì²´í¬
        def safe_error_check(obj):
            if hasattr(obj, '__class__') and obj.__class__.__name__ == 'AdvancedRegretMetrics':
                return True  # AdvancedRegretMetrics ê°ì²´ëŠ” ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            elif hasattr(obj, '__dict__'):
                return 'error' not in obj.__dict__
            elif isinstance(obj, dict):
                return 'error' not in obj and 'disabled' not in obj
            else:
                return True  # ê¸°íƒ€ ê°ì²´ëŠ” ì„±ê³µìœ¼ë¡œ ê°„ì£¼
        
        module_performance = {
            'emotion_success': len([r for r in results if safe_error_check(r.emotion_analysis)]),
            'bentham_success': len([r for r in results if safe_error_check(r.bentham_calculation)]),
            'regret_success': len([r for r in results if safe_error_check(r.regret_analysis) and not (isinstance(r.regret_analysis, dict) and 'disabled' in r.regret_analysis)]),
            'surd_success': len([r for r in results if safe_error_check(r.surd_analysis)])
        }
        
        # ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
        error_patterns = {}
        for result in results:
            for error in result.error_log:
                error_type = error.split(':')[0]
                error_patterns[error_type] = error_patterns.get(error_type, 0) + 1
        
        # ì²˜ë¦¬ ì‹œê°„ ë¶„ì„
        processing_times = [r.processing_time for r in results]
        min_time = min(processing_times)
        max_time = max(processing_times)
        
        # ë°˜ì‚¬ì‹¤ ìƒì„± í†µê³„
        total_counterfactuals = sum(len(r.counterfactual_experiences) for r in results)
        
        # ë°ì´í„° ì†ŒìŠ¤ë³„ ì„±ëŠ¥
        source_performance = {}
        for result in results:
            source = result.source_file
            if source not in source_performance:
                source_performance[source] = {'total': 0, 'success': 0}
            source_performance[source]['total'] += 1
            if result.integration_success:
                source_performance[source]['success'] += 1
        
        # ìµœì¢… ê²°ê³¼
        analysis_result = {
            'training_summary': {
                'total_items': total_items,
                'successful_integrations': successful_items,
                'success_rate': success_rate,
                'total_processing_time': total_time,
                'avg_processing_time': avg_time,
                'min_processing_time': min_time,
                'max_processing_time': max_time
            },
            'module_performance': {
                'emotion_success_rate': (module_performance['emotion_success'] / total_items) * 100,
                'bentham_success_rate': (module_performance['bentham_success'] / total_items) * 100,
                'regret_success_rate': (module_performance['regret_success'] / total_items) * 100,
                'surd_success_rate': (module_performance['surd_success'] / total_items) * 100
            },
            'integration_analysis': {
                'full_integration_rate': success_rate,
                'partial_integration_rate': ((total_items - successful_items) / total_items) * 100,
                'total_counterfactuals_generated': total_counterfactuals,
                'avg_counterfactuals_per_item': total_counterfactuals / total_items
            },
            'error_analysis': {
                'error_patterns': error_patterns,
                'total_errors': sum(error_patterns.values()),
                'error_rate': (sum(error_patterns.values()) / total_items) * 100
            },
            'source_analysis': source_performance,
            'performance_metrics': {
                'items_per_second': total_items / total_time,
                'successful_items_per_second': successful_items / total_time,
                'efficiency_score': success_rate * (total_items / total_time)
            }
        }
        
        return analysis_result

    async def run_complete_learning_system(self) -> Dict[str, Any]:
        """ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰"""
        
        if not self.learning_mode or not self.learning_executor:
            logger.error("âŒ í•™ìŠµ ëª¨ë“œê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {"error": "í•™ìŠµ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"}
        
        logger.info("ğŸ¯ ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹œì‘")
        logger.info("ğŸ“Š 3ë‹¨ê³„ í†µí•© í˜ì´ì¦ˆ ì‹œìŠ¤í…œ:")
        logger.info("   Phase 0: ìì‹  ê°ì • ìº˜ë¦¬ë¸Œë ˆì´ì…˜")
        logger.info("   Phase 1: íƒ€ì¸ ê³µê° í•™ìŠµ")
        logger.info("   Phase 2: ê³µë™ì²´ ì´í•´")
        
        start_time = time.time()
        
        try:
            # 1. ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰
            logger.info("ğŸš€ ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘...")
            learning_results = await self.learning_executor.execute_full_learning()
            
            # 2. í•™ìŠµ ê²°ê³¼ ë¶„ì„
            logger.info("ğŸ“Š í•™ìŠµ ê²°ê³¼ ë¶„ì„ ì¤‘...")
            analysis_results = await self._analyze_learning_results(learning_results)
            
            # 3. í•™ìŠµëœ ì‹œìŠ¤í…œìœ¼ë¡œ ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸
            logger.info("ğŸ¯ í•™ìŠµëœ ì‹œìŠ¤í…œ ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸ ì¤‘...")
            decision_results = await self._test_learned_decision_making()
            
            # 4. ë™ì  ìœ¤ë¦¬ì  ë¶„ì„ í…ŒìŠ¤íŠ¸
            logger.info("ğŸ” ë™ì  ìœ¤ë¦¬ì  ë¶„ì„ í…ŒìŠ¤íŠ¸ ì¤‘...")
            ethical_analysis_results = await self._test_dynamic_ethical_analysis()
            
            total_time = time.time() - start_time
            
            return {
                "learning_success": True,
                "total_learning_time": total_time,
                "learning_results": learning_results,
                "integrated_analysis": analysis_results,
                "decision_test_results": decision_results,
                "ethical_analysis_results": ethical_analysis_results,
                "summary": {
                    "total_learning_time": total_time,
                    "learning_quality": analysis_results.get("learning_quality", {}),
                    "decision_accuracy": decision_results.get("confidence_score", 0.0),
                    "ethical_analysis_quality": ethical_analysis_results.get("analysis_quality", 0.0)
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "learning_success": False}

    async def _analyze_learning_results(self, learning_results: Dict[str, Any]) -> Dict[str, Any]:
        """í•™ìŠµ ê²°ê³¼ ë¶„ì„"""
        
        analysis = {
            "phase_analysis": {},
            "module_performance": {},
            "learning_quality": {}
        }
        
        # í•™ìŠµ í†µê³„ ë¶„ì„
        if "learning_statistics" in learning_results:
            stats = learning_results["learning_statistics"]
            
            # í˜ì´ì¦ˆë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            if "performance_metrics" in stats:
                recent_metrics = stats["performance_metrics"][-10:]  # ìµœê·¼ 10ê°œ
                if recent_metrics:
                    analysis["phase_analysis"]["recent_regret_avg"] = np.mean([m["avg_regret_intensity"] for m in recent_metrics])
                    analysis["phase_analysis"]["recent_hedonic_avg"] = np.mean([m["avg_hedonic_score"] for m in recent_metrics])
            
            # ëª¨ë“ˆ ì„±ëŠ¥
            if "regret_history" in stats:
                regret_data = stats["regret_history"][-50:]  # ìµœê·¼ 50ê°œ
                if regret_data:
                    analysis["module_performance"]["regret_system"] = {
                        "avg_intensity": np.mean([r["intensity"] for r in regret_data]),
                        "total_processed": len(regret_data)
                    }
            
            if "bentham_scores" in stats:
                bentham_data = stats["bentham_scores"][-50:]  # ìµœê·¼ 50ê°œ
                if bentham_data:
                    analysis["module_performance"]["bentham_system"] = {
                        "avg_score": np.mean([b["hedonic_score"] for b in bentham_data]),
                        "total_processed": len(bentham_data)
                    }
        
        # í•™ìŠµ í’ˆì§ˆ í‰ê°€
        if "summary" in learning_results:
            summary = learning_results["summary"]
            analysis["learning_quality"]["scenarios_processed"] = summary.get("total_scenarios_processed", 0)
            analysis["learning_quality"]["total_regrets"] = summary.get("total_regrets", 0)
            analysis["learning_quality"]["total_bentham_calculations"] = summary.get("total_bentham_calculations", 0)
            analysis["learning_quality"]["efficiency"] = summary.get("total_regrets", 0) / max(summary.get("total_scenarios_processed", 1), 1)
        
        return analysis

    async def _test_learned_decision_making(self) -> Dict[str, Any]:
        """í•™ìŠµëœ ì‹œìŠ¤í…œìœ¼ë¡œ ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸"""
        
        # ê°„ë‹¨í•œ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ í…ŒìŠ¤íŠ¸
        test_scenario = {
            "title": "ììœ¨ì£¼í–‰ì°¨ ë”œë ˆë§ˆ",
            "description": "ììœ¨ì£¼í–‰ì°¨ê°€ ê¸‰ë¸Œë ˆì´í¬ë¥¼ ë°Ÿì•„ì•¼ í•˜ëŠ” ìƒí™©ì—ì„œ ë³´í–‰ì 1ëª…ì„ êµ¬í•  ê²ƒì¸ê°€, ì•„ë‹ˆë©´ ì°¨ ì•ˆì˜ íƒ‘ìŠ¹ì 2ëª…ì„ êµ¬í•  ê²ƒì¸ê°€?",
            "context": {"urgency": "high", "stakeholders": ["ë³´í–‰ì", "íƒ‘ìŠ¹ìë“¤"]}
        }
        
        try:
            start_time = time.time()
            
            # ê°ì • ë¶„ì„
            emotion_result = await self.emotion_analyzer.analyze_comprehensive(test_scenario["description"])
            
            # ë²¤ë‹´ ê³„ì‚°
            bentham_result = await self.bentham_calculator.calculate_with_advanced_layers(test_scenario)
            
            # í›„íšŒ ë¶„ì„
            regret_result = await self.regret_analyzer.analyze_regret({
                "text": test_scenario["description"],
                "context": test_scenario["context"]
            })
            
            processing_time = time.time() - start_time
            
            # ìµœì¢… ê²°ì • (ê°„ë‹¨í•œ ë¡œì§)
            decision_scores = {
                "ë³´í–‰ì êµ¬í•˜ê¸°": 0.3,
                "íƒ‘ìŠ¹ì êµ¬í•˜ê¸°": 0.7
            }
            
            final_decision = max(decision_scores, key=decision_scores.get)
            confidence = max(decision_scores.values())
            
            return {
                "test_success": True,
                "final_recommendation": final_decision,
                "confidence_score": confidence,
                "processing_time": processing_time,
                "reasoning_chain": [
                    f"ê°ì • ë¶„ì„: {getattr(emotion_result, 'dominant_emotion', 'N/A')}",
                    f"ë²¤ë‹´ ì ìˆ˜: {getattr(bentham_result, 'final_score', 0.0):.3f}",
                    f"í›„íšŒ ê°•ë„: {getattr(regret_result, 'regret_intensity', 0.0):.3f}",
                    f"ìµœì¢… ê²°ì •: {final_decision}"
                ]
            }
            
        except Exception as e:
            logger.error(f"ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"test_success": False, "error": str(e)}

    async def _test_dynamic_ethical_analysis(self) -> Dict[str, Any]:
        """ë™ì  ìœ¤ë¦¬ì  ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        
        if not self.dynamic_choice_analyzer:
            return {"error": "ë™ì  ì„ íƒì§€ ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"}
        
        try:
            # ë‹¤ì–‘í•œ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ í…ŒìŠ¤íŠ¸
            test_dilemma = "ì˜ì‚¬ê°€ ì¥ê¸° ì´ì‹ì„ ìœ„í•´ ê±´ê°•í•œ í™˜ì 1ëª…ì„ í¬ìƒì‹œì¼œ 5ëª…ì˜ í™˜ìë¥¼ ì‚´ë¦´ ê²ƒì¸ê°€?"
            
            start_time = time.time()
            result = await self.dynamic_choice_analyzer.analyze_ethical_dilemma(
                dilemma_text=test_dilemma,
                title="ì˜ë£Œ ìœ¤ë¦¬ ë”œë ˆë§ˆ í…ŒìŠ¤íŠ¸"
            )
            processing_time = time.time() - start_time
            
            return {
                "analysis_success": True,
                "dilemma_type": result.dilemma_type.value,
                "extracted_choices": len(result.extracted_choices),
                "stakeholders_identified": len(result.stakeholders),
                "recommended_choice": result.recommended_choice.name if result.recommended_choice else None,
                "reasoning_chain": result.reasoning_chain,
                "processing_time": processing_time,
                "analysis_quality": len(result.reasoning_chain) / 5.0  # ê°„ë‹¨í•œ í’ˆì§ˆ ì§€í‘œ
            }
            
        except Exception as e:
            logger.error(f"ë™ì  ìœ¤ë¦¬ì  ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"analysis_success": False, "error": str(e)}


async def main():
    """ì‹¤ì œ í›ˆë ¨ ë©”ì¸ í•¨ìˆ˜"""
    if not MODULES_AVAILABLE:
        logger.error("âŒ í•„ìˆ˜ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    training_system = RealIntegratedTrainingSystem()
    
    # ì‹¤ì œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if not await training_system.initialize_real_system():
        logger.error("âŒ ì‹¤ì œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹¤í–‰ (ì²˜ìŒ 25ê°œ ì•„ì´í…œìœ¼ë¡œ í…ŒìŠ¤íŠ¸)
    results = await training_system.run_real_integrated_training(max_items=25)
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info(f"\n{'='*80}")
    logger.info("ğŸ‰ ì‹¤ì œ Red Heart AI í†µí•© í›ˆë ¨ ì™„ë£Œ")
    logger.info(f"{'='*80}")
    
    if 'error' not in results:
        summary = results['training_summary']
        module_perf = results['module_performance']
        integration = results['integration_analysis']
        
        logger.info(f"\nğŸ“Š ì‹¤ì œ í›ˆë ¨ ìš”ì•½:")
        logger.info(f"  - ì´ ì²˜ë¦¬ ì•„ì´í…œ: {summary['total_items']}ê°œ")
        logger.info(f"  - ì„±ê³µì  í†µí•©: {summary['successful_integrations']}ê°œ")
        logger.info(f"  - í†µí•© ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        logger.info(f"  - ì´ ì²˜ë¦¬ì‹œê°„: {summary['total_processing_time']:.1f}ì´ˆ")
        logger.info(f"  - í‰ê·  ì²˜ë¦¬ì‹œê°„: {summary['avg_processing_time']:.3f}ì´ˆ")
        
        logger.info(f"\nğŸ¯ ëª¨ë“ˆë³„ ì„±ê³µë¥ :")
        logger.info(f"  - ê°ì • ë¶„ì„: {module_perf['emotion_success_rate']:.1f}%")
        logger.info(f"  - ë²¤ë‹´ ê³„ì‚°: {module_perf['bentham_success_rate']:.1f}%")
        logger.info(f"  - í›„íšŒ ë¶„ì„: {module_perf['regret_success_rate']:.1f}%")
        logger.info(f"  - SURD ë¶„ì„: {module_perf['surd_success_rate']:.1f}%")
        
        logger.info(f"\nğŸ”— í†µí•© ë¶„ì„:")
        logger.info(f"  - ì™„ì „ í†µí•©ë¥ : {integration['full_integration_rate']:.1f}%")
        logger.info(f"  - ë°˜ì‚¬ì‹¤ ì‹œë‚˜ë¦¬ì˜¤: {integration['total_counterfactuals_generated']}ê°œ")
        logger.info(f"  - ì•„ì´í…œë‹¹ í‰ê· : {integration['avg_counterfactuals_per_item']:.1f}ê°œ")
        
        logger.info(f"\nâš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        perf = results['performance_metrics']
        logger.info(f"  - ì²˜ë¦¬ ì†ë„: {perf['items_per_second']:.2f} ì•„ì´í…œ/ì´ˆ")
        logger.info(f"  - ì„±ê³µ ì²˜ë¦¬ ì†ë„: {perf['successful_items_per_second']:.2f} ì•„ì´í…œ/ì´ˆ")
        logger.info(f"  - íš¨ìœ¨ì„± ì ìˆ˜: {perf['efficiency_score']:.2f}")
        
        if results['error_analysis']['error_patterns']:
            logger.info(f"\nâš ï¸ ì˜¤ë¥˜ íŒ¨í„´:")
            for error_type, count in results['error_analysis']['error_patterns'].items():
                logger.info(f"  - {error_type}: {count}íšŒ")
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = Path(f'real_integrated_training_results_{timestamp}.json')
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"\nğŸ“„ ìƒì„¸ ê²°ê³¼ ì €ì¥: {result_file}")
    else:
        logger.error(f"âŒ ì‹¤ì œ í›ˆë ¨ ì‹¤íŒ¨: {results['error']}")


async def main():
    """ì‹¤ì œ í›ˆë ¨ ë©”ì¸ í•¨ìˆ˜"""
    if not MODULES_AVAILABLE:
        logger.error("âŒ í•„ìˆ˜ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ ì‹¤í–‰
    system = RealIntegratedTrainingSystem()
    await system.initialize_real_system()
    
    # ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = await system.run_real_integrated_training(max_items=3)
    
    logger.info("ğŸ¯ Red Heart AI ì‹¤ì œ í†µí•© í›ˆë ¨ ì™„ë£Œ")
    return results


async def main_learning_system():
    """í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""
    
    if not MODULES_AVAILABLE:
        logger.error("âŒ í•„ìˆ˜ ëª¨ë“ˆì´ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if not LEARNING_SYSTEM_AVAILABLE:
        logger.error("âŒ í•™ìŠµ ì‹œìŠ¤í…œì´ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = RealIntegratedTrainingSystem()
    success = await system.initialize_real_system(learning_mode=True)
    
    if not success:
        logger.error("âŒ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰
    results = await system.run_complete_learning_system()
    
    # ê²°ê³¼ ì¶œë ¥
    if results.get("learning_success"):
        logger.info("âœ… ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì„±ê³µ!")
        logger.info(f"ì´ í•™ìŠµ ì‹œê°„: {results['total_learning_time']:.2f}ì´ˆ")
        
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = Path(f'complete_learning_system_results_{timestamp}.json')
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“„ í•™ìŠµ ê²°ê³¼ ì €ì¥: {result_file}")
    else:
        logger.error(f"âŒ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {results.get('error', 'Unknown error')}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Red Heart AI ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ')
    parser.add_argument('--learning', action='store_true', help='ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰')
    parser.add_argument('--test', action='store_true', help='ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    if args.learning:
        asyncio.run(main_learning_system())
    elif args.test:
        asyncio.run(main())
    else:
        # ê¸°ë³¸ê°’: ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        asyncio.run(main())
                phase_key = f"phase_{phase}"
                if phase_key in learning_results:
                    analysis["phase_analysis"][phase_key] = {
                        "scenarios_processed": learning_results[phase_key].get("scenarios_processed", 0),
                        "learning_iterations": learning_results[phase_key].get("learning_iterations", 0),
                        "phase_completion_rate": learning_results[phase_key].get("completion_rate", 0.0)
                    }
            
            # ëª¨ë“ˆ ì„±ëŠ¥ ë¶„ì„
            if "module_performance" in learning_results:
                for module, performance in learning_results["module_performance"].items():
                    analysis["module_performance"][module] = {
                        "accuracy_improvement": performance.get("accuracy_improvement", 0.0),
                        "processing_time_avg": performance.get("processing_time_avg", 0.0),
                        "confidence_score": performance.get("confidence_score", 0.0)
                    }
            
            # í•™ìŠµ í’ˆì§ˆ ë¶„ì„
            analysis["learning_quality"] = {
                "overall_improvement": learning_results.get("overall_improvement", 0.0),
                "convergence_rate": learning_results.get("convergence_rate", 0.0),
                "stability_score": learning_results.get("stability_score", 0.0)
            }
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ê²°ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
            analysis["integration_success"] = False
            analysis["error"] = str(e)
        
        return analysis
    
    async def _test_learned_decision_making(self) -> Dict[str, Any]:
        """í•™ìŠµëœ ì˜ì‚¬ê²°ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        
        if not self.integrated_orchestrator:
            logger.warning("í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return {"error": "í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì—†ìŒ"}
        
        # ììœ¨ì£¼í–‰ì°¨ ë”œë ˆë§ˆ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        test_scenario = IntegrationContext(
            session_id="learning_test_001",
            user_input="ììœ¨ì£¼í–‰ì°¨ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ í•™ìŠµ í…ŒìŠ¤íŠ¸",
            scenario_description="ììœ¨ì£¼í–‰ì°¨ê°€ ë¸Œë ˆì´í¬ ê³ ì¥ìœ¼ë¡œ ì¸í•´ ë¶ˆê°€í”¼í•œ ì¶©ëŒ ìƒí™©ì—ì„œ ì–´ë–¤ ì„ íƒì„ í•´ì•¼ í•˜ëŠ”ê°€?",
            urgency_level=0.95,
            complexity_level=0.9,
            stakeholder_count=5,
            ethical_weight=0.9,
            cultural_context="korean"
        )
        
        try:
            # í†µí•© ì‹œìŠ¤í…œìœ¼ë¡œ ì˜ì‚¬ê²°ì • ìˆ˜í–‰
            decision_result = await self.integrated_orchestrator.process_decision_request(test_scenario)
            
            # ê²°ê³¼ ë¶„ì„
            test_results = {
                "decision_id": decision_result.decision_id,
                "final_recommendation": decision_result.final_recommendation,
                "confidence_score": decision_result.confidence_score,
                "module_contributions": decision_result.module_contributions,
                "reasoning_chain": decision_result.reasoning_chain,
                "alternative_options": decision_result.alternative_options,
                "processing_time": decision_result.processing_time,
                "test_success": True
            }
            
            logger.info(f"âœ… í•™ìŠµëœ ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            logger.info(f"   ìµœì¢… ì¶”ì²œ: {decision_result.final_recommendation}")
            logger.info(f"   ì‹ ë¢°ë„: {decision_result.confidence_score:.3f}")
            
            return test_results
            
        except Exception as e:
            logger.error(f"ì˜ì‚¬ê²°ì • í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {
                "error": str(e),
                "test_success": False
            }
    
    async def _test_dynamic_ethical_analysis(self) -> Dict[str, Any]:
        """ë™ì  ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        
        if not self.dynamic_choice_analyzer:
            logger.warning("ë™ì  ìœ¤ë¦¬ì  ì„ íƒì§€ ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return {"error": "ë™ì  ë¶„ì„ê¸° ì—†ìŒ"}
        
        # ë‹¤ì–‘í•œ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ í…ŒìŠ¤íŠ¸
        test_scenarios = [
            {
                "title": "ììœ¨ì£¼í–‰ì°¨ ìœ¤ë¦¬ì  ë”œë ˆë§ˆ",
                "description": "ììœ¨ì£¼í–‰ì°¨ê°€ ë¸Œë ˆì´í¬ ê³ ì¥ìœ¼ë¡œ ì¸í•´ ë¶ˆê°€í”¼í•œ ì¶©ëŒ ìƒí™©ì—ì„œ ì–´ë–¤ ì„ íƒì„ í•´ì•¼ í•˜ëŠ”ê°€? ê¸‰ë¸Œë ˆì´í¬ë¥¼ ë°Ÿì•„ ë’¤ì°¨ ì¶”ëŒì„ ìœ ë°œí•  ê²ƒì¸ê°€, ì•„ë‹ˆë©´ í•¸ë“¤ì„ í‹€ì–´ ë²½ìœ¼ë¡œ í–¥í•  ê²ƒì¸ê°€, ë˜ëŠ” ì§ì§„í•˜ì—¬ ë³´í–‰ìì™€ ì¶©ëŒí•  ê²ƒì¸ê°€?",
            },
            {
                "title": "ì˜ë£Œì§„ ìì› ë°°ë¶„ ë”œë ˆë§ˆ",
                "description": "ì½”ë¡œë‚˜19 ìƒí™©ì—ì„œ ì¸ê³µí˜¸í¡ê¸° 1ëŒ€ë¥¼ ë‘ê³  90ì„¸ ì¤‘ì¦í™˜ìì™€ 30ì„¸ ì¤‘ì¦í™˜ì ì¤‘ ëˆ„êµ¬ë¥¼ ì„ íƒí•  ê²ƒì¸ê°€? ë‚˜ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•  ê²ƒì¸ê°€, ì•„ë‹ˆë©´ ì„ ì°©ìˆœìœ¼ë¡œ í•  ê²ƒì¸ê°€, ë˜ëŠ” ë‹¤ë¥¸ ì˜í•™ì  ê¸°ì¤€ì„ ì ìš©í•  ê²ƒì¸ê°€?",
            },
            {
                "title": "ê°œì¸ì •ë³´ vs ê³µê³µì•ˆì „",
                "description": "í…ŒëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ì‹œë¯¼ë“¤ì˜ ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ê°ì‹œí•  ê²ƒì¸ê°€, ì•„ë‹ˆë©´ ê°œì¸ì˜ í”„ë¼ì´ë²„ì‹œë¥¼ ë³´í˜¸í•  ê²ƒì¸ê°€? ì „ë©´ì  ê°ì‹œë¥¼ í•  ê²ƒì¸ê°€, ì•„ë‹ˆë©´ ì„ íƒì  ê°ì‹œë¥¼ í•  ê²ƒì¸ê°€, ë˜ëŠ” ê°ì‹œë¥¼ í•˜ì§€ ì•Šì„ ê²ƒì¸ê°€?",
            }
        ]
        
        test_results = {
            "tested_scenarios": len(test_scenarios),
            "successful_analyses": 0,
            "scenario_results": [],
            "overall_performance": {},
            "test_success": True
        }
        
        try:
            for i, scenario in enumerate(test_scenarios):
                logger.info(f"   ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ {i+1}/{len(test_scenarios)}: {scenario['title']}")
                
                try:
                    # ë™ì  ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„
                    start_time = time.time()
                    analysis_result = await self.dynamic_choice_analyzer.analyze_ethical_dilemma(
                        dilemma_text=scenario['description'],
                        title=scenario['title']
                    )
                    analysis_time = time.time() - start_time
                    
                    # ê²°ê³¼ ì •ë¦¬
                    scenario_result = {
                        "scenario_id": f"scenario_{i+1}",
                        "title": scenario['title'],
                        "dilemma_type": analysis_result.dilemma_type.value,
                        "extracted_choices": len(analysis_result.extracted_choices),
                        "stakeholders": len(analysis_result.stakeholders),
                        "recommended_choice": analysis_result.recommended_choice.name if analysis_result.recommended_choice else None,
                        "analysis_time": analysis_time,
                        "success": True
                    }
                    
                    # ì„ íƒì§€ë³„ ìƒì„¸ ê²°ê³¼
                    choice_details = []
                    for choice_id, choice_analysis in analysis_result.choice_analyses.items():
                        choice_details.append({
                            "choice_name": choice_analysis.choice.name,
                            "utility_score": choice_analysis.utility_score,
                            "confidence_score": choice_analysis.confidence_score,
                            "risk_adjusted_score": choice_analysis.risk_adjusted_score,
                            "processing_time": choice_analysis.processing_time
                        })
                    
                    scenario_result["choice_analyses"] = choice_details
                    scenario_result["reasoning_chain"] = analysis_result.reasoning_chain
                    
                    test_results["scenario_results"].append(scenario_result)
                    test_results["successful_analyses"] += 1
                    
                    logger.info(f"      âœ… ë¶„ì„ ì™„ë£Œ ({analysis_time:.2f}ì´ˆ)")
                    logger.info(f"      ì„ íƒì§€: {len(analysis_result.extracted_choices)}ê°œ")
                    logger.info(f"      ì´í•´ê´€ê³„ì: {len(analysis_result.stakeholders)}ëª…")
                    if analysis_result.recommended_choice:
                        logger.info(f"      ì¶”ì²œ: {analysis_result.recommended_choice.name}")
                    
                except Exception as e:
                    logger.error(f"      âŒ ì‹œë‚˜ë¦¬ì˜¤ {i+1} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    scenario_result = {
                        "scenario_id": f"scenario_{i+1}",
                        "title": scenario['title'],
                        "success": False,
                        "error": str(e)
                    }
                    test_results["scenario_results"].append(scenario_result)
            
            # ì „ì²´ ì„±ëŠ¥ ë¶„ì„
            if test_results["successful_analyses"] > 0:
                all_choice_analyses = []
                total_analysis_time = 0
                
                for result in test_results["scenario_results"]:
                    if result.get("success"):
                        total_analysis_time += result.get("analysis_time", 0)
                        if "choice_analyses" in result:
                            all_choice_analyses.extend(result["choice_analyses"])
                
                test_results["overall_performance"] = {
                    "success_rate": test_results["successful_analyses"] / test_results["tested_scenarios"],
                    "avg_analysis_time": total_analysis_time / test_results["successful_analyses"],
                    "total_choices_analyzed": len(all_choice_analyses),
                    "avg_utility_score": sum(c["utility_score"] for c in all_choice_analyses) / len(all_choice_analyses) if all_choice_analyses else 0,
                    "avg_confidence_score": sum(c["confidence_score"] for c in all_choice_analyses) / len(all_choice_analyses) if all_choice_analyses else 0
                }
                
                logger.info(f"âœ… ë™ì  ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                logger.info(f"   ì„±ê³µë¥ : {test_results['overall_performance']['success_rate']:.1%}")
                logger.info(f"   í‰ê·  ë¶„ì„ ì‹œê°„: {test_results['overall_performance']['avg_analysis_time']:.2f}ì´ˆ")
                logger.info(f"   ë¶„ì„ëœ ì„ íƒì§€: {test_results['overall_performance']['total_choices_analyzed']}ê°œ")
                logger.info(f"   í‰ê·  ìœ í‹¸ë¦¬í‹° ì ìˆ˜: {test_results['overall_performance']['avg_utility_score']:.3f}")
                logger.info(f"   í‰ê·  ì‹ ë¢°ë„: {test_results['overall_performance']['avg_confidence_score']:.3f}")
            
            return test_results
            
        except Exception as e:
            logger.error(f"ë™ì  ìœ¤ë¦¬ì  ë”œë ˆë§ˆ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            test_results["test_success"] = False
            test_results["error"] = str(e)
            return test_results

async def main_learning_system():
    """í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""
    
    if not MODULES_AVAILABLE:
        logger.error("âŒ í•„ìˆ˜ ëª¨ë“ˆì´ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if not LEARNING_SYSTEM_AVAILABLE:
        logger.error("âŒ í•™ìŠµ ì‹œìŠ¤í…œì´ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = RealIntegratedTrainingSystem()
    success = await system.initialize_real_system(learning_mode=True)
    
    if not success:
        logger.error("âŒ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰
    results = await system.run_complete_learning_system()
    
    # ê²°ê³¼ ì¶œë ¥
    if results.get("learning_success"):
        logger.info("âœ… ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì„±ê³µ!")
        logger.info(f"ì´ í•™ìŠµ ì‹œê°„: {results['total_learning_time']:.2f}ì´ˆ")
        
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = Path(f'complete_learning_system_results_{timestamp}.json')
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“„ í•™ìŠµ ê²°ê³¼ ì €ì¥: {result_file}")
    else:
        logger.error(f"âŒ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {results.get('error', 'Unknown error')}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Red Heart AI ì‹¤ì œ í†µí•© í›ˆë ¨ ì‹œìŠ¤í…œ')
    parser.add_argument('--learning', action='store_true', help='ì™„ì „í•œ í•™ìŠµ ì‹œìŠ¤í…œ ì‹¤í–‰')
    parser.add_argument('--test', action='store_true', help='ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    if args.learning:
        asyncio.run(main_learning_system())
    elif args.test:
        asyncio.run(main())
    else:
        # ê¸°ë³¸ê°’: ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        asyncio.run(main())