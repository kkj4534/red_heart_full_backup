"""
Red Heart Linux Advanced - ì„¤ì • íŒŒì¼
ê³ ê¸‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ Linux ìµœì í™” ì„¤ì •
"""

import os
import platform
import time
import asyncio

import logging
import datetime

# ì „ì—­ logger ì„¤ì •
logger = logging.getLogger('RedHeart.Config')

# dotenv ì˜ì¡´ì„±ì„ ì„ íƒì ìœ¼ë¡œ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("âš ï¸  python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.")
    def load_dotenv():
        pass


# ê¸°ë³¸ ê²½ë¡œ ì„¤ì • - pathlib ì œê±°í•˜ì—¬ WSL hanging ë°©ì§€
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, 'data')
MODELS_DIR = os.path.join(BASE_DIR, 'models')
LOGS_DIR = os.path.join(BASE_DIR, 'logs')
PLOTS_DIR = os.path.join(BASE_DIR, 'plots')
DOCS_DIR = os.path.join(BASE_DIR, 'docs')
TESTS_DIR = os.path.join(BASE_DIR, 'tests')

# ë°ì´í„° í•˜ìœ„ ë””ë ‰í† ë¦¬
RAW_DATA_DIR = os.path.join(DATA_DIR, 'raw')
PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')
EXPERIENCE_DB_DIR = os.path.join(DATA_DIR, 'experience_db')
DECISION_LOGS_DIR = os.path.join(DATA_DIR, 'decision_logs')

# ëª¨ë¸ í•˜ìœ„ ë””ë ‰í† ë¦¬
EMOTION_MODELS_DIR = os.path.join(MODELS_DIR, 'emotion_models')
SEMANTIC_MODELS_DIR = os.path.join(MODELS_DIR, 'semantic_models')
SURD_CACHE_DIR = os.path.join(MODELS_DIR, 'surd_cache')
REGRET_MODELS_DIR = os.path.join(MODELS_DIR, 'regret_models')
HIERARCHICAL_EMOTION_DIR = os.path.join(MODELS_DIR, 'hierarchical_emotion')
SEMANTIC_CACHE_DIR = os.path.join(MODELS_DIR, 'semantic_cache')

# ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
CACHE_DIR = os.path.join(BASE_DIR, 'cache')

# í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬
LEARN_DATASET_DIR = os.path.join(BASE_DIR, 'for_learn_dataset')
PROCESSED_DATASETS_DIR = os.path.join(BASE_DIR, 'processed_datasets')

# ë””ë ‰í† ë¦¬ ìƒì„±ì„ ì§€ì—° í•¨ìˆ˜ë¡œ ë˜í•‘ (WSL /mnt hanging ë°©ì§€)
def _create_directories_if_needed():
    """í•„ìš”ì‹œì—ë§Œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„± (WSL hanging ë°©ì§€)"""
    directories = [DATA_DIR, MODELS_DIR, LOGS_DIR, PLOTS_DIR, DOCS_DIR, TESTS_DIR,
                   RAW_DATA_DIR, PROCESSED_DATA_DIR, EXPERIENCE_DB_DIR, DECISION_LOGS_DIR,
                   EMOTION_MODELS_DIR, SEMANTIC_MODELS_DIR, SURD_CACHE_DIR, REGRET_MODELS_DIR,
                   HIERARCHICAL_EMOTION_DIR, SEMANTIC_CACHE_DIR, CACHE_DIR,
                   LEARN_DATASET_DIR, PROCESSED_DATASETS_DIR]
    
    for directory in directories:
        try:
            os.makedirs(directory, exist_ok=True)
        except Exception as e:
            # WSL /mnt ê²½ë¡œì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” hanging ë¬¸ì œ ë¬´ì‹œ
            logger.warning(f"ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨ (ë¬´ì‹œ): {directory} - {e}")
            pass

# ë””ë ‰í† ë¦¬ëŠ” ì‹¤ì œ ì‚¬ìš©ì‹œì—ë§Œ ìƒì„± (ëª¨ë“ˆ importì‹œ hanging ë°©ì§€)

# ì‹œìŠ¤í…œ ì •ë³´
SYSTEM_INFO = {
    'platform': platform.system(),
    'architecture': platform.architecture(),
    'python_version': platform.python_version(),
    'is_linux': platform.system().lower() == 'linux',
    'cpu_count': os.cpu_count(),
}

# ê³ ê¸‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œì„±í™” ì„¤ì • (800M íŒŒë¼ë¯¸í„° í†µí•© ì•„í‚¤í…ì²˜)
ADVANCED_CONFIG = {
    'use_transformers': True,           # Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
    'use_sentence_transformers': True,  # Sentence Transformers ì‚¬ìš©
    'use_torch': True,                  # PyTorch ì‚¬ìš©
    'use_advanced_nlp': True,           # ê³ ê¸‰ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
    'enable_gpu': True,                 # GPU ì‚¬ìš© (ìë™ ê°ì§€)
    'fallback_mode': False,             # í´ë°± ëª¨ë“œ ì™„ì „ ë¹„í™œì„±í™”
    'strict_mode': True,                # ì—„ê²© ëª¨ë“œ (ê³ ê¸‰ ëª¨ë“ˆ í•„ìˆ˜)
    'korean_advanced': True,            # ê³ ê¸‰ í•œêµ­ì–´ ì²˜ë¦¬ í™œì„±í™”
    'use_multiprocessing': True,        # ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©
    'total_parameters': 450_000_000,    # ì´ 450M íŒŒë¼ë¯¸í„° (68M ë°±ë³¸ + 109M í—¤ë“œ + 232M ë¶„ì„ê¸° + 41M ë³´ì¡°)
    'optimization_target': 'unified_synergy',  # í†µí•© ì‹œë„ˆì§€ì— ì§‘ì¤‘
    'gpu_memory_fraction': 0.85,        # 85%ë¡œ ë³µêµ¬ - ì„ê³„ê°’ê³¼ ë³„ê°œ
    'precision': 'fp16',                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ë°˜ì •ë°€ë„
    'enable_mixed_precision': True,     # í˜¼í•© ì •ë°€ë„ë¡œ ì„±ëŠ¥ í–¥ìƒ
    'disable_counselor_during_training': True,  # í•™ìŠµ ì¤‘ ìƒë‹´ì‚¬ ëª¨ë“ˆ OFF
    'enable_llm_emotion_support': True, # ê°ì • LLM ì§€ì›ì€ ìœ ì§€
    'enable_dynamic_swap': True,        # ë™ì  RAM ìŠ¤ì™‘ í™œì„±í™”
    'unified_backbone': {
        'total_parameters': 68_000_000,     # 68M ê³µìœ  ë°±ë³¸ (50Mì˜ 1.364ë°°)
        'd_model': 896,                     # ëª¨ë¸ ì°¨ì› (768ì˜ 1.17ë°°)
        'num_heads': 14,                    # ì–´í…ì…˜ í—¤ë“œ ìˆ˜
        'num_layers': 8,                    # ë ˆì´ì–´ ìˆ˜
        'feedforward_dim': 3584,            # í”¼ë“œí¬ì›Œë“œ ì°¨ì› (896*4)
        'cross_attention_heads': 14,        # í¬ë¡œìŠ¤ ì–´í…ì…˜ í—¤ë“œ
        'gpu_resident': True,               # ë°±ë³¸ì€ í•­ìƒ GPU ìƒì£¼
    },
    'specialized_heads': {
        'emotion_empathy_head': 30_000_000,       # ê°ì •+ê³µê° í—¤ë“œ (22Mì˜ 1.364ë°°)
        'bentham_fromm_head': 27_000_000,         # ë²¤ë‹´+í”„ë¡¬ í—¤ë“œ (20Mì˜ 1.364ë°°)
        'semantic_surd_head': 22_000_000,         # ì˜ë¯¸+SURD í—¤ë“œ (16Mì˜ 1.364ë°°)
        'regret_learning_head': 30_000_000,       # í›„íšŒ+í•™ìŠµ í—¤ë“œ (22Mì˜ 1.364ë°°)
        'meta_integration_head': 0,               # ë©”íƒ€í†µí•© í—¤ë“œ (ì¶”ê°€ë¡œ í•„ìš”ì‹œ)
        'default_gpu_resident': False,            # í—¤ë“œë“¤ì€ ê¸°ë³¸ì ìœ¼ë¡œ RAMì—ì„œ ìŠ¤ì™‘
        'swap_strategy': 'predictive_preload',    # ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”©
    },
    'dynamic_swap_config': {
        'swap_backend': 'ram',                    # RAM ê¸°ë°˜ ìŠ¤ì™‘
        'compression_enabled': True,              # ëª¨ë¸ ì••ì¶• í™œì„±í™”
        'async_swap': True,                       # ë¹„ë™ê¸° ìŠ¤ì™‘
        'preload_prediction': True,               # ì˜ˆì¸¡ì  í”„ë¦¬ë¡œë”©
        'swap_timeout': 2.0,                      # ìŠ¤ì™‘ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        'memory_threshold': 0.85,                 # GPU ë©”ëª¨ë¦¬ ì„ê³„ì¹˜ 85% ë³µì›
    },
    'rumbaugh': {
        'neural_hidden_dim': 512,
        'learning_rate': 0.001,
        'gpu_allocation': 0.15,
        'max_depth': 10,
        'attention_mechanism': True,
        'layer_norm': True,
        'dropout_rate': 0.1,
        'batch_size': 32,
        'enable_caching': True,
    },
}

# ì´ˆê¸°ê°’ ì„¤ì • (torch ì—†ì´)
ADVANCED_CONFIG['enable_gpu'] = False
ADVANCED_CONFIG['gpu_count'] = 0
DEVICE = 'cpu'
TORCH_DTYPE = None

# ìš°ì„ ìˆœìœ„ í´ë˜ìŠ¤ ì •ì˜ (get_smart_deviceë³´ë‹¤ ë¨¼ì € ì •ì˜)
class ModelPriority:
    """ëª¨ë¸ ìš°ì„ ìˆœìœ„ í´ë˜ìŠ¤"""
    CRITICAL = 4
    HIGH = 3
    MEDIUM = 2
    LOW = 1

# ìš°ì„ ìˆœìœ„ ë¬¸ìì—´ ë§¤í•‘
MODULE_PRIORITY_MAP = {
    'CRITICAL': ModelPriority.CRITICAL,
    'HIGH': ModelPriority.HIGH,
    'MEDIUM': ModelPriority.MEDIUM,
    'LOW': ModelPriority.LOW
}

# GPU ì‚¬ìš© ê°€ëŠ¥ì„± ì²´í¬ ë° ë””ë°”ì´ìŠ¤ ì„¤ì • - torch import ì§€ì—° ë¡œë”©
def _initialize_torch_config():
    """torch ì„¤ì •ì„ ì§€ì—° ì´ˆê¸°í™”"""
    global DEVICE, TORCH_DTYPE
    try:
        import torch
        ADVANCED_CONFIG['enable_gpu'] = torch.cuda.is_available()
        ADVANCED_CONFIG['gpu_count'] = torch.cuda.device_count() if torch.cuda.is_available() else 0
        DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        TORCH_DTYPE = torch.float32
        return True
    except ImportError:
        ADVANCED_CONFIG['enable_gpu'] = False
        ADVANCED_CONFIG['gpu_count'] = 0
        DEVICE = 'cpu'
        TORCH_DTYPE = None
        return False

def get_gpu_memory_info():
    """
    GPU ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ ì •ë°€í•˜ê²Œ ì¶”ì í•˜ëŠ” í•¨ìˆ˜
    
    Returns:
        Dict: GPU ë©”ëª¨ë¦¬ ì •ë³´ (ë‹¨ì¼ ìŠ¤í‚¤ë§ˆ)
            - total_mb: ì „ì²´ ë©”ëª¨ë¦¬ (MB)
            - allocated_mb: í• ë‹¹ëœ ë©”ëª¨ë¦¬ (MB)
            - cached_mb: ì˜ˆì•½ëœ/ìºì‹œëœ ë©”ëª¨ë¦¬ (MB)
            - free_mb: ì—¬ìœ  ë©”ëª¨ë¦¬ (MB)
            - usage_percent: ì‚¬ìš©ë¥  (%)
    """
    try:
        import torch
        
        if not torch.cuda.is_available():
            return None
            
        # GPU 0ë²ˆ ë””ë°”ì´ìŠ¤ ê¸°ì¤€
        device = 0
        
        # ì´ ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸)
        total_memory = torch.cuda.get_device_properties(device).total_memory
        
        # í˜„ì¬ í• ë‹¹ëœ ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸)
        allocated_memory = torch.cuda.memory_allocated(device)
        
        # ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸) - PyTorch ìºì‹œ í¬í•¨
        cached_memory = torch.cuda.memory_reserved(device)
        
        # MB ë‹¨ìœ„ë¡œ ë³€í™˜
        total_mb = total_memory / (1024 * 1024)
        allocated_mb = allocated_memory / (1024 * 1024)
        cached_mb = cached_memory / (1024 * 1024)
        free_mb = total_mb - cached_mb  # ìºì‹œëœ ë©”ëª¨ë¦¬ ê¸°ì¤€
        
        # ì‚¬ìš©ë¥  ê³„ì‚° (í• ë‹¹ëœ ë©”ëª¨ë¦¬ ê¸°ì¤€)
        usage_percent = (allocated_mb / total_mb) * 100
        
        return {
            'total_mb': total_mb,
            'allocated_mb': allocated_mb,
            'cached_mb': cached_mb,
            'free_mb': free_mb,
            'usage_percent': usage_percent
        }
        
    except Exception as e:
        logger.error(f"GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None

# ì „ì—­ MasterMemoryOrchestrator ì¸ìŠ¤í„´ìŠ¤
_master_memory_orchestrator = None

# ì „ì—­ SequentialGPULoader ì¸ìŠ¤í„´ìŠ¤
_gpu_loader = None

def get_gpu_loader():
    """SequentialGPULoader ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _gpu_loader
    
    if _gpu_loader is None:
        _gpu_loader = SequentialGPULoader()
        _gpu_loader.start()
        logger.info("SequentialGPULoader ì‹œì‘ë¨")
    
    return _gpu_loader

def get_master_memory_orchestrator():
    """MasterMemoryOrchestrator ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _master_memory_orchestrator
    
    if _master_memory_orchestrator is None:
        _master_memory_orchestrator = MasterMemoryOrchestrator()
        
        # ì„œë¸Œì‹œìŠ¤í…œ ì—°ê²°
        try:
            gpu_loader = get_gpu_loader()
            
            # ë™ì  ìŠ¤ì™‘ ë§¤ë‹ˆì € ì´ˆê¸°í™”
            try:
                from dynamic_swap_manager import RedHeartDynamicSwapManager
                swap_manager = RedHeartDynamicSwapManager()
                logger.info("ë™ì  ìŠ¤ì™‘ ë§¤ë‹ˆì € ì—°ê²° ì„±ê³µ")
            except Exception as e:
                logger.warning(f"ë™ì  ìŠ¤ì™‘ ë§¤ë‹ˆì € ì—°ê²° ì‹¤íŒ¨: {e}")
                swap_manager = None
                
            # ì••ì¶• ì‹œìŠ¤í…œ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
            compressor = None  # í˜„ì¬ëŠ” ë‚´ì¥ ì••ì¶• ì‹œìŠ¤í…œ ì‚¬ìš©
            predictor = None   # í˜„ì¬ëŠ” MasterMemoryOrchestratorì˜ ë‚´ì¥ ì˜ˆì¸¡ ì‚¬ìš©
            
            _master_memory_orchestrator.connect_subsystems(
                gpu_manager=gpu_loader,
                swap_manager=swap_manager,
                predictor=predictor,
                compressor=compressor
            )
        except Exception as e:
            logger.warning(f"ì„œë¸Œì‹œìŠ¤í…œ ì—°ê²° ì¤‘ ì¼ë¶€ ì‹¤íŒ¨: {e}")
    
    return _master_memory_orchestrator

def get_smart_device(memory_required_mb: int = 500, force_cpu: bool = False, priority: int = ModelPriority.MEDIUM, model_id: str = None):
    """
    MasterMemoryOrchestratorë¥¼ í™œìš©í•œ ì§„ì •í•œ ìŠ¤ë§ˆíŠ¸ ë””ë°”ì´ìŠ¤ ì„ íƒ
    - GPU ë©”ëª¨ë¦¬ë¥¼ 85%ê¹Œì§€ ìµœëŒ€í•œ í™œìš©
    - í•„ìš”ì‹œ ë‹¤ë¥¸ ëª¨ë“ˆì„ RAMìœ¼ë¡œ ìŠ¤ì™‘í•´ì„œ ê³µê°„ í™•ë³´
    - ìˆœì°¨ì  ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ê´€ë¦¬
    
    Args:
        memory_required_mb: í•„ìš”í•œ ë©”ëª¨ë¦¬ ìš©ëŸ‰ (MB)
        force_cpu: CPU ê°•ì œ ì‚¬ìš© ì—¬ë¶€
        priority: ëª¨ë¸ ìš°ì„ ìˆœìœ„ (ModelPriority)
        model_id: ëª¨ë¸ ê³ ìœ  ID
        
    Returns:
        torch.device: ì„ íƒëœ ë””ë°”ì´ìŠ¤
    """
    # torch ëª¨ë“ˆì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ˆê¸°í™”
    if DEVICE == 'cpu' and TORCH_DTYPE is None:
        _initialize_torch_config()
    
    # CPU ê°•ì œ ì‚¬ìš© ëª¨ë“œ
    if force_cpu:
        try:
            import torch
            return torch.device('cpu')
        except ImportError:
            return 'cpu'
    
    try:
        import torch
        
        # GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
        if not torch.cuda.is_available():
            return torch.device('cpu')
        
        # ğŸ”¥ GPU ì ê·¹ í™œìš© ëª¨ë“œ: 85%ê¹Œì§€ ìµœëŒ€í•œ í™œìš©
        memory_info = get_gpu_memory_info()
        if memory_info:
            current_usage = memory_info['usage_percent']
            # 85% ë¯¸ë§Œì´ë©´ ì ê·¹ì ìœ¼ë¡œ GPU ì‚¬ìš©
            if current_usage < 85:
                logger.info(f"GPU ì§ì ‘ ì‚¬ìš©: {model_id} (ì‚¬ìš©ë¥ : {current_usage:.1f}%, í•„ìš”: {memory_required_mb}MB)")
                return torch.device('cuda')
            # 85% ì´ˆê³¼ë©´ ìŠ¤ì™‘ì„ í†µí•œ ê³µê°„ í™•ë³´ ì‹œë„
            elif current_usage >= 85:
                logger.info(f"GPU í¬í™” ìƒíƒœ - ìŠ¤ì™‘ ì‹œë„: {current_usage:.1f}%")
        
        # MasterMemoryOrchestratorë¥¼ í†µí•œ ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ì„ íƒ
        orchestrator = get_master_memory_orchestrator()
        
        # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        try:
            device = run_async_safely(
                orchestrator.intelligent_device_selection(
                    model_id=model_id or f"temp_model_{int(time.time())}",
                    priority=priority,
                    estimated_memory_mb=memory_required_mb,
                    force_gpu=False
                ),
                timeout=5.0
            )
            
            if device is not None:
                return device
            else:
                return torch.device('cpu')
                
        except Exception as e:
            logger.warning(f"ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ì„ íƒ ì‹¤íŒ¨: {e}. ê¸°ë³¸ ë¡œì§ ì‚¬ìš©")
            
            # í´ë°±: ê¸°ë³¸ ë©”ëª¨ë¦¬ ì²´í¬
            memory_info = get_gpu_memory_info()
            if memory_info and memory_info['free_mb'] > memory_required_mb * 1.2:
                return torch.device('cuda')
            else:
                return torch.device('cpu')
                
    except Exception as e:
        logger.error(f"ë””ë°”ì´ìŠ¤ ì„ íƒ ì¤‘ ì˜¤ë¥˜: {e}. CPU ì‚¬ìš©")
        try:
            import torch
            return torch.device('cpu')
        except ImportError:
            return 'cpu'

# ì‹œìŠ¤í…œ ì„¤ì •
SYSTEM_CONFIG = {
    # ì„±ëŠ¥ ì„¤ì • (Linux ìµœì í™”)
    'performance': {
        'batch_size': 16 if ADVANCED_CONFIG['enable_gpu'] else 8,
        'processing_delay': 0.1,        # Linuxì—ì„œ ë” ë¹ ë¥¸ ì²˜ë¦¬
        'max_memory_usage': 0.7,        # Linux ë©”ëª¨ë¦¬ ê´€ë¦¬ íš¨ìœ¨ì„±
        'save_interval': 10,
        'num_workers': min(8, os.cpu_count()),
        'prefetch_factor': 2,
        'pin_memory': ADVANCED_CONFIG['enable_gpu'],
    },
    
    # í•™ìŠµ ì„¤ì • (40M íŒŒë¼ë¯¸í„° - ë©”íƒ€ í†µí•©ì— ì§‘ì¤‘)
    'learning': {
        'initial_learning_rate': 0.005,
        'min_learning_rate': 0.0001,
        'learning_decay': 0.995,
        'regret_threshold': 0.3,
        'max_regret_intensity': 5.0,
        'early_stopping_patience': 10,
        'validation_split': 0.2,
        'total_parameters': 40_000_000,   # ì •í™•í•œ íŒŒë¼ë¯¸í„° ìˆ˜
        'memory_required_mb': 160,        # 40M * 4 bytes (FP32)
        'priority': 'LOW',                # ìš°ì„ ìˆœìœ„
        'model_id': 'meta_integration',   # ë©”ëª¨ë¦¬ ê´€ë¦¬ìš© ID
        'circuit_integration_layers': [256, 128, 64],  # í†µí•© ì„œí‚· ë ˆì´ì–´
        'meta_learning': True,           # ë©”íƒ€ í•™ìŠµ í™œì„±í™”
        'adaptive_weights': True,        # ì ì‘ì  ê°€ì¤‘ì¹˜
        'ensemble_methods': True,        # ì•™ìƒë¸” ë°©ë²•
    },
    
    # ë²¤ë‹´ ê³„ì‚° ì„¤ì • (120M íŒŒë¼ë¯¸í„°)
    'bentham': {
        'weights': {
            'intensity': 0.2,
            'duration': 0.15,
            'certainty': 0.15,
            'propinquity': 0.1,
            'fecundity': 0.15,
            'purity': 0.1,
            'extent': 0.15,
        },
        'total_parameters': 120_000_000,  # ì •í™•í•œ íŒŒë¼ë¯¸í„° ìˆ˜
        'memory_required_mb': 480,        # 120M * 4 bytes (FP32)
        'priority': 'HIGH',               # ìš°ì„ ìˆœìœ„
        'model_id': 'bentham_calculator', # ë©”ëª¨ë¦¬ ê´€ë¦¬ìš© ID
        'neural_predictor_layers': [512, 1024, 512, 256],  # ì‹ ê²½ë§ ì˜ˆì¸¡ê¸°
        'weight_layers': 6,              # 6ì¸µ ê°€ì¤‘ì¹˜ ë ˆì´ì–´
        'dynamic_scaling': True,         # ë™ì  ìŠ¤ì¼€ì¼ë§
        'layer_norm': True,              # ë ˆì´ì–´ ì •ê·œí™”
        'advanced_dropout': 0.1,         # ê³ ê¸‰ ë“œë¡­ì•„ì›ƒ
        'residual_connections': True,    # ì”ì—¬ ì—°ê²°
        },
        'enhancement_layers': {
            'cultural_weight': 0.2,
            'temporal_weight': 0.15,
            'social_weight': 0.2,
            'personal_weight': 0.2,
            'moral_weight': 0.15,
            'situational_weight': 0.1,
        },
    
    # ê³ ê¸‰ ì˜ë¯¸ ë¶„ì„ ì„¤ì • (80M íŒŒë¼ë¯¸í„° - ë°˜ì‚¬ì‹¤ì  ì¶”ë¡ ì— ì§‘ì¤‘)
    'semantic': {
        'sentence_model': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
        'korean_model': 'jhgan/ko-sroberta-multitask',
        'similarity_threshold': 0.75,
        'cache_size': 10000,
        'embedding_dimension': 1024,  # ì„ë² ë”© ì°¨ì› í™•ì¥
        'surface_weight': 0.2,
        'ethical_weight': 0.3,
        'emotional_weight': 0.3,
        'causal_weight': 0.2,
        'use_llm': True,
        'llm_model': 'microsoft/DialoGPT-medium',
        'max_sequence_length': 512,
        'total_parameters': 80_000_000,   # ì •í™•í•œ íŒŒë¼ë¯¸í„° ìˆ˜
        'memory_required_mb': 320,        # 80M * 4 bytes (FP32)
        'priority': 'MEDIUM',             # ìš°ì„ ìˆœìœ„
        'model_id': 'semantic_analyzer',  # ë©”ëª¨ë¦¬ ê´€ë¦¬ìš© ID
        'counterfactual_layers': [512, 256, 128],  # ë°˜ì‚¬ì‹¤ì  ì¶”ë¡  ë„¤íŠ¸ì›Œí¬
        'deep_reasoning': True,       # ê¹Šì€ ì¶”ë¡  ëª¨ë“œ
        'advanced_attention': True,   # ê³ ê¸‰ ì–´í…ì…˜
    },
    
    # SURD ë¶„ì„ ì„¤ì • (ê°•í™”ë¨)
    'surd': {
        'kraskov_k': 5,                 # ë” ì •í™•í•œ ì¶”ì •
        'min_samples': 50,              # ë” ë§ì€ ìƒ˜í”Œ ìš”êµ¬
        'max_variables': 10,
        'min_effect_threshold': 0.001,  # ë” ë¯¼ê°í•œ ê°ì§€
        'simulation_samples': 10000,    # ê³ í’ˆì§ˆ ì‹œë®¬ë ˆì´ì…˜
        'bootstrap_iterations': 1000,
        'confidence_level': 0.95,
        'parallel_processing': True,
    },
    
    # í›„íšŒ ë¶„ì„ ì„¤ì • (120M íŒŒë¼ë¯¸í„°)
    'regret': {
        'learning_rate': 0.0005,
        'hidden_layers': [1024, 768, 512, 256, 128, 64],  # ê¹Šì€ ë„¤íŠ¸ì›Œí¬
        'dropout_rate': 0.15,
        'l2_regularization': 0.01,
        'evaluation_metrics': ['mse', 'mae', 'r2'],
        'cross_validation_folds': 5,
        'total_parameters': 120_000_000,  # ì •í™•í•œ íŒŒë¼ë¯¸í„° ìˆ˜
        'memory_required_mb': 480,        # 120M * 4 bytes (FP32)
        'priority': 'MEDIUM',             # ìš°ì„ ìˆœìœ„
        'model_id': 'regret_analyzer',    # ë©”ëª¨ë¦¬ ê´€ë¦¬ìš© ID
        'attention_mechanism': True,     # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€
        'residual_connections': True,    # ì”ì—¬ ì—°ê²°
        'batch_norm': True,              # ë°°ì¹˜ ì •ê·œí™”
        'advanced_optimization': True,   # ê³ ê¸‰ ìµœì í™”
    },
    
    # ë°ì´í„° ì²˜ë¦¬ ì„¤ì •
    'data': {
        'experience_compression_threshold': 1000,
        'max_active_experiences': 5000,
        'backup_interval_hours': 6,
        'data_validation': True,
        'auto_cleanup': True,
        'korean_preprocessing': True,
    },
    
    # Advanced emotion analysis settings (140M íŒŒë¼ë¯¸í„°)
    'emotion': {
        'model_name': 'cardiffnlp/twitter-roberta-base-emotion',
        'use_transformers': True,
        'confidence_threshold': 0.8,
        'fallback_to_keywords': False,
        'require_advanced': True,
        'enable_biosignal': True,
        'korean_model': 'beomi/KcELECTRA-base-v2022',
        'multilingual_model': 'j-hartmann/emotion-english-distilroberta-base',
        'batch_processing': True,
        'cache_embeddings': True,
        'total_parameters': 140_000_000,  # ì •í™•í•œ íŒŒë¼ë¯¸í„° ìˆ˜
        'memory_required_mb': 560,        # 140M * 4 bytes (FP32)
        'priority': 'HIGH',               # ìš°ì„ ìˆœìœ„
        'model_id': 'emotion_analyzer',   # ë©”ëª¨ë¦¬ ê´€ë¦¬ìš© ID
        'hidden_layers': [1024, 512, 256, 128],
        'attention_heads': 16,
        'layer_norm': True,
        'dropout_rate': 0.1,
    },
    
    # ë²ˆì—­ ì„¤ì • (LocalTranslator - 400MB OPUS-MT)
    'translation': {
        'model_name': 'Helsinki-NLP/opus-mt-ko-en',
        'total_parameters': 74_000_000,   # 74M íŒŒë¼ë¯¸í„° (OPUS-MT í‘œì¤€)
        'memory_required_mb': 400,        # ëª¨ë¸ + í† í¬ë‚˜ì´ì €
        'priority': 'HIGH',               # ë‹¤ë¥¸ ëª¨ë“ˆë“¤ì˜ ì˜ì¡´ì„±
        'model_id': 'translator',         # ë©”ëª¨ë¦¬ ê´€ë¦¬ìš© ID
        'cache_embeddings': True,
        'max_sequence_length': 512,
        'batch_size': 16,
        'num_beams': 3,
        'early_stopping': True,
    },
    
    # ì‹ ê²½ë§ ì»´í¬ë„ŒíŠ¸ ì„¤ì • (HierarchicalPatternStructure - 150MB)
    'neural': {
        'total_parameters': 40_000_000,   # 40M íŒŒë¼ë¯¸í„°
        'memory_required_mb': 150,        # ê°€ë²¼ìš´ ëª¨ë“ˆ
        'priority': 'MEDIUM',             # ì„ íƒì  ëª¨ë“ˆ
        'model_id': 'neural_components',  # ë©”ëª¨ë¦¬ ê´€ë¦¬ìš© ID
        'hidden_dim': 512,
        'num_layers': 4,
        'dropout_rate': 0.1,
        'use_attention': True,
    },
    
    # ë¡œê¹… ì„¤ì •
    'logging': {
        'level': 'INFO',
        'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        'file_rotation': True,
        'max_file_size': '10MB',
        'backup_count': 5,
        'performance_logging': True,
    }
}

def setup_logging():
    """ê³ ê¸‰ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    log_config = SYSTEM_CONFIG['logging']
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    level = getattr(logging, log_config['level'])
    
    # ë¡œê·¸ í¬ë§· ì„¤ì •
    formatter = logging.Formatter(log_config['format'])
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    logger = logging.getLogger('RedHeartLinux')
    logger.setLevel(level)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ë¡œí…Œì´ì…˜)
    if log_config['file_rotation']:
        from logging.handlers import RotatingFileHandler
        log_file = os.path.join(LOGS_DIR, f'red_heart_{datetime.datetime.now().strftime("%Y%m%d")}.log')
        file_handler = RotatingFileHandler(
            log_file,
            maxBytes=int(log_config['max_file_size'].replace('MB', '')) * 1024 * 1024,
            backupCount=log_config['backup_count']
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

# ë””ë°”ì´ìŠ¤ ì„¤ì •ì€ ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë¨ (torch import ì§€ì—° ë°©ì‹)

# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •
BATCH_SIZE = SYSTEM_CONFIG['performance']['batch_size']

# ê³ ê¸‰ ëª¨ë“ˆ í•„ìˆ˜ ëª¨ë“œ ì„¤ì •
REQUIRE_ADVANCED_MODULES = ADVANCED_CONFIG.get('strict_mode', True)

# CUDA Context í”„ë¦¬ë¡œë”© ì‹œìŠ¤í…œ (11ì´ˆ ì§€ì—° í•´ê²°)
_cuda_context_initialized = False
_gpu_memory_cache = None

def preload_cuda_context():
    """CUDA Context ì‚¬ì „ ì´ˆê¸°í™” (11ì´ˆ ì§€ì—° ê·¼ë³¸ í•´ê²°)"""
    global _cuda_context_initialized, _gpu_memory_cache
    
    if _cuda_context_initialized:
        return True
        
    import torch
    if not torch.cuda.is_available():
        return False
        
    try:
        logger.info("ğŸš€ CUDA Context í”„ë¦¬ë¡œë”© ì‹œì‘ (WSL ì§€ì—° í•´ê²°)...")
        start_time = time.time()
        
        # Step 1: ê°„ë‹¨í•œ CUDA ì—°ì‚°ìœ¼ë¡œ context í™œì„±í™”
        device = torch.device('cuda:0')
        dummy_tensor = torch.tensor([1.0], device=device)
        _ = dummy_tensor * 2  # ê°•ì œ context ì´ˆê¸°í™”
        
        # Step 2: GPU ë©”ëª¨ë¦¬ ì •ë³´ ì²« í˜¸ì¶œ (ì§€ì—° í¡ìˆ˜)
        total_memory = torch.cuda.get_device_properties(0).total_memory
        allocated_memory = torch.cuda.memory_allocated(0)
        cached_memory = torch.cuda.memory_reserved(0)
        
        # Step 3: ë©”ëª¨ë¦¬ ì •ë³´ ìºì‹±
        _gpu_memory_cache = {
            'total_mb': total_memory / (1024 * 1024),
            'properties': torch.cuda.get_device_properties(0)
        }
        
        # Step 4: ë”ë¯¸ í…ì„œ ì •ë¦¬
        del dummy_tensor
        torch.cuda.empty_cache()
        
        _cuda_context_initialized = True
        duration = time.time() - start_time
        logger.info(f"âœ… CUDA Context í”„ë¦¬ë¡œë”© ì™„ë£Œ ({duration:.1f}ì´ˆ, í–¥í›„ ì¦‰ì‹œ ì‘ë‹µ)")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ CUDA Context í”„ë¦¬ë¡œë”© ì‹¤íŒ¨: {e}")
        return False

# ì¤‘ë³µ í•¨ìˆ˜ ì œê±° - ìœ„ì˜ get_gpu_memory_info() ì‚¬ìš©

# ì¤‘ë³µëœ get_smart_device í•¨ìˆ˜ ì œê±° - ìœ„ì˜ MasterMemoryOrchestrator ê¸°ë°˜ í•¨ìˆ˜ ì‚¬ìš©

def get_device():
    """ìµœì  ë””ë°”ì´ìŠ¤ ë°˜í™˜ (GPU/CPU) - ë ˆê±°ì‹œ í˜¸í™˜ì„±"""
    return get_smart_device()

class GPUModelContext:
    """GPU ëª¨ë¸ ì„ì‹œ ì‚¬ìš©ì„ ìœ„í•œ Context Manager"""
    
    def __init__(self, model, memory_required_mb=500, force_cpu=False):
        self.model = model
        self.memory_required_mb = memory_required_mb
        self.force_cpu = force_cpu
        self.original_device = None
        self.target_device = None
        self.moved_to_gpu = False
        
    def __enter__(self):
        """GPUë¡œ ëª¨ë¸ ì´ë™"""
        import torch
        
        # í˜„ì¬ ëª¨ë¸ì´ ì–´ëŠ ë””ë°”ì´ìŠ¤ì— ìˆëŠ”ì§€ í™•ì¸
        if hasattr(self.model, 'device'):
            self.original_device = self.model.device
        elif hasattr(self.model, 'parameters'):
            try:
                self.original_device = next(self.model.parameters()).device
            except StopIteration:
                self.original_device = torch.device('cpu')
        else:
            self.original_device = torch.device('cpu')
        
        # íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤ ê²°ì •
        self.target_device = get_smart_device(self.memory_required_mb, self.force_cpu)
        
        # GPUë¡œ ì´ë™ì´ í•„ìš”í•˜ê³  ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ ì´ë™
        if (self.target_device.type == 'cuda' and 
            self.original_device.type != 'cuda' and 
            hasattr(self.model, 'to')):
            try:
                self.model = self.model.to(self.target_device)
                self.moved_to_gpu = True
            except Exception as e:
                # GPU ì´ë™ ì‹¤íŒ¨ ì‹œ CPUì—ì„œ ê³„ì† ì§„í–‰
                self.target_device = torch.device('cpu')
                if self.original_device.type != 'cpu':
                    self.model = self.model.to('cpu')
        
        return self.model
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì›ë˜ ë””ë°”ì´ìŠ¤ë¡œ ëª¨ë¸ ë³µê·€"""
        # GPUì—ì„œ CPUë¡œ ì´ë™í•œ ê²½ìš°ì—ë§Œ ë³µê·€
        if (self.moved_to_gpu and 
            self.original_device.type == 'cpu' and 
            hasattr(self.model, 'to')):
            try:
                self.model = self.model.to(self.original_device)
            except Exception:
                # ë³µê·€ ì‹¤íŒ¨ ì‹œì—ë„ ì—ëŸ¬ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ
                pass

def gpu_model_context(model, memory_required_mb=500, force_cpu=False):
    """GPU ëª¨ë¸ ì„ì‹œ ì‚¬ìš©ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜"""
    return GPUModelContext(model, memory_required_mb, force_cpu)

# GPU ë©”ëª¨ë¦¬ ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ (ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë¨)

# ì „ì—­ GPU ë¡œë”© ìˆœì°¨ ì œì–´ ì‹œìŠ¤í…œ
import queue
import threading
from dataclasses import dataclass, field
from typing import Callable, Any, Optional, Dict, List

@dataclass(order=True)
class GPULoadingRequest:
    """GPU ë¡œë”© ìš”ì²­"""
    priority: int  # ë¹„êµë¥¼ ìœ„í•´ ì²« ë²ˆì§¸ í•„ë“œë¡œ ì´ë™
    model_id: str = field(compare=True)
    estimated_memory_mb: int = field(compare=False)
    loading_function: Callable[[], Any] = field(default=None, compare=False)
    result_queue: queue.Queue = field(default=None, compare=False)
    device_queue: queue.Queue = field(default=None, compare=False)

class SequentialGPULoader:
    """ìˆœì°¨ì  GPU ëª¨ë¸ ë¡œë”© ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.loading_queue = queue.PriorityQueue()
        self.loading_thread = None
        self.is_running = False
        self.current_gpu_usage = 0.0
        self.loaded_models = {}  # {model_id: {'memory_mb': int, 'device': str}}
        self.lock = threading.RLock()
        
    def start(self):
        """ë¡œë”© ìŠ¤ë ˆë“œ ì‹œì‘"""
        if not self.is_running:
            self.is_running = True
            self.loading_thread = threading.Thread(target=self._loading_worker, daemon=True)
            self.loading_thread.start()
            logger.info("ìˆœì°¨ì  GPU ë¡œë”© ì‹œìŠ¤í…œ ì‹œì‘")
    
    def stop(self):
        """ë¡œë”© ìŠ¤ë ˆë“œ ì¤‘ì§€"""
        self.is_running = False
        if self.loading_thread:
            self.loading_thread.join(timeout=5)
    
    def classify_model_risk(self, model_id: str) -> str:
        """ëª¨ë¸ ìœ„í—˜ë„ ë¶„ë¥˜ (SequentialGPULoaderìš© ê¸°ë³¸ êµ¬í˜„)"""
        # ê¸°ë³¸ì ì¸ ìœ„í—˜ë„ ë¶„ë¥˜ ë¡œì§
        if 'backbone' in model_id.lower():
            return 'HIGH'      # ë°±ë³¸ì€ í•­ìƒ ê³ ìœ„í—˜ (ì¤‘ìš”í•¨)
        elif 'head' in model_id.lower():
            return 'MEDIUM'    # í—¤ë“œë“¤ì€ ì¤‘ìœ„í—˜
        elif any(keyword in model_id.lower() for keyword in ['large', 'huge', 'xl']):
            return 'HIGH'      # í° ëª¨ë¸ë“¤ì€ ê³ ìœ„í—˜
        else:
            return 'LOW'       # ë‚˜ë¨¸ì§€ëŠ” ì €ìœ„í—˜
    
    def request_gpu_loading(self, model_id: str, priority: int, estimated_memory_mb: int, loading_function: Callable[[], Any], timeout: float = 30.0):
        """GPU ë¡œë”© ìš”ì²­ (ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì™€ í†µí•©)"""
        import torch  # WSL worker threadì—ì„œ torch ì ‘ê·¼ ë³´ì¥
        if not self.is_running:
            self.start()
        
        # ğŸ”¥ í•µì‹¬ ë³€í™”: MasterMemoryOrchestrator í™œìš©
        master_orchestrator = get_master_orchestrator()
        
        # í—¬í¼ í•¨ìˆ˜ë¡œ ë¹„ë™ê¸° ì‹¤í–‰ì„ ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬
        try:
            device, result = run_async_safely(
                master_orchestrator.intelligent_load_model(
                    model_id, priority, estimated_memory_mb, loading_function
                ),
                timeout=timeout
            )
            
            if device is None:
                logger.error(f"ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë¡œë”© íƒ€ì„ì•„ì›ƒ: {model_id}")
                return torch.device('cpu'), None
                
            return device, result
            
        except Exception as e:
            logger.error(f"ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë¡œë”© ì‹¤íŒ¨: {model_id} - {str(e)}")
            return torch.device('cpu'), None
    
    def _loading_worker(self):
        """ë¡œë”© ì›Œì»¤ ìŠ¤ë ˆë“œ (ìˆœì°¨ì  ì²˜ë¦¬)"""
        import logging
        import torch  # WSL worker threadì—ì„œ torch ì ‘ê·¼ ë³´ì¥
        worker_logger = logging.getLogger(__name__)
        worker_logger.info("GPU ë¡œë”© ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘")
        
        while self.is_running:
            try:
                # ë‹¤ìŒ ë¡œë”© ìš”ì²­ ëŒ€ê¸° (ìš°ì„ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ)
                request = self.loading_queue.get(timeout=1.0)
                
                # try-finallyë¡œ task_done() ì•ˆì „í•˜ê²Œ ë³´ì¥
                try:
                    model_id = request.model_id
                    priority = request.priority
                    
                    with self.lock:
                        # í˜„ì¬ GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                        memory_info = get_gpu_memory_info()
                        if memory_info is None:
                            # GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥
                            request.device_queue.put(torch.device('cpu'))
                            return  # task_done()ì€ finallyì—ì„œ í˜¸ì¶œ
                        
                        current_usage = memory_info['usage_percent']
                        free_mb = memory_info['free_mb']
                        
                        # ì•ˆì „ ì—¬ìœ ë¶„ì„ ê³ ë ¤í•œ ë¡œë”© ê°€ëŠ¥ì„± íŒë‹¨
                        safety_margin = 1000  # 1GB ì•ˆì „ ì—¬ìœ ë¶„
                        required_total = request.estimated_memory_mb + safety_margin
                        
                        # ë¡œë”© ê°€ëŠ¥ì„± íŒë‹¨ (85% ê¸°ì¤€ìœ¼ë¡œ ìµœì í™”)
                        can_load_to_gpu = (
                            current_usage < 82 and  # 82% ë¯¸ë§Œìœ¼ë¡œ 85% ê¸°ì¤€ ì ìš©
                            free_mb > required_total and  # ì—¬ìœ  ë©”ëª¨ë¦¬ ì¶©ë¶„
                            current_usage + (request.estimated_memory_mb / 80) < 83  # ë¡œë”© í›„ ì˜ˆìƒ ì‚¬ìš©ë¥  83% ë¯¸ë§Œ
                        )
                        
                        if can_load_to_gpu:
                            # GPU ë¡œë”© ì§„í–‰
                            worker_logger.info(f"GPU ë¡œë”© ì‹œì‘: {model_id} (í˜„ì¬ ì‚¬ìš©ë¥ : {current_usage:.1f}%, ì—¬ìœ : {free_mb}MB)")
                            request.device_queue.put(torch.device('cuda'))
                            
                            try:
                                # ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì‹¤í–‰
                                result = request.loading_function()
                                
                                # ë¡œë”© í›„ ë©”ëª¨ë¦¬ ìƒíƒœ ì¬í™•ì¸
                                new_memory_info = get_gpu_memory_info()
                                if new_memory_info:
                                    new_usage = new_memory_info['usage_percent']
                                    actual_memory_used = (new_usage - current_usage) * 80  # 8GB * 10
                                    
                                    # ë¡œë”©ëœ ëª¨ë¸ ê¸°ë¡
                                    self.loaded_models[model_id] = {
                                        'memory_mb': actual_memory_used,
                                        'device': 'cuda'
                                    }
                                    
                                    worker_logger.info(f"GPU ë¡œë”© ì™„ë£Œ: {model_id} (ì‚¬ìš©ë¥ : {current_usage:.1f}% â†’ {new_usage:.1f}%, ì‹¤ì œ ì‚¬ìš©: {actual_memory_used:.0f}MB)")
                                    
                                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ 85% ì´ˆê³¼í•˜ë©´ ê¸´ê¸‰ ì¡°ì¹˜
                                    if new_usage > 85:
                                        worker_logger.error(f"GPU ë©”ëª¨ë¦¬ ìœ„í—˜ ìˆ˜ì¤€: {new_usage:.1f}% - ë‹¤ìŒ ëª¨ë¸ë“¤ì€ CPUë¡œ ê°•ì œ ì´ë™")
                                        self._force_cpu_mode()
                                
                                request.result_queue.put(result)
                                
                            except Exception as e:
                                worker_logger.error(f"GPU ë¡œë”© ì‹¤íŒ¨: {model_id} - {e}")
                                request.result_queue.put(None)
                        else:
                            # CPUë¡œ ë¡œë”©
                            worker_logger.info(f"CPU ë¡œë”©: {model_id} (GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: í˜„ì¬ {current_usage:.1f}%, ì—¬ìœ  {free_mb}MB < í•„ìš” {required_total}MB)")
                            request.device_queue.put(torch.device('cpu'))
                            
                            # CPU ë¡œë”©ë„ ê¸°ë¡
                            self.loaded_models[model_id] = {
                                'memory_mb': 0,
                                'device': 'cpu'
                            }
                
                finally:
                    # ëª¨ë“  ì½”ë“œ ê²½ë¡œì—ì„œ ë°˜ë“œì‹œ task_done() í˜¸ì¶œ
                    self.loading_queue.task_done()
                    
            except queue.Empty:
                continue
            except Exception as e:
                worker_logger.error(f"GPU ë¡œë”© ì›Œì»¤ ì˜¤ë¥˜: {e}")
                # queue.get()ì´ ì‹¤íŒ¨í•œ ê²½ìš°ì—ëŠ” task_done() í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
    
    def _force_cpu_mode(self):
        """CPU ê°•ì œ ëª¨ë“œ í™œì„±í™”"""
        import logging
        worker_logger = logging.getLogger(__name__)
        worker_logger.warning("GPU ë©”ëª¨ë¦¬ í•œê³„ë¡œ CPU ê°•ì œ ëª¨ë“œ í™œì„±í™”")
        # ì´í›„ ëª¨ë“  ìš”ì²­ì„ CPUë¡œ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ì •
        # ì´ ë¶€ë¶„ì€ ì¶”í›„ êµ¬í˜„ ê°€ëŠ¥

# ì „ì—­ í†µí•© ë©”ëª¨ë¦¬ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹œìŠ¤í…œ
_gpu_loaded_models = {}  # GPUì— ë¡œë“œëœ ëª¨ë¸ë“¤ ì¶”ì  (ë¯¸ì„ ì–¸ ë²„ê·¸ ìˆ˜ì •)
_gpu_loader = SequentialGPULoader()

# ì „ì—­ ëª¨ë“ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ - ì‹œìŠ¤í…œì˜ ëª¨ë“  ëª¨ë“ˆ ì •ë³´
_global_module_registry = {}
_module_instances = {}  # ì‹¤ì œ ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ë“¤

def register_system_module(module_id: str, module_instance, config_section: str = None, replace: bool = False):
    """ì‹œìŠ¤í…œ ëª¨ë“ˆì„ ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
    
    Args:
        module_id: ëª¨ë“ˆ ê³ ìœ  ID (ì˜ˆ: 'emotion_analyzer')
        module_instance: ì‹¤ì œ ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤
        config_section: ì„¤ì • ì„¹ì…˜ ì´ë¦„ (ì˜ˆ: 'emotion')
        replace: Trueë©´ ê¸°ì¡´ ëª¨ë“ˆ êµì²´ í—ˆìš© (ê¸°ë³¸ê°’: False)
    """
    global _global_module_registry, _module_instances
    
    logger.info(f"ğŸ”¥ register_system_module í˜¸ì¶œ: module_id={module_id}, instance_type={type(module_instance)}, config_section={config_section}")
    
    # STRICT_NO_OVERWRITE: ì´ë¯¸ ë“±ë¡ëœ ëª¨ë“ˆì´ë©´ ë®ì–´ì“°ê¸° ê¸ˆì§€ (replace=Falseì¸ ê²½ìš°)
    if module_id in _module_instances:
        existing_type = type(_module_instances[module_id]).__name__
        new_type = type(module_instance).__name__
        
        if not replace:
            # ë™ì¼ íƒ€ì…ì´ë©´ ê²½ê³ ë§Œ í•˜ê³  ì§„í–‰
            if existing_type == new_type:
                logger.warning(f"âš ï¸ {module_id} ì´ë¯¸ ë“±ë¡ë¨ (ë™ì¼ íƒ€ì…: {existing_type}), ìŠ¤í‚µ")
                return  # ë™ì¼ íƒ€ì…ì´ë©´ ê²½ê³ ë§Œ í•˜ê³  ì§„í–‰
            else:
                logger.error(f"âŒ STRICT_NO_OVERWRITE: {module_id}ê°€ ì´ë¯¸ ë“±ë¡ë¨")
                logger.error(f"   ê¸°ì¡´: {existing_type}")
                logger.error(f"   ì‹ ê·œ: {new_type}")
                logger.error(f"   êµì²´í•˜ë ¤ë©´ replace=True ì‚¬ìš©")
                raise RuntimeError(f"STRICT_NO_OVERWRITE: {module_id} already registered")
        else:
            # replace=Trueì¸ ê²½ìš° ê¸°ì¡´ ëª¨ë“ˆ ì •ë¦¬
            logger.info(f"ğŸ”„ ê¸°ì¡´ ëª¨ë“ˆ êµì²´: {module_id} ({existing_type} â†’ {new_type})")
            old_module = _module_instances[module_id]
            # ê¸°ì¡´ ëª¨ë“ˆ ë©”ëª¨ë¦¬ í•´ì œ ì‹œë„
            if hasattr(old_module, 'cleanup') and callable(old_module.cleanup):
                try:
                    old_module.cleanup()
                    logger.info(f"   ê¸°ì¡´ ëª¨ë“ˆ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"   ê¸°ì¡´ ëª¨ë“ˆ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    # íŠ¹ì • ëª¨ë“ˆì— ëŒ€í•œ ì¸í„°í˜ì´ìŠ¤ ê²€ì¦
    if module_id in {"emotion_analyzer", "bentham_calculator"}:
        if not hasattr(module_instance, "get_pytorch_network"):
            logger.error(f"âŒ {module_id} must implement get_pytorch_network method")
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì„œë“œ ë¡œê¹…
            methods = [m for m in dir(module_instance) if not m.startswith('_') and callable(getattr(module_instance, m, None))]
            logger.error(f"   Available methods: {methods[:10]}...")
            raise AssertionError(f"{module_id} must implement get_pytorch_network")
        else:
            logger.info(f"âœ… {module_id} has get_pytorch_network method")
    
    # ì„¤ì •ì—ì„œ ëª¨ë“ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    if config_section and config_section in SYSTEM_CONFIG:
        module_config = SYSTEM_CONFIG[config_section]
        
        # ëª¨ë“ˆ ë©”íƒ€ë°ì´í„° ìƒì„±
        module_info = {
            'module_id': module_id,
            'config_section': config_section,
            'total_parameters': module_config.get('total_parameters', 0),
            'memory_required_mb': module_config.get('memory_required_mb', 0),
            'priority': MODULE_PRIORITY_MAP.get(module_config.get('priority', 'MEDIUM'), ModelPriority.MEDIUM),
            'device': 'cpu',  # ì´ˆê¸°ê°’
            'loaded': False,
            'last_used': time.time()
        }
        
        _global_module_registry[module_id] = module_info
        _module_instances[module_id] = module_instance
        
        logger.info(f"ì‹œìŠ¤í…œ ëª¨ë“ˆ ë“±ë¡: {module_id} ({module_config.get('total_parameters', 0):,} íŒŒë¼ë¯¸í„°)")
        
        # MasterMemoryOrchestratorì—ë„ ë“±ë¡
        orchestrator = get_master_orchestrator()
        if orchestrator:
            orchestrator.master_model_registry[module_id] = {
                'device': 'cpu',
                'memory_mb': module_config.get('memory_required_mb', 0),
                'priority': MODULE_PRIORITY_MAP.get(module_config.get('priority', 'MEDIUM'), ModelPriority.MEDIUM),
                'total_parameters': module_config.get('total_parameters', 0),
                'load_time': time.time(),
                'access_count': 0,
                'risk_level': 'MEDIUM'
            }
    else:
        # config_sectionì´ ì—†ì–´ë„ _module_instancesì—ëŠ” ë“±ë¡
        _module_instances[module_id] = module_instance
        logger.info(f"ğŸ”¥ ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ ë“±ë¡ (config_section ì—†ìŒ): {module_id} -> {type(module_instance)}")

def get_system_module(module_id: str):
    """ë“±ë¡ëœ ì‹œìŠ¤í…œ ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Args:
        module_id: ëª¨ë“ˆ ê³ ìœ  ID
        
    Returns:
        ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
    """
    logger.info(f"ğŸ” get_system_module í˜¸ì¶œ: module_id={module_id}")
    logger.info(f"ğŸ” í˜„ì¬ ë“±ë¡ëœ ëª¨ë“ˆë“¤: {list(_module_instances.keys())}")
    result = _module_instances.get(module_id)
    logger.info(f"ğŸ” ê²°ê³¼: {type(result) if result else 'None'}")
    return result

def get_module_info(module_id: str):
    """ë“±ë¡ëœ ëª¨ë“ˆì˜ ë©”íƒ€ë°ì´í„° ë°˜í™˜
    
    Args:
        module_id: ëª¨ë“ˆ ê³ ìœ  ID
        
    Returns:
        ëª¨ë“ˆ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
    """
    return _global_module_registry.get(module_id)

def run_async_safely(coro, timeout=60.0):
    """ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ëŠ” í—¬í¼"""
    try:
        # í˜„ì¬ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
        loop = asyncio.get_running_loop()
        
        # ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        import concurrent.futures
        import threading
        
        result_holder = {'result': None, 'exception': None}
        
        def run_in_new_loop():
            try:
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                result_holder['result'] = new_loop.run_until_complete(coro)
                new_loop.close()
            except Exception as e:
                result_holder['exception'] = e
            finally:
                asyncio.set_event_loop(None)
        
        thread = threading.Thread(target=run_in_new_loop)
        thread.start()
        thread.join(timeout=timeout)
        
        if thread.is_alive():
            logger.error("ë¹„ë™ê¸° ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ")
            return None
            
        if result_holder['exception']:
            raise result_holder['exception']
            
        return result_holder['result']
        
    except RuntimeError:
        # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì‹¤í–‰
        return asyncio.run(coro)

# ëª¨ë¸ ì‹¤í–‰ ìƒíƒœ ì—´ê±°í˜•
from enum import Enum

class ModelExecutionState(Enum):
    """ëª¨ë¸ ì‹¤í–‰ ìƒíƒœ"""
    IDLE = "idle"                    # ìœ íœ´ ìƒíƒœ - ìŠ¤ì™‘ ê°€ëŠ¥
    BUSY = "busy"                    # ì‘ì—… ì‹¤í–‰ ì¤‘ - ìŠ¤ì™‘ ë¶ˆê°€
    LOADING = "loading"              # ë¡œë”© ì¤‘ - ìŠ¤ì™‘ ë¶ˆê°€
    PENDING_SWAP = "pending_swap"    # ìŠ¤ì™‘ ëŒ€ê¸° ì¤‘
    SWAPPING = "swapping"            # ìŠ¤ì™‘ ì§„í–‰ ì¤‘
    ERROR = "error"                  # ì—ëŸ¬ ìƒíƒœ

class TaskSequenceType(Enum):
    """ì—°ê³„ ì‘ì—… ìœ í˜•"""
    STANDALONE = "standalone"        # ë…ë¦½ ì‘ì—…
    BACKBONE_HEAD = "backbone_head"  # ë°±ë³¸ + í—¤ë“œ ì¡°í•©
    PIPELINE = "pipeline"            # íŒŒì´í”„ë¼ì¸ ì—°ê³„
    BATCH = "batch"                  # ë°°ì¹˜ ì²˜ë¦¬

@dataclass
class ModelExecutionContext:
    """ëª¨ë¸ ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸"""
    model_id: str
    state: ModelExecutionState = ModelExecutionState.IDLE
    current_task_id: Optional[str] = None
    start_time: Optional[float] = None
    estimated_completion_time: Optional[float] = None
    sequence_type: TaskSequenceType = TaskSequenceType.STANDALONE
    related_models: List[str] = field(default_factory=list)
    is_critical_for_sequence: bool = False
    last_access_time: float = field(default_factory=time.time)
    access_count: int = 0

class MasterMemoryOrchestrator:
    """ì‘ì—… ì™„ë£Œ ê¸°ë°˜ ì§€ëŠ¥ì  ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ
    
    í•µì‹¬ ê°œì„ ì‚¬í•­:
    - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ â†’ ì‘ì—… ì™„ë£Œ ê¸°ë°˜ ìŠ¤ì™‘
    - ì‹¤ì‹œê°„ ì‘ì—… ìƒíƒœ ì¶”ì  ë° ëª¨ë‹ˆí„°ë§
    - ì—°ê³„ ì‘ì—… (backbone+head) ê°ì§€ ë° ì•ˆì „ ëŒ€ê¸°
    - ì‘ì—… ìˆœì„œ ê¸°ë°˜ ì•ˆì „í•œ ìŠ¤ì™‘ íƒ€ì´ë° ì œì–´
    """
    
    def __init__(self):
        # ë‹¨ì¼ ì§„ì‹¤ ì†ŒìŠ¤ (Single Source of Truth)
        self.master_model_registry = {}  # {model_id: ModelMetadata}
        self.gpu_memory_map = {}         # ì‹¤ì œ GPU ìƒíƒœ ì¶”ì 
        self.compressed_cache = {}       # ì••ì¶•ëœ ëª¨ë¸ë“¤
        
        # ğŸ”¥ ìƒˆë¡œìš´ ì‘ì—… ìƒíƒœ ì¶”ì  ì‹œìŠ¤í…œ
        self.execution_contexts = {}     # {model_id: ModelExecutionContext}
        self.active_task_sequences = {}  # {sequence_id: [model_ids]}
        self.pending_swap_queue = []     # ìŠ¤ì™‘ ëŒ€ê¸° í
        
        # ğŸ”¥ ì‘ì—… ì™„ë£Œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        self.task_completion_callbacks = {}  # {task_id: callback_function}
        self.sequence_dependencies = {}      # {model_id: [dependent_model_ids]}
        
        # ê¸°ì¡´ ê³ ê¸‰ ì‹œìŠ¤í…œë“¤ê³¼ ì—°ê²°
        self._dynamic_gpu_manager = None
        self._swap_manager = None
        self._predictor = None
        self._compressor = None
        
        # ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë“¤ì˜ ì•½í•œ ì°¸ì¡° (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
        import weakref
        self.active_model_refs = {}      # {model_id: weakref.ref(model)}
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self.lock = threading.RLock()
        
        # ğŸ”¥ ì•ˆì „í•œ ìŠ¤ì™‘ ìŠ¤ì¼€ì¤„ëŸ¬
        self.safe_swap_scheduler_running = False
        self.swap_scheduler_thread = None
    
    def connect_subsystems(self, gpu_manager=None, swap_manager=None, predictor=None, compressor=None):
        """ê¸°ì¡´ ì„œë¸Œì‹œìŠ¤í…œë“¤ê³¼ ì—°ê²°"""
        self._dynamic_gpu_manager = gpu_manager
        self._swap_manager = swap_manager  
        self._predictor = predictor
        self._compressor = compressor
        
        # ì•ˆì „í•œ ìŠ¤ì™‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
        self._start_safe_swap_scheduler()
    
    def _start_safe_swap_scheduler(self):
        """ì•ˆì „í•œ ìŠ¤ì™‘ ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤ë ˆë“œ ì‹œì‘"""
        if not self.safe_swap_scheduler_running:
            self.safe_swap_scheduler_running = True
            self.swap_scheduler_thread = threading.Thread(
                target=self._safe_swap_scheduler_worker, 
                daemon=True
            )
            self.swap_scheduler_thread.start()
            logger.info("ì•ˆì „í•œ ìŠ¤ì™‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨")
    
    def _safe_swap_scheduler_worker(self):
        """ì•ˆì „í•œ ìŠ¤ì™‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì›Œì»¤ ìŠ¤ë ˆë“œ"""
        while self.safe_swap_scheduler_running:
            try:
                with self.lock:
                    # ìŠ¤ì™‘ ëŒ€ê¸° íì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥í•œ í•­ëª© í™•ì¸
                    if self.pending_swap_queue:
                        self._process_pending_swaps()
                
                # 1ì´ˆë§ˆë‹¤ ì²´í¬
                time.sleep(1.0)
                
            except Exception as e:
                logger.error(f"ìŠ¤ì™‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì˜¤ë¥˜: {e}")
                time.sleep(5.0)  # ì—ëŸ¬ ì‹œ 5ì´ˆ ëŒ€ê¸°
    
    def register_model_execution_start(self, model_id: str, task_id: str, sequence_type: TaskSequenceType = TaskSequenceType.STANDALONE, related_models: List[str] = None):
        """
        ëª¨ë¸ ì‘ì—… ì‹œì‘ ë“±ë¡
        
        Args:
            model_id: ëª¨ë¸ ID
            task_id: ì‘ì—… ID
            sequence_type: ì—°ê³„ ì‘ì—… ìœ í˜•
            related_models: ì—°ê³„ëœ ëª¨ë¸ë“¤
        """
        with self.lock:
            if model_id not in self.execution_contexts:
                self.execution_contexts[model_id] = ModelExecutionContext(model_id=model_id)
            
            context = self.execution_contexts[model_id]
            context.state = ModelExecutionState.BUSY
            context.current_task_id = task_id
            context.start_time = time.time()
            context.sequence_type = sequence_type
            context.related_models = related_models or []
            context.access_count += 1
            context.last_access_time = time.time()
            
            # ì—°ê³„ ì‘ì—… ë“±ë¡
            if sequence_type != TaskSequenceType.STANDALONE and related_models:
                sequence_id = f"{task_id}_{sequence_type.value}"
                self.active_task_sequences[sequence_id] = [model_id] + related_models
                
                # ì˜ì¡´ì„± ë“±ë¡
                for related_model in related_models:
                    if related_model not in self.sequence_dependencies:
                        self.sequence_dependencies[related_model] = []
                    self.sequence_dependencies[related_model].append(model_id)
            
            logger.info(f"ì‘ì—… ì‹œì‘ ë“±ë¡: {model_id} (ì‘ì—…: {task_id}, ìœ í˜•: {sequence_type.value})")
    
    def register_model_execution_complete(self, model_id: str, task_id: str):
        """
        ëª¨ë¸ ì‘ì—… ì™„ë£Œ ë“±ë¡
        
        Args:
            model_id: ëª¨ë¸ ID
            task_id: ì‘ì—… ID
        """
        with self.lock:
            if model_id not in self.execution_contexts:
                logger.warning(f"ì‘ì—… ì™„ë£Œ ë“±ë¡ ì‹¤íŒ¨ - ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ: {model_id}")
                return
            
            context = self.execution_contexts[model_id]
            
            # ì‘ì—… ID ë§¤ì¹­ í™•ì¸
            if context.current_task_id != task_id:
                logger.warning(f"ì‘ì—… ID ë¶ˆì¼ì¹˜: {model_id}, ì˜ˆìƒ: {context.current_task_id}, ì‹¤ì œ: {task_id}")
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            context.state = ModelExecutionState.IDLE
            context.current_task_id = None
            context.start_time = None
            context.last_access_time = time.time()
            
            logger.info(f"ì‘ì—… ì™„ë£Œ ë“±ë¡: {model_id} (ì‘ì—…: {task_id})")
            
            # ì™„ë£Œ ì½œë°± ì‹¤í–‰
            if task_id in self.task_completion_callbacks:
                try:
                    callback = self.task_completion_callbacks[task_id]
                    callback(model_id, task_id)
                    del self.task_completion_callbacks[task_id]
                except Exception as e:
                    logger.error(f"ì‘ì—… ì™„ë£Œ ì½œë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    def is_model_safe_to_swap(self, model_id: str) -> bool:
        """
        ëª¨ë¸ì´ ì•ˆì „í•˜ê²Œ ìŠ¤ì™‘ ê°€ëŠ¥í•œì§€ í™•ì¸
        
        Args:
            model_id: ëª¨ë¸ ID
            
        Returns:
            bool: ìŠ¤ì™‘ ê°€ëŠ¥ ì—¬ë¶€
        """
        with self.lock:
            # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ í™•ì¸
            if model_id not in self.execution_contexts:
                return True  # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ìŠ¤ì™‘ ê°€ëŠ¥
            
            context = self.execution_contexts[model_id]
            
            # í˜„ì¬ ì‘ì—… ì¤‘ì´ë©´ ìŠ¤ì™‘ ë¶ˆê°€
            if context.state in [ModelExecutionState.BUSY, ModelExecutionState.LOADING, ModelExecutionState.SWAPPING]:
                logger.debug(f"ìŠ¤ì™‘ ë¶ˆê°€ - ì‘ì—… ì¤‘: {model_id} (ìƒíƒœ: {context.state.value})")
                return False
            
            # ì—°ê³„ ì‘ì—… í™•ì¸
            if self._is_part_of_active_sequence(model_id):
                logger.debug(f"ìŠ¤ì™‘ ë¶ˆê°€ - ì—°ê³„ ì‘ì—… ì§„í–‰ ì¤‘: {model_id}")
                return False
            
            # ì˜ì¡´ì„± ìˆëŠ” ëª¨ë¸ë“¤ì´ ì‘ì—… ì¤‘ì¸ì§€ í™•ì¸
            if model_id in self.sequence_dependencies:
                for dependent_model in self.sequence_dependencies[model_id]:
                    if not self.is_model_safe_to_swap(dependent_model):
                        logger.debug(f"ìŠ¤ì™‘ ë¶ˆê°€ - ì˜ì¡´ì„± ëª¨ë¸ ì‘ì—… ì¤‘: {model_id} -> {dependent_model}")
                        return False
            
            return True
    
    def _is_part_of_active_sequence(self, model_id: str) -> bool:
        """ëª¨ë¸ì´ í™œì„± ì—°ê³„ ì‘ì—…ì˜ ì¼ë¶€ì¸ì§€ í™•ì¸"""
        for sequence_id, models in self.active_task_sequences.items():
            if model_id in models:
                # ì‹œí€€ìŠ¤ ë‚´ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì´ ì‘ì—… ì¤‘ì¸ì§€ í™•ì¸
                for other_model in models:
                    if other_model != model_id and other_model in self.execution_contexts:
                        other_context = self.execution_contexts[other_model]
                        if other_context.state == ModelExecutionState.BUSY:
                            return True
        return False
    
    def request_safe_swap(self, model_id: str, reason: str = "memory_needed"):
        """
        ì•ˆì „í•œ ìŠ¤ì™‘ ìš”ì²­ (ì¦‰ì‹œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ëŒ€ê¸°ì—´ì— ì¶”ê°€)
        
        Args:
            model_id: ìŠ¤ì™‘í•  ëª¨ë¸ ID
            reason: ìŠ¤ì™‘ ì‚¬ìœ 
        """
        with self.lock:
            # ì´ë¯¸ ëŒ€ê¸°ì—´ì— ìˆëŠ”ì§€ í™•ì¸
            for pending_model, pending_reason, pending_time in self.pending_swap_queue:
                if pending_model == model_id:
                    logger.debug(f"ìŠ¤ì™‘ ì´ë¯¸ ëŒ€ê¸° ì¤‘: {model_id}")
                    return
            
            # ëŒ€ê¸°ì—´ì— ì¶”ê°€
            self.pending_swap_queue.append((model_id, reason, time.time()))
            logger.info(f"ì•ˆì „í•œ ìŠ¤ì™‘ ëŒ€ê¸°ì—´ ì¶”ê°€: {model_id} (ì‚¬ìœ : {reason})")
    
    def _process_pending_swaps(self):
        """ëŒ€ê¸° ì¤‘ì¸ ìŠ¤ì™‘ ìš”ì²­ë“¤ ì²˜ë¦¬"""
        processed_count = 0
        remaining_queue = []
        
        for model_id, reason, request_time in self.pending_swap_queue:
            if self.is_model_safe_to_swap(model_id):
                # ì•ˆì „í•˜ê²Œ ìŠ¤ì™‘ ì‹¤í–‰
                try:
                    logger.info(f"ì•ˆì „í•œ ìŠ¤ì™‘ ì‹¤í–‰: {model_id} (ëŒ€ê¸° ì‹œê°„: {time.time() - request_time:.1f}ì´ˆ)")
                    
                    # ë¹„ë™ê¸° ì–¸ë¡œë“œë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
                    success = run_async_safely(
                        self.intelligent_unload_model(model_id, force=True),
                        timeout=30.0
                    )
                    
                    if success:
                        processed_count += 1
                        logger.info(f"ì•ˆì „í•œ ìŠ¤ì™‘ ì™„ë£Œ: {model_id}")
                    else:
                        remaining_queue.append((model_id, reason, request_time))
                        logger.warning(f"ìŠ¤ì™‘ ì‹¤íŒ¨, ì¬ì‹œë„ ì˜ˆì •: {model_id}")
                        
                except Exception as e:
                    logger.error(f"ìŠ¤ì™‘ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {model_id} - {e}")
                    remaining_queue.append((model_id, reason, request_time))
            else:
                # ì•„ì§ ì•ˆì „í•˜ì§€ ì•Šìœ¼ë©´ ëŒ€ê¸°ì—´ì— ìœ ì§€
                remaining_queue.append((model_id, reason, request_time))
        
        # ëŒ€ê¸°ì—´ ì—…ë°ì´íŠ¸
        self.pending_swap_queue = remaining_queue
        
        if processed_count > 0:
            logger.info(f"ì•ˆì „í•œ ìŠ¤ì™‘ ë°°ì¹˜ ì™„ë£Œ: {processed_count}ê°œ ëª¨ë¸ ì²˜ë¦¬")
    
    async def intelligent_device_selection(self, model_id: str, priority: int, estimated_memory_mb: int, force_gpu: bool = False):
        """
        ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ì„ íƒ (ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì—†ì´)
        - GPU 85% í™œìš© ëª©í‘œ
        - í•„ìš”ì‹œ ìŠ¤ì™‘ìœ¼ë¡œ ê³µê°„ í™•ë³´
        - ìœ„í—˜ë„ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì ìš©
        
        Args:
            model_id: ëª¨ë¸ ê³ ìœ  ID
            priority: ëª¨ë¸ ìš°ì„ ìˆœìœ„
            estimated_memory_mb: ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
            force_gpu: GPU ê°•ì œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            torch.device: ì„ íƒëœ ë””ë°”ì´ìŠ¤
        """
        import torch
        
        with self.lock:
            # 1. GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì •ë°€ ì²´í¬
            memory_info = get_gpu_memory_info()
            if not memory_info:
                logger.info(f"GPU ì‚¬ìš© ë¶ˆê°€: {model_id}")
                return torch.device('cpu')
            
            current_usage = memory_info['usage_percent']
            free_mb = memory_info['free_mb']
            
            # 2. ìœ„í—˜ë„ ë¶„ë¥˜ ë° ì•ˆì „ ì—¬ìœ ë¶„ ê³„ì‚°
            try:
                if self._dynamic_gpu_manager:
                    risk_level = self._dynamic_gpu_manager.classify_model_risk(model_id)
                else:
                    # ê¸°ë³¸ ìœ„í—˜ë„ ë¶„ë¥˜
                    if 'backbone' in model_id.lower():
                        risk_level = 'HIGH'
                    elif 'head' in model_id.lower():
                        risk_level = 'MEDIUM'
                    else:
                        risk_level = 'LOW'
                        
                safety_margin = {
                    'HIGH': 1000,    # ê³ ìœ„í—˜ ëª¨ë¸ì€ 1GB ì—¬ìœ ë¶„
                    'MEDIUM': 800,   # ì¤‘ìœ„í—˜ ëª¨ë¸ì€ 800MB ì—¬ìœ ë¶„  
                    'LOW': 500       # ì €ìœ„í—˜ ëª¨ë¸ì€ 500MB ì—¬ìœ ë¶„
                }.get(risk_level, 800)
                
            except Exception as e:
                logger.warning(f"ìœ„í—˜ë„ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)} - ê¸°ë³¸ê°’ ì‚¬ìš©")
                risk_level = 'MEDIUM'
                safety_margin = 800
            
            required_total = estimated_memory_mb + safety_margin
            
            # 3. 85% ê¸°ì¤€ ì ìš© - ìš°ì„ ìˆœìœ„ë³„ ì°¨ë“± ì ìš©
            if priority <= ModelPriority.HIGH:
                # HIGH/CRITICAL ìš°ì„ ìˆœìœ„ëŠ” 85%ê¹Œì§€ ì ê·¹ í™œìš©
                max_usage_threshold = 85
            else:
                # MEDIUM/LOW ìš°ì„ ìˆœìœ„ëŠ” 80%ê¹Œì§€ë§Œ í™œìš© (ì•ˆì „)
                max_usage_threshold = 80
            
            # 4. í˜„ì¬ ìƒíƒœë¡œ GPU ì‚¬ìš© ê°€ëŠ¥í•œì§€ ì²´í¬
            if current_usage < max_usage_threshold and free_mb > required_total:
                logger.info(f"GPU ì§ì ‘ ì‚¬ìš©: {model_id} (ì‚¬ìš©ë¥ : {current_usage:.1f}%, í•„ìš”: {required_total}MB)")
                return torch.device('cuda')
            
            # 5. ìŠ¤ì™‘ ê³µê°„ í™•ë³´ ì‹œë„
            if current_usage >= 82 or free_mb < required_total:
                logger.info(f"GPU ê³µê°„ ë¶€ì¡±: {model_id} (ì‚¬ìš©ë¥ : {current_usage:.1f}%, ê°€ìš©: {free_mb}MB, í•„ìš”: {required_total}MB)")
                
                if self._swap_manager:
                    try:
                        # ìŠ¤ì™‘ ë§¤ë‹ˆì €ë¥¼ í†µí•œ ê³µê°„ í™•ë³´ ì‹œë„
                        freed_space = await self._swap_manager.free_gpu_space_intelligent(required_total)
                        if freed_space >= required_total:
                            logger.info(f"ìŠ¤ì™‘ìœ¼ë¡œ ê³µê°„ í™•ë³´ ì„±ê³µ: {model_id} ({freed_space}MB í™•ë³´)")
                            return torch.device('cuda')
                        else:
                            logger.warning(f"ìŠ¤ì™‘ ê³µê°„ ë¶€ì¡±: {model_id} (í•„ìš”: {required_total}MB, í™•ë³´: {freed_space}MB)")
                    except Exception as e:
                        logger.error(f"ìŠ¤ì™‘ ê³µê°„ í™•ë³´ ì‹¤íŒ¨: {model_id} - {str(e)}")
                
                # 6. ğŸ”¥ ì‘ì—… ì™„ë£Œ ê¸°ë°˜ ì•ˆì „í•œ ê³µê°„ í™•ë³´ ì‹œë„
                if len(self.master_model_registry) > 0:
                    logger.info(f"ì‘ì—… ì™„ë£Œ ê¸°ë°˜ ê³µê°„ í™•ë³´ ì‹œë„: {model_id}")
                    
                    # ìŠ¤ì™‘ ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ì°¾ê¸° (ì‘ì—… ìƒíƒœ ê¸°ë°˜)
                    swappable_models = []
                    for existing_model_id, model_info in self.master_model_registry.items():
                        if self.is_model_safe_to_swap(existing_model_id):
                            # ì¶”ê°€ ì¡°ê±´: í˜„ì¬ ìš”ì²­ë³´ë‹¤ ëœ ì¤‘ìš”í•œ ëª¨ë¸
                            existing_priority = model_info.get('priority', ModelPriority.LOW)
                            if existing_priority >= priority:  # ê°™ì€ ìš°ì„ ìˆœìœ„ë„ í¬í•¨ (LRU ë°©ì‹)
                                swappable_models.append((
                                    existing_model_id, 
                                    model_info.get('memory_mb', 0),
                                    model_info.get('last_access_time', 0)
                                ))
                    
                    if swappable_models:
                        # LRU (Least Recently Used) ìˆœìœ¼ë¡œ ì •ë ¬
                        swappable_models.sort(key=lambda x: x[2])  # last_access_time ê¸°ì¤€
                        
                        logger.info(f"ìŠ¤ì™‘ ê°€ëŠ¥í•œ ëª¨ë¸ {len(swappable_models)}ê°œ ë°œê²¬")
                        
                        # í•„ìš”í•œ ë©”ëª¨ë¦¬ë§Œí¼ ì•ˆì „í•œ ìŠ¤ì™‘ ìš”ì²­
                        freed_memory = 0
                        swap_requested_count = 0
                        
                        for swap_model_id, model_memory, last_access_time in swappable_models:
                            if freed_memory >= required_total:
                                break
                                
                            # ì•ˆì „í•œ ìŠ¤ì™‘ ëŒ€ê¸°ì—´ì— ì¶”ê°€
                            self.request_safe_swap(
                                model_id=swap_model_id,
                                reason=f"space_for_{model_id}"
                            )
                            
                            freed_memory += model_memory
                            swap_requested_count += 1
                            
                            logger.info(f"ìŠ¤ì™‘ ìš”ì²­: {swap_model_id} ({model_memory}MB, ë§ˆì§€ë§‰ ì‚¬ìš©: {time.time() - last_access_time:.1f}ì´ˆ ì „)")
                        
                        # ìŠ¤ì™‘ ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
                        if swap_requested_count > 0:
                            logger.info(f"ìŠ¤ì™‘ ì™„ë£Œ ëŒ€ê¸° ì¤‘: {swap_requested_count}ê°œ ëª¨ë¸, ì˜ˆìƒ í™•ë³´: {freed_memory}MB")
                            
                            wait_start_time = time.time()
                            max_wait_time = 30.0
                            check_interval = 1.0
                            
                            while time.time() - wait_start_time < max_wait_time:
                                # í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¬í™•ì¸
                                updated_memory_info = get_gpu_memory_info()
                                if updated_memory_info and updated_memory_info['free_mb'] > required_total:
                                    wait_time = time.time() - wait_start_time
                                    logger.info(f"ì‘ì—… ì™„ë£Œ ê¸°ë°˜ ê³µê°„ í™•ë³´ ì„±ê³µ: {model_id} (ëŒ€ê¸° ì‹œê°„: {wait_time:.1f}ì´ˆ)")
                                    return torch.device('cuda')
                                
                                # ì ì‹œ ëŒ€ê¸° í›„ ì¬í™•ì¸
                                await asyncio.sleep(check_interval)
                            
                            logger.warning(f"ìŠ¤ì™‘ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ: {model_id} (ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ {max_wait_time}ì´ˆ ì´ˆê³¼)")
                    else:
                        logger.info(f"í˜„ì¬ ìŠ¤ì™‘ ê°€ëŠ¥í•œ ëª¨ë¸ ì—†ìŒ: {model_id} (ëª¨ë“  ëª¨ë¸ ì‘ì—… ì¤‘)")
            
            # 7. ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ CPU ì‚¬ìš©
            logger.info(f"ì‘ì—… ì™„ë£Œ ê¸°ë°˜ ê³µê°„ í™•ë³´ ì‹¤íŒ¨, CPU ì‚¬ìš©: {model_id}")
            return torch.device('cpu')
    
    async def intelligent_load_model(self, model_id: str, priority: int, estimated_memory_mb: int, loading_function: Callable, force_gpu: bool = False):
        """ì§€ëŠ¥ì  í†µí•© ëª¨ë¸ ë¡œë”©
        
        ê¸°ì¡´ ê¸°ìˆ ë“¤ì„ ëª¨ë‘ í™œìš©:
        1. TaskSequencePredictorë¡œ í•„ìš”ì„± ì˜ˆì¸¡
        2. ModelCompressorë¡œ ì••ì¶• í•´ì œ ìµœì í™”  
        3. DynamicGPUManagerë¡œ ì•ˆì „í•œ ë©”ëª¨ë¦¬ í• ë‹¹
        4. RedHeartDynamicSwapManagerë¡œ ìŠ¤ì™‘ ê³µê°„ í™•ë³´
        5. ì‹¤ì œ GPU ë©”ëª¨ë¦¬ ì¶”ì ìœ¼ë¡œ ì •í™•í•œ ìƒíƒœ ê´€ë¦¬
        """
        import torch  # ë¹„ë™ê¸° í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ torch ì ‘ê·¼ ë³´ì¥
        with self.lock:
            # 1. ê¸°ì¡´ ì˜ˆì¸¡ ì‹œìŠ¤í…œ í™œìš©
            if self._predictor:
                prediction_confidence = await self._predictor.predict_model_need(model_id)
                if prediction_confidence < 0.3 and not force_gpu:
                    logger.info(f"ì˜ˆì¸¡ ì‹œìŠ¤í…œ: {model_id} ë¶ˆí•„ìš” íŒì • (ì‹ ë¢°ë„: {prediction_confidence:.2f})")
                    return torch.device('cpu'), None
            
            # 2. ì••ì¶•ëœ ë²„ì „ì´ ìˆìœ¼ë©´ í™œìš©
            if model_id in self.compressed_cache and self._compressor:
                logger.info(f"ì••ì¶• ë²„ì „ ë°œê²¬: {model_id} - ì••ì¶• í•´ì œ ì¤‘")
                # ì••ì¶• í•´ì œ ë¡œì§ì€ ê¸°ì¡´ ModelCompressor í™œìš©
            
            # 3. GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì •ë°€ ì²´í¬
            memory_info = get_gpu_memory_info()
            if not memory_info:
                return torch.device('cpu'), None
            
            current_usage = memory_info['usage_percent']
            free_mb = memory_info['free_mb']
            
            # 4. ìœ„í—˜ë„ ë¶„ë¥˜ ë° ì•ˆì „ ì—¬ìœ ë¶„ ê³„ì‚°
            # GPU ë¡œë”ì˜ ìœ„í—˜ë„ ë¶„ë¥˜ í™œìš© (SequentialGPULoaderì— êµ¬í˜„ë¨)
            try:
                risk_level = _gpu_loader.classify_model_risk(model_id)
                safety_margin = {
                    'HIGH': 2000,    # ê³ ìœ„í—˜ ëª¨ë¸ì€ 2GB ì—¬ìœ ë¶„ í•„ìš”
                    'MEDIUM': 1500,  # ì¤‘ìœ„í—˜ ëª¨ë¸ì€ 1.5GB ì—¬ìœ ë¶„
                    'LOW': 1000      # ì €ìœ„í—˜ ëª¨ë¸ì€ 1GB ì—¬ìœ ë¶„
                }.get(risk_level, 1500)
            except Exception as e:
                logger.warning(f"ìœ„í—˜ë„ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)} - ê¸°ë³¸ê°’ ì‚¬ìš©")
                risk_level = 'MEDIUM'
                safety_margin = 1500
            
            required_total = estimated_memory_mb + safety_margin
            
            # 5. 85% ê¸°ì¤€ ì ìš© ë° ìŠ¤ì™‘ ê³µê°„ í™•ë³´
            if current_usage >= 82 or free_mb < required_total:
                # ê¸°ì¡´ SwapManager í™œìš©í•˜ì—¬ ê³µê°„ í™•ë³´
                if self._swap_manager:
                    freed_space = await self._swap_manager.free_gpu_space_intelligent(required_total)
                    if freed_space >= required_total:
                        logger.info(f"ìŠ¤ì™‘ ë§¤ë‹ˆì €ê°€ {freed_space}MB ê³µê°„ í™•ë³´ ì™„ë£Œ")
                    else:
                        logger.warning(f"ìŠ¤ì™‘ ê³µê°„ ë¶€ì¡±: í•„ìš” {required_total}MB, í™•ë³´ {freed_space}MB")
                        return torch.device('cpu'), None
                else:
                    return torch.device('cpu'), None
            
            # 6. ì‹¤ì œ GPU ë¡œë”© ìˆ˜í–‰
            try:
                device = torch.device('cuda')
                logger.info(f"GPU ë¡œë”© ì‹œì‘: {model_id} (ì˜ˆìƒ ë©”ëª¨ë¦¬: {estimated_memory_mb}MB)")
                
                result = loading_function()
                
                # 7. ë¡œë”© í›„ ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
                new_memory_info = get_gpu_memory_info()
                if new_memory_info:
                    new_usage = new_memory_info['usage_percent']
                    actual_memory_used = (new_usage - current_usage) * 80  # 8GB ê¸°ì¤€
                    
                    # 8. ë§ˆìŠ¤í„° ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡ (ë‹¨ì¼ ì§„ì‹¤ ì†ŒìŠ¤)
                    self.master_model_registry[model_id] = {
                        'device': 'cuda',
                        'memory_mb': actual_memory_used,
                        'priority': priority,
                        'load_time': time.time(),
                        'access_count': 1,
                        'risk_level': risk_level if self._dynamic_gpu_manager else 'MEDIUM'
                    }
                    
                    # 9. ì•½í•œ ì°¸ì¡°ë¡œ ì‹¤ì œ ëª¨ë¸ ì¶”ì  (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
                    if hasattr(result, 'to'):  # PyTorch ëª¨ë¸ì¸ ê²½ìš°
                        import weakref
                        self.active_model_refs[model_id] = weakref.ref(result, 
                            lambda ref: self._on_model_garbage_collected(model_id))
                    
                    # 10. ëª¨ë“  ì„œë¸Œì‹œìŠ¤í…œì— ìƒíƒœ ë™ê¸°í™”
                    await self._sync_to_all_subsystems(model_id, 'cuda', actual_memory_used)
                    
                    logger.info(f"GPU ë¡œë”© ì„±ê³µ: {model_id} (ì‚¬ìš©ë¥ : {current_usage:.1f}% â†’ {new_usage:.1f}%, ì‹¤ì œ: {actual_memory_used:.0f}MB)")
                    
                    # 11. 85% ì´ˆê³¼ ì‹œ ê¸´ê¸‰ ì¡°ì¹˜
                    if new_usage > 85:
                        logger.error(f"ğŸš¨ GPU ë©”ëª¨ë¦¬ ìœ„í—˜: {new_usage:.1f}% - ê¸´ê¸‰ ì •ë¦¬ ì‹œì‘")
                        await self._emergency_intelligent_cleanup()
                
                return device, result
                
            except Exception as e:
                logger.error(f"GPU ë¡œë”© ì‹¤íŒ¨: {model_id} - {str(e)}")
                return torch.device('cpu'), None
    
    async def _sync_to_all_subsystems(self, model_id: str, device: str, memory_mb: float):
        """ëª¨ë“  ì„œë¸Œì‹œìŠ¤í…œì˜ loaded_modelsë¥¼ ë§ˆìŠ¤í„°ì™€ ë™ê¸°í™”"""
        model_info = {
            'device': device,
            'memory_mb': memory_mb,
            'last_sync': time.time()
        }
        
        # ê¸°ì¡´ ì‹œìŠ¤í…œë“¤ê³¼ ë™ê¸°í™”
        if hasattr(self._dynamic_gpu_manager, 'loaded_models'):
            self._dynamic_gpu_manager.loaded_models[model_id] = model_info
        
        if hasattr(self._swap_manager, 'gpu_resident_models'):
            if device == 'cuda':
                self._swap_manager.gpu_resident_models[model_id] = model_info
            elif model_id in self._swap_manager.gpu_resident_models:
                del self._swap_manager.gpu_resident_models[model_id]
    
    async def intelligent_unload_model(self, model_id: str, force: bool = False):
        """ì§„ì •í•œ ëª¨ë¸ ì–¸ë¡œë“œ - ë”•ì…”ë„ˆë¦¬ë¿ë§Œ ì•„ë‹ˆë¼ ì‹¤ì œ GPU ë©”ëª¨ë¦¬ë„ í•´ì œ"""
        import torch
        with self.lock:
            if model_id not in self.master_model_registry:
                logger.warning(f"ì–¸ë¡œë“œ ìš”ì²­ëœ ëª¨ë¸ì´ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ì—†ìŒ: {model_id}")
                return True
            
            model_info = self.master_model_registry[model_id]
            
            # 1. ìš°ì„ ìˆœìœ„ ì²´í¬ (CRITICAL ëª¨ë¸ì€ ë³´í˜¸)
            if not force and model_info.get('priority', ModelPriority.MEDIUM) <= ModelPriority.CRITICAL:
                logger.info(f"CRITICAL ëª¨ë¸ ë³´í˜¸: {model_id} ì–¸ë¡œë“œ ê±°ë¶€")
                return False
            
            # 2. ì‹¤ì œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ GPUâ†’CPU ì´ë™ (2025 PyTorch ëª¨ë²” ì‚¬ë¡€ ì ìš©)
            model_instance = None
            if model_id in self.active_model_refs:
                model_ref = self.active_model_refs[model_id]
                model_instance = model_ref()  # ì•½í•œ ì°¸ì¡°ì—ì„œ ì‹¤ì œ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
                
                if model_instance is not None and hasattr(model_instance, 'to'):
                    try:
                        logger.info(f"ğŸ”„ ì‹¤ì œ GPUâ†’CPU ì´ë™ ì‹œì‘: {model_id}")
                        
                        # ğŸ”¥ Step 1: GPUâ†’CPU ì´ë™
                        model_instance.to('cpu')
                        logger.info(f"âœ… GPUâ†’CPU ì´ë™ ì™„ë£Œ: {model_id}")
                        
                        # ğŸ”¥ Step 2: ì‹¤ì œ ëª¨ë¸ ê°ì²´ ì‚­ì œ (í•µì‹¬!)
                        # WeakRefì—ì„œ ì œê±°
                        del self.active_model_refs[model_id]
                        
                        # ğŸ”¥ ì‹¤ì œ Python ê°ì²´ ì°¸ì¡° ì‚­ì œ - ì´ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•¨!
                        logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ê°ì²´ ì°¸ì¡° ì‚­ì œ ì‹œì‘: {model_id}")
                        del model_instance
                        model_instance = None
                        logger.info(f"âœ… ëª¨ë¸ ê°ì²´ ì°¸ì¡° ì‚­ì œ ì™„ë£Œ: {model_id}")
                        
                    except Exception as e:
                        logger.error(f"âŒ ëª¨ë¸ ì´ë™/ì‚­ì œ ì‹¤íŒ¨: {model_id} - {str(e)}")
                else:
                    # ì´ë¯¸ ì‚­ì œëœ ëª¨ë¸ì˜ ê²½ìš° WeakRefë§Œ ì •ë¦¬
                    del self.active_model_refs[model_id]
            
            # 3. ë§ˆìŠ¤í„° ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ì œê±°
            memory_freed = model_info.get('memory_mb', 0)
            del self.master_model_registry[model_id]
            
            # 4. ëª¨ë“  ì„œë¸Œì‹œìŠ¤í…œì—ì„œ ë™ê¸°í™” ì œê±°
            await self._unsync_from_all_subsystems(model_id)
            
            # ğŸ”¥ Step 3: ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ (Python ê°ì²´ ì™„ì „ ì •ë¦¬)
            import gc
            gc.collect()
            logger.info(f"ğŸ§¹ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì™„ë£Œ: {model_id}")
            
            # ğŸ”¥ Step 4: CUDA ìºì‹œ ì •ë¦¬ (GPU ë©”ëª¨ë¦¬ í•´ì œ)
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                logger.info(f"ğŸš€ CUDA ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {model_id}")
            
            logger.info(f"ì™„ì „ ì–¸ë¡œë“œ ì™„ë£Œ: {model_id} ({memory_freed:.0f}MB í•´ì œ)")
            return True
    
    async def _unsync_from_all_subsystems(self, model_id: str):
        """ëª¨ë“  ì„œë¸Œì‹œìŠ¤í…œì—ì„œ ëª¨ë¸ ì •ë³´ ì œê±°"""
        # ê¸°ì¡´ ì‹œìŠ¤í…œë“¤ê³¼ ë™ê¸°í™” í•´ì œ
        if hasattr(self._dynamic_gpu_manager, 'loaded_models'):
            if model_id in self._dynamic_gpu_manager.loaded_models:
                del self._dynamic_gpu_manager.loaded_models[model_id]
        
        if hasattr(self._swap_manager, 'gpu_resident_models'):
            if model_id in self._swap_manager.gpu_resident_models:
                del self._swap_manager.gpu_resident_models[model_id]
        
        if hasattr(self._swap_manager, 'ram_models'):
            if model_id in self._swap_manager.ram_models:
                del self._swap_manager.ram_models[model_id]
    
    def _sync_cleanup_model(self, model_id: str):
        """ë™ê¸°ì  ëª¨ë¸ ì •ë¦¬ - asyncio RuntimeError ë°©ì§€ìš©"""
        try:
            # ë™ê¸°ì ìœ¼ë¡œ ëª¨ë“  ì„œë¸Œì‹œìŠ¤í…œì—ì„œ ëª¨ë¸ ì •ë³´ ì œê±°
            if hasattr(self._dynamic_gpu_manager, 'loaded_models'):
                if model_id in self._dynamic_gpu_manager.loaded_models:
                    del self._dynamic_gpu_manager.loaded_models[model_id]
            
            if hasattr(self._swap_manager, 'gpu_resident_models'):
                if model_id in self._swap_manager.gpu_resident_models:
                    del self._swap_manager.gpu_resident_models[model_id]
            
            if hasattr(self._swap_manager, 'ram_models'):
                if model_id in self._swap_manager.ram_models:
                    del self._swap_manager.ram_models[model_id]
                    
            logger.info(f"ë™ê¸°ì  ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ: {model_id}")
        except Exception as e:
            logger.warning(f"ë™ê¸°ì  ëª¨ë¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ({model_id}): {str(e)}")
    
    def _on_model_garbage_collected(self, model_id: str):
        """ëª¨ë¸ì´ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ë  ë•Œ ìë™ ì •ë¦¬"""
        logger.warning(f"ëª¨ë¸ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°ì§€: {model_id} - ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì •ë¦¬")
        if model_id in self.master_model_registry:
            # ğŸ”§ asyncio RuntimeError í•´ê²°: ì´ë²¤íŠ¸ ë£¨í”„ ìƒíƒœ í™•ì¸ í›„ ì•ˆì „ ì²˜ë¦¬
            try:
                # ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
                loop = asyncio.get_running_loop()
                if loop and not loop.is_closed():
                    # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ë¹„ë™ê¸° ì‘ì—… ìˆ˜í–‰
                    asyncio.create_task(self._unsync_from_all_subsystems(model_id))
                else:
                    # ë£¨í”„ê°€ ì—†ìœ¼ë©´ ë™ê¸°ì  ì •ë¦¬ë§Œ ìˆ˜í–‰
                    logger.info(f"ì´ë²¤íŠ¸ ë£¨í”„ ì—†ìŒ - {model_id} ë™ê¸°ì  ì •ë¦¬")
                    self._sync_cleanup_model(model_id)
            except RuntimeError:
                # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš° - ë™ê¸°ì  ì •ë¦¬ë§Œ ìˆ˜í–‰
                logger.info(f"RuntimeError ë°©ì§€ - {model_id} ë™ê¸°ì  ì •ë¦¬")
                self._sync_cleanup_model(model_id)
            
            # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ì œê±°
            del self.master_model_registry[model_id]
    
    async def ensure_gpu_space(self, required_mb: float):
        """GPU ë©”ëª¨ë¦¬ ê³µê°„ í™•ë³´
        
        Args:
            required_mb: í•„ìš”í•œ ë©”ëª¨ë¦¬ í¬ê¸° (MB)
        
        Returns:
            bool: ê³µê°„ í™•ë³´ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # í˜„ì¬ GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            memory_info = get_gpu_memory_info()
            if not memory_info:
                logger.warning("GPU ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ - CPU ëª¨ë“œë¡œ ì§„í–‰")
                return False
            
            current_usage_percent = memory_info.get('usage_percent', 0)
            free_mb = memory_info.get('free_mb', 0)
            total_mb = memory_info.get('memory_total_gb', 8) * 1024
            
            logger.info(f"ğŸ” GPU ë©”ëª¨ë¦¬ í˜„í™©: {current_usage_percent:.1f}% ì‚¬ìš© ì¤‘, {free_mb}MB ì—¬ìœ ")
            logger.info(f"ğŸ¯ ìš”ì²­ëœ ë©”ëª¨ë¦¬: {required_mb}MB")
            
            # 85% ì´í•˜ì´ê³  ì¶©ë¶„í•œ ì—¬ìœ  ê³µê°„ì´ ìˆìœ¼ë©´ ì¶”ê°€ ì •ë¦¬ ë¶ˆí•„ìš”
            if current_usage_percent <= 85 and free_mb >= required_mb * 1.2:  # 20% ì—¬ìœ ë¶„ í¬í•¨
                logger.info(f"âœ… ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬ ì—¬ìœ  - ì •ë¦¬ ë¶ˆí•„ìš”")
                return True
            
            # 90% ì´ìƒì´ê±°ë‚˜ ì—¬ìœ  ê³µê°„ì´ ë¶€ì¡±í•˜ë©´ ì •ë¦¬ í•„ìš”
            if current_usage_percent > 90 or free_mb < required_mb:
                logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - ì •ë¦¬ ì‹œì‘ (ì‚¬ìš©ë¥ : {current_usage_percent:.1f}%, ì—¬ìœ : {free_mb}MB)")
                
                # ê¸´ê¸‰ ì •ë¦¬ ì‹¤í–‰
                await self._emergency_intelligent_cleanup()
                
                # ì •ë¦¬ í›„ ë‹¤ì‹œ í™•ì¸
                memory_info_after = get_gpu_memory_info()
                if memory_info_after:
                    new_usage = memory_info_after.get('usage_percent', 0)
                    new_free = memory_info_after.get('free_mb', 0)
                    logger.info(f"ğŸ§¹ ì •ë¦¬ í›„ GPU ë©”ëª¨ë¦¬: {new_usage:.1f}% ì‚¬ìš© ì¤‘, {new_free}MB ì—¬ìœ ")
                    
                    if new_free >= required_mb:
                        logger.info(f"âœ… GPU ë©”ëª¨ë¦¬ ê³µê°„ í™•ë³´ ì„±ê³µ")
                        return True
                    else:
                        logger.error(f"âŒ GPU ë©”ëª¨ë¦¬ ê³µê°„ í™•ë³´ ì‹¤íŒ¨ - í•„ìš”: {required_mb}MB, ì—¬ìœ : {new_free}MB")
                        return False
                else:
                    logger.error("âŒ ì •ë¦¬ í›„ GPU ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸ ì‹¤íŒ¨")
                    return False
            else:
                logger.info(f"âœ… GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì–‘í˜¸ - ì •ë¦¬ ë¶ˆí•„ìš”")
                return True
                
        except Exception as e:
            logger.error(f"âŒ ensure_gpu_space ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    async def _emergency_intelligent_cleanup(self):
        """ê¸´ê¸‰ ìƒí™© ì‹œ ì§€ëŠ¥ì  ì •ë¦¬ - 2025 PyTorch ê°•í™” ë²„ì „"""
        import torch
        logger.error("ğŸš¨ ê¸´ê¸‰ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘")
        
        # ğŸ”¥ Step 1: í˜„ì¬ GPU ë©”ëª¨ë¦¬ ìƒí™© ì •í™•í•œ íŒŒì•…
        initial_memory = get_gpu_memory_info()
        if initial_memory:
            logger.error(f"ğŸš¨ ì´ˆê¸° GPU ë©”ëª¨ë¦¬: {initial_memory['usage_percent']:.1f}% ì‚¬ìš© ì¤‘")
        
        # ğŸ”¥ Step 2: ì‹¤ì œ GPU ìƒì£¼ ëª¨ë¸ë“¤ì„ ì „ë©´ ìŠ¤ìº” ë° ë“±ë¡ 
        logger.error("ğŸ” ì‹¤ì œ GPU ìƒì£¼ ëª¨ë¸ ì „ë©´ ìŠ¤ìº” ì‹œì‘...")
        await self._discover_and_register_gpu_models()
        
        # ğŸ”¥ Step 3: PyTorch ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ì‹¤ì œ ìƒíƒœ í™•ì¸
        try:
            if torch.cuda.is_available():
                # PyTorch 2025 ë©”ëª¨ë¦¬ ë””ë²„ê¹… ë„êµ¬ í™œìš©
                allocated = torch.cuda.memory_allocated() / (1024**3)  # GB
                reserved = torch.cuda.memory_reserved() / (1024**3)    # GB
                logger.error(f"ğŸ” PyTorch ì‹¤ì œ ë©”ëª¨ë¦¬: í• ë‹¹={allocated:.2f}GB, ì˜ˆì•½={reserved:.2f}GB")
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ì‹¤íŒ¨: {str(e)}")
        
        # ğŸ”¥ Step 4: ì ê·¹ì  ëª¨ë¸ ì–¸ë¡œë“œ (CRITICAL ì™¸ ëª¨ë“  ëª¨ë¸ ëŒ€ìƒ)
        models_by_priority = sorted(
            self.master_model_registry.items(),
            key=lambda x: (x[1].get('priority', 10), -x[1].get('access_count', 0))
        )
        
        freed_memory = 0
        target_usage = 85  # 85% ëª©í‘œë¡œ ì„¤ì • (ê³¼ë„í•œ ì–¸ë¡œë“œ ë°©ì§€)
        unloaded_count = 0
        
        logger.error(f"ğŸ—‘ï¸ ì´ {len(models_by_priority)}ê°œ ëª¨ë¸ ì¤‘ CRITICAL ì™¸ ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ ì‹œì‘")
        
        for model_id, model_info in models_by_priority:
            priority = model_info.get('priority', ModelPriority.MEDIUM)
            
            # CRITICAL ì™¸ ëª¨ë“  ëª¨ë¸ ê°•ì œ ì–¸ë¡œë“œ
            if priority > ModelPriority.CRITICAL:
                logger.error(f"ğŸ—‘ï¸ ê°•ì œ ì–¸ë¡œë“œ ì‹œì‘: {model_id} (ìš°ì„ ìˆœìœ„: {priority})")
                
                success = await self.intelligent_unload_model(model_id, force=True)
                if success:
                    freed_memory += model_info.get('memory_mb', 0)
                    unloaded_count += 1
                    
                    # ì¤‘ê°„ ìƒíƒœ ì²´í¬
                    current_memory = get_gpu_memory_info()
                    if current_memory:
                        logger.error(f"ğŸ“Š ì§„í–‰ ìƒí™©: {model_id} ì–¸ë¡œë“œ í›„ {current_memory['usage_percent']:.1f}%")
                        
                        # ëª©í‘œ ë‹¬ì„± ì²´í¬ (75% ì´í•˜)
                        if current_memory['usage_percent'] <= target_usage:
                            logger.info(f"âœ… ê¸´ê¸‰ ì •ë¦¬ ëª©í‘œ ë‹¬ì„±: {freed_memory:.0f}MB í•´ì œ, ì‚¬ìš©ë¥ : {current_memory['usage_percent']:.1f}%")
                            break
                else:
                    logger.error(f"âŒ ì–¸ë¡œë“œ ì‹¤íŒ¨: {model_id}")
        
        # ğŸ”¥ Step 5: ìµœì¢… ì „ë©´ ë©”ëª¨ë¦¬ ì •ë¦¬ (2025 ëª¨ë²” ì‚¬ë¡€)
        logger.error("ğŸ§¹ ìµœì¢… ì „ë©´ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘...")
        import gc
        
        # ê°•ë ¥í•œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        for i in range(3):  # 3íšŒ ë°˜ë³µìœ¼ë¡œ í™•ì‹¤íˆ ì •ë¦¬
            collected = gc.collect()
            logger.error(f"ğŸ§¹ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ {i+1}íšŒ: {collected}ê°œ ê°ì²´ ì •ë¦¬")
        
        # CUDA ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            logger.error("ğŸš€ CUDA ìºì‹œ ì™„ì „ ì •ë¦¬ ì™„ë£Œ")
        
        # ğŸ”¥ Step 6: ìµœì¢… ê²°ê³¼ ê²€ì¦
        final_memory = get_gpu_memory_info()
        if final_memory:
            improvement = initial_memory['usage_percent'] - final_memory['usage_percent'] if initial_memory else 0
            logger.error(f"âœ… ê¸´ê¸‰ ì •ë¦¬ ì™„ë£Œ:")
            logger.error(f"   ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {initial_memory['usage_percent']:.1f}% â†’ {final_memory['usage_percent']:.1f}% ({improvement:+.1f}%)")
            logger.error(f"   ğŸ—‘ï¸ ì–¸ë¡œë“œëœ ëª¨ë¸: {unloaded_count}ê°œ")
            logger.error(f"   ğŸ’¾ í•´ì œëœ ë©”ëª¨ë¦¬: {freed_memory:.0f}MB")
            
            # ì—¬ì „íˆ ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ ê²½ê³ 
            if final_memory['usage_percent'] > 85:
                logger.error("âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ì—¬ì „íˆ ë†’ìŒ - ì¶”ê°€ ì¡°ì¹˜ í•„ìš”")
        else:
            logger.error(f"ê¸´ê¸‰ ì •ë¦¬ ì¢…ë£Œ: {unloaded_count}ê°œ ëª¨ë¸ ì–¸ë¡œë“œ, {freed_memory:.0f}MB í•´ì œ")
    
    async def _discover_and_register_gpu_models(self):
        """ì‹¤ì œ GPU ìƒì£¼ ëª¨ë¸ë“¤ì„ ìŠ¤ìº”í•´ì„œ registryì— ë“±ë¡"""
        logger.info("ğŸ” GPU ìƒì£¼ ëª¨ë¸ ìŠ¤ìº” ë° registry ë“±ë¡ ì‹œì‘")
        
        import gc
        import torch.nn as nn
        discovered_models = 0
        
        try:
            # 1. ê°€ë¹„ì§€ ì»¬ë ‰í„°ë¡œ ëª¨ë“  ê°ì²´ ìŠ¤ìº”
            for obj in gc.get_objects():
                try:
                    # 2. PyTorch ëª¨ë¸ì¸ì§€ í™•ì¸
                    if isinstance(obj, nn.Module) and hasattr(obj, 'parameters'):
                        # 3. GPUì— ìƒì£¼í•˜ëŠ” ëª¨ë¸ì¸ì§€ í™•ì¸
                        gpu_params = []
                        total_params = 0
                        gpu_memory_mb = 0
                        
                        for param in obj.parameters():
                            total_params += param.numel()
                            if param.device.type == 'cuda':
                                gpu_params.append(param)
                                gpu_memory_mb += param.numel() * param.element_size() / (1024 * 1024)
                        
                        # 4. GPU íŒŒë¼ë¯¸í„°ê°€ ìˆê³  ì¶©ë¶„í•œ í¬ê¸°ì¸ ëª¨ë¸ë§Œ ë“±ë¡
                        if gpu_params and gpu_memory_mb > 10:  # 10MB ì´ìƒ
                            # 5. ëª¨ë¸ ID ìƒì„± (í´ë˜ìŠ¤ëª… + ë©”ëª¨ë¦¬ í¬ê¸° ê¸°ë°˜)
                            model_class = obj.__class__.__name__
                            model_id = f"discovered_{model_class}_{int(gpu_memory_mb)}MB_{id(obj)}"
                            
                            # 6. Registryì— ì—†ëŠ” ê²½ìš°ë§Œ ë“±ë¡
                            if model_id not in self.master_model_registry:
                                logger.info(f"ğŸ” ë°œê²¬ëœ GPU ëª¨ë¸: {model_id} ({gpu_memory_mb:.1f}MB)")
                                
                                # Registryì— ë“±ë¡
                                self.master_model_registry[model_id] = {
                                    'device': 'cuda',
                                    'memory_mb': gpu_memory_mb,
                                    'priority': ModelPriority.MEDIUM,  # ë°œê²¬ëœ ëª¨ë¸ì€ ì¤‘ê°„ ìš°ì„ ìˆœìœ„
                                    'load_time': time.time(),
                                    'access_count': 0,
                                    'risk_level': 'MEDIUM',
                                    'discovered': True  # ìŠ¤ìº”ìœ¼ë¡œ ë°œê²¬ëœ ëª¨ë¸ í‘œì‹œ
                                }
                                
                                # WeakRef ë“±ë¡
                                import weakref
                                self.active_model_refs[model_id] = weakref.ref(obj, 
                                    lambda ref, mid=model_id: self._on_model_garbage_collected(mid))
                                
                                discovered_models += 1
                
                except Exception as e:
                    # ê°œë³„ ê°ì²´ ìŠ¤ìº” ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì†
                    continue
            
            logger.info(f"âœ… GPU ëª¨ë¸ ìŠ¤ìº” ì™„ë£Œ: {discovered_models}ê°œ ëª¨ë¸ ë°œê²¬ ë° ë“±ë¡")
            logger.info(f"ğŸ“Š í˜„ì¬ Registry ìƒíƒœ: {len(self.master_model_registry)}ê°œ ëª¨ë¸ ì¶”ì  ì¤‘")
            
        except Exception as e:
            logger.error(f"âŒ GPU ëª¨ë¸ ìŠ¤ìº” ì‹¤íŒ¨: {str(e)}")
            # ìŠ¤ìº” ì‹¤íŒ¨í•´ë„ ê¸°ì¡´ registryë¡œ ê³„ì† ì§„í–‰

# ì „ì—­ ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì¸ìŠ¤í„´ìŠ¤
_master_orchestrator = MasterMemoryOrchestrator()

def get_master_orchestrator() -> MasterMemoryOrchestrator:
    """ì „ì—­ ë§ˆìŠ¤í„° ë©”ëª¨ë¦¬ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë°˜í™˜"""
    return _master_orchestrator

def initialize_unified_memory_system():
    """í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    
    ì´ í•¨ìˆ˜ëŠ” ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ í•œ ë²ˆ í˜¸ì¶œë˜ì–´ì•¼ í•˜ë©°,
    ëª¨ë“  ì„œë¸Œì‹œìŠ¤í…œë“¤ì„ ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì™€ ì—°ê²°í•©ë‹ˆë‹¤.
    """
    master_orchestrator = get_master_orchestrator()
    
    logger.info("ğŸ”§ í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
    
    try:
        # ğŸš€ Step 0: CUDA Context í”„ë¦¬ë¡œë”© (11ì´ˆ ì§€ì—° ê·¼ë³¸ í•´ê²°)
        logger.info("ğŸš€ CUDA Context í”„ë¦¬ë¡œë”© ì‹¤í–‰...")
        preload_success = preload_cuda_context()
        if preload_success:
            logger.info("âœ… CUDA ì§€ì—° ë¬¸ì œ ê·¼ë³¸ í•´ê²° ì™„ë£Œ - í–¥í›„ ì¦‰ì‹œ ì‘ë‹µ")
        else:
            logger.warning("âš ï¸ CUDA í”„ë¦¬ë¡œë”© ì‹¤íŒ¨ - CPU ëª¨ë“œë¡œ ê³„ì† ì§„í–‰")
        
        # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: ì‹¤ì œ ì„œë¸Œì‹œìŠ¤í…œë“¤ê³¼ì˜ ì—°ê²° í™œì„±í™”
        logger.info("ğŸ”§ ì„œë¸Œì‹œìŠ¤í…œë“¤ê³¼ì˜ ì—°ê²° ì‹œë„ ì¤‘...")
        
        # ê¸°ì¡´ ì‹œìŠ¤í…œë“¤ê³¼ ì—°ê²° (ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        try:
            # GPU ë¡œë”ì™€ ì—°ê²°
            master_orchestrator.connect_subsystems(
                gpu_manager=_gpu_loader,  # ì „ì—­ GPU ë¡œë” ì‚¬ìš©
                swap_manager=None,        # ì¶”í›„ ì—°ê²° ì˜ˆì •
                predictor=None,           # ì¶”í›„ ì—°ê²° ì˜ˆì •  
                compressor=None           # ì¶”í›„ ì—°ê²° ì˜ˆì •
            )
            logger.info("âœ… ê¸°ë³¸ ì„œë¸Œì‹œìŠ¤í…œ ì—°ê²° ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ì„œë¸Œì‹œìŠ¤í…œ ì—°ê²° ë¶€ë¶„ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {str(e)}")
        
        # ê°•ì œ GPUâ†’RAM ìŠ¤ì™‘ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë™ê¸°ì  ì²˜ë¦¬)
        logger.info("ğŸ§ª ë©”ëª¨ë¦¬ ê´€ë¦¬ ê¸°ë³¸ ì„¤ì • ì™„ë£Œ")
        
        logger.info("âœ… í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“Š ë§ˆìŠ¤í„° ë ˆì§€ìŠ¤íŠ¸ë¦¬ ìƒíƒœ: {len(master_orchestrator.master_model_registry)}ê°œ ëª¨ë¸ ì¶”ì  ì¤‘")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return False

def get_memory_system_status() -> dict:
    """í˜„ì¬ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
    master_orchestrator = get_master_orchestrator()
    memory_info = get_gpu_memory_info()
    
    status = {
        'gpu_available': memory_info is not None,
        'gpu_usage_percent': memory_info['usage_percent'] if memory_info else 0,
        'gpu_free_mb': memory_info['free_mb'] if memory_info else 0,
        'loaded_models_count': len(master_orchestrator.master_model_registry),
        'active_models': list(master_orchestrator.master_model_registry.keys()),
        'memory_by_priority': {}
    }
    
    # ìš°ì„ ìˆœìœ„ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì§‘ê³„
    for model_id, info in master_orchestrator.master_model_registry.items():
        priority = info.get('priority', 'UNKNOWN')
        if priority not in status['memory_by_priority']:
            status['memory_by_priority'][priority] = {'count': 0, 'total_mb': 0}
        
        status['memory_by_priority'][priority]['count'] += 1
        status['memory_by_priority'][priority]['total_mb'] += info.get('memory_mb', 0)
    
    return status

def force_unified_cleanup(target_usage_percent: float = 75.0):
    """í†µí•© ì‹œìŠ¤í…œì„ í†µí•œ ê°•ì œ ì •ë¦¬
    
    Args:
        target_usage_percent: ëª©í‘œ GPU ì‚¬ìš©ë¥  (ê¸°ë³¸ê°’: 75%)
    """
    logger.info(f"ğŸ§¹ í†µí•© ì‹œìŠ¤í…œ ê°•ì œ ì •ë¦¬ ì‹œì‘ (ëª©í‘œ: {target_usage_percent}%)")
    
    master_orchestrator = get_master_orchestrator()
    initial_memory_info = get_gpu_memory_info()
    
    if not initial_memory_info:
        logger.warning("GPU ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ì •ë¦¬ ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤")
        return False
    
    initial_usage = initial_memory_info['usage_percent']
    logger.info(f"ğŸ“Š ì •ë¦¬ ì „ GPU ì‚¬ìš©ë¥ : {initial_usage:.1f}%")
    
    if initial_usage <= target_usage_percent:
        logger.info("ì´ë¯¸ ëª©í‘œ ì‚¬ìš©ë¥  ì´í•˜ì…ë‹ˆë‹¤")
        return True
    
    # 1ë‹¨ê³„: LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ ì œê±°
    logger.info("1ë‹¨ê³„: LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ ì •ë¦¬")
    _force_swap_low_priority_models()
    
    # ì¤‘ê°„ ì²´í¬
    mid_memory_info = get_gpu_memory_info()
    if mid_memory_info and mid_memory_info['usage_percent'] <= target_usage_percent:
        logger.info(f"âœ… 1ë‹¨ê³„ ì •ë¦¬ë¡œ ëª©í‘œ ë‹¬ì„±: {mid_memory_info['usage_percent']:.1f}%")
        return True
    
    # 2ë‹¨ê³„: ê¸´ê¸‰ ì •ë¦¬ (MEDIUM ìš°ì„ ìˆœìœ„ê¹Œì§€)
    logger.info("2ë‹¨ê³„: ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬")
    _emergency_gpu_cleanup()
    
    # ìµœì¢… ì²´í¬
    final_memory_info = get_gpu_memory_info()
    if final_memory_info:
        final_usage = final_memory_info['usage_percent']
        freed_mb = (initial_usage - final_usage) * 80  # 8GB ê¸°ì¤€
        
        logger.info(f"ğŸ ì •ë¦¬ ì™„ë£Œ: {initial_usage:.1f}% â†’ {final_usage:.1f}% ({freed_mb:.0f}MB í•´ì œ)")
        
        if final_usage <= target_usage_percent:
            logger.info("âœ… ëª©í‘œ ì‚¬ìš©ë¥  ë‹¬ì„±")
            return True
        else:
            logger.warning(f"âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_usage_percent}%, í˜„ì¬: {final_usage:.1f}%)")
            return False
    else:
        logger.error("ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return False

def get_gpu_loader():
    """ì „ì—­ GPU ë¡œë” ë°˜í™˜"""
    return _gpu_loader

def get_priority_based_device(memory_required_mb=500, priority=ModelPriority.MEDIUM, model_id=None, loading_function=None):
    """ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ë¥¼ í†µí•œ ì§€ëŠ¥ì  ë””ë°”ì´ìŠ¤ ì„ íƒ"""
    import torch
    import asyncio
    
    # CPU ê°•ì œ ëª¨ë“œ
    if not ADVANCED_CONFIG['enable_gpu'] or not torch.cuda.is_available():
        return torch.device('cpu')
    
    # loading_functionì´ ì—†ìœ¼ë©´ ë¹ ë¥¸ ì²´í¬ (ì¦‰ì‹œ ê²°ì •)
    if loading_function is None:
        memory_info = get_gpu_memory_info()
        if memory_info is None or memory_info['usage_percent'] > 82:
            return torch.device('cpu')
        return torch.device('cuda')
    
    # loading_functionì´ ìˆìœ¼ë©´ ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í™œìš©
    master_orchestrator = get_master_orchestrator()
    
    try:
        device, result = run_async_safely(
            master_orchestrator.intelligent_load_model(
                model_id or "unknown_model", priority, memory_required_mb, loading_function
            ),
            timeout=30.0
        )
        
        if device is None:
            return torch.device('cpu')
            
        return device
        
    except Exception as e:
        logger.error(f"ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë””ë°”ì´ìŠ¤ ì„ íƒ ì‹¤íŒ¨: {str(e)}")
        return torch.device('cpu')

def _track_gpu_model(model_id, priority, memory_mb):
    """GPU ë¡œë“œëœ ëª¨ë¸ ì¶”ì  (ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì™€ ì—°ë™)"""
    if model_id:
        # ë ˆê±°ì‹œ ì‹œìŠ¤í…œ í˜¸í™˜ì„±
        _gpu_loaded_models[model_id] = {
            'priority': priority,
            'memory_mb': memory_mb
        }
        
        # ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì—ë„ ë™ê¸°í™”
        master_orchestrator = get_master_orchestrator()
        if model_id not in master_orchestrator.master_model_registry:
            master_orchestrator.master_model_registry[model_id] = {
                'device': 'cuda',
                'memory_mb': memory_mb,
                'priority': priority,
                'load_time': time.time(),
                'access_count': 1,
                'risk_level': 'MEDIUM'
            }

def _force_swap_low_priority_models():
    """ë‚®ì€ ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ ê°•ì œ ìŠ¤ì™‘ (ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í™œìš©)"""
    import asyncio
    
    master_orchestrator = get_master_orchestrator()
    
    # LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ ì°¾ê¸°
    low_priority_models = [
        model_id for model_id, info in master_orchestrator.master_model_registry.items()
        if info.get('priority', ModelPriority.MEDIUM) >= ModelPriority.LOW
    ]
    
    if not low_priority_models:
        logger.info("ê°•ì œ ìŠ¤ì™‘í•  LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ì´ ì—†ìŒ")
        return
    
    logger.info(f"LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ë“¤ ê°•ì œ ìŠ¤ì™‘ ì‹œì‘: {low_priority_models}")
    
    # ë¹„ë™ê¸° ì–¸ë¡œë“œë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ (í—¬í¼ ì‚¬ìš©)
    async def _async_swap():
        tasks = []
        for model_id in low_priority_models:
            task = master_orchestrator.intelligent_unload_model(model_id, force=False)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
    
    try:
        results = run_async_safely(_async_swap(), timeout=60.0)
        
        if results is not None:
            success_count = sum(1 for r in results if r is True)
            logger.info(f"LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ ìŠ¤ì™‘ ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ")
        else:
            logger.error("LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ ìŠ¤ì™‘ íƒ€ì„ì•„ì›ƒ")
            _legacy_force_cleanup()
        
    except Exception as e:
        logger.error(f"LOW ìš°ì„ ìˆœìœ„ ëª¨ë¸ ìŠ¤ì™‘ ì‹¤íŒ¨: {str(e)}")
        # í´ë°±ìœ¼ë¡œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        _legacy_force_cleanup()

def _emergency_gpu_cleanup():
    """ê¸´ê¸‰ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§ˆìŠ¤í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í™œìš©)"""
    logger.error("ğŸš¨ ê¸´ê¸‰ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘")
    
    master_orchestrator = get_master_orchestrator()
    
    # ë¹„ë™ê¸° ê¸´ê¸‰ ì •ë¦¬ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ (í—¬í¼ ì‚¬ìš©)
    try:
        result = run_async_safely(
            master_orchestrator._emergency_intelligent_cleanup(),
            timeout=120.0
        )
        
        if result is not None:
            logger.info("ğŸš¨ ê¸´ê¸‰ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        else:
            logger.error("ğŸš¨ ê¸´ê¸‰ ì •ë¦¬ íƒ€ì„ì•„ì›ƒ - ë ˆê±°ì‹œ ì •ë¦¬ ëª¨ë“œë¡œ ì „í™˜")
            _legacy_emergency_cleanup()
        
    except Exception as e:
        logger.error(f"ğŸš¨ ê¸´ê¸‰ ì •ë¦¬ ì‹¤íŒ¨: {str(e)} - ë ˆê±°ì‹œ ì •ë¦¬ ëª¨ë“œë¡œ ì „í™˜")
        _legacy_emergency_cleanup()

def _legacy_force_cleanup():
    """ë ˆê±°ì‹œ ê°•ì œ ì •ë¦¬ (í´ë°± ì „ìš©)"""
    import gc
    import torch
    
    logger.warning("ë ˆê±°ì‹œ ê°•ì œ ì •ë¦¬ ëª¨ë“œ ì‹¤í–‰")
    
    to_remove = []
    for model_id, info in _gpu_loaded_models.items():
        if info['priority'] >= ModelPriority.LOW:
            to_remove.append(model_id)
    
    for model_id in to_remove:
        del _gpu_loaded_models[model_id]
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

def _legacy_emergency_cleanup():
    """ë ˆê±°ì‹œ ê¸´ê¸‰ ì •ë¦¬ (í´ë°± ì „ìš©)"""
    import gc
    import torch
    
    logger.warning("ë ˆê±°ì‹œ ê¸´ê¸‰ ì •ë¦¬ ëª¨ë“œ ì‹¤í–‰")
    
    to_remove = []
    for model_id, info in _gpu_loaded_models.items():
        if info['priority'] > ModelPriority.CRITICAL:
            to_remove.append(model_id)
    
    for model_id in to_remove:
        del _gpu_loaded_models[model_id]
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()

def update_smart_device_with_priority():
    """ê¸°ì¡´ get_smart_deviceë¥¼ ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œìœ¼ë¡œ ì—…ë°ì´íŠ¸"""
    global get_smart_device
    
    def enhanced_get_smart_device(memory_required_mb=500, force_cpu=False, priority=ModelPriority.MEDIUM, model_id=None):
        import torch  # ì§€ì—° ë¡œë”©ìœ¼ë¡œ torch import ì¶”ê°€
        if force_cpu:
            return torch.device('cpu')
        return get_priority_based_device(memory_required_mb, priority, model_id)
    
    # ê¸°ì¡´ í•¨ìˆ˜ ëŒ€ì²´
    get_smart_device = enhanced_get_smart_device

def print_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""
    print("="*60)
    print("Red Heart Linux Advanced - System Information")
    print("="*60)
    print(f"Platform: {SYSTEM_INFO['platform']}")
    print(f"Architecture: {SYSTEM_INFO['architecture']}")
    print(f"Python Version: {SYSTEM_INFO['python_version']}")
    print(f"CPU Count: {SYSTEM_INFO['cpu_count']}")
    print(f"GPU Available: {ADVANCED_CONFIG['enable_gpu']}")
    print(f"GPU Count: {ADVANCED_CONFIG['gpu_count']}")
    print(f"Advanced Libraries: {'Enabled' if not ADVANCED_CONFIG['fallback_mode'] else 'Fallback Mode'}")
    print("="*60)

if __name__ == "__main__":
    print_system_info()