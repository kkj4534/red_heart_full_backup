# Red Heart AI μ‹μ¤ν… μµμΆ… μ κ²€ λ³΄κ³ μ„
μ‘μ„±μΌ: 2025-08-21

## β… μμ • μ™„λ£ μ‚¬ν•­

### 1. Fallback/Dummy μ κ±°
- **unified_training_final.py**: 
  - β torch.randn λ”λ―Έ μ„λ² λ”© β†’ β… RuntimeError λ°μƒ
  - SentenceTransformer μ‹¤ν¨ μ‹ μ‹μ¤ν… μΆ…λ£
  
- **run_hierarchical_lr_sweep.py**: 
  - β torch.randn λ”λ―Έ μ„λ² λ”© β†’ β… RuntimeError λ°μƒ
  - SentenceTransformer ν•„μ λ¨λ“λ΅ λ³€κ²½

- **hierarchical_lr_sweep.py**:
  - β torch.rand SURD λ”λ―Έ νƒ€κ² β†’ β… μ‹¤μ  μ •λ³΄μ΄λ΅  κΈ°λ° κ³„μ‚°
  - SURD = Synergy + Unique + Redundant + Deterministic

### 2. SURD νƒ€κ² μμ •
```python
# μ΄μ „ (μλ»λ κµ¬ν„)
surd_target = torch.rand(batch_size, 4)  # λ”λ―Έ!

# ν„μ¬ (μ •λ³΄μ΄λ΅  κΈ°λ°)
surd_target[:, 0] = emotion_entropy / np.log(7)  # Synergy
surd_target[:, 1] = label_unique.max()           # Unique  
surd_target[:, 2] = 1.0 - (std/mean).clamp(0,1)  # Redundant
surd_target[:, 3] = regret.abs()                 # Deterministic
```

## π” μ‹μ¤ν… μ—°κ²° κ²€μ¦

### 1. λ°μ΄ν„° νλ¦„
```
claude_preprocessed_complete.json
    β†“
μ„λ² λ”© μ²΄ν¬ β†’ μ—†μΌλ©΄ SentenceTransformer μƒμ„± (NO FALLBACK)
    β†“
.embedded.json μ €μ¥
    β†“
DataLoader β†’ ν•™μµ
```

### 2. λ¨λΈ κµ¬μ΅° (730M λ©ν‘)
- β… **λ°±λ³Έ**: 90.6M - GPU λ΅λ“λ¨
- β… **ν—¤λ“λ“¤**: 63M 
  - EmotionHead: 17.25M
  - BenthamHead: 13.87M  
  - RegretHead: 19.90M
  - SURDHead: 12.03M
- β… **Neural Analyzers**: 368.2M - nn.ModuleDict
- β… **Advanced Wrappers**: 112M - nn.ModuleDict
- β… **Phase Networks**: 4.3M
- β… **DSP & Kalman**: 2.3M

### 3. ν•™μµ λ£¨ν”„ κ²€μ¦
```python
# Stage 1: λ°±λ³Έ
backbone_outputs = self.model.backbone(inputs, return_all_tasks=True) β…

# Stage 2: ν—¤λ“λ“¤
emotion_loss = self.model.emotion_head.compute_loss() β…
bentham_loss = self.model.bentham_head.compute_loss() β…
regret_loss = self.model.regret_head.compute_loss() β…
surd_loss = self.model.surd_head.compute_loss(surd_pred, surd_target) β…

# Stage 3: Neural Analyzers (1127-1252 λΌμΈ)
for name, analyzer in self.model.neural_analyzers.items():
    analyzer_output = analyzer(features) β…
    
# Stage 4: Advanced Wrappers (1253-1280 λΌμΈ)
for name, wrapper in self.model.advanced_wrappers.items():
    wrapper_output = wrapper(features) β…
```

### 4. νλΌλ―Έν„° μ—…λ°μ΄νΈ ν™•μΈ
- 100λ°°μΉλ§λ‹¤ μ—…λ°μ΄νΈ μ „ν›„ κ°’ λΉ„κµ
- λ―Έμ—…λ°μ΄νΈ λ¨λ“ κ²½κ³  μ¶λ ¥
- Optimizer λ“±λ΅ νλΌλ―Έν„° μ ν™•μΈ

## β οΈ μ£Όμμ‚¬ν•­

### 1. μ„λ² λ”© ν•„μ
- SentenceTransformer λ΅λ“ μ‹¤ν¨ μ‹ μ‹μ¤ν… μΆ…λ£
- λ”λ―Έ λ°μ΄ν„° μ™„μ „ μ κ±°λ¨
- μ²« μ‹¤ν–‰ μ‹ μ„λ² λ”© μƒμ„± μ‹κ°„ ν•„μ” (~25λ¶„/150K μƒν”)

### 2. SURD νƒ€κ²
- 4μ°¨μ› μ •λ³΄μ΄λ΅  λ©”νΈλ¦­
- λ‹¨μ λ¶„λ¥κ°€ μ•„λ‹ multi-dimensional regression
- Dynamic threshold μ μ© (μ—ν­μ— λ”°λΌ 0.5β†’0.35β†’0.25)

### 3. LR μ¤μ•
- λ„μ  κ²°κ³Ό μ €μ¥ (lr_sweep_cumulative.json)
- μ¤‘λ³µ LR μλ™ 10% μ΅°μ •
- κ° LRμ€ λ…λ¦½μ  μ΄κΈ° κ°€μ¤‘μΉμ—μ„ μ‹μ‘

## π€ μ‹¤ν–‰ μ¤€λΉ„ μ™„λ£

### ν…μ¤νΈ λ…λ Ήμ–΄
```bash
# LR μ¤μ• ν¬ν•¨ (μμ •λ¨)
bash run_learning.sh unified-test --lr-sweep --samples 3 --no-param-update --debug --verbose

# λ°±κ·ΈλΌμ΄λ“ μ‹¤ν–‰
nohup timeout 3600 bash run_learning.sh unified-test --lr-sweep --samples 3 --no-param-update --debug --verbose > test_$(date +%Y%m%d_%H%M%S).txt 2>&1 &
```

### μμƒ λ΅κ·Έ
```
π“ μ„λ² λ”© μƒνƒ:
  - μ „μ²΄ λ°μ΄ν„°: 150000κ°
  - μ„λ² λ”© μμ: 0κ° (0.0%)
  - μ„λ² λ”© μ—†μ: 150000κ° (100.0%)
β οΈ 150000κ° ν•­λ©μ— μ„λ² λ”©μ΄ μ—†μµλ‹λ‹¤. μλ™ μƒμ„±λ©λ‹λ‹¤.

β… λ¨λΈ μ΄κΈ°ν™” μ™„λ£: μ΄ XXX.XM νλΌλ―Έν„°
β οΈ νλΌλ―Έν„° κ°μ λ¶μΌμΉ! (730M νƒ€κ²κ³Ό μ°¨μ΄ μμΌλ©΄)

π“‚ κΈ°μ΅΄ LR ν…μ¤νΈ κ²°κ³Ό λ΅λ“: 0 κ° LR κΈ°λ΅
π€ Hierarchical LR Sweep μ‹μ‘...

β… νλΌλ―Έν„° μ—…λ°μ΄νΈλ¨ (batch 0): backbone, emotion_head, ...
β οΈ νλΌλ―Έν„° λ―Έμ—…λ°μ΄νΈ (batch 0): neural_analyzers, ... (μμΌλ©΄)
```

## π“‹ μ²΄ν¬λ¦¬μ¤νΈ

- [x] NO DUMMY - λ¨λ“  λ”λ―Έ λ°μ΄ν„° μ κ±°
- [x] NO FALLBACK - μ‹¤ν¨ μ‹ μ—λ¬ λ°μƒ
- [x] NO MOCK - μ‹¤μ  λ°μ΄ν„°λ§ μ‚¬μ©
- [x] SURD μ •λ³΄μ΄λ΅  νƒ€κ² κµ¬ν„
- [x] 730M νλΌλ―Έν„° κ²€μ¦ λ΅μ§
- [x] Neural Analyzers/Wrappers ν•™μµ μ°Έμ—¬
- [x] μ„λ² λ”© μΊμ‹± μ‹μ¤ν…
- [x] LR λ„μ /μ¤‘λ³µ μ²΄ν¬
- [ ] μ‹¤μ  ν…μ¤νΈ μ‹¤ν–‰

---
μƒνƒ: μ‹¤ν–‰ μ¤€λΉ„ μ™„λ£